diff -uparN ffmpeg-4.1/adapt_liteos_config.sh ffmpeg-y/adapt_liteos_config.sh
--- ffmpeg-4.1/adapt_liteos_config.sh	1970-01-01 08:00:00.000000000 +0800
+++ ffmpeg-y/adapt_liteos_config.sh	2019-06-29 11:49:36.833017667 +0800
@@ -0,0 +1,59 @@
+#!/bin/sh
+
+
+function cancel_opt
+{
+    pattern=$1
+    sed -i "s/$pattern 1/$pattern 0/g" config.h
+    sed -i "s/$pattern/!$pattern/g" ffbuild/config.mak
+}
+
+function effect_opt
+{
+    pattern=$1
+    echo $pattern
+    sed -i "s/$pattern 0/$pattern 1/g" config.h
+    sed -i "s/!$pattern/$pattern/g" ffbuild/config.mak
+}
+
+function adapt_liteos
+{
+echo "adapt ffmpeg for liteos"
+
+sed -i "/^CFLAGS=/ s/$/ \$(LITEOS_MACRO) \$(LITEOS_OSDRV_INCLUDE) \$(LITEOS_USR_INCLUDE) \$(LITEOS_CMACRO) \$(LITEOS_INCLUDE) \-I\$(LITEOSTOPDIR)\/lib\/cxxstl\/gccinclude/g" ffbuild/config.mak
+
+sed -i "s:atomics\/gcc:atomics\/pthread:g" ffbuild/config.mak
+
+sed -i "/getenv/ s:^://:g" config.h
+
+effect_opt HAVE_UNISTD_H
+effect_opt HAVE_CBRT
+effect_opt HAVE_CBRTF
+effect_opt HAVE_COPYSIGN
+effect_opt HAVE_ERF
+effect_opt HAVE_HYPOT
+effect_opt HAVE_RINT
+effect_opt HAVE_LRINT
+effect_opt HAVE_LRINTF
+effect_opt HAVE_ROUND
+effect_opt HAVE_ROUNDF
+effect_opt HAVE_TRUNC
+effect_opt HAVE_TRUNCF
+effect_opt HAVE_GMTIME_R
+effect_opt HAVE_LOCALTIME_R
+effect_opt HAVE_PTHREAD_CANCEL
+effect_opt HAVE_PTHREADS
+effect_opt HAVE_SIMD_ALIGN_16
+
+cancel_opt HAVE_SYSCONF
+cancel_opt HAVE_SYSCTL
+cancel_opt HAVE_ATOMICS_NATIVE
+cancel_opt HAVE_LLRINTF
+cancel_opt HAVE_MMAP
+
+make clean
+
+echo "already adapt ffmpeg to liteos"
+}
+CURDIR=$(pwd)
+adapt_liteos
diff -uparN ffmpeg-4.1/config.h ffmpeg-y/config.h
--- ffmpeg-4.1/config.h	1970-01-01 08:00:00.000000000 +0800
+++ ffmpeg-y/config.h	2019-06-29 11:49:36.905017669 +0800
@@ -0,0 +1,2467 @@
+/* Automatically generated by configure - do not modify! */
+#ifndef FFMPEG_CONFIG_H
+#define FFMPEG_CONFIG_H
+#define FFMPEG_CONFIGURATION "--prefix=./install --enable-cross-compile --disable-doc --disable-htmlpages --target-os=linux --enable-static --disable-shared --disable-debug --disable-iconv --enable-small --disable-network --disable-filters --disable-devices --disable-programs --disable-swresample --disable-swscale --disable-avdevice --disable-postproc --disable-avfilter --disable-protocols --disable-pthreads --disable-runtime-cpudetect --disable-everything --enable-pic --enable-protocol=file --disable-muxers --enable-demuxer=mov --disable-neon --disable-inline-asm --disable-asm --disable-armv6 --disable-armv6t2 --disable-armv5te --disable-vfp --disable-hardcoded-tables --disable-mediacodec --enable-bsf=h264_mp4toannexb --enable-bsf=hevc_mp4toannexb --disable-pixelutils --enable-demuxer=wav --disable-gpl --cpu=cortex-a7 --arch=armv7-a --cross-prefix=arm-himix100-linux-"
+#define FFMPEG_LICENSE "LGPL version 2.1 or later"
+#define CONFIG_THIS_YEAR 2018
+#define FFMPEG_DATADIR "./install/share/ffmpeg"
+#define AVCONV_DATADIR "./install/share/ffmpeg"
+#define CC_IDENT "gcc 6.3.0 (HC&C V100R002C00B032_20190114)"
+#define av_restrict restrict
+#define EXTERN_PREFIX ""
+#define EXTERN_ASM 
+#define BUILDSUF ""
+#define SLIBSUF ".so"
+#define HAVE_MMX2 HAVE_MMXEXT
+#define SWS_MAX_FILTER_SIZE 256
+#define ARCH_AARCH64 0
+#define ARCH_ALPHA 0
+#define ARCH_ARM 0
+#define ARCH_AVR32 0
+#define ARCH_AVR32_AP 0
+#define ARCH_AVR32_UC 0
+#define ARCH_BFIN 0
+#define ARCH_IA64 0
+#define ARCH_M68K 0
+#define ARCH_MIPS 0
+#define ARCH_MIPS64 0
+#define ARCH_PARISC 0
+#define ARCH_PPC 0
+#define ARCH_PPC64 0
+#define ARCH_S390 0
+#define ARCH_SH4 0
+#define ARCH_SPARC 0
+#define ARCH_SPARC64 0
+#define ARCH_TILEGX 0
+#define ARCH_TILEPRO 0
+#define ARCH_TOMI 0
+#define ARCH_X86 0
+#define ARCH_X86_32 0
+#define ARCH_X86_64 0
+#define HAVE_ARMV5TE 0
+#define HAVE_ARMV6 0
+#define HAVE_ARMV6T2 0
+#define HAVE_ARMV8 0
+#define HAVE_NEON 0
+#define HAVE_VFP 0
+#define HAVE_VFPV3 0
+#define HAVE_SETEND 0
+#define HAVE_ALTIVEC 0
+#define HAVE_DCBZL 0
+#define HAVE_LDBRX 0
+#define HAVE_POWER8 0
+#define HAVE_PPC4XX 0
+#define HAVE_VSX 0
+#define HAVE_AESNI 0
+#define HAVE_AMD3DNOW 0
+#define HAVE_AMD3DNOWEXT 0
+#define HAVE_AVX 0
+#define HAVE_AVX2 0
+#define HAVE_AVX512 0
+#define HAVE_FMA3 0
+#define HAVE_FMA4 0
+#define HAVE_MMX 0
+#define HAVE_MMXEXT 0
+#define HAVE_SSE 0
+#define HAVE_SSE2 0
+#define HAVE_SSE3 0
+#define HAVE_SSE4 0
+#define HAVE_SSE42 0
+#define HAVE_SSSE3 0
+#define HAVE_XOP 0
+#define HAVE_CPUNOP 0
+#define HAVE_I686 0
+#define HAVE_MIPSFPU 0
+#define HAVE_MIPS32R2 0
+#define HAVE_MIPS32R5 0
+#define HAVE_MIPS64R2 0
+#define HAVE_MIPS32R6 0
+#define HAVE_MIPS64R6 0
+#define HAVE_MIPSDSP 0
+#define HAVE_MIPSDSPR2 0
+#define HAVE_MSA 0
+#define HAVE_LOONGSON2 0
+#define HAVE_LOONGSON3 0
+#define HAVE_MMI 0
+#define HAVE_ARMV5TE_EXTERNAL 0
+#define HAVE_ARMV6_EXTERNAL 0
+#define HAVE_ARMV6T2_EXTERNAL 0
+#define HAVE_ARMV8_EXTERNAL 0
+#define HAVE_NEON_EXTERNAL 0
+#define HAVE_VFP_EXTERNAL 0
+#define HAVE_VFPV3_EXTERNAL 0
+#define HAVE_SETEND_EXTERNAL 1
+#define HAVE_ALTIVEC_EXTERNAL 0
+#define HAVE_DCBZL_EXTERNAL 0
+#define HAVE_LDBRX_EXTERNAL 0
+#define HAVE_POWER8_EXTERNAL 0
+#define HAVE_PPC4XX_EXTERNAL 0
+#define HAVE_VSX_EXTERNAL 0
+#define HAVE_AESNI_EXTERNAL 0
+#define HAVE_AMD3DNOW_EXTERNAL 0
+#define HAVE_AMD3DNOWEXT_EXTERNAL 0
+#define HAVE_AVX_EXTERNAL 0
+#define HAVE_AVX2_EXTERNAL 0
+#define HAVE_AVX512_EXTERNAL 0
+#define HAVE_FMA3_EXTERNAL 0
+#define HAVE_FMA4_EXTERNAL 0
+#define HAVE_MMX_EXTERNAL 0
+#define HAVE_MMXEXT_EXTERNAL 0
+#define HAVE_SSE_EXTERNAL 0
+#define HAVE_SSE2_EXTERNAL 0
+#define HAVE_SSE3_EXTERNAL 0
+#define HAVE_SSE4_EXTERNAL 0
+#define HAVE_SSE42_EXTERNAL 0
+#define HAVE_SSSE3_EXTERNAL 0
+#define HAVE_XOP_EXTERNAL 0
+#define HAVE_CPUNOP_EXTERNAL 0
+#define HAVE_I686_EXTERNAL 0
+#define HAVE_MIPSFPU_EXTERNAL 0
+#define HAVE_MIPS32R2_EXTERNAL 0
+#define HAVE_MIPS32R5_EXTERNAL 0
+#define HAVE_MIPS64R2_EXTERNAL 0
+#define HAVE_MIPS32R6_EXTERNAL 0
+#define HAVE_MIPS64R6_EXTERNAL 0
+#define HAVE_MIPSDSP_EXTERNAL 0
+#define HAVE_MIPSDSPR2_EXTERNAL 0
+#define HAVE_MSA_EXTERNAL 0
+#define HAVE_LOONGSON2_EXTERNAL 0
+#define HAVE_LOONGSON3_EXTERNAL 0
+#define HAVE_MMI_EXTERNAL 0
+#define HAVE_ARMV5TE_INLINE 0
+#define HAVE_ARMV6_INLINE 0
+#define HAVE_ARMV6T2_INLINE 0
+#define HAVE_ARMV8_INLINE 0
+#define HAVE_NEON_INLINE 0
+#define HAVE_VFP_INLINE 0
+#define HAVE_VFPV3_INLINE 0
+#define HAVE_SETEND_INLINE 0
+#define HAVE_ALTIVEC_INLINE 0
+#define HAVE_DCBZL_INLINE 0
+#define HAVE_LDBRX_INLINE 0
+#define HAVE_POWER8_INLINE 0
+#define HAVE_PPC4XX_INLINE 0
+#define HAVE_VSX_INLINE 0
+#define HAVE_AESNI_INLINE 0
+#define HAVE_AMD3DNOW_INLINE 0
+#define HAVE_AMD3DNOWEXT_INLINE 0
+#define HAVE_AVX_INLINE 0
+#define HAVE_AVX2_INLINE 0
+#define HAVE_AVX512_INLINE 0
+#define HAVE_FMA3_INLINE 0
+#define HAVE_FMA4_INLINE 0
+#define HAVE_MMX_INLINE 0
+#define HAVE_MMXEXT_INLINE 0
+#define HAVE_SSE_INLINE 0
+#define HAVE_SSE2_INLINE 0
+#define HAVE_SSE3_INLINE 0
+#define HAVE_SSE4_INLINE 0
+#define HAVE_SSE42_INLINE 0
+#define HAVE_SSSE3_INLINE 0
+#define HAVE_XOP_INLINE 0
+#define HAVE_CPUNOP_INLINE 0
+#define HAVE_I686_INLINE 0
+#define HAVE_MIPSFPU_INLINE 0
+#define HAVE_MIPS32R2_INLINE 0
+#define HAVE_MIPS32R5_INLINE 0
+#define HAVE_MIPS64R2_INLINE 0
+#define HAVE_MIPS32R6_INLINE 0
+#define HAVE_MIPS64R6_INLINE 0
+#define HAVE_MIPSDSP_INLINE 0
+#define HAVE_MIPSDSPR2_INLINE 0
+#define HAVE_MSA_INLINE 0
+#define HAVE_LOONGSON2_INLINE 0
+#define HAVE_LOONGSON3_INLINE 0
+#define HAVE_MMI_INLINE 0
+#define HAVE_ALIGNED_STACK 0
+#define HAVE_FAST_64BIT 0
+#define HAVE_FAST_CLZ 1
+#define HAVE_FAST_CMOV 0
+#define HAVE_LOCAL_ALIGNED 0
+#define HAVE_SIMD_ALIGN_16 0
+#define HAVE_SIMD_ALIGN_32 0
+#define HAVE_SIMD_ALIGN_64 0
+#define HAVE_ATOMIC_CAS_PTR 0
+#define HAVE_MACHINE_RW_BARRIER 0
+#define HAVE_MEMORYBARRIER 0
+#define HAVE_MM_EMPTY 0
+#define HAVE_RDTSC 0
+#define HAVE_SEM_TIMEDWAIT 0
+#define HAVE_SYNC_VAL_COMPARE_AND_SWAP 1
+#define HAVE_CABS 0
+#define HAVE_CEXP 0
+#define HAVE_INLINE_ASM 0
+#define HAVE_SYMVER 1
+#define HAVE_X86ASM 0
+#define HAVE_BIGENDIAN 0
+#define HAVE_FAST_UNALIGNED 1
+#define HAVE_ARPA_INET_H 0
+#define HAVE_ASM_TYPES_H 1
+#define HAVE_CDIO_PARANOIA_H 0
+#define HAVE_CDIO_PARANOIA_PARANOIA_H 0
+#define HAVE_CUDA_H 0
+#define HAVE_DISPATCH_DISPATCH_H 0
+#define HAVE_DEV_BKTR_IOCTL_BT848_H 0
+#define HAVE_DEV_BKTR_IOCTL_METEOR_H 0
+#define HAVE_DEV_IC_BT8XX_H 0
+#define HAVE_DEV_VIDEO_BKTR_IOCTL_BT848_H 0
+#define HAVE_DEV_VIDEO_METEOR_IOCTL_METEOR_H 0
+#define HAVE_DIRECT_H 0
+#define HAVE_DIRENT_H 1
+#define HAVE_DXGIDEBUG_H 0
+#define HAVE_DXVA_H 0
+#define HAVE_ES2_GL_H 0
+#define HAVE_GSM_H 0
+#define HAVE_IO_H 0
+#define HAVE_LINUX_PERF_EVENT_H 1
+#define HAVE_MACHINE_IOCTL_BT848_H 0
+#define HAVE_MACHINE_IOCTL_METEOR_H 0
+#define HAVE_MALLOC_H 1
+#define HAVE_OPENCV2_CORE_CORE_C_H 0
+#define HAVE_OPENGL_GL3_H 0
+#define HAVE_POLL_H 1
+#define HAVE_SYS_PARAM_H 1
+#define HAVE_SYS_RESOURCE_H 1
+#define HAVE_SYS_SELECT_H 1
+#define HAVE_SYS_SOUNDCARD_H 1
+#define HAVE_SYS_TIME_H 1
+#define HAVE_SYS_UN_H 1
+#define HAVE_SYS_VIDEOIO_H 0
+#define HAVE_TERMIOS_H 1
+#define HAVE_UDPLITE_H 0
+#define HAVE_UNISTD_H 1
+#define HAVE_VALGRIND_VALGRIND_H 0
+#define HAVE_WINDOWS_H 0
+#define HAVE_WINSOCK2_H 0
+#define HAVE_INTRINSICS_NEON 0
+#define HAVE_ATANF 1
+#define HAVE_ATAN2F 1
+#define HAVE_CBRT 1
+#define HAVE_CBRTF 1
+#define HAVE_COPYSIGN 1
+#define HAVE_COSF 1
+#define HAVE_ERF 1
+#define HAVE_EXP2 1
+#define HAVE_EXP2F 1
+#define HAVE_EXPF 1
+#define HAVE_HYPOT 1
+#define HAVE_ISFINITE 1
+#define HAVE_ISINF 1
+#define HAVE_ISNAN 1
+#define HAVE_LDEXPF 1
+#define HAVE_LLRINT 1
+#define HAVE_LLRINTF 1
+#define HAVE_LOG2 1
+#define HAVE_LOG2F 1
+#define HAVE_LOG10F 1
+#define HAVE_LRINT 1
+#define HAVE_LRINTF 1
+#define HAVE_POWF 1
+#define HAVE_RINT 1
+#define HAVE_ROUND 1
+#define HAVE_ROUNDF 1
+#define HAVE_SINF 1
+#define HAVE_TRUNC 1
+#define HAVE_TRUNCF 1
+#define HAVE_DOS_PATHS 0
+#define HAVE_LIBC_MSVCRT 0
+#define HAVE_MMAL_PARAMETER_VIDEO_MAX_NUM_CALLBACKS 0
+#define HAVE_SECTION_DATA_REL_RO 1
+#define HAVE_THREADS 0
+#define HAVE_UWP 0
+#define HAVE_WINRT 0
+#define HAVE_ACCESS 1
+#define HAVE_ALIGNED_MALLOC 0
+#define HAVE_ARC4RANDOM 0
+#define HAVE_CLOCK_GETTIME 1
+#define HAVE_CLOSESOCKET 0
+#define HAVE_COMMANDLINETOARGVW 0
+#define HAVE_FCNTL 1
+#define HAVE_GETADDRINFO 0
+#define HAVE_GETHRTIME 0
+#define HAVE_GETOPT 1
+#define HAVE_GETPROCESSAFFINITYMASK 0
+#define HAVE_GETPROCESSMEMORYINFO 0
+#define HAVE_GETPROCESSTIMES 0
+#define HAVE_GETRUSAGE 1
+#define HAVE_GETSYSTEMTIMEASFILETIME 0
+#define HAVE_GETTIMEOFDAY 1
+#define HAVE_GLOB 1
+#define HAVE_GLXGETPROCADDRESS 0
+#define HAVE_GMTIME_R 1
+#define HAVE_INET_ATON 0
+#define HAVE_ISATTY 1
+#define HAVE_KBHIT 0
+#define HAVE_LOCALTIME_R 1
+#define HAVE_LSTAT 1
+#define HAVE_LZO1X_999_COMPRESS 0
+#define HAVE_MACH_ABSOLUTE_TIME 0
+#define HAVE_MAPVIEWOFFILE 0
+#define HAVE_MEMALIGN 1
+#define HAVE_MKSTEMP 1
+#define HAVE_MMAP 1
+#define HAVE_MPROTECT 1
+#define HAVE_NANOSLEEP 1
+#define HAVE_PEEKNAMEDPIPE 0
+#define HAVE_POSIX_MEMALIGN 1
+#define HAVE_PTHREAD_CANCEL 0
+#define HAVE_SCHED_GETAFFINITY 1
+#define HAVE_SECITEMIMPORT 0
+#define HAVE_SETCONSOLETEXTATTRIBUTE 0
+#define HAVE_SETCONSOLECTRLHANDLER 0
+#define HAVE_SETMODE 0
+#define HAVE_SETRLIMIT 1
+#define HAVE_SLEEP 0
+#define HAVE_STRERROR_R 1
+#define HAVE_SYSCONF 1
+#define HAVE_SYSCTL 1
+#define HAVE_USLEEP 1
+#define HAVE_UTGETOSTYPEFROMSTRING 0
+#define HAVE_VIRTUALALLOC 0
+#define HAVE_WGLGETPROCADDRESS 0
+#define HAVE_BCRYPT 0
+#define HAVE_VAAPI_DRM 0
+#define HAVE_VAAPI_X11 0
+#define HAVE_VDPAU_X11 0
+#define HAVE_PTHREADS 0
+#define HAVE_OS2THREADS 0
+#define HAVE_W32THREADS 0
+#define HAVE_AS_ARCH_DIRECTIVE 1
+#define HAVE_AS_DN_DIRECTIVE 1
+#define HAVE_AS_FPU_DIRECTIVE 1
+#define HAVE_AS_FUNC 0
+#define HAVE_AS_OBJECT_ARCH 1
+#define HAVE_ASM_MOD_Q 1
+#define HAVE_BLOCKS_EXTENSION 0
+#define HAVE_EBP_AVAILABLE 0
+#define HAVE_EBX_AVAILABLE 0
+#define HAVE_GNU_AS 0
+#define HAVE_GNU_WINDRES 0
+#define HAVE_IBM_ASM 0
+#define HAVE_INLINE_ASM_DIRECT_SYMBOL_REFS 0
+#define HAVE_INLINE_ASM_LABELS 1
+#define HAVE_INLINE_ASM_NONLOCAL_LABELS 1
+#define HAVE_PRAGMA_DEPRECATED 1
+#define HAVE_RSYNC_CONTIMEOUT 1
+#define HAVE_SYMVER_ASM_LABEL 0
+#define HAVE_SYMVER_GNU_ASM 1
+#define HAVE_VFP_ARGS 0
+#define HAVE_XFORM_ASM 0
+#define HAVE_XMM_CLOBBERS 0
+#define HAVE_KCMVIDEOCODECTYPE_HEVC 0
+#define HAVE_SOCKLEN_T 0
+#define HAVE_STRUCT_ADDRINFO 0
+#define HAVE_STRUCT_GROUP_SOURCE_REQ 0
+#define HAVE_STRUCT_IP_MREQ_SOURCE 0
+#define HAVE_STRUCT_IPV6_MREQ 0
+#define HAVE_STRUCT_MSGHDR_MSG_FLAGS 0
+#define HAVE_STRUCT_POLLFD 0
+#define HAVE_STRUCT_RUSAGE_RU_MAXRSS 1
+#define HAVE_STRUCT_SCTP_EVENT_SUBSCRIBE 0
+#define HAVE_STRUCT_SOCKADDR_IN6 0
+#define HAVE_STRUCT_SOCKADDR_SA_LEN 0
+#define HAVE_STRUCT_SOCKADDR_STORAGE 0
+#define HAVE_STRUCT_STAT_ST_MTIM_TV_NSEC 1
+#define HAVE_STRUCT_V4L2_FRMIVALENUM_DISCRETE 1
+#define HAVE_MAKEINFO 1
+#define HAVE_MAKEINFO_HTML 1
+#define HAVE_OPENCL_D3D11 0
+#define HAVE_OPENCL_DRM_ARM 0
+#define HAVE_OPENCL_DRM_BEIGNET 0
+#define HAVE_OPENCL_DXVA2 0
+#define HAVE_OPENCL_VAAPI_BEIGNET 0
+#define HAVE_OPENCL_VAAPI_INTEL_MEDIA 0
+#define HAVE_PERL 1
+#define HAVE_POD2MAN 1
+#define HAVE_TEXI2HTML 0
+#define CONFIG_DOC 0
+#define CONFIG_HTMLPAGES 0
+#define CONFIG_MANPAGES 1
+#define CONFIG_PODPAGES 1
+#define CONFIG_TXTPAGES 1
+#define CONFIG_AVIO_DIR_CMD_EXAMPLE 1
+#define CONFIG_AVIO_READING_EXAMPLE 1
+#define CONFIG_DECODE_AUDIO_EXAMPLE 1
+#define CONFIG_DECODE_VIDEO_EXAMPLE 1
+#define CONFIG_DEMUXING_DECODING_EXAMPLE 1
+#define CONFIG_ENCODE_AUDIO_EXAMPLE 1
+#define CONFIG_ENCODE_VIDEO_EXAMPLE 1
+#define CONFIG_EXTRACT_MVS_EXAMPLE 1
+#define CONFIG_FILTER_AUDIO_EXAMPLE 0
+#define CONFIG_FILTERING_AUDIO_EXAMPLE 0
+#define CONFIG_FILTERING_VIDEO_EXAMPLE 0
+#define CONFIG_HTTP_MULTICLIENT_EXAMPLE 1
+#define CONFIG_HW_DECODE_EXAMPLE 1
+#define CONFIG_METADATA_EXAMPLE 1
+#define CONFIG_MUXING_EXAMPLE 0
+#define CONFIG_QSVDEC_EXAMPLE 0
+#define CONFIG_REMUXING_EXAMPLE 1
+#define CONFIG_RESAMPLING_AUDIO_EXAMPLE 0
+#define CONFIG_SCALING_VIDEO_EXAMPLE 0
+#define CONFIG_TRANSCODE_AAC_EXAMPLE 0
+#define CONFIG_TRANSCODING_EXAMPLE 0
+#define CONFIG_VAAPI_ENCODE_EXAMPLE 0
+#define CONFIG_VAAPI_TRANSCODE_EXAMPLE 0
+#define CONFIG_AVISYNTH 0
+#define CONFIG_FREI0R 0
+#define CONFIG_LIBCDIO 0
+#define CONFIG_LIBDAVS2 0
+#define CONFIG_LIBRUBBERBAND 0
+#define CONFIG_LIBVIDSTAB 0
+#define CONFIG_LIBX264 0
+#define CONFIG_LIBX265 0
+#define CONFIG_LIBXAVS 0
+#define CONFIG_LIBXAVS2 0
+#define CONFIG_LIBXVID 0
+#define CONFIG_DECKLINK 0
+#define CONFIG_LIBNDI_NEWTEK 0
+#define CONFIG_LIBFDK_AAC 0
+#define CONFIG_OPENSSL 0
+#define CONFIG_LIBTLS 0
+#define CONFIG_GMP 0
+#define CONFIG_LIBLENSFUN 0
+#define CONFIG_LIBOPENCORE_AMRNB 0
+#define CONFIG_LIBOPENCORE_AMRWB 0
+#define CONFIG_LIBVMAF 0
+#define CONFIG_LIBVO_AMRWBENC 0
+#define CONFIG_MBEDTLS 0
+#define CONFIG_RKMPP 0
+#define CONFIG_LIBSMBCLIENT 0
+#define CONFIG_CHROMAPRINT 0
+#define CONFIG_GCRYPT 0
+#define CONFIG_GNUTLS 0
+#define CONFIG_JNI 0
+#define CONFIG_LADSPA 0
+#define CONFIG_LIBAOM 0
+#define CONFIG_LIBASS 0
+#define CONFIG_LIBBLURAY 0
+#define CONFIG_LIBBS2B 0
+#define CONFIG_LIBCACA 0
+#define CONFIG_LIBCELT 0
+#define CONFIG_LIBCODEC2 0
+#define CONFIG_LIBDC1394 0
+#define CONFIG_LIBDRM 0
+#define CONFIG_LIBFLITE 0
+#define CONFIG_LIBFONTCONFIG 0
+#define CONFIG_LIBFREETYPE 0
+#define CONFIG_LIBFRIBIDI 0
+#define CONFIG_LIBGME 0
+#define CONFIG_LIBGSM 0
+#define CONFIG_LIBIEC61883 0
+#define CONFIG_LIBILBC 0
+#define CONFIG_LIBJACK 0
+#define CONFIG_LIBKLVANC 0
+#define CONFIG_LIBKVAZAAR 0
+#define CONFIG_LIBMODPLUG 0
+#define CONFIG_LIBMP3LAME 0
+#define CONFIG_LIBMYSOFA 0
+#define CONFIG_LIBOPENCV 0
+#define CONFIG_LIBOPENH264 0
+#define CONFIG_LIBOPENJPEG 0
+#define CONFIG_LIBOPENMPT 0
+#define CONFIG_LIBOPUS 0
+#define CONFIG_LIBPULSE 0
+#define CONFIG_LIBRSVG 0
+#define CONFIG_LIBRTMP 0
+#define CONFIG_LIBSHINE 0
+#define CONFIG_LIBSMBCLIENT 0
+#define CONFIG_LIBSNAPPY 0
+#define CONFIG_LIBSOXR 0
+#define CONFIG_LIBSPEEX 0
+#define CONFIG_LIBSRT 0
+#define CONFIG_LIBSSH 0
+#define CONFIG_LIBTENSORFLOW 0
+#define CONFIG_LIBTESSERACT 0
+#define CONFIG_LIBTHEORA 0
+#define CONFIG_LIBTWOLAME 0
+#define CONFIG_LIBV4L2 0
+#define CONFIG_LIBVORBIS 0
+#define CONFIG_LIBVPX 0
+#define CONFIG_LIBWAVPACK 0
+#define CONFIG_LIBWEBP 0
+#define CONFIG_LIBXML2 0
+#define CONFIG_LIBZIMG 0
+#define CONFIG_LIBZMQ 0
+#define CONFIG_LIBZVBI 0
+#define CONFIG_LV2 0
+#define CONFIG_MEDIACODEC 0
+#define CONFIG_OPENAL 0
+#define CONFIG_OPENGL 0
+#define CONFIG_VAPOURSYNTH 0
+#define CONFIG_ALSA 0
+#define CONFIG_APPKIT 0
+#define CONFIG_AVFOUNDATION 0
+#define CONFIG_BZLIB 0
+#define CONFIG_COREIMAGE 0
+#define CONFIG_ICONV 0
+#define CONFIG_LIBXCB 0
+#define CONFIG_LIBXCB_SHM 0
+#define CONFIG_LIBXCB_SHAPE 0
+#define CONFIG_LIBXCB_XFIXES 0
+#define CONFIG_LZMA 0
+#define CONFIG_SCHANNEL 0
+#define CONFIG_SDL2 0
+#define CONFIG_SECURETRANSPORT 0
+#define CONFIG_SNDIO 0
+#define CONFIG_XLIB 0
+#define CONFIG_ZLIB 0
+#define CONFIG_CUDA_SDK 0
+#define CONFIG_LIBNPP 0
+#define CONFIG_LIBMFX 0
+#define CONFIG_MMAL 0
+#define CONFIG_OMX 0
+#define CONFIG_OPENCL 0
+#define CONFIG_AMF 0
+#define CONFIG_AUDIOTOOLBOX 0
+#define CONFIG_CRYSTALHD 0
+#define CONFIG_CUDA 0
+#define CONFIG_CUVID 0
+#define CONFIG_D3D11VA 0
+#define CONFIG_DXVA2 0
+#define CONFIG_FFNVCODEC 0
+#define CONFIG_NVDEC 0
+#define CONFIG_NVENC 0
+#define CONFIG_VAAPI 0
+#define CONFIG_VDPAU 0
+#define CONFIG_VIDEOTOOLBOX 0
+#define CONFIG_V4L2_M2M 0
+#define CONFIG_XVMC 0
+#define CONFIG_FTRAPV 0
+#define CONFIG_GRAY 0
+#define CONFIG_HARDCODED_TABLES 0
+#define CONFIG_OMX_RPI 0
+#define CONFIG_RUNTIME_CPUDETECT 0
+#define CONFIG_SAFE_BITSTREAM_READER 1
+#define CONFIG_SHARED 0
+#define CONFIG_SMALL 1
+#define CONFIG_STATIC 1
+#define CONFIG_SWSCALE_ALPHA 1
+#define CONFIG_GPL 0
+#define CONFIG_NONFREE 0
+#define CONFIG_VERSION3 0
+#define CONFIG_AVDEVICE 0
+#define CONFIG_AVFILTER 0
+#define CONFIG_SWSCALE 0
+#define CONFIG_POSTPROC 0
+#define CONFIG_AVFORMAT 1
+#define CONFIG_AVCODEC 1
+#define CONFIG_SWRESAMPLE 0
+#define CONFIG_AVRESAMPLE 0
+#define CONFIG_AVUTIL 1
+#define CONFIG_FFPLAY 0
+#define CONFIG_FFPROBE 0
+#define CONFIG_FFMPEG 0
+#define CONFIG_DCT 0
+#define CONFIG_DWT 0
+#define CONFIG_ERROR_RESILIENCE 0
+#define CONFIG_FAAN 1
+#define CONFIG_FAST_UNALIGNED 1
+#define CONFIG_FFT 0
+#define CONFIG_LSP 0
+#define CONFIG_LZO 0
+#define CONFIG_MDCT 0
+#define CONFIG_PIXELUTILS 0
+#define CONFIG_NETWORK 0
+#define CONFIG_RDFT 0
+#define CONFIG_AUTODETECT 0
+#define CONFIG_FONTCONFIG 0
+#define CONFIG_LINUX_PERF 1
+#define CONFIG_MEMORY_POISONING 0
+#define CONFIG_NEON_CLOBBER_TEST 0
+#define CONFIG_OSSFUZZ 0
+#define CONFIG_PIC 1
+#define CONFIG_THUMB 0
+#define CONFIG_VALGRIND_BACKTRACE 0
+#define CONFIG_XMM_CLOBBER_TEST 0
+#define CONFIG_BSFS 1
+#define CONFIG_DECODERS 0
+#define CONFIG_ENCODERS 0
+#define CONFIG_HWACCELS 0
+#define CONFIG_PARSERS 0
+#define CONFIG_INDEVS 0
+#define CONFIG_OUTDEVS 0
+#define CONFIG_FILTERS 0
+#define CONFIG_DEMUXERS 1
+#define CONFIG_MUXERS 0
+#define CONFIG_PROTOCOLS 1
+#define CONFIG_AANDCTTABLES 0
+#define CONFIG_AC3DSP 0
+#define CONFIG_ADTS_HEADER 0
+#define CONFIG_AUDIO_FRAME_QUEUE 0
+#define CONFIG_AUDIODSP 0
+#define CONFIG_BLOCKDSP 0
+#define CONFIG_BSWAPDSP 0
+#define CONFIG_CABAC 0
+#define CONFIG_CBS 0
+#define CONFIG_CBS_AV1 0
+#define CONFIG_CBS_H264 0
+#define CONFIG_CBS_H265 0
+#define CONFIG_CBS_JPEG 0
+#define CONFIG_CBS_MPEG2 0
+#define CONFIG_CBS_VP9 0
+#define CONFIG_DIRAC_PARSE 0
+#define CONFIG_DNN 0
+#define CONFIG_DVPROFILE 0
+#define CONFIG_EXIF 0
+#define CONFIG_FAANDCT 1
+#define CONFIG_FAANIDCT 1
+#define CONFIG_FDCTDSP 1
+#define CONFIG_FLACDSP 0
+#define CONFIG_FMTCONVERT 0
+#define CONFIG_FRAME_THREAD_ENCODER 0
+#define CONFIG_G722DSP 0
+#define CONFIG_GOLOMB 0
+#define CONFIG_GPLV3 0
+#define CONFIG_H263DSP 0
+#define CONFIG_H264CHROMA 0
+#define CONFIG_H264DSP 0
+#define CONFIG_H264PARSE 0
+#define CONFIG_H264PRED 0
+#define CONFIG_H264QPEL 0
+#define CONFIG_HEVCPARSE 0
+#define CONFIG_HPELDSP 0
+#define CONFIG_HUFFMAN 0
+#define CONFIG_HUFFYUVDSP 0
+#define CONFIG_HUFFYUVENCDSP 0
+#define CONFIG_IDCTDSP 1
+#define CONFIG_IIRFILTER 0
+#define CONFIG_MDCT15 0
+#define CONFIG_INTRAX8 0
+#define CONFIG_ISO_MEDIA 1
+#define CONFIG_IVIDSP 0
+#define CONFIG_JPEGTABLES 0
+#define CONFIG_LGPLV3 0
+#define CONFIG_LIBX262 0
+#define CONFIG_LLAUDDSP 0
+#define CONFIG_LLVIDDSP 0
+#define CONFIG_LLVIDENCDSP 0
+#define CONFIG_LPC 0
+#define CONFIG_LZF 0
+#define CONFIG_ME_CMP 0
+#define CONFIG_MPEG_ER 0
+#define CONFIG_MPEGAUDIO 0
+#define CONFIG_MPEGAUDIODSP 0
+#define CONFIG_MPEGAUDIOHEADER 0
+#define CONFIG_MPEGVIDEO 0
+#define CONFIG_MPEGVIDEOENC 0
+#define CONFIG_MSS34DSP 0
+#define CONFIG_PIXBLOCKDSP 0
+#define CONFIG_QPELDSP 0
+#define CONFIG_QSV 0
+#define CONFIG_QSVDEC 0
+#define CONFIG_QSVENC 0
+#define CONFIG_QSVVPP 0
+#define CONFIG_RANGECODER 0
+#define CONFIG_RIFFDEC 1
+#define CONFIG_RIFFENC 0
+#define CONFIG_RTPDEC 0
+#define CONFIG_RTPENC_CHAIN 0
+#define CONFIG_RV34DSP 0
+#define CONFIG_SINEWIN 0
+#define CONFIG_SNAPPY 0
+#define CONFIG_SRTP 0
+#define CONFIG_STARTCODE 0
+#define CONFIG_TEXTUREDSP 0
+#define CONFIG_TEXTUREDSPENC 0
+#define CONFIG_TPELDSP 0
+#define CONFIG_VAAPI_1 0
+#define CONFIG_VAAPI_ENCODE 0
+#define CONFIG_VC1DSP 0
+#define CONFIG_VIDEODSP 0
+#define CONFIG_VP3DSP 0
+#define CONFIG_VP56DSP 0
+#define CONFIG_VP8DSP 0
+#define CONFIG_WMA_FREQS 0
+#define CONFIG_WMV2DSP 0
+#define CONFIG_AAC_ADTSTOASC_BSF 0
+#define CONFIG_AV1_METADATA_BSF 0
+#define CONFIG_CHOMP_BSF 0
+#define CONFIG_DUMP_EXTRADATA_BSF 0
+#define CONFIG_DCA_CORE_BSF 0
+#define CONFIG_EAC3_CORE_BSF 0
+#define CONFIG_EXTRACT_EXTRADATA_BSF 0
+#define CONFIG_FILTER_UNITS_BSF 0
+#define CONFIG_H264_METADATA_BSF 0
+#define CONFIG_H264_MP4TOANNEXB_BSF 1
+#define CONFIG_H264_REDUNDANT_PPS_BSF 0
+#define CONFIG_HAPQA_EXTRACT_BSF 0
+#define CONFIG_HEVC_METADATA_BSF 0
+#define CONFIG_HEVC_MP4TOANNEXB_BSF 1
+#define CONFIG_IMX_DUMP_HEADER_BSF 0
+#define CONFIG_MJPEG2JPEG_BSF 0
+#define CONFIG_MJPEGA_DUMP_HEADER_BSF 0
+#define CONFIG_MP3_HEADER_DECOMPRESS_BSF 0
+#define CONFIG_MPEG2_METADATA_BSF 0
+#define CONFIG_MPEG4_UNPACK_BFRAMES_BSF 0
+#define CONFIG_MOV2TEXTSUB_BSF 0
+#define CONFIG_NOISE_BSF 0
+#define CONFIG_NULL_BSF 1
+#define CONFIG_REMOVE_EXTRADATA_BSF 0
+#define CONFIG_TEXT2MOVSUB_BSF 0
+#define CONFIG_TRACE_HEADERS_BSF 0
+#define CONFIG_VP9_METADATA_BSF 0
+#define CONFIG_VP9_RAW_REORDER_BSF 0
+#define CONFIG_VP9_SUPERFRAME_BSF 0
+#define CONFIG_VP9_SUPERFRAME_SPLIT_BSF 0
+#define CONFIG_AASC_DECODER 0
+#define CONFIG_AIC_DECODER 0
+#define CONFIG_ALIAS_PIX_DECODER 0
+#define CONFIG_AMV_DECODER 0
+#define CONFIG_ANM_DECODER 0
+#define CONFIG_ANSI_DECODER 0
+#define CONFIG_APNG_DECODER 0
+#define CONFIG_ASV1_DECODER 0
+#define CONFIG_ASV2_DECODER 0
+#define CONFIG_AURA_DECODER 0
+#define CONFIG_AURA2_DECODER 0
+#define CONFIG_AVRP_DECODER 0
+#define CONFIG_AVRN_DECODER 0
+#define CONFIG_AVS_DECODER 0
+#define CONFIG_AVUI_DECODER 0
+#define CONFIG_AYUV_DECODER 0
+#define CONFIG_BETHSOFTVID_DECODER 0
+#define CONFIG_BFI_DECODER 0
+#define CONFIG_BINK_DECODER 0
+#define CONFIG_BITPACKED_DECODER 0
+#define CONFIG_BMP_DECODER 0
+#define CONFIG_BMV_VIDEO_DECODER 0
+#define CONFIG_BRENDER_PIX_DECODER 0
+#define CONFIG_C93_DECODER 0
+#define CONFIG_CAVS_DECODER 0
+#define CONFIG_CDGRAPHICS_DECODER 0
+#define CONFIG_CDXL_DECODER 0
+#define CONFIG_CFHD_DECODER 0
+#define CONFIG_CINEPAK_DECODER 0
+#define CONFIG_CLEARVIDEO_DECODER 0
+#define CONFIG_CLJR_DECODER 0
+#define CONFIG_CLLC_DECODER 0
+#define CONFIG_COMFORTNOISE_DECODER 0
+#define CONFIG_CPIA_DECODER 0
+#define CONFIG_CSCD_DECODER 0
+#define CONFIG_CYUV_DECODER 0
+#define CONFIG_DDS_DECODER 0
+#define CONFIG_DFA_DECODER 0
+#define CONFIG_DIRAC_DECODER 0
+#define CONFIG_DNXHD_DECODER 0
+#define CONFIG_DPX_DECODER 0
+#define CONFIG_DSICINVIDEO_DECODER 0
+#define CONFIG_DVAUDIO_DECODER 0
+#define CONFIG_DVVIDEO_DECODER 0
+#define CONFIG_DXA_DECODER 0
+#define CONFIG_DXTORY_DECODER 0
+#define CONFIG_DXV_DECODER 0
+#define CONFIG_EACMV_DECODER 0
+#define CONFIG_EAMAD_DECODER 0
+#define CONFIG_EATGQ_DECODER 0
+#define CONFIG_EATGV_DECODER 0
+#define CONFIG_EATQI_DECODER 0
+#define CONFIG_EIGHTBPS_DECODER 0
+#define CONFIG_EIGHTSVX_EXP_DECODER 0
+#define CONFIG_EIGHTSVX_FIB_DECODER 0
+#define CONFIG_ESCAPE124_DECODER 0
+#define CONFIG_ESCAPE130_DECODER 0
+#define CONFIG_EXR_DECODER 0
+#define CONFIG_FFV1_DECODER 0
+#define CONFIG_FFVHUFF_DECODER 0
+#define CONFIG_FIC_DECODER 0
+#define CONFIG_FITS_DECODER 0
+#define CONFIG_FLASHSV_DECODER 0
+#define CONFIG_FLASHSV2_DECODER 0
+#define CONFIG_FLIC_DECODER 0
+#define CONFIG_FLV_DECODER 0
+#define CONFIG_FMVC_DECODER 0
+#define CONFIG_FOURXM_DECODER 0
+#define CONFIG_FRAPS_DECODER 0
+#define CONFIG_FRWU_DECODER 0
+#define CONFIG_G2M_DECODER 0
+#define CONFIG_GDV_DECODER 0
+#define CONFIG_GIF_DECODER 0
+#define CONFIG_H261_DECODER 0
+#define CONFIG_H263_DECODER 0
+#define CONFIG_H263I_DECODER 0
+#define CONFIG_H263P_DECODER 0
+#define CONFIG_H263_V4L2M2M_DECODER 0
+#define CONFIG_H264_DECODER 0
+#define CONFIG_H264_CRYSTALHD_DECODER 0
+#define CONFIG_H264_V4L2M2M_DECODER 0
+#define CONFIG_H264_MEDIACODEC_DECODER 0
+#define CONFIG_H264_MMAL_DECODER 0
+#define CONFIG_H264_QSV_DECODER 0
+#define CONFIG_H264_RKMPP_DECODER 0
+#define CONFIG_HAP_DECODER 0
+#define CONFIG_HEVC_DECODER 0
+#define CONFIG_HEVC_QSV_DECODER 0
+#define CONFIG_HEVC_RKMPP_DECODER 0
+#define CONFIG_HEVC_V4L2M2M_DECODER 0
+#define CONFIG_HNM4_VIDEO_DECODER 0
+#define CONFIG_HQ_HQA_DECODER 0
+#define CONFIG_HQX_DECODER 0
+#define CONFIG_HUFFYUV_DECODER 0
+#define CONFIG_IDCIN_DECODER 0
+#define CONFIG_IFF_ILBM_DECODER 0
+#define CONFIG_IMM4_DECODER 0
+#define CONFIG_INDEO2_DECODER 0
+#define CONFIG_INDEO3_DECODER 0
+#define CONFIG_INDEO4_DECODER 0
+#define CONFIG_INDEO5_DECODER 0
+#define CONFIG_INTERPLAY_VIDEO_DECODER 0
+#define CONFIG_JPEG2000_DECODER 0
+#define CONFIG_JPEGLS_DECODER 0
+#define CONFIG_JV_DECODER 0
+#define CONFIG_KGV1_DECODER 0
+#define CONFIG_KMVC_DECODER 0
+#define CONFIG_LAGARITH_DECODER 0
+#define CONFIG_LOCO_DECODER 0
+#define CONFIG_M101_DECODER 0
+#define CONFIG_MAGICYUV_DECODER 0
+#define CONFIG_MDEC_DECODER 0
+#define CONFIG_MIMIC_DECODER 0
+#define CONFIG_MJPEG_DECODER 0
+#define CONFIG_MJPEGB_DECODER 0
+#define CONFIG_MMVIDEO_DECODER 0
+#define CONFIG_MOTIONPIXELS_DECODER 0
+#define CONFIG_MPEG1VIDEO_DECODER 0
+#define CONFIG_MPEG2VIDEO_DECODER 0
+#define CONFIG_MPEG4_DECODER 0
+#define CONFIG_MPEG4_CRYSTALHD_DECODER 0
+#define CONFIG_MPEG4_V4L2M2M_DECODER 0
+#define CONFIG_MPEG4_MMAL_DECODER 0
+#define CONFIG_MPEGVIDEO_DECODER 0
+#define CONFIG_MPEG1_V4L2M2M_DECODER 0
+#define CONFIG_MPEG2_MMAL_DECODER 0
+#define CONFIG_MPEG2_CRYSTALHD_DECODER 0
+#define CONFIG_MPEG2_V4L2M2M_DECODER 0
+#define CONFIG_MPEG2_QSV_DECODER 0
+#define CONFIG_MPEG2_MEDIACODEC_DECODER 0
+#define CONFIG_MSA1_DECODER 0
+#define CONFIG_MSCC_DECODER 0
+#define CONFIG_MSMPEG4V1_DECODER 0
+#define CONFIG_MSMPEG4V2_DECODER 0
+#define CONFIG_MSMPEG4V3_DECODER 0
+#define CONFIG_MSMPEG4_CRYSTALHD_DECODER 0
+#define CONFIG_MSRLE_DECODER 0
+#define CONFIG_MSS1_DECODER 0
+#define CONFIG_MSS2_DECODER 0
+#define CONFIG_MSVIDEO1_DECODER 0
+#define CONFIG_MSZH_DECODER 0
+#define CONFIG_MTS2_DECODER 0
+#define CONFIG_MVC1_DECODER 0
+#define CONFIG_MVC2_DECODER 0
+#define CONFIG_MWSC_DECODER 0
+#define CONFIG_MXPEG_DECODER 0
+#define CONFIG_NUV_DECODER 0
+#define CONFIG_PAF_VIDEO_DECODER 0
+#define CONFIG_PAM_DECODER 0
+#define CONFIG_PBM_DECODER 0
+#define CONFIG_PCX_DECODER 0
+#define CONFIG_PGM_DECODER 0
+#define CONFIG_PGMYUV_DECODER 0
+#define CONFIG_PICTOR_DECODER 0
+#define CONFIG_PIXLET_DECODER 0
+#define CONFIG_PNG_DECODER 0
+#define CONFIG_PPM_DECODER 0
+#define CONFIG_PRORES_DECODER 0
+#define CONFIG_PROSUMER_DECODER 0
+#define CONFIG_PSD_DECODER 0
+#define CONFIG_PTX_DECODER 0
+#define CONFIG_QDRAW_DECODER 0
+#define CONFIG_QPEG_DECODER 0
+#define CONFIG_QTRLE_DECODER 0
+#define CONFIG_R10K_DECODER 0
+#define CONFIG_R210_DECODER 0
+#define CONFIG_RASC_DECODER 0
+#define CONFIG_RAWVIDEO_DECODER 0
+#define CONFIG_RL2_DECODER 0
+#define CONFIG_ROQ_DECODER 0
+#define CONFIG_RPZA_DECODER 0
+#define CONFIG_RSCC_DECODER 0
+#define CONFIG_RV10_DECODER 0
+#define CONFIG_RV20_DECODER 0
+#define CONFIG_RV30_DECODER 0
+#define CONFIG_RV40_DECODER 0
+#define CONFIG_S302M_DECODER 0
+#define CONFIG_SANM_DECODER 0
+#define CONFIG_SCPR_DECODER 0
+#define CONFIG_SCREENPRESSO_DECODER 0
+#define CONFIG_SDX2_DPCM_DECODER 0
+#define CONFIG_SGI_DECODER 0
+#define CONFIG_SGIRLE_DECODER 0
+#define CONFIG_SHEERVIDEO_DECODER 0
+#define CONFIG_SMACKER_DECODER 0
+#define CONFIG_SMC_DECODER 0
+#define CONFIG_SMVJPEG_DECODER 0
+#define CONFIG_SNOW_DECODER 0
+#define CONFIG_SP5X_DECODER 0
+#define CONFIG_SPEEDHQ_DECODER 0
+#define CONFIG_SRGC_DECODER 0
+#define CONFIG_SUNRAST_DECODER 0
+#define CONFIG_SVQ1_DECODER 0
+#define CONFIG_SVQ3_DECODER 0
+#define CONFIG_TARGA_DECODER 0
+#define CONFIG_TARGA_Y216_DECODER 0
+#define CONFIG_TDSC_DECODER 0
+#define CONFIG_THEORA_DECODER 0
+#define CONFIG_THP_DECODER 0
+#define CONFIG_TIERTEXSEQVIDEO_DECODER 0
+#define CONFIG_TIFF_DECODER 0
+#define CONFIG_TMV_DECODER 0
+#define CONFIG_TRUEMOTION1_DECODER 0
+#define CONFIG_TRUEMOTION2_DECODER 0
+#define CONFIG_TRUEMOTION2RT_DECODER 0
+#define CONFIG_TSCC_DECODER 0
+#define CONFIG_TSCC2_DECODER 0
+#define CONFIG_TXD_DECODER 0
+#define CONFIG_ULTI_DECODER 0
+#define CONFIG_UTVIDEO_DECODER 0
+#define CONFIG_V210_DECODER 0
+#define CONFIG_V210X_DECODER 0
+#define CONFIG_V308_DECODER 0
+#define CONFIG_V408_DECODER 0
+#define CONFIG_V410_DECODER 0
+#define CONFIG_VB_DECODER 0
+#define CONFIG_VBLE_DECODER 0
+#define CONFIG_VC1_DECODER 0
+#define CONFIG_VC1_CRYSTALHD_DECODER 0
+#define CONFIG_VC1IMAGE_DECODER 0
+#define CONFIG_VC1_MMAL_DECODER 0
+#define CONFIG_VC1_QSV_DECODER 0
+#define CONFIG_VC1_V4L2M2M_DECODER 0
+#define CONFIG_VCR1_DECODER 0
+#define CONFIG_VMDVIDEO_DECODER 0
+#define CONFIG_VMNC_DECODER 0
+#define CONFIG_VP3_DECODER 0
+#define CONFIG_VP5_DECODER 0
+#define CONFIG_VP6_DECODER 0
+#define CONFIG_VP6A_DECODER 0
+#define CONFIG_VP6F_DECODER 0
+#define CONFIG_VP7_DECODER 0
+#define CONFIG_VP8_DECODER 0
+#define CONFIG_VP8_RKMPP_DECODER 0
+#define CONFIG_VP8_V4L2M2M_DECODER 0
+#define CONFIG_VP9_DECODER 0
+#define CONFIG_VP9_RKMPP_DECODER 0
+#define CONFIG_VP9_V4L2M2M_DECODER 0
+#define CONFIG_VQA_DECODER 0
+#define CONFIG_WEBP_DECODER 0
+#define CONFIG_WCMV_DECODER 0
+#define CONFIG_WRAPPED_AVFRAME_DECODER 0
+#define CONFIG_WMV1_DECODER 0
+#define CONFIG_WMV2_DECODER 0
+#define CONFIG_WMV3_DECODER 0
+#define CONFIG_WMV3_CRYSTALHD_DECODER 0
+#define CONFIG_WMV3IMAGE_DECODER 0
+#define CONFIG_WNV1_DECODER 0
+#define CONFIG_XAN_WC3_DECODER 0
+#define CONFIG_XAN_WC4_DECODER 0
+#define CONFIG_XBM_DECODER 0
+#define CONFIG_XFACE_DECODER 0
+#define CONFIG_XL_DECODER 0
+#define CONFIG_XPM_DECODER 0
+#define CONFIG_XWD_DECODER 0
+#define CONFIG_Y41P_DECODER 0
+#define CONFIG_YLC_DECODER 0
+#define CONFIG_YOP_DECODER 0
+#define CONFIG_YUV4_DECODER 0
+#define CONFIG_ZERO12V_DECODER 0
+#define CONFIG_ZEROCODEC_DECODER 0
+#define CONFIG_ZLIB_DECODER 0
+#define CONFIG_ZMBV_DECODER 0
+#define CONFIG_AAC_DECODER 0
+#define CONFIG_AAC_FIXED_DECODER 0
+#define CONFIG_AAC_LATM_DECODER 0
+#define CONFIG_AC3_DECODER 0
+#define CONFIG_AC3_FIXED_DECODER 0
+#define CONFIG_ALAC_DECODER 0
+#define CONFIG_ALS_DECODER 0
+#define CONFIG_AMRNB_DECODER 0
+#define CONFIG_AMRWB_DECODER 0
+#define CONFIG_APE_DECODER 0
+#define CONFIG_APTX_DECODER 0
+#define CONFIG_APTX_HD_DECODER 0
+#define CONFIG_ATRAC1_DECODER 0
+#define CONFIG_ATRAC3_DECODER 0
+#define CONFIG_ATRAC3AL_DECODER 0
+#define CONFIG_ATRAC3P_DECODER 0
+#define CONFIG_ATRAC3PAL_DECODER 0
+#define CONFIG_ATRAC9_DECODER 0
+#define CONFIG_BINKAUDIO_DCT_DECODER 0
+#define CONFIG_BINKAUDIO_RDFT_DECODER 0
+#define CONFIG_BMV_AUDIO_DECODER 0
+#define CONFIG_COOK_DECODER 0
+#define CONFIG_DCA_DECODER 0
+#define CONFIG_DOLBY_E_DECODER 0
+#define CONFIG_DSD_LSBF_DECODER 0
+#define CONFIG_DSD_MSBF_DECODER 0
+#define CONFIG_DSD_LSBF_PLANAR_DECODER 0
+#define CONFIG_DSD_MSBF_PLANAR_DECODER 0
+#define CONFIG_DSICINAUDIO_DECODER 0
+#define CONFIG_DSS_SP_DECODER 0
+#define CONFIG_DST_DECODER 0
+#define CONFIG_EAC3_DECODER 0
+#define CONFIG_EVRC_DECODER 0
+#define CONFIG_FFWAVESYNTH_DECODER 0
+#define CONFIG_FLAC_DECODER 0
+#define CONFIG_G723_1_DECODER 0
+#define CONFIG_G729_DECODER 0
+#define CONFIG_GSM_DECODER 0
+#define CONFIG_GSM_MS_DECODER 0
+#define CONFIG_IAC_DECODER 0
+#define CONFIG_ILBC_DECODER 0
+#define CONFIG_IMC_DECODER 0
+#define CONFIG_INTERPLAY_ACM_DECODER 0
+#define CONFIG_MACE3_DECODER 0
+#define CONFIG_MACE6_DECODER 0
+#define CONFIG_METASOUND_DECODER 0
+#define CONFIG_MLP_DECODER 0
+#define CONFIG_MP1_DECODER 0
+#define CONFIG_MP1FLOAT_DECODER 0
+#define CONFIG_MP2_DECODER 0
+#define CONFIG_MP2FLOAT_DECODER 0
+#define CONFIG_MP3FLOAT_DECODER 0
+#define CONFIG_MP3_DECODER 0
+#define CONFIG_MP3ADUFLOAT_DECODER 0
+#define CONFIG_MP3ADU_DECODER 0
+#define CONFIG_MP3ON4FLOAT_DECODER 0
+#define CONFIG_MP3ON4_DECODER 0
+#define CONFIG_MPC7_DECODER 0
+#define CONFIG_MPC8_DECODER 0
+#define CONFIG_NELLYMOSER_DECODER 0
+#define CONFIG_ON2AVC_DECODER 0
+#define CONFIG_OPUS_DECODER 0
+#define CONFIG_PAF_AUDIO_DECODER 0
+#define CONFIG_QCELP_DECODER 0
+#define CONFIG_QDM2_DECODER 0
+#define CONFIG_QDMC_DECODER 0
+#define CONFIG_RA_144_DECODER 0
+#define CONFIG_RA_288_DECODER 0
+#define CONFIG_RALF_DECODER 0
+#define CONFIG_SBC_DECODER 0
+#define CONFIG_SHORTEN_DECODER 0
+#define CONFIG_SIPR_DECODER 0
+#define CONFIG_SMACKAUD_DECODER 0
+#define CONFIG_SONIC_DECODER 0
+#define CONFIG_TAK_DECODER 0
+#define CONFIG_TRUEHD_DECODER 0
+#define CONFIG_TRUESPEECH_DECODER 0
+#define CONFIG_TTA_DECODER 0
+#define CONFIG_TWINVQ_DECODER 0
+#define CONFIG_VMDAUDIO_DECODER 0
+#define CONFIG_VORBIS_DECODER 0
+#define CONFIG_WAVPACK_DECODER 0
+#define CONFIG_WMALOSSLESS_DECODER 0
+#define CONFIG_WMAPRO_DECODER 0
+#define CONFIG_WMAV1_DECODER 0
+#define CONFIG_WMAV2_DECODER 0
+#define CONFIG_WMAVOICE_DECODER 0
+#define CONFIG_WS_SND1_DECODER 0
+#define CONFIG_XMA1_DECODER 0
+#define CONFIG_XMA2_DECODER 0
+#define CONFIG_PCM_ALAW_DECODER 0
+#define CONFIG_PCM_BLURAY_DECODER 0
+#define CONFIG_PCM_DVD_DECODER 0
+#define CONFIG_PCM_F16LE_DECODER 0
+#define CONFIG_PCM_F24LE_DECODER 0
+#define CONFIG_PCM_F32BE_DECODER 0
+#define CONFIG_PCM_F32LE_DECODER 0
+#define CONFIG_PCM_F64BE_DECODER 0
+#define CONFIG_PCM_F64LE_DECODER 0
+#define CONFIG_PCM_LXF_DECODER 0
+#define CONFIG_PCM_MULAW_DECODER 0
+#define CONFIG_PCM_S8_DECODER 0
+#define CONFIG_PCM_S8_PLANAR_DECODER 0
+#define CONFIG_PCM_S16BE_DECODER 0
+#define CONFIG_PCM_S16BE_PLANAR_DECODER 0
+#define CONFIG_PCM_S16LE_DECODER 0
+#define CONFIG_PCM_S16LE_PLANAR_DECODER 0
+#define CONFIG_PCM_S24BE_DECODER 0
+#define CONFIG_PCM_S24DAUD_DECODER 0
+#define CONFIG_PCM_S24LE_DECODER 0
+#define CONFIG_PCM_S24LE_PLANAR_DECODER 0
+#define CONFIG_PCM_S32BE_DECODER 0
+#define CONFIG_PCM_S32LE_DECODER 0
+#define CONFIG_PCM_S32LE_PLANAR_DECODER 0
+#define CONFIG_PCM_S64BE_DECODER 0
+#define CONFIG_PCM_S64LE_DECODER 0
+#define CONFIG_PCM_U8_DECODER 0
+#define CONFIG_PCM_U16BE_DECODER 0
+#define CONFIG_PCM_U16LE_DECODER 0
+#define CONFIG_PCM_U24BE_DECODER 0
+#define CONFIG_PCM_U24LE_DECODER 0
+#define CONFIG_PCM_U32BE_DECODER 0
+#define CONFIG_PCM_U32LE_DECODER 0
+#define CONFIG_PCM_VIDC_DECODER 0
+#define CONFIG_PCM_ZORK_DECODER 0
+#define CONFIG_GREMLIN_DPCM_DECODER 0
+#define CONFIG_INTERPLAY_DPCM_DECODER 0
+#define CONFIG_ROQ_DPCM_DECODER 0
+#define CONFIG_SOL_DPCM_DECODER 0
+#define CONFIG_XAN_DPCM_DECODER 0
+#define CONFIG_ADPCM_4XM_DECODER 0
+#define CONFIG_ADPCM_ADX_DECODER 0
+#define CONFIG_ADPCM_AFC_DECODER 0
+#define CONFIG_ADPCM_AICA_DECODER 0
+#define CONFIG_ADPCM_CT_DECODER 0
+#define CONFIG_ADPCM_DTK_DECODER 0
+#define CONFIG_ADPCM_EA_DECODER 0
+#define CONFIG_ADPCM_EA_MAXIS_XA_DECODER 0
+#define CONFIG_ADPCM_EA_R1_DECODER 0
+#define CONFIG_ADPCM_EA_R2_DECODER 0
+#define CONFIG_ADPCM_EA_R3_DECODER 0
+#define CONFIG_ADPCM_EA_XAS_DECODER 0
+#define CONFIG_ADPCM_G722_DECODER 0
+#define CONFIG_ADPCM_G726_DECODER 0
+#define CONFIG_ADPCM_G726LE_DECODER 0
+#define CONFIG_ADPCM_IMA_AMV_DECODER 0
+#define CONFIG_ADPCM_IMA_APC_DECODER 0
+#define CONFIG_ADPCM_IMA_DAT4_DECODER 0
+#define CONFIG_ADPCM_IMA_DK3_DECODER 0
+#define CONFIG_ADPCM_IMA_DK4_DECODER 0
+#define CONFIG_ADPCM_IMA_EA_EACS_DECODER 0
+#define CONFIG_ADPCM_IMA_EA_SEAD_DECODER 0
+#define CONFIG_ADPCM_IMA_ISS_DECODER 0
+#define CONFIG_ADPCM_IMA_OKI_DECODER 0
+#define CONFIG_ADPCM_IMA_QT_DECODER 0
+#define CONFIG_ADPCM_IMA_RAD_DECODER 0
+#define CONFIG_ADPCM_IMA_SMJPEG_DECODER 0
+#define CONFIG_ADPCM_IMA_WAV_DECODER 0
+#define CONFIG_ADPCM_IMA_WS_DECODER 0
+#define CONFIG_ADPCM_MS_DECODER 0
+#define CONFIG_ADPCM_MTAF_DECODER 0
+#define CONFIG_ADPCM_PSX_DECODER 0
+#define CONFIG_ADPCM_SBPRO_2_DECODER 0
+#define CONFIG_ADPCM_SBPRO_3_DECODER 0
+#define CONFIG_ADPCM_SBPRO_4_DECODER 0
+#define CONFIG_ADPCM_SWF_DECODER 0
+#define CONFIG_ADPCM_THP_DECODER 0
+#define CONFIG_ADPCM_THP_LE_DECODER 0
+#define CONFIG_ADPCM_VIMA_DECODER 0
+#define CONFIG_ADPCM_XA_DECODER 0
+#define CONFIG_ADPCM_YAMAHA_DECODER 0
+#define CONFIG_SSA_DECODER 0
+#define CONFIG_ASS_DECODER 0
+#define CONFIG_CCAPTION_DECODER 0
+#define CONFIG_DVBSUB_DECODER 0
+#define CONFIG_DVDSUB_DECODER 0
+#define CONFIG_JACOSUB_DECODER 0
+#define CONFIG_MICRODVD_DECODER 0
+#define CONFIG_MOVTEXT_DECODER 0
+#define CONFIG_MPL2_DECODER 0
+#define CONFIG_PGSSUB_DECODER 0
+#define CONFIG_PJS_DECODER 0
+#define CONFIG_REALTEXT_DECODER 0
+#define CONFIG_SAMI_DECODER 0
+#define CONFIG_SRT_DECODER 0
+#define CONFIG_STL_DECODER 0
+#define CONFIG_SUBRIP_DECODER 0
+#define CONFIG_SUBVIEWER_DECODER 0
+#define CONFIG_SUBVIEWER1_DECODER 0
+#define CONFIG_TEXT_DECODER 0
+#define CONFIG_VPLAYER_DECODER 0
+#define CONFIG_WEBVTT_DECODER 0
+#define CONFIG_XSUB_DECODER 0
+#define CONFIG_AAC_AT_DECODER 0
+#define CONFIG_AC3_AT_DECODER 0
+#define CONFIG_ADPCM_IMA_QT_AT_DECODER 0
+#define CONFIG_ALAC_AT_DECODER 0
+#define CONFIG_AMR_NB_AT_DECODER 0
+#define CONFIG_EAC3_AT_DECODER 0
+#define CONFIG_GSM_MS_AT_DECODER 0
+#define CONFIG_ILBC_AT_DECODER 0
+#define CONFIG_MP1_AT_DECODER 0
+#define CONFIG_MP2_AT_DECODER 0
+#define CONFIG_MP3_AT_DECODER 0
+#define CONFIG_PCM_ALAW_AT_DECODER 0
+#define CONFIG_PCM_MULAW_AT_DECODER 0
+#define CONFIG_QDMC_AT_DECODER 0
+#define CONFIG_QDM2_AT_DECODER 0
+#define CONFIG_LIBAOM_AV1_DECODER 0
+#define CONFIG_LIBCELT_DECODER 0
+#define CONFIG_LIBCODEC2_DECODER 0
+#define CONFIG_LIBDAVS2_DECODER 0
+#define CONFIG_LIBFDK_AAC_DECODER 0
+#define CONFIG_LIBGSM_DECODER 0
+#define CONFIG_LIBGSM_MS_DECODER 0
+#define CONFIG_LIBILBC_DECODER 0
+#define CONFIG_LIBOPENCORE_AMRNB_DECODER 0
+#define CONFIG_LIBOPENCORE_AMRWB_DECODER 0
+#define CONFIG_LIBOPENJPEG_DECODER 0
+#define CONFIG_LIBOPUS_DECODER 0
+#define CONFIG_LIBRSVG_DECODER 0
+#define CONFIG_LIBSPEEX_DECODER 0
+#define CONFIG_LIBVORBIS_DECODER 0
+#define CONFIG_LIBVPX_VP8_DECODER 0
+#define CONFIG_LIBVPX_VP9_DECODER 0
+#define CONFIG_LIBZVBI_TELETEXT_DECODER 0
+#define CONFIG_BINTEXT_DECODER 0
+#define CONFIG_XBIN_DECODER 0
+#define CONFIG_IDF_DECODER 0
+#define CONFIG_LIBOPENH264_DECODER 0
+#define CONFIG_H264_CUVID_DECODER 0
+#define CONFIG_HEVC_CUVID_DECODER 0
+#define CONFIG_HEVC_MEDIACODEC_DECODER 0
+#define CONFIG_MJPEG_CUVID_DECODER 0
+#define CONFIG_MPEG1_CUVID_DECODER 0
+#define CONFIG_MPEG2_CUVID_DECODER 0
+#define CONFIG_MPEG4_CUVID_DECODER 0
+#define CONFIG_MPEG4_MEDIACODEC_DECODER 0
+#define CONFIG_VC1_CUVID_DECODER 0
+#define CONFIG_VP8_CUVID_DECODER 0
+#define CONFIG_VP8_MEDIACODEC_DECODER 0
+#define CONFIG_VP8_QSV_DECODER 0
+#define CONFIG_VP9_CUVID_DECODER 0
+#define CONFIG_VP9_MEDIACODEC_DECODER 0
+#define CONFIG_A64MULTI_ENCODER 0
+#define CONFIG_A64MULTI5_ENCODER 0
+#define CONFIG_ALIAS_PIX_ENCODER 0
+#define CONFIG_AMV_ENCODER 0
+#define CONFIG_APNG_ENCODER 0
+#define CONFIG_ASV1_ENCODER 0
+#define CONFIG_ASV2_ENCODER 0
+#define CONFIG_AVRP_ENCODER 0
+#define CONFIG_AVUI_ENCODER 0
+#define CONFIG_AYUV_ENCODER 0
+#define CONFIG_BMP_ENCODER 0
+#define CONFIG_CINEPAK_ENCODER 0
+#define CONFIG_CLJR_ENCODER 0
+#define CONFIG_COMFORTNOISE_ENCODER 0
+#define CONFIG_DNXHD_ENCODER 0
+#define CONFIG_DPX_ENCODER 0
+#define CONFIG_DVVIDEO_ENCODER 0
+#define CONFIG_FFV1_ENCODER 0
+#define CONFIG_FFVHUFF_ENCODER 0
+#define CONFIG_FITS_ENCODER 0
+#define CONFIG_FLASHSV_ENCODER 0
+#define CONFIG_FLASHSV2_ENCODER 0
+#define CONFIG_FLV_ENCODER 0
+#define CONFIG_GIF_ENCODER 0
+#define CONFIG_H261_ENCODER 0
+#define CONFIG_H263_ENCODER 0
+#define CONFIG_H263P_ENCODER 0
+#define CONFIG_HAP_ENCODER 0
+#define CONFIG_HUFFYUV_ENCODER 0
+#define CONFIG_JPEG2000_ENCODER 0
+#define CONFIG_JPEGLS_ENCODER 0
+#define CONFIG_LJPEG_ENCODER 0
+#define CONFIG_MAGICYUV_ENCODER 0
+#define CONFIG_MJPEG_ENCODER 0
+#define CONFIG_MPEG1VIDEO_ENCODER 0
+#define CONFIG_MPEG2VIDEO_ENCODER 0
+#define CONFIG_MPEG4_ENCODER 0
+#define CONFIG_MSMPEG4V2_ENCODER 0
+#define CONFIG_MSMPEG4V3_ENCODER 0
+#define CONFIG_MSVIDEO1_ENCODER 0
+#define CONFIG_PAM_ENCODER 0
+#define CONFIG_PBM_ENCODER 0
+#define CONFIG_PCX_ENCODER 0
+#define CONFIG_PGM_ENCODER 0
+#define CONFIG_PGMYUV_ENCODER 0
+#define CONFIG_PNG_ENCODER 0
+#define CONFIG_PPM_ENCODER 0
+#define CONFIG_PRORES_ENCODER 0
+#define CONFIG_PRORES_AW_ENCODER 0
+#define CONFIG_PRORES_KS_ENCODER 0
+#define CONFIG_QTRLE_ENCODER 0
+#define CONFIG_R10K_ENCODER 0
+#define CONFIG_R210_ENCODER 0
+#define CONFIG_RAWVIDEO_ENCODER 0
+#define CONFIG_ROQ_ENCODER 0
+#define CONFIG_RV10_ENCODER 0
+#define CONFIG_RV20_ENCODER 0
+#define CONFIG_S302M_ENCODER 0
+#define CONFIG_SGI_ENCODER 0
+#define CONFIG_SNOW_ENCODER 0
+#define CONFIG_SUNRAST_ENCODER 0
+#define CONFIG_SVQ1_ENCODER 0
+#define CONFIG_TARGA_ENCODER 0
+#define CONFIG_TIFF_ENCODER 0
+#define CONFIG_UTVIDEO_ENCODER 0
+#define CONFIG_V210_ENCODER 0
+#define CONFIG_V308_ENCODER 0
+#define CONFIG_V408_ENCODER 0
+#define CONFIG_V410_ENCODER 0
+#define CONFIG_VC2_ENCODER 0
+#define CONFIG_WRAPPED_AVFRAME_ENCODER 0
+#define CONFIG_WMV1_ENCODER 0
+#define CONFIG_WMV2_ENCODER 0
+#define CONFIG_XBM_ENCODER 0
+#define CONFIG_XFACE_ENCODER 0
+#define CONFIG_XWD_ENCODER 0
+#define CONFIG_Y41P_ENCODER 0
+#define CONFIG_YUV4_ENCODER 0
+#define CONFIG_ZLIB_ENCODER 0
+#define CONFIG_ZMBV_ENCODER 0
+#define CONFIG_AAC_ENCODER 0
+#define CONFIG_AC3_ENCODER 0
+#define CONFIG_AC3_FIXED_ENCODER 0
+#define CONFIG_ALAC_ENCODER 0
+#define CONFIG_APTX_ENCODER 0
+#define CONFIG_APTX_HD_ENCODER 0
+#define CONFIG_DCA_ENCODER 0
+#define CONFIG_EAC3_ENCODER 0
+#define CONFIG_FLAC_ENCODER 0
+#define CONFIG_G723_1_ENCODER 0
+#define CONFIG_MLP_ENCODER 0
+#define CONFIG_MP2_ENCODER 0
+#define CONFIG_MP2FIXED_ENCODER 0
+#define CONFIG_NELLYMOSER_ENCODER 0
+#define CONFIG_OPUS_ENCODER 0
+#define CONFIG_RA_144_ENCODER 0
+#define CONFIG_SBC_ENCODER 0
+#define CONFIG_SONIC_ENCODER 0
+#define CONFIG_SONIC_LS_ENCODER 0
+#define CONFIG_TRUEHD_ENCODER 0
+#define CONFIG_TTA_ENCODER 0
+#define CONFIG_VORBIS_ENCODER 0
+#define CONFIG_WAVPACK_ENCODER 0
+#define CONFIG_WMAV1_ENCODER 0
+#define CONFIG_WMAV2_ENCODER 0
+#define CONFIG_PCM_ALAW_ENCODER 0
+#define CONFIG_PCM_F32BE_ENCODER 0
+#define CONFIG_PCM_F32LE_ENCODER 0
+#define CONFIG_PCM_F64BE_ENCODER 0
+#define CONFIG_PCM_F64LE_ENCODER 0
+#define CONFIG_PCM_MULAW_ENCODER 0
+#define CONFIG_PCM_S8_ENCODER 0
+#define CONFIG_PCM_S8_PLANAR_ENCODER 0
+#define CONFIG_PCM_S16BE_ENCODER 0
+#define CONFIG_PCM_S16BE_PLANAR_ENCODER 0
+#define CONFIG_PCM_S16LE_ENCODER 0
+#define CONFIG_PCM_S16LE_PLANAR_ENCODER 0
+#define CONFIG_PCM_S24BE_ENCODER 0
+#define CONFIG_PCM_S24DAUD_ENCODER 0
+#define CONFIG_PCM_S24LE_ENCODER 0
+#define CONFIG_PCM_S24LE_PLANAR_ENCODER 0
+#define CONFIG_PCM_S32BE_ENCODER 0
+#define CONFIG_PCM_S32LE_ENCODER 0
+#define CONFIG_PCM_S32LE_PLANAR_ENCODER 0
+#define CONFIG_PCM_S64BE_ENCODER 0
+#define CONFIG_PCM_S64LE_ENCODER 0
+#define CONFIG_PCM_U8_ENCODER 0
+#define CONFIG_PCM_U16BE_ENCODER 0
+#define CONFIG_PCM_U16LE_ENCODER 0
+#define CONFIG_PCM_U24BE_ENCODER 0
+#define CONFIG_PCM_U24LE_ENCODER 0
+#define CONFIG_PCM_U32BE_ENCODER 0
+#define CONFIG_PCM_U32LE_ENCODER 0
+#define CONFIG_PCM_VIDC_ENCODER 0
+#define CONFIG_ROQ_DPCM_ENCODER 0
+#define CONFIG_ADPCM_ADX_ENCODER 0
+#define CONFIG_ADPCM_G722_ENCODER 0
+#define CONFIG_ADPCM_G726_ENCODER 0
+#define CONFIG_ADPCM_G726LE_ENCODER 0
+#define CONFIG_ADPCM_IMA_QT_ENCODER 0
+#define CONFIG_ADPCM_IMA_WAV_ENCODER 0
+#define CONFIG_ADPCM_MS_ENCODER 0
+#define CONFIG_ADPCM_SWF_ENCODER 0
+#define CONFIG_ADPCM_YAMAHA_ENCODER 0
+#define CONFIG_SSA_ENCODER 0
+#define CONFIG_ASS_ENCODER 0
+#define CONFIG_DVBSUB_ENCODER 0
+#define CONFIG_DVDSUB_ENCODER 0
+#define CONFIG_MOVTEXT_ENCODER 0
+#define CONFIG_SRT_ENCODER 0
+#define CONFIG_SUBRIP_ENCODER 0
+#define CONFIG_TEXT_ENCODER 0
+#define CONFIG_WEBVTT_ENCODER 0
+#define CONFIG_XSUB_ENCODER 0
+#define CONFIG_AAC_AT_ENCODER 0
+#define CONFIG_ALAC_AT_ENCODER 0
+#define CONFIG_ILBC_AT_ENCODER 0
+#define CONFIG_PCM_ALAW_AT_ENCODER 0
+#define CONFIG_PCM_MULAW_AT_ENCODER 0
+#define CONFIG_LIBAOM_AV1_ENCODER 0
+#define CONFIG_LIBCODEC2_ENCODER 0
+#define CONFIG_LIBFDK_AAC_ENCODER 0
+#define CONFIG_LIBGSM_ENCODER 0
+#define CONFIG_LIBGSM_MS_ENCODER 0
+#define CONFIG_LIBILBC_ENCODER 0
+#define CONFIG_LIBMP3LAME_ENCODER 0
+#define CONFIG_LIBOPENCORE_AMRNB_ENCODER 0
+#define CONFIG_LIBOPENJPEG_ENCODER 0
+#define CONFIG_LIBOPUS_ENCODER 0
+#define CONFIG_LIBSHINE_ENCODER 0
+#define CONFIG_LIBSPEEX_ENCODER 0
+#define CONFIG_LIBTHEORA_ENCODER 0
+#define CONFIG_LIBTWOLAME_ENCODER 0
+#define CONFIG_LIBVO_AMRWBENC_ENCODER 0
+#define CONFIG_LIBVORBIS_ENCODER 0
+#define CONFIG_LIBVPX_VP8_ENCODER 0
+#define CONFIG_LIBVPX_VP9_ENCODER 0
+#define CONFIG_LIBWAVPACK_ENCODER 0
+#define CONFIG_LIBWEBP_ANIM_ENCODER 0
+#define CONFIG_LIBWEBP_ENCODER 0
+#define CONFIG_LIBX262_ENCODER 0
+#define CONFIG_LIBX264_ENCODER 0
+#define CONFIG_LIBX264RGB_ENCODER 0
+#define CONFIG_LIBX265_ENCODER 0
+#define CONFIG_LIBXAVS_ENCODER 0
+#define CONFIG_LIBXAVS2_ENCODER 0
+#define CONFIG_LIBXVID_ENCODER 0
+#define CONFIG_H263_V4L2M2M_ENCODER 0
+#define CONFIG_LIBOPENH264_ENCODER 0
+#define CONFIG_H264_AMF_ENCODER 0
+#define CONFIG_H264_NVENC_ENCODER 0
+#define CONFIG_H264_OMX_ENCODER 0
+#define CONFIG_H264_QSV_ENCODER 0
+#define CONFIG_H264_V4L2M2M_ENCODER 0
+#define CONFIG_H264_VAAPI_ENCODER 0
+#define CONFIG_H264_VIDEOTOOLBOX_ENCODER 0
+#define CONFIG_NVENC_ENCODER 0
+#define CONFIG_NVENC_H264_ENCODER 0
+#define CONFIG_NVENC_HEVC_ENCODER 0
+#define CONFIG_HEVC_AMF_ENCODER 0
+#define CONFIG_HEVC_NVENC_ENCODER 0
+#define CONFIG_HEVC_QSV_ENCODER 0
+#define CONFIG_HEVC_V4L2M2M_ENCODER 0
+#define CONFIG_HEVC_VAAPI_ENCODER 0
+#define CONFIG_HEVC_VIDEOTOOLBOX_ENCODER 0
+#define CONFIG_LIBKVAZAAR_ENCODER 0
+#define CONFIG_MJPEG_QSV_ENCODER 0
+#define CONFIG_MJPEG_VAAPI_ENCODER 0
+#define CONFIG_MPEG2_QSV_ENCODER 0
+#define CONFIG_MPEG2_VAAPI_ENCODER 0
+#define CONFIG_MPEG4_V4L2M2M_ENCODER 0
+#define CONFIG_VP8_V4L2M2M_ENCODER 0
+#define CONFIG_VP8_VAAPI_ENCODER 0
+#define CONFIG_VP9_VAAPI_ENCODER 0
+#define CONFIG_H263_VAAPI_HWACCEL 0
+#define CONFIG_H263_VIDEOTOOLBOX_HWACCEL 0
+#define CONFIG_H264_D3D11VA_HWACCEL 0
+#define CONFIG_H264_D3D11VA2_HWACCEL 0
+#define CONFIG_H264_DXVA2_HWACCEL 0
+#define CONFIG_H264_NVDEC_HWACCEL 0
+#define CONFIG_H264_VAAPI_HWACCEL 0
+#define CONFIG_H264_VDPAU_HWACCEL 0
+#define CONFIG_H264_VIDEOTOOLBOX_HWACCEL 0
+#define CONFIG_HEVC_D3D11VA_HWACCEL 0
+#define CONFIG_HEVC_D3D11VA2_HWACCEL 0
+#define CONFIG_HEVC_DXVA2_HWACCEL 0
+#define CONFIG_HEVC_NVDEC_HWACCEL 0
+#define CONFIG_HEVC_VAAPI_HWACCEL 0
+#define CONFIG_HEVC_VDPAU_HWACCEL 0
+#define CONFIG_HEVC_VIDEOTOOLBOX_HWACCEL 0
+#define CONFIG_MJPEG_NVDEC_HWACCEL 0
+#define CONFIG_MJPEG_VAAPI_HWACCEL 0
+#define CONFIG_MPEG1_NVDEC_HWACCEL 0
+#define CONFIG_MPEG1_VDPAU_HWACCEL 0
+#define CONFIG_MPEG1_VIDEOTOOLBOX_HWACCEL 0
+#define CONFIG_MPEG1_XVMC_HWACCEL 0
+#define CONFIG_MPEG2_D3D11VA_HWACCEL 0
+#define CONFIG_MPEG2_D3D11VA2_HWACCEL 0
+#define CONFIG_MPEG2_NVDEC_HWACCEL 0
+#define CONFIG_MPEG2_DXVA2_HWACCEL 0
+#define CONFIG_MPEG2_VAAPI_HWACCEL 0
+#define CONFIG_MPEG2_VDPAU_HWACCEL 0
+#define CONFIG_MPEG2_VIDEOTOOLBOX_HWACCEL 0
+#define CONFIG_MPEG2_XVMC_HWACCEL 0
+#define CONFIG_MPEG4_NVDEC_HWACCEL 0
+#define CONFIG_MPEG4_VAAPI_HWACCEL 0
+#define CONFIG_MPEG4_VDPAU_HWACCEL 0
+#define CONFIG_MPEG4_VIDEOTOOLBOX_HWACCEL 0
+#define CONFIG_VC1_D3D11VA_HWACCEL 0
+#define CONFIG_VC1_D3D11VA2_HWACCEL 0
+#define CONFIG_VC1_DXVA2_HWACCEL 0
+#define CONFIG_VC1_NVDEC_HWACCEL 0
+#define CONFIG_VC1_VAAPI_HWACCEL 0
+#define CONFIG_VC1_VDPAU_HWACCEL 0
+#define CONFIG_VP8_NVDEC_HWACCEL 0
+#define CONFIG_VP8_VAAPI_HWACCEL 0
+#define CONFIG_VP9_D3D11VA_HWACCEL 0
+#define CONFIG_VP9_D3D11VA2_HWACCEL 0
+#define CONFIG_VP9_DXVA2_HWACCEL 0
+#define CONFIG_VP9_NVDEC_HWACCEL 0
+#define CONFIG_VP9_VAAPI_HWACCEL 0
+#define CONFIG_WMV3_D3D11VA_HWACCEL 0
+#define CONFIG_WMV3_D3D11VA2_HWACCEL 0
+#define CONFIG_WMV3_DXVA2_HWACCEL 0
+#define CONFIG_WMV3_NVDEC_HWACCEL 0
+#define CONFIG_WMV3_VAAPI_HWACCEL 0
+#define CONFIG_WMV3_VDPAU_HWACCEL 0
+#define CONFIG_AAC_PARSER 0
+#define CONFIG_AAC_LATM_PARSER 0
+#define CONFIG_AC3_PARSER 0
+#define CONFIG_ADX_PARSER 0
+#define CONFIG_AV1_PARSER 0
+#define CONFIG_AVS2_PARSER 0
+#define CONFIG_BMP_PARSER 0
+#define CONFIG_CAVSVIDEO_PARSER 0
+#define CONFIG_COOK_PARSER 0
+#define CONFIG_DCA_PARSER 0
+#define CONFIG_DIRAC_PARSER 0
+#define CONFIG_DNXHD_PARSER 0
+#define CONFIG_DPX_PARSER 0
+#define CONFIG_DVAUDIO_PARSER 0
+#define CONFIG_DVBSUB_PARSER 0
+#define CONFIG_DVDSUB_PARSER 0
+#define CONFIG_DVD_NAV_PARSER 0
+#define CONFIG_FLAC_PARSER 0
+#define CONFIG_G729_PARSER 0
+#define CONFIG_GSM_PARSER 0
+#define CONFIG_H261_PARSER 0
+#define CONFIG_H263_PARSER 0
+#define CONFIG_H264_PARSER 0
+#define CONFIG_HEVC_PARSER 0
+#define CONFIG_MJPEG_PARSER 0
+#define CONFIG_MLP_PARSER 0
+#define CONFIG_MPEG4VIDEO_PARSER 0
+#define CONFIG_MPEGAUDIO_PARSER 0
+#define CONFIG_MPEGVIDEO_PARSER 0
+#define CONFIG_OPUS_PARSER 0
+#define CONFIG_PNG_PARSER 0
+#define CONFIG_PNM_PARSER 0
+#define CONFIG_RV30_PARSER 0
+#define CONFIG_RV40_PARSER 0
+#define CONFIG_SBC_PARSER 0
+#define CONFIG_SIPR_PARSER 0
+#define CONFIG_TAK_PARSER 0
+#define CONFIG_VC1_PARSER 0
+#define CONFIG_VORBIS_PARSER 0
+#define CONFIG_VP3_PARSER 0
+#define CONFIG_VP8_PARSER 0
+#define CONFIG_VP9_PARSER 0
+#define CONFIG_XMA_PARSER 0
+#define CONFIG_ALSA_INDEV 0
+#define CONFIG_ANDROID_CAMERA_INDEV 0
+#define CONFIG_AVFOUNDATION_INDEV 0
+#define CONFIG_BKTR_INDEV 0
+#define CONFIG_DECKLINK_INDEV 0
+#define CONFIG_LIBNDI_NEWTEK_INDEV 0
+#define CONFIG_DSHOW_INDEV 0
+#define CONFIG_FBDEV_INDEV 0
+#define CONFIG_GDIGRAB_INDEV 0
+#define CONFIG_IEC61883_INDEV 0
+#define CONFIG_JACK_INDEV 0
+#define CONFIG_KMSGRAB_INDEV 0
+#define CONFIG_LAVFI_INDEV 0
+#define CONFIG_OPENAL_INDEV 0
+#define CONFIG_OSS_INDEV 0
+#define CONFIG_PULSE_INDEV 0
+#define CONFIG_SNDIO_INDEV 0
+#define CONFIG_V4L2_INDEV 0
+#define CONFIG_VFWCAP_INDEV 0
+#define CONFIG_XCBGRAB_INDEV 0
+#define CONFIG_LIBCDIO_INDEV 0
+#define CONFIG_LIBDC1394_INDEV 0
+#define CONFIG_ALSA_OUTDEV 0
+#define CONFIG_CACA_OUTDEV 0
+#define CONFIG_DECKLINK_OUTDEV 0
+#define CONFIG_LIBNDI_NEWTEK_OUTDEV 0
+#define CONFIG_FBDEV_OUTDEV 0
+#define CONFIG_OPENGL_OUTDEV 0
+#define CONFIG_OSS_OUTDEV 0
+#define CONFIG_PULSE_OUTDEV 0
+#define CONFIG_SDL2_OUTDEV 0
+#define CONFIG_SNDIO_OUTDEV 0
+#define CONFIG_V4L2_OUTDEV 0
+#define CONFIG_XV_OUTDEV 0
+#define CONFIG_ABENCH_FILTER 0
+#define CONFIG_ACOMPRESSOR_FILTER 0
+#define CONFIG_ACONTRAST_FILTER 0
+#define CONFIG_ACOPY_FILTER 0
+#define CONFIG_ACUE_FILTER 0
+#define CONFIG_ACROSSFADE_FILTER 0
+#define CONFIG_ACROSSOVER_FILTER 0
+#define CONFIG_ACRUSHER_FILTER 0
+#define CONFIG_ADECLICK_FILTER 0
+#define CONFIG_ADECLIP_FILTER 0
+#define CONFIG_ADELAY_FILTER 0
+#define CONFIG_ADERIVATIVE_FILTER 0
+#define CONFIG_AECHO_FILTER 0
+#define CONFIG_AEMPHASIS_FILTER 0
+#define CONFIG_AEVAL_FILTER 0
+#define CONFIG_AFADE_FILTER 0
+#define CONFIG_AFFTDN_FILTER 0
+#define CONFIG_AFFTFILT_FILTER 0
+#define CONFIG_AFIR_FILTER 0
+#define CONFIG_AFORMAT_FILTER 0
+#define CONFIG_AGATE_FILTER 0
+#define CONFIG_AIIR_FILTER 0
+#define CONFIG_AINTEGRAL_FILTER 0
+#define CONFIG_AINTERLEAVE_FILTER 0
+#define CONFIG_ALIMITER_FILTER 0
+#define CONFIG_ALLPASS_FILTER 0
+#define CONFIG_ALOOP_FILTER 0
+#define CONFIG_AMERGE_FILTER 0
+#define CONFIG_AMETADATA_FILTER 0
+#define CONFIG_AMIX_FILTER 0
+#define CONFIG_AMULTIPLY_FILTER 0
+#define CONFIG_ANEQUALIZER_FILTER 0
+#define CONFIG_ANULL_FILTER 0
+#define CONFIG_APAD_FILTER 0
+#define CONFIG_APERMS_FILTER 0
+#define CONFIG_APHASER_FILTER 0
+#define CONFIG_APULSATOR_FILTER 0
+#define CONFIG_AREALTIME_FILTER 0
+#define CONFIG_ARESAMPLE_FILTER 0
+#define CONFIG_AREVERSE_FILTER 0
+#define CONFIG_ASELECT_FILTER 0
+#define CONFIG_ASENDCMD_FILTER 0
+#define CONFIG_ASETNSAMPLES_FILTER 0
+#define CONFIG_ASETPTS_FILTER 0
+#define CONFIG_ASETRATE_FILTER 0
+#define CONFIG_ASETTB_FILTER 0
+#define CONFIG_ASHOWINFO_FILTER 0
+#define CONFIG_ASIDEDATA_FILTER 0
+#define CONFIG_ASPLIT_FILTER 0
+#define CONFIG_ASTATS_FILTER 0
+#define CONFIG_ASTREAMSELECT_FILTER 0
+#define CONFIG_ATEMPO_FILTER 0
+#define CONFIG_ATRIM_FILTER 0
+#define CONFIG_AZMQ_FILTER 0
+#define CONFIG_BANDPASS_FILTER 0
+#define CONFIG_BANDREJECT_FILTER 0
+#define CONFIG_BASS_FILTER 0
+#define CONFIG_BIQUAD_FILTER 0
+#define CONFIG_BS2B_FILTER 0
+#define CONFIG_CHANNELMAP_FILTER 0
+#define CONFIG_CHANNELSPLIT_FILTER 0
+#define CONFIG_CHORUS_FILTER 0
+#define CONFIG_COMPAND_FILTER 0
+#define CONFIG_COMPENSATIONDELAY_FILTER 0
+#define CONFIG_CROSSFEED_FILTER 0
+#define CONFIG_CRYSTALIZER_FILTER 0
+#define CONFIG_DCSHIFT_FILTER 0
+#define CONFIG_DRMETER_FILTER 0
+#define CONFIG_DYNAUDNORM_FILTER 0
+#define CONFIG_EARWAX_FILTER 0
+#define CONFIG_EBUR128_FILTER 0
+#define CONFIG_EQUALIZER_FILTER 0
+#define CONFIG_EXTRASTEREO_FILTER 0
+#define CONFIG_FIREQUALIZER_FILTER 0
+#define CONFIG_FLANGER_FILTER 0
+#define CONFIG_HAAS_FILTER 0
+#define CONFIG_HDCD_FILTER 0
+#define CONFIG_HEADPHONE_FILTER 0
+#define CONFIG_HIGHPASS_FILTER 0
+#define CONFIG_HIGHSHELF_FILTER 0
+#define CONFIG_JOIN_FILTER 0
+#define CONFIG_LADSPA_FILTER 0
+#define CONFIG_LOUDNORM_FILTER 0
+#define CONFIG_LOWPASS_FILTER 0
+#define CONFIG_LOWSHELF_FILTER 0
+#define CONFIG_LV2_FILTER 0
+#define CONFIG_MCOMPAND_FILTER 0
+#define CONFIG_PAN_FILTER 0
+#define CONFIG_REPLAYGAIN_FILTER 0
+#define CONFIG_RESAMPLE_FILTER 0
+#define CONFIG_RUBBERBAND_FILTER 0
+#define CONFIG_SIDECHAINCOMPRESS_FILTER 0
+#define CONFIG_SIDECHAINGATE_FILTER 0
+#define CONFIG_SILENCEDETECT_FILTER 0
+#define CONFIG_SILENCEREMOVE_FILTER 0
+#define CONFIG_SOFALIZER_FILTER 0
+#define CONFIG_STEREOTOOLS_FILTER 0
+#define CONFIG_STEREOWIDEN_FILTER 0
+#define CONFIG_SUPEREQUALIZER_FILTER 0
+#define CONFIG_SURROUND_FILTER 0
+#define CONFIG_TREBLE_FILTER 0
+#define CONFIG_TREMOLO_FILTER 0
+#define CONFIG_VIBRATO_FILTER 0
+#define CONFIG_VOLUME_FILTER 0
+#define CONFIG_VOLUMEDETECT_FILTER 0
+#define CONFIG_AEVALSRC_FILTER 0
+#define CONFIG_ANOISESRC_FILTER 0
+#define CONFIG_ANULLSRC_FILTER 0
+#define CONFIG_FLITE_FILTER 0
+#define CONFIG_HILBERT_FILTER 0
+#define CONFIG_SINC_FILTER 0
+#define CONFIG_SINE_FILTER 0
+#define CONFIG_ANULLSINK_FILTER 0
+#define CONFIG_ALPHAEXTRACT_FILTER 0
+#define CONFIG_ALPHAMERGE_FILTER 0
+#define CONFIG_AMPLIFY_FILTER 0
+#define CONFIG_ASS_FILTER 0
+#define CONFIG_ATADENOISE_FILTER 0
+#define CONFIG_AVGBLUR_FILTER 0
+#define CONFIG_AVGBLUR_OPENCL_FILTER 0
+#define CONFIG_BBOX_FILTER 0
+#define CONFIG_BENCH_FILTER 0
+#define CONFIG_BITPLANENOISE_FILTER 0
+#define CONFIG_BLACKDETECT_FILTER 0
+#define CONFIG_BLACKFRAME_FILTER 0
+#define CONFIG_BLEND_FILTER 0
+#define CONFIG_BM3D_FILTER 0
+#define CONFIG_BOXBLUR_FILTER 0
+#define CONFIG_BOXBLUR_OPENCL_FILTER 0
+#define CONFIG_BWDIF_FILTER 0
+#define CONFIG_CHROMAHOLD_FILTER 0
+#define CONFIG_CHROMAKEY_FILTER 0
+#define CONFIG_CIESCOPE_FILTER 0
+#define CONFIG_CODECVIEW_FILTER 0
+#define CONFIG_COLORBALANCE_FILTER 0
+#define CONFIG_COLORCHANNELMIXER_FILTER 0
+#define CONFIG_COLORKEY_FILTER 0
+#define CONFIG_COLORLEVELS_FILTER 0
+#define CONFIG_COLORMATRIX_FILTER 0
+#define CONFIG_COLORSPACE_FILTER 0
+#define CONFIG_CONVOLUTION_FILTER 0
+#define CONFIG_CONVOLUTION_OPENCL_FILTER 0
+#define CONFIG_CONVOLVE_FILTER 0
+#define CONFIG_COPY_FILTER 0
+#define CONFIG_COREIMAGE_FILTER 0
+#define CONFIG_COVER_RECT_FILTER 0
+#define CONFIG_CROP_FILTER 0
+#define CONFIG_CROPDETECT_FILTER 0
+#define CONFIG_CUE_FILTER 0
+#define CONFIG_CURVES_FILTER 0
+#define CONFIG_DATASCOPE_FILTER 0
+#define CONFIG_DCTDNOIZ_FILTER 0
+#define CONFIG_DEBAND_FILTER 0
+#define CONFIG_DEBLOCK_FILTER 0
+#define CONFIG_DECIMATE_FILTER 0
+#define CONFIG_DECONVOLVE_FILTER 0
+#define CONFIG_DEFLATE_FILTER 0
+#define CONFIG_DEFLICKER_FILTER 0
+#define CONFIG_DEINTERLACE_QSV_FILTER 0
+#define CONFIG_DEINTERLACE_VAAPI_FILTER 0
+#define CONFIG_DEJUDDER_FILTER 0
+#define CONFIG_DELOGO_FILTER 0
+#define CONFIG_DENOISE_VAAPI_FILTER 0
+#define CONFIG_DESHAKE_FILTER 0
+#define CONFIG_DESPILL_FILTER 0
+#define CONFIG_DETELECINE_FILTER 0
+#define CONFIG_DILATION_FILTER 0
+#define CONFIG_DILATION_OPENCL_FILTER 0
+#define CONFIG_DISPLACE_FILTER 0
+#define CONFIG_DOUBLEWEAVE_FILTER 0
+#define CONFIG_DRAWBOX_FILTER 0
+#define CONFIG_DRAWGRAPH_FILTER 0
+#define CONFIG_DRAWGRID_FILTER 0
+#define CONFIG_DRAWTEXT_FILTER 0
+#define CONFIG_EDGEDETECT_FILTER 0
+#define CONFIG_ELBG_FILTER 0
+#define CONFIG_ENTROPY_FILTER 0
+#define CONFIG_EQ_FILTER 0
+#define CONFIG_EROSION_FILTER 0
+#define CONFIG_EROSION_OPENCL_FILTER 0
+#define CONFIG_EXTRACTPLANES_FILTER 0
+#define CONFIG_FADE_FILTER 0
+#define CONFIG_FFTDNOIZ_FILTER 0
+#define CONFIG_FFTFILT_FILTER 0
+#define CONFIG_FIELD_FILTER 0
+#define CONFIG_FIELDHINT_FILTER 0
+#define CONFIG_FIELDMATCH_FILTER 0
+#define CONFIG_FIELDORDER_FILTER 0
+#define CONFIG_FILLBORDERS_FILTER 0
+#define CONFIG_FIND_RECT_FILTER 0
+#define CONFIG_FLOODFILL_FILTER 0
+#define CONFIG_FORMAT_FILTER 0
+#define CONFIG_FPS_FILTER 0
+#define CONFIG_FRAMEPACK_FILTER 0
+#define CONFIG_FRAMERATE_FILTER 0
+#define CONFIG_FRAMESTEP_FILTER 0
+#define CONFIG_FREI0R_FILTER 0
+#define CONFIG_FSPP_FILTER 0
+#define CONFIG_GBLUR_FILTER 0
+#define CONFIG_GEQ_FILTER 0
+#define CONFIG_GRADFUN_FILTER 0
+#define CONFIG_GRAPHMONITOR_FILTER 0
+#define CONFIG_GREYEDGE_FILTER 0
+#define CONFIG_HALDCLUT_FILTER 0
+#define CONFIG_HFLIP_FILTER 0
+#define CONFIG_HISTEQ_FILTER 0
+#define CONFIG_HISTOGRAM_FILTER 0
+#define CONFIG_HQDN3D_FILTER 0
+#define CONFIG_HQX_FILTER 0
+#define CONFIG_HSTACK_FILTER 0
+#define CONFIG_HUE_FILTER 0
+#define CONFIG_HWDOWNLOAD_FILTER 0
+#define CONFIG_HWMAP_FILTER 0
+#define CONFIG_HWUPLOAD_FILTER 0
+#define CONFIG_HWUPLOAD_CUDA_FILTER 0
+#define CONFIG_HYSTERESIS_FILTER 0
+#define CONFIG_IDET_FILTER 0
+#define CONFIG_IL_FILTER 0
+#define CONFIG_INFLATE_FILTER 0
+#define CONFIG_INTERLACE_FILTER 0
+#define CONFIG_INTERLEAVE_FILTER 0
+#define CONFIG_KERNDEINT_FILTER 0
+#define CONFIG_LENSCORRECTION_FILTER 0
+#define CONFIG_LENSFUN_FILTER 0
+#define CONFIG_LIBVMAF_FILTER 0
+#define CONFIG_LIMITER_FILTER 0
+#define CONFIG_LOOP_FILTER 0
+#define CONFIG_LUMAKEY_FILTER 0
+#define CONFIG_LUT_FILTER 0
+#define CONFIG_LUT1D_FILTER 0
+#define CONFIG_LUT2_FILTER 0
+#define CONFIG_LUT3D_FILTER 0
+#define CONFIG_LUTRGB_FILTER 0
+#define CONFIG_LUTYUV_FILTER 0
+#define CONFIG_MASKEDCLAMP_FILTER 0
+#define CONFIG_MASKEDMERGE_FILTER 0
+#define CONFIG_MCDEINT_FILTER 0
+#define CONFIG_MERGEPLANES_FILTER 0
+#define CONFIG_MESTIMATE_FILTER 0
+#define CONFIG_METADATA_FILTER 0
+#define CONFIG_MIDEQUALIZER_FILTER 0
+#define CONFIG_MINTERPOLATE_FILTER 0
+#define CONFIG_MIX_FILTER 0
+#define CONFIG_MPDECIMATE_FILTER 0
+#define CONFIG_NEGATE_FILTER 0
+#define CONFIG_NLMEANS_FILTER 0
+#define CONFIG_NNEDI_FILTER 0
+#define CONFIG_NOFORMAT_FILTER 0
+#define CONFIG_NOISE_FILTER 0
+#define CONFIG_NORMALIZE_FILTER 0
+#define CONFIG_NULL_FILTER 0
+#define CONFIG_OCR_FILTER 0
+#define CONFIG_OCV_FILTER 0
+#define CONFIG_OSCILLOSCOPE_FILTER 0
+#define CONFIG_OVERLAY_FILTER 0
+#define CONFIG_OVERLAY_OPENCL_FILTER 0
+#define CONFIG_OVERLAY_QSV_FILTER 0
+#define CONFIG_OWDENOISE_FILTER 0
+#define CONFIG_PAD_FILTER 0
+#define CONFIG_PALETTEGEN_FILTER 0
+#define CONFIG_PALETTEUSE_FILTER 0
+#define CONFIG_PERMS_FILTER 0
+#define CONFIG_PERSPECTIVE_FILTER 0
+#define CONFIG_PHASE_FILTER 0
+#define CONFIG_PIXDESCTEST_FILTER 0
+#define CONFIG_PIXSCOPE_FILTER 0
+#define CONFIG_PP_FILTER 0
+#define CONFIG_PP7_FILTER 0
+#define CONFIG_PREMULTIPLY_FILTER 0
+#define CONFIG_PREWITT_FILTER 0
+#define CONFIG_PREWITT_OPENCL_FILTER 0
+#define CONFIG_PROCAMP_VAAPI_FILTER 0
+#define CONFIG_PROGRAM_OPENCL_FILTER 0
+#define CONFIG_PSEUDOCOLOR_FILTER 0
+#define CONFIG_PSNR_FILTER 0
+#define CONFIG_PULLUP_FILTER 0
+#define CONFIG_QP_FILTER 0
+#define CONFIG_RANDOM_FILTER 0
+#define CONFIG_READEIA608_FILTER 0
+#define CONFIG_READVITC_FILTER 0
+#define CONFIG_REALTIME_FILTER 0
+#define CONFIG_REMAP_FILTER 0
+#define CONFIG_REMOVEGRAIN_FILTER 0
+#define CONFIG_REMOVELOGO_FILTER 0
+#define CONFIG_REPEATFIELDS_FILTER 0
+#define CONFIG_REVERSE_FILTER 0
+#define CONFIG_ROBERTS_FILTER 0
+#define CONFIG_ROBERTS_OPENCL_FILTER 0
+#define CONFIG_ROTATE_FILTER 0
+#define CONFIG_SAB_FILTER 0
+#define CONFIG_SCALE_FILTER 0
+#define CONFIG_SCALE_CUDA_FILTER 0
+#define CONFIG_SCALE_NPP_FILTER 0
+#define CONFIG_SCALE_QSV_FILTER 0
+#define CONFIG_SCALE_VAAPI_FILTER 0
+#define CONFIG_SCALE2REF_FILTER 0
+#define CONFIG_SELECT_FILTER 0
+#define CONFIG_SELECTIVECOLOR_FILTER 0
+#define CONFIG_SENDCMD_FILTER 0
+#define CONFIG_SEPARATEFIELDS_FILTER 0
+#define CONFIG_SETDAR_FILTER 0
+#define CONFIG_SETFIELD_FILTER 0
+#define CONFIG_SETPARAMS_FILTER 0
+#define CONFIG_SETPTS_FILTER 0
+#define CONFIG_SETRANGE_FILTER 0
+#define CONFIG_SETSAR_FILTER 0
+#define CONFIG_SETTB_FILTER 0
+#define CONFIG_SHARPNESS_VAAPI_FILTER 0
+#define CONFIG_SHOWINFO_FILTER 0
+#define CONFIG_SHOWPALETTE_FILTER 0
+#define CONFIG_SHUFFLEFRAMES_FILTER 0
+#define CONFIG_SHUFFLEPLANES_FILTER 0
+#define CONFIG_SIDEDATA_FILTER 0
+#define CONFIG_SIGNALSTATS_FILTER 0
+#define CONFIG_SIGNATURE_FILTER 0
+#define CONFIG_SMARTBLUR_FILTER 0
+#define CONFIG_SOBEL_FILTER 0
+#define CONFIG_SOBEL_OPENCL_FILTER 0
+#define CONFIG_SPLIT_FILTER 0
+#define CONFIG_SPP_FILTER 0
+#define CONFIG_SR_FILTER 0
+#define CONFIG_SSIM_FILTER 0
+#define CONFIG_STEREO3D_FILTER 0
+#define CONFIG_STREAMSELECT_FILTER 0
+#define CONFIG_SUBTITLES_FILTER 0
+#define CONFIG_SUPER2XSAI_FILTER 0
+#define CONFIG_SWAPRECT_FILTER 0
+#define CONFIG_SWAPUV_FILTER 0
+#define CONFIG_TBLEND_FILTER 0
+#define CONFIG_TELECINE_FILTER 0
+#define CONFIG_THRESHOLD_FILTER 0
+#define CONFIG_THUMBNAIL_FILTER 0
+#define CONFIG_THUMBNAIL_CUDA_FILTER 0
+#define CONFIG_TILE_FILTER 0
+#define CONFIG_TINTERLACE_FILTER 0
+#define CONFIG_TLUT2_FILTER 0
+#define CONFIG_TMIX_FILTER 0
+#define CONFIG_TONEMAP_FILTER 0
+#define CONFIG_TONEMAP_OPENCL_FILTER 0
+#define CONFIG_TRANSPOSE_FILTER 0
+#define CONFIG_TRANSPOSE_NPP_FILTER 0
+#define CONFIG_TRIM_FILTER 0
+#define CONFIG_UNPREMULTIPLY_FILTER 0
+#define CONFIG_UNSHARP_FILTER 0
+#define CONFIG_UNSHARP_OPENCL_FILTER 0
+#define CONFIG_USPP_FILTER 0
+#define CONFIG_VAGUEDENOISER_FILTER 0
+#define CONFIG_VECTORSCOPE_FILTER 0
+#define CONFIG_VFLIP_FILTER 0
+#define CONFIG_VFRDET_FILTER 0
+#define CONFIG_VIBRANCE_FILTER 0
+#define CONFIG_VIDSTABDETECT_FILTER 0
+#define CONFIG_VIDSTABTRANSFORM_FILTER 0
+#define CONFIG_VIGNETTE_FILTER 0
+#define CONFIG_VMAFMOTION_FILTER 0
+#define CONFIG_VPP_QSV_FILTER 0
+#define CONFIG_VSTACK_FILTER 0
+#define CONFIG_W3FDIF_FILTER 0
+#define CONFIG_WAVEFORM_FILTER 0
+#define CONFIG_WEAVE_FILTER 0
+#define CONFIG_XBR_FILTER 0
+#define CONFIG_XSTACK_FILTER 0
+#define CONFIG_YADIF_FILTER 0
+#define CONFIG_YADIF_CUDA_FILTER 0
+#define CONFIG_ZMQ_FILTER 0
+#define CONFIG_ZOOMPAN_FILTER 0
+#define CONFIG_ZSCALE_FILTER 0
+#define CONFIG_ALLRGB_FILTER 0
+#define CONFIG_ALLYUV_FILTER 0
+#define CONFIG_CELLAUTO_FILTER 0
+#define CONFIG_COLOR_FILTER 0
+#define CONFIG_COREIMAGESRC_FILTER 0
+#define CONFIG_FREI0R_SRC_FILTER 0
+#define CONFIG_HALDCLUTSRC_FILTER 0
+#define CONFIG_LIFE_FILTER 0
+#define CONFIG_MANDELBROT_FILTER 0
+#define CONFIG_MPTESTSRC_FILTER 0
+#define CONFIG_NULLSRC_FILTER 0
+#define CONFIG_OPENCLSRC_FILTER 0
+#define CONFIG_PAL75BARS_FILTER 0
+#define CONFIG_PAL100BARS_FILTER 0
+#define CONFIG_RGBTESTSRC_FILTER 0
+#define CONFIG_SMPTEBARS_FILTER 0
+#define CONFIG_SMPTEHDBARS_FILTER 0
+#define CONFIG_TESTSRC_FILTER 0
+#define CONFIG_TESTSRC2_FILTER 0
+#define CONFIG_YUVTESTSRC_FILTER 0
+#define CONFIG_NULLSINK_FILTER 0
+#define CONFIG_ABITSCOPE_FILTER 0
+#define CONFIG_ADRAWGRAPH_FILTER 0
+#define CONFIG_AGRAPHMONITOR_FILTER 0
+#define CONFIG_AHISTOGRAM_FILTER 0
+#define CONFIG_APHASEMETER_FILTER 0
+#define CONFIG_AVECTORSCOPE_FILTER 0
+#define CONFIG_CONCAT_FILTER 0
+#define CONFIG_SHOWCQT_FILTER 0
+#define CONFIG_SHOWFREQS_FILTER 0
+#define CONFIG_SHOWSPECTRUM_FILTER 0
+#define CONFIG_SHOWSPECTRUMPIC_FILTER 0
+#define CONFIG_SHOWVOLUME_FILTER 0
+#define CONFIG_SHOWWAVES_FILTER 0
+#define CONFIG_SHOWWAVESPIC_FILTER 0
+#define CONFIG_SPECTRUMSYNTH_FILTER 0
+#define CONFIG_AMOVIE_FILTER 0
+#define CONFIG_MOVIE_FILTER 0
+#define CONFIG_AFIFO_FILTER 0
+#define CONFIG_FIFO_FILTER 0
+#define CONFIG_AA_DEMUXER 0
+#define CONFIG_AAC_DEMUXER 0
+#define CONFIG_AC3_DEMUXER 0
+#define CONFIG_ACM_DEMUXER 0
+#define CONFIG_ACT_DEMUXER 0
+#define CONFIG_ADF_DEMUXER 0
+#define CONFIG_ADP_DEMUXER 0
+#define CONFIG_ADS_DEMUXER 0
+#define CONFIG_ADX_DEMUXER 0
+#define CONFIG_AEA_DEMUXER 0
+#define CONFIG_AFC_DEMUXER 0
+#define CONFIG_AIFF_DEMUXER 0
+#define CONFIG_AIX_DEMUXER 0
+#define CONFIG_AMR_DEMUXER 0
+#define CONFIG_AMRNB_DEMUXER 0
+#define CONFIG_AMRWB_DEMUXER 0
+#define CONFIG_ANM_DEMUXER 0
+#define CONFIG_APC_DEMUXER 0
+#define CONFIG_APE_DEMUXER 0
+#define CONFIG_APNG_DEMUXER 0
+#define CONFIG_APTX_DEMUXER 0
+#define CONFIG_APTX_HD_DEMUXER 0
+#define CONFIG_AQTITLE_DEMUXER 0
+#define CONFIG_ASF_DEMUXER 0
+#define CONFIG_ASF_O_DEMUXER 0
+#define CONFIG_ASS_DEMUXER 0
+#define CONFIG_AST_DEMUXER 0
+#define CONFIG_AU_DEMUXER 0
+#define CONFIG_AVI_DEMUXER 0
+#define CONFIG_AVISYNTH_DEMUXER 0
+#define CONFIG_AVR_DEMUXER 0
+#define CONFIG_AVS_DEMUXER 0
+#define CONFIG_AVS2_DEMUXER 0
+#define CONFIG_BETHSOFTVID_DEMUXER 0
+#define CONFIG_BFI_DEMUXER 0
+#define CONFIG_BINTEXT_DEMUXER 0
+#define CONFIG_BINK_DEMUXER 0
+#define CONFIG_BIT_DEMUXER 0
+#define CONFIG_BMV_DEMUXER 0
+#define CONFIG_BFSTM_DEMUXER 0
+#define CONFIG_BRSTM_DEMUXER 0
+#define CONFIG_BOA_DEMUXER 0
+#define CONFIG_C93_DEMUXER 0
+#define CONFIG_CAF_DEMUXER 0
+#define CONFIG_CAVSVIDEO_DEMUXER 0
+#define CONFIG_CDG_DEMUXER 0
+#define CONFIG_CDXL_DEMUXER 0
+#define CONFIG_CINE_DEMUXER 0
+#define CONFIG_CODEC2_DEMUXER 0
+#define CONFIG_CODEC2RAW_DEMUXER 0
+#define CONFIG_CONCAT_DEMUXER 0
+#define CONFIG_DASH_DEMUXER 0
+#define CONFIG_DATA_DEMUXER 0
+#define CONFIG_DAUD_DEMUXER 0
+#define CONFIG_DCSTR_DEMUXER 0
+#define CONFIG_DFA_DEMUXER 0
+#define CONFIG_DIRAC_DEMUXER 0
+#define CONFIG_DNXHD_DEMUXER 0
+#define CONFIG_DSF_DEMUXER 0
+#define CONFIG_DSICIN_DEMUXER 0
+#define CONFIG_DSS_DEMUXER 0
+#define CONFIG_DTS_DEMUXER 0
+#define CONFIG_DTSHD_DEMUXER 0
+#define CONFIG_DV_DEMUXER 0
+#define CONFIG_DVBSUB_DEMUXER 0
+#define CONFIG_DVBTXT_DEMUXER 0
+#define CONFIG_DXA_DEMUXER 0
+#define CONFIG_EA_DEMUXER 0
+#define CONFIG_EA_CDATA_DEMUXER 0
+#define CONFIG_EAC3_DEMUXER 0
+#define CONFIG_EPAF_DEMUXER 0
+#define CONFIG_FFMETADATA_DEMUXER 0
+#define CONFIG_FILMSTRIP_DEMUXER 0
+#define CONFIG_FITS_DEMUXER 0
+#define CONFIG_FLAC_DEMUXER 0
+#define CONFIG_FLIC_DEMUXER 0
+#define CONFIG_FLV_DEMUXER 0
+#define CONFIG_LIVE_FLV_DEMUXER 0
+#define CONFIG_FOURXM_DEMUXER 0
+#define CONFIG_FRM_DEMUXER 0
+#define CONFIG_FSB_DEMUXER 0
+#define CONFIG_G722_DEMUXER 0
+#define CONFIG_G723_1_DEMUXER 0
+#define CONFIG_G726_DEMUXER 0
+#define CONFIG_G726LE_DEMUXER 0
+#define CONFIG_G729_DEMUXER 0
+#define CONFIG_GDV_DEMUXER 0
+#define CONFIG_GENH_DEMUXER 0
+#define CONFIG_GIF_DEMUXER 0
+#define CONFIG_GSM_DEMUXER 0
+#define CONFIG_GXF_DEMUXER 0
+#define CONFIG_H261_DEMUXER 0
+#define CONFIG_H263_DEMUXER 0
+#define CONFIG_H264_DEMUXER 0
+#define CONFIG_HEVC_DEMUXER 0
+#define CONFIG_HLS_DEMUXER 0
+#define CONFIG_HNM_DEMUXER 0
+#define CONFIG_ICO_DEMUXER 0
+#define CONFIG_IDCIN_DEMUXER 0
+#define CONFIG_IDF_DEMUXER 0
+#define CONFIG_IFF_DEMUXER 0
+#define CONFIG_ILBC_DEMUXER 0
+#define CONFIG_IMAGE2_DEMUXER 0
+#define CONFIG_IMAGE2PIPE_DEMUXER 0
+#define CONFIG_IMAGE2_ALIAS_PIX_DEMUXER 0
+#define CONFIG_IMAGE2_BRENDER_PIX_DEMUXER 0
+#define CONFIG_INGENIENT_DEMUXER 0
+#define CONFIG_IPMOVIE_DEMUXER 0
+#define CONFIG_IRCAM_DEMUXER 0
+#define CONFIG_ISS_DEMUXER 0
+#define CONFIG_IV8_DEMUXER 0
+#define CONFIG_IVF_DEMUXER 0
+#define CONFIG_IVR_DEMUXER 0
+#define CONFIG_JACOSUB_DEMUXER 0
+#define CONFIG_JV_DEMUXER 0
+#define CONFIG_LMLM4_DEMUXER 0
+#define CONFIG_LOAS_DEMUXER 0
+#define CONFIG_LRC_DEMUXER 0
+#define CONFIG_LVF_DEMUXER 0
+#define CONFIG_LXF_DEMUXER 0
+#define CONFIG_M4V_DEMUXER 0
+#define CONFIG_MATROSKA_DEMUXER 0
+#define CONFIG_MGSTS_DEMUXER 0
+#define CONFIG_MICRODVD_DEMUXER 0
+#define CONFIG_MJPEG_DEMUXER 0
+#define CONFIG_MJPEG_2000_DEMUXER 0
+#define CONFIG_MLP_DEMUXER 0
+#define CONFIG_MLV_DEMUXER 0
+#define CONFIG_MM_DEMUXER 0
+#define CONFIG_MMF_DEMUXER 0
+#define CONFIG_MOV_DEMUXER 1
+#define CONFIG_MP3_DEMUXER 0
+#define CONFIG_MPC_DEMUXER 0
+#define CONFIG_MPC8_DEMUXER 0
+#define CONFIG_MPEGPS_DEMUXER 0
+#define CONFIG_MPEGTS_DEMUXER 0
+#define CONFIG_MPEGTSRAW_DEMUXER 0
+#define CONFIG_MPEGVIDEO_DEMUXER 0
+#define CONFIG_MPJPEG_DEMUXER 0
+#define CONFIG_MPL2_DEMUXER 0
+#define CONFIG_MPSUB_DEMUXER 0
+#define CONFIG_MSF_DEMUXER 0
+#define CONFIG_MSNWC_TCP_DEMUXER 0
+#define CONFIG_MTAF_DEMUXER 0
+#define CONFIG_MTV_DEMUXER 0
+#define CONFIG_MUSX_DEMUXER 0
+#define CONFIG_MV_DEMUXER 0
+#define CONFIG_MVI_DEMUXER 0
+#define CONFIG_MXF_DEMUXER 0
+#define CONFIG_MXG_DEMUXER 0
+#define CONFIG_NC_DEMUXER 0
+#define CONFIG_NISTSPHERE_DEMUXER 0
+#define CONFIG_NSP_DEMUXER 0
+#define CONFIG_NSV_DEMUXER 0
+#define CONFIG_NUT_DEMUXER 0
+#define CONFIG_NUV_DEMUXER 0
+#define CONFIG_OGG_DEMUXER 0
+#define CONFIG_OMA_DEMUXER 0
+#define CONFIG_PAF_DEMUXER 0
+#define CONFIG_PCM_ALAW_DEMUXER 0
+#define CONFIG_PCM_MULAW_DEMUXER 0
+#define CONFIG_PCM_VIDC_DEMUXER 0
+#define CONFIG_PCM_F64BE_DEMUXER 0
+#define CONFIG_PCM_F64LE_DEMUXER 0
+#define CONFIG_PCM_F32BE_DEMUXER 0
+#define CONFIG_PCM_F32LE_DEMUXER 0
+#define CONFIG_PCM_S32BE_DEMUXER 0
+#define CONFIG_PCM_S32LE_DEMUXER 0
+#define CONFIG_PCM_S24BE_DEMUXER 0
+#define CONFIG_PCM_S24LE_DEMUXER 0
+#define CONFIG_PCM_S16BE_DEMUXER 0
+#define CONFIG_PCM_S16LE_DEMUXER 0
+#define CONFIG_PCM_S8_DEMUXER 0
+#define CONFIG_PCM_U32BE_DEMUXER 0
+#define CONFIG_PCM_U32LE_DEMUXER 0
+#define CONFIG_PCM_U24BE_DEMUXER 0
+#define CONFIG_PCM_U24LE_DEMUXER 0
+#define CONFIG_PCM_U16BE_DEMUXER 0
+#define CONFIG_PCM_U16LE_DEMUXER 0
+#define CONFIG_PCM_U8_DEMUXER 0
+#define CONFIG_PJS_DEMUXER 0
+#define CONFIG_PMP_DEMUXER 0
+#define CONFIG_PVA_DEMUXER 0
+#define CONFIG_PVF_DEMUXER 0
+#define CONFIG_QCP_DEMUXER 0
+#define CONFIG_R3D_DEMUXER 0
+#define CONFIG_RAWVIDEO_DEMUXER 0
+#define CONFIG_REALTEXT_DEMUXER 0
+#define CONFIG_REDSPARK_DEMUXER 0
+#define CONFIG_RL2_DEMUXER 0
+#define CONFIG_RM_DEMUXER 0
+#define CONFIG_ROQ_DEMUXER 0
+#define CONFIG_RPL_DEMUXER 0
+#define CONFIG_RSD_DEMUXER 0
+#define CONFIG_RSO_DEMUXER 0
+#define CONFIG_RTP_DEMUXER 0
+#define CONFIG_RTSP_DEMUXER 0
+#define CONFIG_S337M_DEMUXER 0
+#define CONFIG_SAMI_DEMUXER 0
+#define CONFIG_SAP_DEMUXER 0
+#define CONFIG_SBC_DEMUXER 0
+#define CONFIG_SBG_DEMUXER 0
+#define CONFIG_SCC_DEMUXER 0
+#define CONFIG_SDP_DEMUXER 0
+#define CONFIG_SDR2_DEMUXER 0
+#define CONFIG_SDS_DEMUXER 0
+#define CONFIG_SDX_DEMUXER 0
+#define CONFIG_SEGAFILM_DEMUXER 0
+#define CONFIG_SER_DEMUXER 0
+#define CONFIG_SHORTEN_DEMUXER 0
+#define CONFIG_SIFF_DEMUXER 0
+#define CONFIG_SLN_DEMUXER 0
+#define CONFIG_SMACKER_DEMUXER 0
+#define CONFIG_SMJPEG_DEMUXER 0
+#define CONFIG_SMUSH_DEMUXER 0
+#define CONFIG_SOL_DEMUXER 0
+#define CONFIG_SOX_DEMUXER 0
+#define CONFIG_SPDIF_DEMUXER 0
+#define CONFIG_SRT_DEMUXER 0
+#define CONFIG_STR_DEMUXER 0
+#define CONFIG_STL_DEMUXER 0
+#define CONFIG_SUBVIEWER1_DEMUXER 0
+#define CONFIG_SUBVIEWER_DEMUXER 0
+#define CONFIG_SUP_DEMUXER 0
+#define CONFIG_SVAG_DEMUXER 0
+#define CONFIG_SWF_DEMUXER 0
+#define CONFIG_TAK_DEMUXER 0
+#define CONFIG_TEDCAPTIONS_DEMUXER 0
+#define CONFIG_THP_DEMUXER 0
+#define CONFIG_THREEDOSTR_DEMUXER 0
+#define CONFIG_TIERTEXSEQ_DEMUXER 0
+#define CONFIG_TMV_DEMUXER 0
+#define CONFIG_TRUEHD_DEMUXER 0
+#define CONFIG_TTA_DEMUXER 0
+#define CONFIG_TXD_DEMUXER 0
+#define CONFIG_TTY_DEMUXER 0
+#define CONFIG_TY_DEMUXER 0
+#define CONFIG_V210_DEMUXER 0
+#define CONFIG_V210X_DEMUXER 0
+#define CONFIG_VAG_DEMUXER 0
+#define CONFIG_VC1_DEMUXER 0
+#define CONFIG_VC1T_DEMUXER 0
+#define CONFIG_VIVO_DEMUXER 0
+#define CONFIG_VMD_DEMUXER 0
+#define CONFIG_VOBSUB_DEMUXER 0
+#define CONFIG_VOC_DEMUXER 0
+#define CONFIG_VPK_DEMUXER 0
+#define CONFIG_VPLAYER_DEMUXER 0
+#define CONFIG_VQF_DEMUXER 0
+#define CONFIG_W64_DEMUXER 0
+#define CONFIG_WAV_DEMUXER 1
+#define CONFIG_WC3_DEMUXER 0
+#define CONFIG_WEBM_DASH_MANIFEST_DEMUXER 0
+#define CONFIG_WEBVTT_DEMUXER 0
+#define CONFIG_WSAUD_DEMUXER 0
+#define CONFIG_WSD_DEMUXER 0
+#define CONFIG_WSVQA_DEMUXER 0
+#define CONFIG_WTV_DEMUXER 0
+#define CONFIG_WVE_DEMUXER 0
+#define CONFIG_WV_DEMUXER 0
+#define CONFIG_XA_DEMUXER 0
+#define CONFIG_XBIN_DEMUXER 0
+#define CONFIG_XMV_DEMUXER 0
+#define CONFIG_XVAG_DEMUXER 0
+#define CONFIG_XWMA_DEMUXER 0
+#define CONFIG_YOP_DEMUXER 0
+#define CONFIG_YUV4MPEGPIPE_DEMUXER 0
+#define CONFIG_IMAGE_BMP_PIPE_DEMUXER 0
+#define CONFIG_IMAGE_DDS_PIPE_DEMUXER 0
+#define CONFIG_IMAGE_DPX_PIPE_DEMUXER 0
+#define CONFIG_IMAGE_EXR_PIPE_DEMUXER 0
+#define CONFIG_IMAGE_J2K_PIPE_DEMUXER 0
+#define CONFIG_IMAGE_JPEG_PIPE_DEMUXER 0
+#define CONFIG_IMAGE_JPEGLS_PIPE_DEMUXER 0
+#define CONFIG_IMAGE_PAM_PIPE_DEMUXER 0
+#define CONFIG_IMAGE_PBM_PIPE_DEMUXER 0
+#define CONFIG_IMAGE_PCX_PIPE_DEMUXER 0
+#define CONFIG_IMAGE_PGMYUV_PIPE_DEMUXER 0
+#define CONFIG_IMAGE_PGM_PIPE_DEMUXER 0
+#define CONFIG_IMAGE_PICTOR_PIPE_DEMUXER 0
+#define CONFIG_IMAGE_PNG_PIPE_DEMUXER 0
+#define CONFIG_IMAGE_PPM_PIPE_DEMUXER 0
+#define CONFIG_IMAGE_PSD_PIPE_DEMUXER 0
+#define CONFIG_IMAGE_QDRAW_PIPE_DEMUXER 0
+#define CONFIG_IMAGE_SGI_PIPE_DEMUXER 0
+#define CONFIG_IMAGE_SVG_PIPE_DEMUXER 0
+#define CONFIG_IMAGE_SUNRAST_PIPE_DEMUXER 0
+#define CONFIG_IMAGE_TIFF_PIPE_DEMUXER 0
+#define CONFIG_IMAGE_WEBP_PIPE_DEMUXER 0
+#define CONFIG_IMAGE_XPM_PIPE_DEMUXER 0
+#define CONFIG_IMAGE_XWD_PIPE_DEMUXER 0
+#define CONFIG_LIBGME_DEMUXER 0
+#define CONFIG_LIBMODPLUG_DEMUXER 0
+#define CONFIG_LIBOPENMPT_DEMUXER 0
+#define CONFIG_VAPOURSYNTH_DEMUXER 0
+#define CONFIG_A64_MUXER 0
+#define CONFIG_AC3_MUXER 0
+#define CONFIG_ADTS_MUXER 0
+#define CONFIG_ADX_MUXER 0
+#define CONFIG_AIFF_MUXER 0
+#define CONFIG_AMR_MUXER 0
+#define CONFIG_APNG_MUXER 0
+#define CONFIG_APTX_MUXER 0
+#define CONFIG_APTX_HD_MUXER 0
+#define CONFIG_ASF_MUXER 0
+#define CONFIG_ASS_MUXER 0
+#define CONFIG_AST_MUXER 0
+#define CONFIG_ASF_STREAM_MUXER 0
+#define CONFIG_AU_MUXER 0
+#define CONFIG_AVI_MUXER 0
+#define CONFIG_AVM2_MUXER 0
+#define CONFIG_AVS2_MUXER 0
+#define CONFIG_BIT_MUXER 0
+#define CONFIG_CAF_MUXER 0
+#define CONFIG_CAVSVIDEO_MUXER 0
+#define CONFIG_CODEC2_MUXER 0
+#define CONFIG_CODEC2RAW_MUXER 0
+#define CONFIG_CRC_MUXER 0
+#define CONFIG_DASH_MUXER 0
+#define CONFIG_DATA_MUXER 0
+#define CONFIG_DAUD_MUXER 0
+#define CONFIG_DIRAC_MUXER 0
+#define CONFIG_DNXHD_MUXER 0
+#define CONFIG_DTS_MUXER 0
+#define CONFIG_DV_MUXER 0
+#define CONFIG_EAC3_MUXER 0
+#define CONFIG_F4V_MUXER 0
+#define CONFIG_FFMETADATA_MUXER 0
+#define CONFIG_FIFO_MUXER 0
+#define CONFIG_FIFO_TEST_MUXER 0
+#define CONFIG_FILMSTRIP_MUXER 0
+#define CONFIG_FITS_MUXER 0
+#define CONFIG_FLAC_MUXER 0
+#define CONFIG_FLV_MUXER 0
+#define CONFIG_FRAMECRC_MUXER 0
+#define CONFIG_FRAMEHASH_MUXER 0
+#define CONFIG_FRAMEMD5_MUXER 0
+#define CONFIG_G722_MUXER 0
+#define CONFIG_G723_1_MUXER 0
+#define CONFIG_G726_MUXER 0
+#define CONFIG_G726LE_MUXER 0
+#define CONFIG_GIF_MUXER 0
+#define CONFIG_GSM_MUXER 0
+#define CONFIG_GXF_MUXER 0
+#define CONFIG_H261_MUXER 0
+#define CONFIG_H263_MUXER 0
+#define CONFIG_H264_MUXER 0
+#define CONFIG_HASH_MUXER 0
+#define CONFIG_HDS_MUXER 0
+#define CONFIG_HEVC_MUXER 0
+#define CONFIG_HLS_MUXER 0
+#define CONFIG_ICO_MUXER 0
+#define CONFIG_ILBC_MUXER 0
+#define CONFIG_IMAGE2_MUXER 0
+#define CONFIG_IMAGE2PIPE_MUXER 0
+#define CONFIG_IPOD_MUXER 0
+#define CONFIG_IRCAM_MUXER 0
+#define CONFIG_ISMV_MUXER 0
+#define CONFIG_IVF_MUXER 0
+#define CONFIG_JACOSUB_MUXER 0
+#define CONFIG_LATM_MUXER 0
+#define CONFIG_LRC_MUXER 0
+#define CONFIG_M4V_MUXER 0
+#define CONFIG_MD5_MUXER 0
+#define CONFIG_MATROSKA_MUXER 0
+#define CONFIG_MATROSKA_AUDIO_MUXER 0
+#define CONFIG_MICRODVD_MUXER 0
+#define CONFIG_MJPEG_MUXER 0
+#define CONFIG_MLP_MUXER 0
+#define CONFIG_MMF_MUXER 0
+#define CONFIG_MOV_MUXER 0
+#define CONFIG_MP2_MUXER 0
+#define CONFIG_MP3_MUXER 0
+#define CONFIG_MP4_MUXER 0
+#define CONFIG_MPEG1SYSTEM_MUXER 0
+#define CONFIG_MPEG1VCD_MUXER 0
+#define CONFIG_MPEG1VIDEO_MUXER 0
+#define CONFIG_MPEG2DVD_MUXER 0
+#define CONFIG_MPEG2SVCD_MUXER 0
+#define CONFIG_MPEG2VIDEO_MUXER 0
+#define CONFIG_MPEG2VOB_MUXER 0
+#define CONFIG_MPEGTS_MUXER 0
+#define CONFIG_MPJPEG_MUXER 0
+#define CONFIG_MXF_MUXER 0
+#define CONFIG_MXF_D10_MUXER 0
+#define CONFIG_MXF_OPATOM_MUXER 0
+#define CONFIG_NULL_MUXER 0
+#define CONFIG_NUT_MUXER 0
+#define CONFIG_OGA_MUXER 0
+#define CONFIG_OGG_MUXER 0
+#define CONFIG_OGV_MUXER 0
+#define CONFIG_OMA_MUXER 0
+#define CONFIG_OPUS_MUXER 0
+#define CONFIG_PCM_ALAW_MUXER 0
+#define CONFIG_PCM_MULAW_MUXER 0
+#define CONFIG_PCM_VIDC_MUXER 0
+#define CONFIG_PCM_F64BE_MUXER 0
+#define CONFIG_PCM_F64LE_MUXER 0
+#define CONFIG_PCM_F32BE_MUXER 0
+#define CONFIG_PCM_F32LE_MUXER 0
+#define CONFIG_PCM_S32BE_MUXER 0
+#define CONFIG_PCM_S32LE_MUXER 0
+#define CONFIG_PCM_S24BE_MUXER 0
+#define CONFIG_PCM_S24LE_MUXER 0
+#define CONFIG_PCM_S16BE_MUXER 0
+#define CONFIG_PCM_S16LE_MUXER 0
+#define CONFIG_PCM_S8_MUXER 0
+#define CONFIG_PCM_U32BE_MUXER 0
+#define CONFIG_PCM_U32LE_MUXER 0
+#define CONFIG_PCM_U24BE_MUXER 0
+#define CONFIG_PCM_U24LE_MUXER 0
+#define CONFIG_PCM_U16BE_MUXER 0
+#define CONFIG_PCM_U16LE_MUXER 0
+#define CONFIG_PCM_U8_MUXER 0
+#define CONFIG_PSP_MUXER 0
+#define CONFIG_RAWVIDEO_MUXER 0
+#define CONFIG_RM_MUXER 0
+#define CONFIG_ROQ_MUXER 0
+#define CONFIG_RSO_MUXER 0
+#define CONFIG_RTP_MUXER 0
+#define CONFIG_RTP_MPEGTS_MUXER 0
+#define CONFIG_RTSP_MUXER 0
+#define CONFIG_SAP_MUXER 0
+#define CONFIG_SBC_MUXER 0
+#define CONFIG_SCC_MUXER 0
+#define CONFIG_SEGAFILM_MUXER 0
+#define CONFIG_SEGMENT_MUXER 0
+#define CONFIG_STREAM_SEGMENT_MUXER 0
+#define CONFIG_SINGLEJPEG_MUXER 0
+#define CONFIG_SMJPEG_MUXER 0
+#define CONFIG_SMOOTHSTREAMING_MUXER 0
+#define CONFIG_SOX_MUXER 0
+#define CONFIG_SPX_MUXER 0
+#define CONFIG_SPDIF_MUXER 0
+#define CONFIG_SRT_MUXER 0
+#define CONFIG_SUP_MUXER 0
+#define CONFIG_SWF_MUXER 0
+#define CONFIG_TEE_MUXER 0
+#define CONFIG_TG2_MUXER 0
+#define CONFIG_TGP_MUXER 0
+#define CONFIG_MKVTIMESTAMP_V2_MUXER 0
+#define CONFIG_TRUEHD_MUXER 0
+#define CONFIG_TTA_MUXER 0
+#define CONFIG_UNCODEDFRAMECRC_MUXER 0
+#define CONFIG_VC1_MUXER 0
+#define CONFIG_VC1T_MUXER 0
+#define CONFIG_VOC_MUXER 0
+#define CONFIG_W64_MUXER 0
+#define CONFIG_WAV_MUXER 0
+#define CONFIG_WEBM_MUXER 0
+#define CONFIG_WEBM_DASH_MANIFEST_MUXER 0
+#define CONFIG_WEBM_CHUNK_MUXER 0
+#define CONFIG_WEBP_MUXER 0
+#define CONFIG_WEBVTT_MUXER 0
+#define CONFIG_WTV_MUXER 0
+#define CONFIG_WV_MUXER 0
+#define CONFIG_YUV4MPEGPIPE_MUXER 0
+#define CONFIG_CHROMAPRINT_MUXER 0
+#define CONFIG_ASYNC_PROTOCOL 0
+#define CONFIG_BLURAY_PROTOCOL 0
+#define CONFIG_CACHE_PROTOCOL 0
+#define CONFIG_CONCAT_PROTOCOL 0
+#define CONFIG_CRYPTO_PROTOCOL 0
+#define CONFIG_DATA_PROTOCOL 0
+#define CONFIG_FFRTMPCRYPT_PROTOCOL 0
+#define CONFIG_FFRTMPHTTP_PROTOCOL 0
+#define CONFIG_FILE_PROTOCOL 1
+#define CONFIG_FTP_PROTOCOL 0
+#define CONFIG_GOPHER_PROTOCOL 0
+#define CONFIG_HLS_PROTOCOL 0
+#define CONFIG_HTTP_PROTOCOL 0
+#define CONFIG_HTTPPROXY_PROTOCOL 0
+#define CONFIG_HTTPS_PROTOCOL 0
+#define CONFIG_ICECAST_PROTOCOL 0
+#define CONFIG_MMSH_PROTOCOL 0
+#define CONFIG_MMST_PROTOCOL 0
+#define CONFIG_MD5_PROTOCOL 0
+#define CONFIG_PIPE_PROTOCOL 0
+#define CONFIG_PROMPEG_PROTOCOL 0
+#define CONFIG_RTMP_PROTOCOL 0
+#define CONFIG_RTMPE_PROTOCOL 0
+#define CONFIG_RTMPS_PROTOCOL 0
+#define CONFIG_RTMPT_PROTOCOL 0
+#define CONFIG_RTMPTE_PROTOCOL 0
+#define CONFIG_RTMPTS_PROTOCOL 0
+#define CONFIG_RTP_PROTOCOL 0
+#define CONFIG_SCTP_PROTOCOL 0
+#define CONFIG_SRTP_PROTOCOL 0
+#define CONFIG_SUBFILE_PROTOCOL 0
+#define CONFIG_TEE_PROTOCOL 0
+#define CONFIG_TCP_PROTOCOL 0
+#define CONFIG_TLS_PROTOCOL 0
+#define CONFIG_UDP_PROTOCOL 0
+#define CONFIG_UDPLITE_PROTOCOL 0
+#define CONFIG_UNIX_PROTOCOL 0
+#define CONFIG_LIBRTMP_PROTOCOL 0
+#define CONFIG_LIBRTMPE_PROTOCOL 0
+#define CONFIG_LIBRTMPS_PROTOCOL 0
+#define CONFIG_LIBRTMPT_PROTOCOL 0
+#define CONFIG_LIBRTMPTE_PROTOCOL 0
+#define CONFIG_LIBSRT_PROTOCOL 0
+#define CONFIG_LIBSSH_PROTOCOL 0
+#define CONFIG_LIBSMBCLIENT_PROTOCOL 0
+#endif /* FFMPEG_CONFIG_H */
diff -uparN ffmpeg-4.1/disable_decoder_config.sh ffmpeg-y/disable_decoder_config.sh
--- ffmpeg-4.1/disable_decoder_config.sh	1970-01-01 08:00:00.000000000 +0800
+++ ffmpeg-y/disable_decoder_config.sh	2019-06-29 11:49:36.997017672 +0800
@@ -0,0 +1,76 @@
+echo `pwd`
+CUR_DIR=$(pwd)
+HIBERRY_DIR=$(pwd)/../../..
+source ${HIBERRY_DIR}/cfg.mak
+
+configure_attr=" --prefix=./install \
+    --enable-cross-compile \
+    --disable-doc \
+    --disable-htmlpages \
+    --target-os=linux \
+    --enable-static \
+    --disable-shared \
+    --disable-debug \
+    --disable-iconv  \
+    --enable-small \
+    --disable-network \
+    --disable-filters \
+    --disable-devices \
+    --disable-programs \
+    --disable-swresample \
+    --disable-swscale \
+    --disable-avdevice \
+    --disable-postproc \
+    --disable-avfilter \
+    --disable-protocols \
+    --disable-pthreads \
+    --disable-runtime-cpudetect \
+    --disable-everything   \
+    --enable-pic   \
+    --enable-protocol=file \
+    --disable-muxers \
+    --enable-demuxer=mov\
+    --disable-neon \
+    --disable-inline-asm \
+    --disable-asm \
+    --disable-armv6 \
+    --disable-armv6t2 \
+    --disable-armv5te \
+    --disable-vfp \
+    --disable-hardcoded-tables \
+    --disable-mediacodec \
+    --enable-bsf=h264_mp4toannexb \
+    --enable-bsf=hevc_mp4toannexb \
+    --disable-pixelutils \
+    --enable-demuxer=wav \
+    --disable-gpl "
+
+
+if [ ${CFG_CHIP_TYPE} == "hi3559av100" ]; then
+echo "hi3559av100 =? ${CFG_CHIP_TYPE}"
+configure_attr+=" --arch=arm64  --cross-prefix=${CFG_SDK_TOOLCHAIN} "
+fi
+
+if [ ${CFG_CHIP_TYPE} == "hi3559"  -o  ${CFG_CHIP_TYPE} == "hi3556" ]; then
+echo "hi3559/hi3556 =? ${CFG_CHIP_TYPE}"
+configure_attr+=" --cpu=cortex-a7 --arch=armv7-a --cross-prefix=${CFG_SDK_TOOLCHAIN} "
+fi
+
+if [ ${CFG_CHIP_TYPE} == "hi3556av100" ]; then
+echo "hi3556av100 =? ${CFG_CHIP_TYPE}"
+configure_attr+=" --arch=armv7-a --cross-prefix=${CFG_SDK_TOOLCHAIN} "
+fi
+
+if [ ${CFG_CHIP_TYPE} == "hi3559v200" ]; then
+echo "hi3556av100 =? ${CFG_CHIP_TYPE}"
+configure_attr+=" --cpu=cortex-a7 --arch=armv7-a --cross-prefix=${CFG_SDK_TOOLCHAIN} "
+fi
+
+if [ ${CFG_CHIP_TYPE} == "hi3519av100" ]; then
+echo "hi3519av100 =? ${CFG_CHIP_TYPE}"
+configure_attr+=" --arch=armv7-a --cross-prefix=${CFG_SDK_TOOLCHAIN} "
+fi
+
+echo ${configure_attr}
+
+./configure  ${configure_attr}
diff -uparN ffmpeg-4.1/doc/config.texi ffmpeg-y/doc/config.texi
--- ffmpeg-4.1/doc/config.texi	1970-01-01 08:00:00.000000000 +0800
+++ ffmpeg-y/doc/config.texi	2019-06-29 11:49:36.865017668 +0800
@@ -0,0 +1,2451 @@
+@c auto-generated by configure - do not modify! 
+@c @set arch-aarch64 no
+@c @set arch-alpha no
+@c @set arch-arm no
+@c @set arch-avr32 no
+@c @set arch-avr32-ap no
+@c @set arch-avr32-uc no
+@c @set arch-bfin no
+@c @set arch-ia64 no
+@c @set arch-m68k no
+@c @set arch-mips no
+@c @set arch-mips64 no
+@c @set arch-parisc no
+@c @set arch-ppc no
+@c @set arch-ppc64 no
+@c @set arch-s390 no
+@c @set arch-sh4 no
+@c @set arch-sparc no
+@c @set arch-sparc64 no
+@c @set arch-tilegx no
+@c @set arch-tilepro no
+@c @set arch-tomi no
+@c @set arch-x86 no
+@c @set arch-x86-32 no
+@c @set arch-x86-64 no
+@c @set have-armv5te no
+@c @set have-armv6 no
+@c @set have-armv6t2 no
+@c @set have-armv8 no
+@c @set have-neon no
+@c @set have-vfp no
+@c @set have-vfpv3 no
+@c @set have-setend no
+@c @set have-altivec no
+@c @set have-dcbzl no
+@c @set have-ldbrx no
+@c @set have-power8 no
+@c @set have-ppc4xx no
+@c @set have-vsx no
+@c @set have-aesni no
+@c @set have-amd3dnow no
+@c @set have-amd3dnowext no
+@c @set have-avx no
+@c @set have-avx2 no
+@c @set have-avx512 no
+@c @set have-fma3 no
+@c @set have-fma4 no
+@c @set have-mmx no
+@c @set have-mmxext no
+@c @set have-sse no
+@c @set have-sse2 no
+@c @set have-sse3 no
+@c @set have-sse4 no
+@c @set have-sse42 no
+@c @set have-ssse3 no
+@c @set have-xop no
+@c @set have-cpunop no
+@c @set have-i686 no
+@c @set have-mipsfpu no
+@c @set have-mips32r2 no
+@c @set have-mips32r5 no
+@c @set have-mips64r2 no
+@c @set have-mips32r6 no
+@c @set have-mips64r6 no
+@c @set have-mipsdsp no
+@c @set have-mipsdspr2 no
+@c @set have-msa no
+@c @set have-loongson2 no
+@c @set have-loongson3 no
+@c @set have-mmi no
+@c @set have-armv5te-external no
+@c @set have-armv6-external no
+@c @set have-armv6t2-external no
+@c @set have-armv8-external no
+@c @set have-neon-external no
+@c @set have-vfp-external no
+@c @set have-vfpv3-external no
+@set have-setend-external yes
+@c @set have-altivec-external no
+@c @set have-dcbzl-external no
+@c @set have-ldbrx-external no
+@c @set have-power8-external no
+@c @set have-ppc4xx-external no
+@c @set have-vsx-external no
+@c @set have-aesni-external no
+@c @set have-amd3dnow-external no
+@c @set have-amd3dnowext-external no
+@c @set have-avx-external no
+@c @set have-avx2-external no
+@c @set have-avx512-external no
+@c @set have-fma3-external no
+@c @set have-fma4-external no
+@c @set have-mmx-external no
+@c @set have-mmxext-external no
+@c @set have-sse-external no
+@c @set have-sse2-external no
+@c @set have-sse3-external no
+@c @set have-sse4-external no
+@c @set have-sse42-external no
+@c @set have-ssse3-external no
+@c @set have-xop-external no
+@c @set have-cpunop-external no
+@c @set have-i686-external no
+@c @set have-mipsfpu-external no
+@c @set have-mips32r2-external no
+@c @set have-mips32r5-external no
+@c @set have-mips64r2-external no
+@c @set have-mips32r6-external no
+@c @set have-mips64r6-external no
+@c @set have-mipsdsp-external no
+@c @set have-mipsdspr2-external no
+@c @set have-msa-external no
+@c @set have-loongson2-external no
+@c @set have-loongson3-external no
+@c @set have-mmi-external no
+@c @set have-armv5te-inline no
+@c @set have-armv6-inline no
+@c @set have-armv6t2-inline no
+@c @set have-armv8-inline no
+@c @set have-neon-inline no
+@c @set have-vfp-inline no
+@c @set have-vfpv3-inline no
+@c @set have-setend-inline no
+@c @set have-altivec-inline no
+@c @set have-dcbzl-inline no
+@c @set have-ldbrx-inline no
+@c @set have-power8-inline no
+@c @set have-ppc4xx-inline no
+@c @set have-vsx-inline no
+@c @set have-aesni-inline no
+@c @set have-amd3dnow-inline no
+@c @set have-amd3dnowext-inline no
+@c @set have-avx-inline no
+@c @set have-avx2-inline no
+@c @set have-avx512-inline no
+@c @set have-fma3-inline no
+@c @set have-fma4-inline no
+@c @set have-mmx-inline no
+@c @set have-mmxext-inline no
+@c @set have-sse-inline no
+@c @set have-sse2-inline no
+@c @set have-sse3-inline no
+@c @set have-sse4-inline no
+@c @set have-sse42-inline no
+@c @set have-ssse3-inline no
+@c @set have-xop-inline no
+@c @set have-cpunop-inline no
+@c @set have-i686-inline no
+@c @set have-mipsfpu-inline no
+@c @set have-mips32r2-inline no
+@c @set have-mips32r5-inline no
+@c @set have-mips64r2-inline no
+@c @set have-mips32r6-inline no
+@c @set have-mips64r6-inline no
+@c @set have-mipsdsp-inline no
+@c @set have-mipsdspr2-inline no
+@c @set have-msa-inline no
+@c @set have-loongson2-inline no
+@c @set have-loongson3-inline no
+@c @set have-mmi-inline no
+@c @set have-aligned-stack no
+@c @set have-fast-64bit no
+@set have-fast-clz yes
+@c @set have-fast-cmov no
+@c @set have-local-aligned no
+@c @set have-simd-align-16 no
+@c @set have-simd-align-32 no
+@c @set have-simd-align-64 no
+@c @set have-atomic-cas-ptr no
+@c @set have-machine-rw-barrier no
+@c @set have-memorybarrier no
+@c @set have-mm-empty no
+@c @set have-rdtsc no
+@c @set have-sem-timedwait no
+@set have-sync-val-compare-and-swap yes
+@c @set have-cabs no
+@c @set have-cexp no
+@c @set have-inline-asm no
+@set have-symver yes
+@c @set have-x86asm no
+@c @set have-bigendian no
+@set have-fast-unaligned yes
+@c @set have-arpa-inet-h no
+@set have-asm-types-h yes
+@c @set have-cdio-paranoia-h no
+@c @set have-cdio-paranoia-paranoia-h no
+@c @set have-cuda-h no
+@c @set have-dispatch-dispatch-h no
+@c @set have-dev-bktr-ioctl-bt848-h no
+@c @set have-dev-bktr-ioctl-meteor-h no
+@c @set have-dev-ic-bt8xx-h no
+@c @set have-dev-video-bktr-ioctl-bt848-h no
+@c @set have-dev-video-meteor-ioctl-meteor-h no
+@c @set have-direct-h no
+@set have-dirent-h yes
+@c @set have-dxgidebug-h no
+@c @set have-dxva-h no
+@c @set have-es2-gl-h no
+@c @set have-gsm-h no
+@c @set have-io-h no
+@set have-linux-perf-event-h yes
+@c @set have-machine-ioctl-bt848-h no
+@c @set have-machine-ioctl-meteor-h no
+@set have-malloc-h yes
+@c @set have-opencv2-core-core-c-h no
+@c @set have-opengl-gl3-h no
+@set have-poll-h yes
+@set have-sys-param-h yes
+@set have-sys-resource-h yes
+@set have-sys-select-h yes
+@set have-sys-soundcard-h yes
+@set have-sys-time-h yes
+@set have-sys-un-h yes
+@c @set have-sys-videoio-h no
+@set have-termios-h yes
+@c @set have-udplite-h no
+@set have-unistd-h yes
+@c @set have-valgrind-valgrind-h no
+@c @set have-windows-h no
+@c @set have-winsock2-h no
+@c @set have-intrinsics-neon no
+@set have-atanf yes
+@set have-atan2f yes
+@set have-cbrt yes
+@set have-cbrtf yes
+@set have-copysign yes
+@set have-cosf yes
+@set have-erf yes
+@set have-exp2 yes
+@set have-exp2f yes
+@set have-expf yes
+@set have-hypot yes
+@set have-isfinite yes
+@set have-isinf yes
+@set have-isnan yes
+@set have-ldexpf yes
+@set have-llrint yes
+@set have-llrintf yes
+@set have-log2 yes
+@set have-log2f yes
+@set have-log10f yes
+@set have-lrint yes
+@set have-lrintf yes
+@set have-powf yes
+@set have-rint yes
+@set have-round yes
+@set have-roundf yes
+@set have-sinf yes
+@set have-trunc yes
+@set have-truncf yes
+@c @set have-dos-paths no
+@c @set have-libc-msvcrt no
+@c @set have-mmal-parameter-video-max-num-callbacks no
+@set have-section-data-rel-ro yes
+@c @set have-threads no
+@c @set have-uwp no
+@c @set have-winrt no
+@set have-access yes
+@c @set have-aligned-malloc no
+@c @set have-arc4random no
+@set have-clock-gettime yes
+@c @set have-closesocket no
+@c @set have-commandlinetoargvw no
+@set have-fcntl yes
+@c @set have-getaddrinfo no
+@c @set have-gethrtime no
+@set have-getopt yes
+@c @set have-getprocessaffinitymask no
+@c @set have-getprocessmemoryinfo no
+@c @set have-getprocesstimes no
+@set have-getrusage yes
+@c @set have-getsystemtimeasfiletime no
+@set have-gettimeofday yes
+@set have-glob yes
+@c @set have-glxgetprocaddress no
+@set have-gmtime-r yes
+@c @set have-inet-aton no
+@set have-isatty yes
+@c @set have-kbhit no
+@set have-localtime-r yes
+@set have-lstat yes
+@c @set have-lzo1x-999-compress no
+@c @set have-mach-absolute-time no
+@c @set have-mapviewoffile no
+@set have-memalign yes
+@set have-mkstemp yes
+@set have-mmap yes
+@set have-mprotect yes
+@set have-nanosleep yes
+@c @set have-peeknamedpipe no
+@set have-posix-memalign yes
+@c @set have-pthread-cancel no
+@set have-sched-getaffinity yes
+@c @set have-secitemimport no
+@c @set have-setconsoletextattribute no
+@c @set have-setconsolectrlhandler no
+@c @set have-setmode no
+@set have-setrlimit yes
+@c @set have-sleep no
+@set have-strerror-r yes
+@set have-sysconf yes
+@set have-sysctl yes
+@set have-usleep yes
+@c @set have-utgetostypefromstring no
+@c @set have-virtualalloc no
+@c @set have-wglgetprocaddress no
+@c @set have-bcrypt no
+@c @set have-vaapi-drm no
+@c @set have-vaapi-x11 no
+@c @set have-vdpau-x11 no
+@c @set have-pthreads no
+@c @set have-os2threads no
+@c @set have-w32threads no
+@set have-as-arch-directive yes
+@set have-as-dn-directive yes
+@set have-as-fpu-directive yes
+@c @set have-as-func no
+@set have-as-object-arch yes
+@set have-asm-mod-q yes
+@c @set have-blocks-extension no
+@c @set have-ebp-available no
+@c @set have-ebx-available no
+@c @set have-gnu-as no
+@c @set have-gnu-windres no
+@c @set have-ibm-asm no
+@c @set have-inline-asm-direct-symbol-refs no
+@set have-inline-asm-labels yes
+@set have-inline-asm-nonlocal-labels yes
+@set have-pragma-deprecated yes
+@set have-rsync-contimeout yes
+@c @set have-symver-asm-label no
+@set have-symver-gnu-asm yes
+@c @set have-vfp-args no
+@c @set have-xform-asm no
+@c @set have-xmm-clobbers no
+@c @set have-kcmvideocodectype-hevc no
+@c @set have-socklen-t no
+@c @set have-struct-addrinfo no
+@c @set have-struct-group-source-req no
+@c @set have-struct-ip-mreq-source no
+@c @set have-struct-ipv6-mreq no
+@c @set have-struct-msghdr-msg-flags no
+@c @set have-struct-pollfd no
+@set have-struct-rusage-ru-maxrss yes
+@c @set have-struct-sctp-event-subscribe no
+@c @set have-struct-sockaddr-in6 no
+@c @set have-struct-sockaddr-sa-len no
+@c @set have-struct-sockaddr-storage no
+@set have-struct-stat-st-mtim-tv-nsec yes
+@set have-struct-v4l2-frmivalenum-discrete yes
+@set have-makeinfo yes
+@set have-makeinfo-html yes
+@c @set have-opencl-d3d11 no
+@c @set have-opencl-drm-arm no
+@c @set have-opencl-drm-beignet no
+@c @set have-opencl-dxva2 no
+@c @set have-opencl-vaapi-beignet no
+@c @set have-opencl-vaapi-intel-media no
+@set have-perl yes
+@set have-pod2man yes
+@c @set have-texi2html no
+@c @set config-doc no
+@c @set config-htmlpages no
+@set config-manpages yes
+@set config-podpages yes
+@set config-txtpages yes
+@set config-avio-dir-cmd-example yes
+@set config-avio-reading-example yes
+@set config-decode-audio-example yes
+@set config-decode-video-example yes
+@set config-demuxing-decoding-example yes
+@set config-encode-audio-example yes
+@set config-encode-video-example yes
+@set config-extract-mvs-example yes
+@c @set config-filter-audio-example no
+@c @set config-filtering-audio-example no
+@c @set config-filtering-video-example no
+@set config-http-multiclient-example yes
+@set config-hw-decode-example yes
+@set config-metadata-example yes
+@c @set config-muxing-example no
+@c @set config-qsvdec-example no
+@set config-remuxing-example yes
+@c @set config-resampling-audio-example no
+@c @set config-scaling-video-example no
+@c @set config-transcode-aac-example no
+@c @set config-transcoding-example no
+@c @set config-vaapi-encode-example no
+@c @set config-vaapi-transcode-example no
+@c @set config-avisynth no
+@c @set config-frei0r no
+@c @set config-libcdio no
+@c @set config-libdavs2 no
+@c @set config-librubberband no
+@c @set config-libvidstab no
+@c @set config-libx264 no
+@c @set config-libx265 no
+@c @set config-libxavs no
+@c @set config-libxavs2 no
+@c @set config-libxvid no
+@c @set config-decklink no
+@c @set config-libndi-newtek no
+@c @set config-libfdk-aac no
+@c @set config-openssl no
+@c @set config-libtls no
+@c @set config-gmp no
+@c @set config-liblensfun no
+@c @set config-libopencore-amrnb no
+@c @set config-libopencore-amrwb no
+@c @set config-libvmaf no
+@c @set config-libvo-amrwbenc no
+@c @set config-mbedtls no
+@c @set config-rkmpp no
+@c @set config-libsmbclient no
+@c @set config-chromaprint no
+@c @set config-gcrypt no
+@c @set config-gnutls no
+@c @set config-jni no
+@c @set config-ladspa no
+@c @set config-libaom no
+@c @set config-libass no
+@c @set config-libbluray no
+@c @set config-libbs2b no
+@c @set config-libcaca no
+@c @set config-libcelt no
+@c @set config-libcodec2 no
+@c @set config-libdc1394 no
+@c @set config-libdrm no
+@c @set config-libflite no
+@c @set config-libfontconfig no
+@c @set config-libfreetype no
+@c @set config-libfribidi no
+@c @set config-libgme no
+@c @set config-libgsm no
+@c @set config-libiec61883 no
+@c @set config-libilbc no
+@c @set config-libjack no
+@c @set config-libklvanc no
+@c @set config-libkvazaar no
+@c @set config-libmodplug no
+@c @set config-libmp3lame no
+@c @set config-libmysofa no
+@c @set config-libopencv no
+@c @set config-libopenh264 no
+@c @set config-libopenjpeg no
+@c @set config-libopenmpt no
+@c @set config-libopus no
+@c @set config-libpulse no
+@c @set config-librsvg no
+@c @set config-librtmp no
+@c @set config-libshine no
+@c @set config-libsmbclient no
+@c @set config-libsnappy no
+@c @set config-libsoxr no
+@c @set config-libspeex no
+@c @set config-libsrt no
+@c @set config-libssh no
+@c @set config-libtensorflow no
+@c @set config-libtesseract no
+@c @set config-libtheora no
+@c @set config-libtwolame no
+@c @set config-libv4l2 no
+@c @set config-libvorbis no
+@c @set config-libvpx no
+@c @set config-libwavpack no
+@c @set config-libwebp no
+@c @set config-libxml2 no
+@c @set config-libzimg no
+@c @set config-libzmq no
+@c @set config-libzvbi no
+@c @set config-lv2 no
+@c @set config-mediacodec no
+@c @set config-openal no
+@c @set config-opengl no
+@c @set config-vapoursynth no
+@c @set config-alsa no
+@c @set config-appkit no
+@c @set config-avfoundation no
+@c @set config-bzlib no
+@c @set config-coreimage no
+@c @set config-iconv no
+@c @set config-libxcb no
+@c @set config-libxcb-shm no
+@c @set config-libxcb-shape no
+@c @set config-libxcb-xfixes no
+@c @set config-lzma no
+@c @set config-schannel no
+@c @set config-sdl2 no
+@c @set config-securetransport no
+@c @set config-sndio no
+@c @set config-xlib no
+@c @set config-zlib no
+@c @set config-cuda-sdk no
+@c @set config-libnpp no
+@c @set config-libmfx no
+@c @set config-mmal no
+@c @set config-omx no
+@c @set config-opencl no
+@c @set config-amf no
+@c @set config-audiotoolbox no
+@c @set config-crystalhd no
+@c @set config-cuda no
+@c @set config-cuvid no
+@c @set config-d3d11va no
+@c @set config-dxva2 no
+@c @set config-ffnvcodec no
+@c @set config-nvdec no
+@c @set config-nvenc no
+@c @set config-vaapi no
+@c @set config-vdpau no
+@c @set config-videotoolbox no
+@c @set config-v4l2-m2m no
+@c @set config-xvmc no
+@c @set config-ftrapv no
+@c @set config-gray no
+@c @set config-hardcoded-tables no
+@c @set config-omx-rpi no
+@c @set config-runtime-cpudetect no
+@set config-safe-bitstream-reader yes
+@c @set config-shared no
+@set config-small yes
+@set config-static yes
+@set config-swscale-alpha yes
+@c @set config-gpl no
+@c @set config-nonfree no
+@c @set config-version3 no
+@c @set config-avdevice no
+@c @set config-avfilter no
+@c @set config-swscale no
+@c @set config-postproc no
+@set config-avformat yes
+@set config-avcodec yes
+@c @set config-swresample no
+@c @set config-avresample no
+@set config-avutil yes
+@c @set config-ffplay no
+@c @set config-ffprobe no
+@c @set config-ffmpeg no
+@c @set config-dct no
+@c @set config-dwt no
+@c @set config-error-resilience no
+@set config-faan yes
+@set config-fast-unaligned yes
+@c @set config-fft no
+@c @set config-lsp no
+@c @set config-lzo no
+@c @set config-mdct no
+@c @set config-pixelutils no
+@c @set config-network no
+@c @set config-rdft no
+@c @set config-autodetect no
+@c @set config-fontconfig no
+@set config-linux-perf yes
+@c @set config-memory-poisoning no
+@c @set config-neon-clobber-test no
+@c @set config-ossfuzz no
+@set config-pic yes
+@c @set config-thumb no
+@c @set config-valgrind-backtrace no
+@c @set config-xmm-clobber-test no
+@set config-bsfs yes
+@c @set config-decoders no
+@c @set config-encoders no
+@c @set config-hwaccels no
+@c @set config-parsers no
+@c @set config-indevs no
+@c @set config-outdevs no
+@c @set config-filters no
+@set config-demuxers yes
+@c @set config-muxers no
+@set config-protocols yes
+@c @set config-aandcttables no
+@c @set config-ac3dsp no
+@c @set config-adts-header no
+@c @set config-audio-frame-queue no
+@c @set config-audiodsp no
+@c @set config-blockdsp no
+@c @set config-bswapdsp no
+@c @set config-cabac no
+@c @set config-cbs no
+@c @set config-cbs-av1 no
+@c @set config-cbs-h264 no
+@c @set config-cbs-h265 no
+@c @set config-cbs-jpeg no
+@c @set config-cbs-mpeg2 no
+@c @set config-cbs-vp9 no
+@c @set config-dirac-parse no
+@c @set config-dnn no
+@c @set config-dvprofile no
+@c @set config-exif no
+@set config-faandct yes
+@set config-faanidct yes
+@set config-fdctdsp yes
+@c @set config-flacdsp no
+@c @set config-fmtconvert no
+@c @set config-frame-thread-encoder no
+@c @set config-g722dsp no
+@c @set config-golomb no
+@c @set config-gplv3 no
+@c @set config-h263dsp no
+@c @set config-h264chroma no
+@c @set config-h264dsp no
+@c @set config-h264parse no
+@c @set config-h264pred no
+@c @set config-h264qpel no
+@c @set config-hevcparse no
+@c @set config-hpeldsp no
+@c @set config-huffman no
+@c @set config-huffyuvdsp no
+@c @set config-huffyuvencdsp no
+@set config-idctdsp yes
+@c @set config-iirfilter no
+@c @set config-mdct15 no
+@c @set config-intrax8 no
+@set config-iso-media yes
+@c @set config-ividsp no
+@c @set config-jpegtables no
+@c @set config-lgplv3 no
+@c @set config-libx262 no
+@c @set config-llauddsp no
+@c @set config-llviddsp no
+@c @set config-llvidencdsp no
+@c @set config-lpc no
+@c @set config-lzf no
+@c @set config-me-cmp no
+@c @set config-mpeg-er no
+@c @set config-mpegaudio no
+@c @set config-mpegaudiodsp no
+@c @set config-mpegaudioheader no
+@c @set config-mpegvideo no
+@c @set config-mpegvideoenc no
+@c @set config-mss34dsp no
+@c @set config-pixblockdsp no
+@c @set config-qpeldsp no
+@c @set config-qsv no
+@c @set config-qsvdec no
+@c @set config-qsvenc no
+@c @set config-qsvvpp no
+@c @set config-rangecoder no
+@set config-riffdec yes
+@c @set config-riffenc no
+@c @set config-rtpdec no
+@c @set config-rtpenc-chain no
+@c @set config-rv34dsp no
+@c @set config-sinewin no
+@c @set config-snappy no
+@c @set config-srtp no
+@c @set config-startcode no
+@c @set config-texturedsp no
+@c @set config-texturedspenc no
+@c @set config-tpeldsp no
+@c @set config-vaapi-1 no
+@c @set config-vaapi-encode no
+@c @set config-vc1dsp no
+@c @set config-videodsp no
+@c @set config-vp3dsp no
+@c @set config-vp56dsp no
+@c @set config-vp8dsp no
+@c @set config-wma-freqs no
+@c @set config-wmv2dsp no
+@c @set config-aac-adtstoasc-bsf no
+@c @set config-av1-metadata-bsf no
+@c @set config-chomp-bsf no
+@c @set config-dump-extradata-bsf no
+@c @set config-dca-core-bsf no
+@c @set config-eac3-core-bsf no
+@c @set config-extract-extradata-bsf no
+@c @set config-filter-units-bsf no
+@c @set config-h264-metadata-bsf no
+@set config-h264-mp4toannexb-bsf yes
+@c @set config-h264-redundant-pps-bsf no
+@c @set config-hapqa-extract-bsf no
+@c @set config-hevc-metadata-bsf no
+@set config-hevc-mp4toannexb-bsf yes
+@c @set config-imx-dump-header-bsf no
+@c @set config-mjpeg2jpeg-bsf no
+@c @set config-mjpega-dump-header-bsf no
+@c @set config-mp3-header-decompress-bsf no
+@c @set config-mpeg2-metadata-bsf no
+@c @set config-mpeg4-unpack-bframes-bsf no
+@c @set config-mov2textsub-bsf no
+@c @set config-noise-bsf no
+@set config-null-bsf yes
+@c @set config-remove-extradata-bsf no
+@c @set config-text2movsub-bsf no
+@c @set config-trace-headers-bsf no
+@c @set config-vp9-metadata-bsf no
+@c @set config-vp9-raw-reorder-bsf no
+@c @set config-vp9-superframe-bsf no
+@c @set config-vp9-superframe-split-bsf no
+@c @set config-aasc-decoder no
+@c @set config-aic-decoder no
+@c @set config-alias-pix-decoder no
+@c @set config-amv-decoder no
+@c @set config-anm-decoder no
+@c @set config-ansi-decoder no
+@c @set config-apng-decoder no
+@c @set config-asv1-decoder no
+@c @set config-asv2-decoder no
+@c @set config-aura-decoder no
+@c @set config-aura2-decoder no
+@c @set config-avrp-decoder no
+@c @set config-avrn-decoder no
+@c @set config-avs-decoder no
+@c @set config-avui-decoder no
+@c @set config-ayuv-decoder no
+@c @set config-bethsoftvid-decoder no
+@c @set config-bfi-decoder no
+@c @set config-bink-decoder no
+@c @set config-bitpacked-decoder no
+@c @set config-bmp-decoder no
+@c @set config-bmv-video-decoder no
+@c @set config-brender-pix-decoder no
+@c @set config-c93-decoder no
+@c @set config-cavs-decoder no
+@c @set config-cdgraphics-decoder no
+@c @set config-cdxl-decoder no
+@c @set config-cfhd-decoder no
+@c @set config-cinepak-decoder no
+@c @set config-clearvideo-decoder no
+@c @set config-cljr-decoder no
+@c @set config-cllc-decoder no
+@c @set config-comfortnoise-decoder no
+@c @set config-cpia-decoder no
+@c @set config-cscd-decoder no
+@c @set config-cyuv-decoder no
+@c @set config-dds-decoder no
+@c @set config-dfa-decoder no
+@c @set config-dirac-decoder no
+@c @set config-dnxhd-decoder no
+@c @set config-dpx-decoder no
+@c @set config-dsicinvideo-decoder no
+@c @set config-dvaudio-decoder no
+@c @set config-dvvideo-decoder no
+@c @set config-dxa-decoder no
+@c @set config-dxtory-decoder no
+@c @set config-dxv-decoder no
+@c @set config-eacmv-decoder no
+@c @set config-eamad-decoder no
+@c @set config-eatgq-decoder no
+@c @set config-eatgv-decoder no
+@c @set config-eatqi-decoder no
+@c @set config-eightbps-decoder no
+@c @set config-eightsvx-exp-decoder no
+@c @set config-eightsvx-fib-decoder no
+@c @set config-escape124-decoder no
+@c @set config-escape130-decoder no
+@c @set config-exr-decoder no
+@c @set config-ffv1-decoder no
+@c @set config-ffvhuff-decoder no
+@c @set config-fic-decoder no
+@c @set config-fits-decoder no
+@c @set config-flashsv-decoder no
+@c @set config-flashsv2-decoder no
+@c @set config-flic-decoder no
+@c @set config-flv-decoder no
+@c @set config-fmvc-decoder no
+@c @set config-fourxm-decoder no
+@c @set config-fraps-decoder no
+@c @set config-frwu-decoder no
+@c @set config-g2m-decoder no
+@c @set config-gdv-decoder no
+@c @set config-gif-decoder no
+@c @set config-h261-decoder no
+@c @set config-h263-decoder no
+@c @set config-h263i-decoder no
+@c @set config-h263p-decoder no
+@c @set config-h263-v4l2m2m-decoder no
+@c @set config-h264-decoder no
+@c @set config-h264-crystalhd-decoder no
+@c @set config-h264-v4l2m2m-decoder no
+@c @set config-h264-mediacodec-decoder no
+@c @set config-h264-mmal-decoder no
+@c @set config-h264-qsv-decoder no
+@c @set config-h264-rkmpp-decoder no
+@c @set config-hap-decoder no
+@c @set config-hevc-decoder no
+@c @set config-hevc-qsv-decoder no
+@c @set config-hevc-rkmpp-decoder no
+@c @set config-hevc-v4l2m2m-decoder no
+@c @set config-hnm4-video-decoder no
+@c @set config-hq-hqa-decoder no
+@c @set config-hqx-decoder no
+@c @set config-huffyuv-decoder no
+@c @set config-idcin-decoder no
+@c @set config-iff-ilbm-decoder no
+@c @set config-imm4-decoder no
+@c @set config-indeo2-decoder no
+@c @set config-indeo3-decoder no
+@c @set config-indeo4-decoder no
+@c @set config-indeo5-decoder no
+@c @set config-interplay-video-decoder no
+@c @set config-jpeg2000-decoder no
+@c @set config-jpegls-decoder no
+@c @set config-jv-decoder no
+@c @set config-kgv1-decoder no
+@c @set config-kmvc-decoder no
+@c @set config-lagarith-decoder no
+@c @set config-loco-decoder no
+@c @set config-m101-decoder no
+@c @set config-magicyuv-decoder no
+@c @set config-mdec-decoder no
+@c @set config-mimic-decoder no
+@c @set config-mjpeg-decoder no
+@c @set config-mjpegb-decoder no
+@c @set config-mmvideo-decoder no
+@c @set config-motionpixels-decoder no
+@c @set config-mpeg1video-decoder no
+@c @set config-mpeg2video-decoder no
+@c @set config-mpeg4-decoder no
+@c @set config-mpeg4-crystalhd-decoder no
+@c @set config-mpeg4-v4l2m2m-decoder no
+@c @set config-mpeg4-mmal-decoder no
+@c @set config-mpegvideo-decoder no
+@c @set config-mpeg1-v4l2m2m-decoder no
+@c @set config-mpeg2-mmal-decoder no
+@c @set config-mpeg2-crystalhd-decoder no
+@c @set config-mpeg2-v4l2m2m-decoder no
+@c @set config-mpeg2-qsv-decoder no
+@c @set config-mpeg2-mediacodec-decoder no
+@c @set config-msa1-decoder no
+@c @set config-mscc-decoder no
+@c @set config-msmpeg4v1-decoder no
+@c @set config-msmpeg4v2-decoder no
+@c @set config-msmpeg4v3-decoder no
+@c @set config-msmpeg4-crystalhd-decoder no
+@c @set config-msrle-decoder no
+@c @set config-mss1-decoder no
+@c @set config-mss2-decoder no
+@c @set config-msvideo1-decoder no
+@c @set config-mszh-decoder no
+@c @set config-mts2-decoder no
+@c @set config-mvc1-decoder no
+@c @set config-mvc2-decoder no
+@c @set config-mwsc-decoder no
+@c @set config-mxpeg-decoder no
+@c @set config-nuv-decoder no
+@c @set config-paf-video-decoder no
+@c @set config-pam-decoder no
+@c @set config-pbm-decoder no
+@c @set config-pcx-decoder no
+@c @set config-pgm-decoder no
+@c @set config-pgmyuv-decoder no
+@c @set config-pictor-decoder no
+@c @set config-pixlet-decoder no
+@c @set config-png-decoder no
+@c @set config-ppm-decoder no
+@c @set config-prores-decoder no
+@c @set config-prosumer-decoder no
+@c @set config-psd-decoder no
+@c @set config-ptx-decoder no
+@c @set config-qdraw-decoder no
+@c @set config-qpeg-decoder no
+@c @set config-qtrle-decoder no
+@c @set config-r10k-decoder no
+@c @set config-r210-decoder no
+@c @set config-rasc-decoder no
+@c @set config-rawvideo-decoder no
+@c @set config-rl2-decoder no
+@c @set config-roq-decoder no
+@c @set config-rpza-decoder no
+@c @set config-rscc-decoder no
+@c @set config-rv10-decoder no
+@c @set config-rv20-decoder no
+@c @set config-rv30-decoder no
+@c @set config-rv40-decoder no
+@c @set config-s302m-decoder no
+@c @set config-sanm-decoder no
+@c @set config-scpr-decoder no
+@c @set config-screenpresso-decoder no
+@c @set config-sdx2-dpcm-decoder no
+@c @set config-sgi-decoder no
+@c @set config-sgirle-decoder no
+@c @set config-sheervideo-decoder no
+@c @set config-smacker-decoder no
+@c @set config-smc-decoder no
+@c @set config-smvjpeg-decoder no
+@c @set config-snow-decoder no
+@c @set config-sp5x-decoder no
+@c @set config-speedhq-decoder no
+@c @set config-srgc-decoder no
+@c @set config-sunrast-decoder no
+@c @set config-svq1-decoder no
+@c @set config-svq3-decoder no
+@c @set config-targa-decoder no
+@c @set config-targa-y216-decoder no
+@c @set config-tdsc-decoder no
+@c @set config-theora-decoder no
+@c @set config-thp-decoder no
+@c @set config-tiertexseqvideo-decoder no
+@c @set config-tiff-decoder no
+@c @set config-tmv-decoder no
+@c @set config-truemotion1-decoder no
+@c @set config-truemotion2-decoder no
+@c @set config-truemotion2rt-decoder no
+@c @set config-tscc-decoder no
+@c @set config-tscc2-decoder no
+@c @set config-txd-decoder no
+@c @set config-ulti-decoder no
+@c @set config-utvideo-decoder no
+@c @set config-v210-decoder no
+@c @set config-v210x-decoder no
+@c @set config-v308-decoder no
+@c @set config-v408-decoder no
+@c @set config-v410-decoder no
+@c @set config-vb-decoder no
+@c @set config-vble-decoder no
+@c @set config-vc1-decoder no
+@c @set config-vc1-crystalhd-decoder no
+@c @set config-vc1image-decoder no
+@c @set config-vc1-mmal-decoder no
+@c @set config-vc1-qsv-decoder no
+@c @set config-vc1-v4l2m2m-decoder no
+@c @set config-vcr1-decoder no
+@c @set config-vmdvideo-decoder no
+@c @set config-vmnc-decoder no
+@c @set config-vp3-decoder no
+@c @set config-vp5-decoder no
+@c @set config-vp6-decoder no
+@c @set config-vp6a-decoder no
+@c @set config-vp6f-decoder no
+@c @set config-vp7-decoder no
+@c @set config-vp8-decoder no
+@c @set config-vp8-rkmpp-decoder no
+@c @set config-vp8-v4l2m2m-decoder no
+@c @set config-vp9-decoder no
+@c @set config-vp9-rkmpp-decoder no
+@c @set config-vp9-v4l2m2m-decoder no
+@c @set config-vqa-decoder no
+@c @set config-webp-decoder no
+@c @set config-wcmv-decoder no
+@c @set config-wrapped-avframe-decoder no
+@c @set config-wmv1-decoder no
+@c @set config-wmv2-decoder no
+@c @set config-wmv3-decoder no
+@c @set config-wmv3-crystalhd-decoder no
+@c @set config-wmv3image-decoder no
+@c @set config-wnv1-decoder no
+@c @set config-xan-wc3-decoder no
+@c @set config-xan-wc4-decoder no
+@c @set config-xbm-decoder no
+@c @set config-xface-decoder no
+@c @set config-xl-decoder no
+@c @set config-xpm-decoder no
+@c @set config-xwd-decoder no
+@c @set config-y41p-decoder no
+@c @set config-ylc-decoder no
+@c @set config-yop-decoder no
+@c @set config-yuv4-decoder no
+@c @set config-zero12v-decoder no
+@c @set config-zerocodec-decoder no
+@c @set config-zlib-decoder no
+@c @set config-zmbv-decoder no
+@c @set config-aac-decoder no
+@c @set config-aac-fixed-decoder no
+@c @set config-aac-latm-decoder no
+@c @set config-ac3-decoder no
+@c @set config-ac3-fixed-decoder no
+@c @set config-alac-decoder no
+@c @set config-als-decoder no
+@c @set config-amrnb-decoder no
+@c @set config-amrwb-decoder no
+@c @set config-ape-decoder no
+@c @set config-aptx-decoder no
+@c @set config-aptx-hd-decoder no
+@c @set config-atrac1-decoder no
+@c @set config-atrac3-decoder no
+@c @set config-atrac3al-decoder no
+@c @set config-atrac3p-decoder no
+@c @set config-atrac3pal-decoder no
+@c @set config-atrac9-decoder no
+@c @set config-binkaudio-dct-decoder no
+@c @set config-binkaudio-rdft-decoder no
+@c @set config-bmv-audio-decoder no
+@c @set config-cook-decoder no
+@c @set config-dca-decoder no
+@c @set config-dolby-e-decoder no
+@c @set config-dsd-lsbf-decoder no
+@c @set config-dsd-msbf-decoder no
+@c @set config-dsd-lsbf-planar-decoder no
+@c @set config-dsd-msbf-planar-decoder no
+@c @set config-dsicinaudio-decoder no
+@c @set config-dss-sp-decoder no
+@c @set config-dst-decoder no
+@c @set config-eac3-decoder no
+@c @set config-evrc-decoder no
+@c @set config-ffwavesynth-decoder no
+@c @set config-flac-decoder no
+@c @set config-g723-1-decoder no
+@c @set config-g729-decoder no
+@c @set config-gsm-decoder no
+@c @set config-gsm-ms-decoder no
+@c @set config-iac-decoder no
+@c @set config-ilbc-decoder no
+@c @set config-imc-decoder no
+@c @set config-interplay-acm-decoder no
+@c @set config-mace3-decoder no
+@c @set config-mace6-decoder no
+@c @set config-metasound-decoder no
+@c @set config-mlp-decoder no
+@c @set config-mp1-decoder no
+@c @set config-mp1float-decoder no
+@c @set config-mp2-decoder no
+@c @set config-mp2float-decoder no
+@c @set config-mp3float-decoder no
+@c @set config-mp3-decoder no
+@c @set config-mp3adufloat-decoder no
+@c @set config-mp3adu-decoder no
+@c @set config-mp3on4float-decoder no
+@c @set config-mp3on4-decoder no
+@c @set config-mpc7-decoder no
+@c @set config-mpc8-decoder no
+@c @set config-nellymoser-decoder no
+@c @set config-on2avc-decoder no
+@c @set config-opus-decoder no
+@c @set config-paf-audio-decoder no
+@c @set config-qcelp-decoder no
+@c @set config-qdm2-decoder no
+@c @set config-qdmc-decoder no
+@c @set config-ra-144-decoder no
+@c @set config-ra-288-decoder no
+@c @set config-ralf-decoder no
+@c @set config-sbc-decoder no
+@c @set config-shorten-decoder no
+@c @set config-sipr-decoder no
+@c @set config-smackaud-decoder no
+@c @set config-sonic-decoder no
+@c @set config-tak-decoder no
+@c @set config-truehd-decoder no
+@c @set config-truespeech-decoder no
+@c @set config-tta-decoder no
+@c @set config-twinvq-decoder no
+@c @set config-vmdaudio-decoder no
+@c @set config-vorbis-decoder no
+@c @set config-wavpack-decoder no
+@c @set config-wmalossless-decoder no
+@c @set config-wmapro-decoder no
+@c @set config-wmav1-decoder no
+@c @set config-wmav2-decoder no
+@c @set config-wmavoice-decoder no
+@c @set config-ws-snd1-decoder no
+@c @set config-xma1-decoder no
+@c @set config-xma2-decoder no
+@c @set config-pcm-alaw-decoder no
+@c @set config-pcm-bluray-decoder no
+@c @set config-pcm-dvd-decoder no
+@c @set config-pcm-f16le-decoder no
+@c @set config-pcm-f24le-decoder no
+@c @set config-pcm-f32be-decoder no
+@c @set config-pcm-f32le-decoder no
+@c @set config-pcm-f64be-decoder no
+@c @set config-pcm-f64le-decoder no
+@c @set config-pcm-lxf-decoder no
+@c @set config-pcm-mulaw-decoder no
+@c @set config-pcm-s8-decoder no
+@c @set config-pcm-s8-planar-decoder no
+@c @set config-pcm-s16be-decoder no
+@c @set config-pcm-s16be-planar-decoder no
+@c @set config-pcm-s16le-decoder no
+@c @set config-pcm-s16le-planar-decoder no
+@c @set config-pcm-s24be-decoder no
+@c @set config-pcm-s24daud-decoder no
+@c @set config-pcm-s24le-decoder no
+@c @set config-pcm-s24le-planar-decoder no
+@c @set config-pcm-s32be-decoder no
+@c @set config-pcm-s32le-decoder no
+@c @set config-pcm-s32le-planar-decoder no
+@c @set config-pcm-s64be-decoder no
+@c @set config-pcm-s64le-decoder no
+@c @set config-pcm-u8-decoder no
+@c @set config-pcm-u16be-decoder no
+@c @set config-pcm-u16le-decoder no
+@c @set config-pcm-u24be-decoder no
+@c @set config-pcm-u24le-decoder no
+@c @set config-pcm-u32be-decoder no
+@c @set config-pcm-u32le-decoder no
+@c @set config-pcm-vidc-decoder no
+@c @set config-pcm-zork-decoder no
+@c @set config-gremlin-dpcm-decoder no
+@c @set config-interplay-dpcm-decoder no
+@c @set config-roq-dpcm-decoder no
+@c @set config-sol-dpcm-decoder no
+@c @set config-xan-dpcm-decoder no
+@c @set config-adpcm-4xm-decoder no
+@c @set config-adpcm-adx-decoder no
+@c @set config-adpcm-afc-decoder no
+@c @set config-adpcm-aica-decoder no
+@c @set config-adpcm-ct-decoder no
+@c @set config-adpcm-dtk-decoder no
+@c @set config-adpcm-ea-decoder no
+@c @set config-adpcm-ea-maxis-xa-decoder no
+@c @set config-adpcm-ea-r1-decoder no
+@c @set config-adpcm-ea-r2-decoder no
+@c @set config-adpcm-ea-r3-decoder no
+@c @set config-adpcm-ea-xas-decoder no
+@c @set config-adpcm-g722-decoder no
+@c @set config-adpcm-g726-decoder no
+@c @set config-adpcm-g726le-decoder no
+@c @set config-adpcm-ima-amv-decoder no
+@c @set config-adpcm-ima-apc-decoder no
+@c @set config-adpcm-ima-dat4-decoder no
+@c @set config-adpcm-ima-dk3-decoder no
+@c @set config-adpcm-ima-dk4-decoder no
+@c @set config-adpcm-ima-ea-eacs-decoder no
+@c @set config-adpcm-ima-ea-sead-decoder no
+@c @set config-adpcm-ima-iss-decoder no
+@c @set config-adpcm-ima-oki-decoder no
+@c @set config-adpcm-ima-qt-decoder no
+@c @set config-adpcm-ima-rad-decoder no
+@c @set config-adpcm-ima-smjpeg-decoder no
+@c @set config-adpcm-ima-wav-decoder no
+@c @set config-adpcm-ima-ws-decoder no
+@c @set config-adpcm-ms-decoder no
+@c @set config-adpcm-mtaf-decoder no
+@c @set config-adpcm-psx-decoder no
+@c @set config-adpcm-sbpro-2-decoder no
+@c @set config-adpcm-sbpro-3-decoder no
+@c @set config-adpcm-sbpro-4-decoder no
+@c @set config-adpcm-swf-decoder no
+@c @set config-adpcm-thp-decoder no
+@c @set config-adpcm-thp-le-decoder no
+@c @set config-adpcm-vima-decoder no
+@c @set config-adpcm-xa-decoder no
+@c @set config-adpcm-yamaha-decoder no
+@c @set config-ssa-decoder no
+@c @set config-ass-decoder no
+@c @set config-ccaption-decoder no
+@c @set config-dvbsub-decoder no
+@c @set config-dvdsub-decoder no
+@c @set config-jacosub-decoder no
+@c @set config-microdvd-decoder no
+@c @set config-movtext-decoder no
+@c @set config-mpl2-decoder no
+@c @set config-pgssub-decoder no
+@c @set config-pjs-decoder no
+@c @set config-realtext-decoder no
+@c @set config-sami-decoder no
+@c @set config-srt-decoder no
+@c @set config-stl-decoder no
+@c @set config-subrip-decoder no
+@c @set config-subviewer-decoder no
+@c @set config-subviewer1-decoder no
+@c @set config-text-decoder no
+@c @set config-vplayer-decoder no
+@c @set config-webvtt-decoder no
+@c @set config-xsub-decoder no
+@c @set config-aac-at-decoder no
+@c @set config-ac3-at-decoder no
+@c @set config-adpcm-ima-qt-at-decoder no
+@c @set config-alac-at-decoder no
+@c @set config-amr-nb-at-decoder no
+@c @set config-eac3-at-decoder no
+@c @set config-gsm-ms-at-decoder no
+@c @set config-ilbc-at-decoder no
+@c @set config-mp1-at-decoder no
+@c @set config-mp2-at-decoder no
+@c @set config-mp3-at-decoder no
+@c @set config-pcm-alaw-at-decoder no
+@c @set config-pcm-mulaw-at-decoder no
+@c @set config-qdmc-at-decoder no
+@c @set config-qdm2-at-decoder no
+@c @set config-libaom-av1-decoder no
+@c @set config-libcelt-decoder no
+@c @set config-libcodec2-decoder no
+@c @set config-libdavs2-decoder no
+@c @set config-libfdk-aac-decoder no
+@c @set config-libgsm-decoder no
+@c @set config-libgsm-ms-decoder no
+@c @set config-libilbc-decoder no
+@c @set config-libopencore-amrnb-decoder no
+@c @set config-libopencore-amrwb-decoder no
+@c @set config-libopenjpeg-decoder no
+@c @set config-libopus-decoder no
+@c @set config-librsvg-decoder no
+@c @set config-libspeex-decoder no
+@c @set config-libvorbis-decoder no
+@c @set config-libvpx-vp8-decoder no
+@c @set config-libvpx-vp9-decoder no
+@c @set config-libzvbi-teletext-decoder no
+@c @set config-bintext-decoder no
+@c @set config-xbin-decoder no
+@c @set config-idf-decoder no
+@c @set config-libopenh264-decoder no
+@c @set config-h264-cuvid-decoder no
+@c @set config-hevc-cuvid-decoder no
+@c @set config-hevc-mediacodec-decoder no
+@c @set config-mjpeg-cuvid-decoder no
+@c @set config-mpeg1-cuvid-decoder no
+@c @set config-mpeg2-cuvid-decoder no
+@c @set config-mpeg4-cuvid-decoder no
+@c @set config-mpeg4-mediacodec-decoder no
+@c @set config-vc1-cuvid-decoder no
+@c @set config-vp8-cuvid-decoder no
+@c @set config-vp8-mediacodec-decoder no
+@c @set config-vp8-qsv-decoder no
+@c @set config-vp9-cuvid-decoder no
+@c @set config-vp9-mediacodec-decoder no
+@c @set config-a64multi-encoder no
+@c @set config-a64multi5-encoder no
+@c @set config-alias-pix-encoder no
+@c @set config-amv-encoder no
+@c @set config-apng-encoder no
+@c @set config-asv1-encoder no
+@c @set config-asv2-encoder no
+@c @set config-avrp-encoder no
+@c @set config-avui-encoder no
+@c @set config-ayuv-encoder no
+@c @set config-bmp-encoder no
+@c @set config-cinepak-encoder no
+@c @set config-cljr-encoder no
+@c @set config-comfortnoise-encoder no
+@c @set config-dnxhd-encoder no
+@c @set config-dpx-encoder no
+@c @set config-dvvideo-encoder no
+@c @set config-ffv1-encoder no
+@c @set config-ffvhuff-encoder no
+@c @set config-fits-encoder no
+@c @set config-flashsv-encoder no
+@c @set config-flashsv2-encoder no
+@c @set config-flv-encoder no
+@c @set config-gif-encoder no
+@c @set config-h261-encoder no
+@c @set config-h263-encoder no
+@c @set config-h263p-encoder no
+@c @set config-hap-encoder no
+@c @set config-huffyuv-encoder no
+@c @set config-jpeg2000-encoder no
+@c @set config-jpegls-encoder no
+@c @set config-ljpeg-encoder no
+@c @set config-magicyuv-encoder no
+@c @set config-mjpeg-encoder no
+@c @set config-mpeg1video-encoder no
+@c @set config-mpeg2video-encoder no
+@c @set config-mpeg4-encoder no
+@c @set config-msmpeg4v2-encoder no
+@c @set config-msmpeg4v3-encoder no
+@c @set config-msvideo1-encoder no
+@c @set config-pam-encoder no
+@c @set config-pbm-encoder no
+@c @set config-pcx-encoder no
+@c @set config-pgm-encoder no
+@c @set config-pgmyuv-encoder no
+@c @set config-png-encoder no
+@c @set config-ppm-encoder no
+@c @set config-prores-encoder no
+@c @set config-prores-aw-encoder no
+@c @set config-prores-ks-encoder no
+@c @set config-qtrle-encoder no
+@c @set config-r10k-encoder no
+@c @set config-r210-encoder no
+@c @set config-rawvideo-encoder no
+@c @set config-roq-encoder no
+@c @set config-rv10-encoder no
+@c @set config-rv20-encoder no
+@c @set config-s302m-encoder no
+@c @set config-sgi-encoder no
+@c @set config-snow-encoder no
+@c @set config-sunrast-encoder no
+@c @set config-svq1-encoder no
+@c @set config-targa-encoder no
+@c @set config-tiff-encoder no
+@c @set config-utvideo-encoder no
+@c @set config-v210-encoder no
+@c @set config-v308-encoder no
+@c @set config-v408-encoder no
+@c @set config-v410-encoder no
+@c @set config-vc2-encoder no
+@c @set config-wrapped-avframe-encoder no
+@c @set config-wmv1-encoder no
+@c @set config-wmv2-encoder no
+@c @set config-xbm-encoder no
+@c @set config-xface-encoder no
+@c @set config-xwd-encoder no
+@c @set config-y41p-encoder no
+@c @set config-yuv4-encoder no
+@c @set config-zlib-encoder no
+@c @set config-zmbv-encoder no
+@c @set config-aac-encoder no
+@c @set config-ac3-encoder no
+@c @set config-ac3-fixed-encoder no
+@c @set config-alac-encoder no
+@c @set config-aptx-encoder no
+@c @set config-aptx-hd-encoder no
+@c @set config-dca-encoder no
+@c @set config-eac3-encoder no
+@c @set config-flac-encoder no
+@c @set config-g723-1-encoder no
+@c @set config-mlp-encoder no
+@c @set config-mp2-encoder no
+@c @set config-mp2fixed-encoder no
+@c @set config-nellymoser-encoder no
+@c @set config-opus-encoder no
+@c @set config-ra-144-encoder no
+@c @set config-sbc-encoder no
+@c @set config-sonic-encoder no
+@c @set config-sonic-ls-encoder no
+@c @set config-truehd-encoder no
+@c @set config-tta-encoder no
+@c @set config-vorbis-encoder no
+@c @set config-wavpack-encoder no
+@c @set config-wmav1-encoder no
+@c @set config-wmav2-encoder no
+@c @set config-pcm-alaw-encoder no
+@c @set config-pcm-f32be-encoder no
+@c @set config-pcm-f32le-encoder no
+@c @set config-pcm-f64be-encoder no
+@c @set config-pcm-f64le-encoder no
+@c @set config-pcm-mulaw-encoder no
+@c @set config-pcm-s8-encoder no
+@c @set config-pcm-s8-planar-encoder no
+@c @set config-pcm-s16be-encoder no
+@c @set config-pcm-s16be-planar-encoder no
+@c @set config-pcm-s16le-encoder no
+@c @set config-pcm-s16le-planar-encoder no
+@c @set config-pcm-s24be-encoder no
+@c @set config-pcm-s24daud-encoder no
+@c @set config-pcm-s24le-encoder no
+@c @set config-pcm-s24le-planar-encoder no
+@c @set config-pcm-s32be-encoder no
+@c @set config-pcm-s32le-encoder no
+@c @set config-pcm-s32le-planar-encoder no
+@c @set config-pcm-s64be-encoder no
+@c @set config-pcm-s64le-encoder no
+@c @set config-pcm-u8-encoder no
+@c @set config-pcm-u16be-encoder no
+@c @set config-pcm-u16le-encoder no
+@c @set config-pcm-u24be-encoder no
+@c @set config-pcm-u24le-encoder no
+@c @set config-pcm-u32be-encoder no
+@c @set config-pcm-u32le-encoder no
+@c @set config-pcm-vidc-encoder no
+@c @set config-roq-dpcm-encoder no
+@c @set config-adpcm-adx-encoder no
+@c @set config-adpcm-g722-encoder no
+@c @set config-adpcm-g726-encoder no
+@c @set config-adpcm-g726le-encoder no
+@c @set config-adpcm-ima-qt-encoder no
+@c @set config-adpcm-ima-wav-encoder no
+@c @set config-adpcm-ms-encoder no
+@c @set config-adpcm-swf-encoder no
+@c @set config-adpcm-yamaha-encoder no
+@c @set config-ssa-encoder no
+@c @set config-ass-encoder no
+@c @set config-dvbsub-encoder no
+@c @set config-dvdsub-encoder no
+@c @set config-movtext-encoder no
+@c @set config-srt-encoder no
+@c @set config-subrip-encoder no
+@c @set config-text-encoder no
+@c @set config-webvtt-encoder no
+@c @set config-xsub-encoder no
+@c @set config-aac-at-encoder no
+@c @set config-alac-at-encoder no
+@c @set config-ilbc-at-encoder no
+@c @set config-pcm-alaw-at-encoder no
+@c @set config-pcm-mulaw-at-encoder no
+@c @set config-libaom-av1-encoder no
+@c @set config-libcodec2-encoder no
+@c @set config-libfdk-aac-encoder no
+@c @set config-libgsm-encoder no
+@c @set config-libgsm-ms-encoder no
+@c @set config-libilbc-encoder no
+@c @set config-libmp3lame-encoder no
+@c @set config-libopencore-amrnb-encoder no
+@c @set config-libopenjpeg-encoder no
+@c @set config-libopus-encoder no
+@c @set config-libshine-encoder no
+@c @set config-libspeex-encoder no
+@c @set config-libtheora-encoder no
+@c @set config-libtwolame-encoder no
+@c @set config-libvo-amrwbenc-encoder no
+@c @set config-libvorbis-encoder no
+@c @set config-libvpx-vp8-encoder no
+@c @set config-libvpx-vp9-encoder no
+@c @set config-libwavpack-encoder no
+@c @set config-libwebp-anim-encoder no
+@c @set config-libwebp-encoder no
+@c @set config-libx262-encoder no
+@c @set config-libx264-encoder no
+@c @set config-libx264rgb-encoder no
+@c @set config-libx265-encoder no
+@c @set config-libxavs-encoder no
+@c @set config-libxavs2-encoder no
+@c @set config-libxvid-encoder no
+@c @set config-h263-v4l2m2m-encoder no
+@c @set config-libopenh264-encoder no
+@c @set config-h264-amf-encoder no
+@c @set config-h264-nvenc-encoder no
+@c @set config-h264-omx-encoder no
+@c @set config-h264-qsv-encoder no
+@c @set config-h264-v4l2m2m-encoder no
+@c @set config-h264-vaapi-encoder no
+@c @set config-h264-videotoolbox-encoder no
+@c @set config-nvenc-encoder no
+@c @set config-nvenc-h264-encoder no
+@c @set config-nvenc-hevc-encoder no
+@c @set config-hevc-amf-encoder no
+@c @set config-hevc-nvenc-encoder no
+@c @set config-hevc-qsv-encoder no
+@c @set config-hevc-v4l2m2m-encoder no
+@c @set config-hevc-vaapi-encoder no
+@c @set config-hevc-videotoolbox-encoder no
+@c @set config-libkvazaar-encoder no
+@c @set config-mjpeg-qsv-encoder no
+@c @set config-mjpeg-vaapi-encoder no
+@c @set config-mpeg2-qsv-encoder no
+@c @set config-mpeg2-vaapi-encoder no
+@c @set config-mpeg4-v4l2m2m-encoder no
+@c @set config-vp8-v4l2m2m-encoder no
+@c @set config-vp8-vaapi-encoder no
+@c @set config-vp9-vaapi-encoder no
+@c @set config-h263-vaapi-hwaccel no
+@c @set config-h263-videotoolbox-hwaccel no
+@c @set config-h264-d3d11va-hwaccel no
+@c @set config-h264-d3d11va2-hwaccel no
+@c @set config-h264-dxva2-hwaccel no
+@c @set config-h264-nvdec-hwaccel no
+@c @set config-h264-vaapi-hwaccel no
+@c @set config-h264-vdpau-hwaccel no
+@c @set config-h264-videotoolbox-hwaccel no
+@c @set config-hevc-d3d11va-hwaccel no
+@c @set config-hevc-d3d11va2-hwaccel no
+@c @set config-hevc-dxva2-hwaccel no
+@c @set config-hevc-nvdec-hwaccel no
+@c @set config-hevc-vaapi-hwaccel no
+@c @set config-hevc-vdpau-hwaccel no
+@c @set config-hevc-videotoolbox-hwaccel no
+@c @set config-mjpeg-nvdec-hwaccel no
+@c @set config-mjpeg-vaapi-hwaccel no
+@c @set config-mpeg1-nvdec-hwaccel no
+@c @set config-mpeg1-vdpau-hwaccel no
+@c @set config-mpeg1-videotoolbox-hwaccel no
+@c @set config-mpeg1-xvmc-hwaccel no
+@c @set config-mpeg2-d3d11va-hwaccel no
+@c @set config-mpeg2-d3d11va2-hwaccel no
+@c @set config-mpeg2-nvdec-hwaccel no
+@c @set config-mpeg2-dxva2-hwaccel no
+@c @set config-mpeg2-vaapi-hwaccel no
+@c @set config-mpeg2-vdpau-hwaccel no
+@c @set config-mpeg2-videotoolbox-hwaccel no
+@c @set config-mpeg2-xvmc-hwaccel no
+@c @set config-mpeg4-nvdec-hwaccel no
+@c @set config-mpeg4-vaapi-hwaccel no
+@c @set config-mpeg4-vdpau-hwaccel no
+@c @set config-mpeg4-videotoolbox-hwaccel no
+@c @set config-vc1-d3d11va-hwaccel no
+@c @set config-vc1-d3d11va2-hwaccel no
+@c @set config-vc1-dxva2-hwaccel no
+@c @set config-vc1-nvdec-hwaccel no
+@c @set config-vc1-vaapi-hwaccel no
+@c @set config-vc1-vdpau-hwaccel no
+@c @set config-vp8-nvdec-hwaccel no
+@c @set config-vp8-vaapi-hwaccel no
+@c @set config-vp9-d3d11va-hwaccel no
+@c @set config-vp9-d3d11va2-hwaccel no
+@c @set config-vp9-dxva2-hwaccel no
+@c @set config-vp9-nvdec-hwaccel no
+@c @set config-vp9-vaapi-hwaccel no
+@c @set config-wmv3-d3d11va-hwaccel no
+@c @set config-wmv3-d3d11va2-hwaccel no
+@c @set config-wmv3-dxva2-hwaccel no
+@c @set config-wmv3-nvdec-hwaccel no
+@c @set config-wmv3-vaapi-hwaccel no
+@c @set config-wmv3-vdpau-hwaccel no
+@c @set config-aac-parser no
+@c @set config-aac-latm-parser no
+@c @set config-ac3-parser no
+@c @set config-adx-parser no
+@c @set config-av1-parser no
+@c @set config-avs2-parser no
+@c @set config-bmp-parser no
+@c @set config-cavsvideo-parser no
+@c @set config-cook-parser no
+@c @set config-dca-parser no
+@c @set config-dirac-parser no
+@c @set config-dnxhd-parser no
+@c @set config-dpx-parser no
+@c @set config-dvaudio-parser no
+@c @set config-dvbsub-parser no
+@c @set config-dvdsub-parser no
+@c @set config-dvd-nav-parser no
+@c @set config-flac-parser no
+@c @set config-g729-parser no
+@c @set config-gsm-parser no
+@c @set config-h261-parser no
+@c @set config-h263-parser no
+@c @set config-h264-parser no
+@c @set config-hevc-parser no
+@c @set config-mjpeg-parser no
+@c @set config-mlp-parser no
+@c @set config-mpeg4video-parser no
+@c @set config-mpegaudio-parser no
+@c @set config-mpegvideo-parser no
+@c @set config-opus-parser no
+@c @set config-png-parser no
+@c @set config-pnm-parser no
+@c @set config-rv30-parser no
+@c @set config-rv40-parser no
+@c @set config-sbc-parser no
+@c @set config-sipr-parser no
+@c @set config-tak-parser no
+@c @set config-vc1-parser no
+@c @set config-vorbis-parser no
+@c @set config-vp3-parser no
+@c @set config-vp8-parser no
+@c @set config-vp9-parser no
+@c @set config-xma-parser no
+@c @set config-alsa-indev no
+@c @set config-android-camera-indev no
+@c @set config-avfoundation-indev no
+@c @set config-bktr-indev no
+@c @set config-decklink-indev no
+@c @set config-libndi-newtek-indev no
+@c @set config-dshow-indev no
+@c @set config-fbdev-indev no
+@c @set config-gdigrab-indev no
+@c @set config-iec61883-indev no
+@c @set config-jack-indev no
+@c @set config-kmsgrab-indev no
+@c @set config-lavfi-indev no
+@c @set config-openal-indev no
+@c @set config-oss-indev no
+@c @set config-pulse-indev no
+@c @set config-sndio-indev no
+@c @set config-v4l2-indev no
+@c @set config-vfwcap-indev no
+@c @set config-xcbgrab-indev no
+@c @set config-libcdio-indev no
+@c @set config-libdc1394-indev no
+@c @set config-alsa-outdev no
+@c @set config-caca-outdev no
+@c @set config-decklink-outdev no
+@c @set config-libndi-newtek-outdev no
+@c @set config-fbdev-outdev no
+@c @set config-opengl-outdev no
+@c @set config-oss-outdev no
+@c @set config-pulse-outdev no
+@c @set config-sdl2-outdev no
+@c @set config-sndio-outdev no
+@c @set config-v4l2-outdev no
+@c @set config-xv-outdev no
+@c @set config-abench-filter no
+@c @set config-acompressor-filter no
+@c @set config-acontrast-filter no
+@c @set config-acopy-filter no
+@c @set config-acue-filter no
+@c @set config-acrossfade-filter no
+@c @set config-acrossover-filter no
+@c @set config-acrusher-filter no
+@c @set config-adeclick-filter no
+@c @set config-adeclip-filter no
+@c @set config-adelay-filter no
+@c @set config-aderivative-filter no
+@c @set config-aecho-filter no
+@c @set config-aemphasis-filter no
+@c @set config-aeval-filter no
+@c @set config-afade-filter no
+@c @set config-afftdn-filter no
+@c @set config-afftfilt-filter no
+@c @set config-afir-filter no
+@c @set config-aformat-filter no
+@c @set config-agate-filter no
+@c @set config-aiir-filter no
+@c @set config-aintegral-filter no
+@c @set config-ainterleave-filter no
+@c @set config-alimiter-filter no
+@c @set config-allpass-filter no
+@c @set config-aloop-filter no
+@c @set config-amerge-filter no
+@c @set config-ametadata-filter no
+@c @set config-amix-filter no
+@c @set config-amultiply-filter no
+@c @set config-anequalizer-filter no
+@c @set config-anull-filter no
+@c @set config-apad-filter no
+@c @set config-aperms-filter no
+@c @set config-aphaser-filter no
+@c @set config-apulsator-filter no
+@c @set config-arealtime-filter no
+@c @set config-aresample-filter no
+@c @set config-areverse-filter no
+@c @set config-aselect-filter no
+@c @set config-asendcmd-filter no
+@c @set config-asetnsamples-filter no
+@c @set config-asetpts-filter no
+@c @set config-asetrate-filter no
+@c @set config-asettb-filter no
+@c @set config-ashowinfo-filter no
+@c @set config-asidedata-filter no
+@c @set config-asplit-filter no
+@c @set config-astats-filter no
+@c @set config-astreamselect-filter no
+@c @set config-atempo-filter no
+@c @set config-atrim-filter no
+@c @set config-azmq-filter no
+@c @set config-bandpass-filter no
+@c @set config-bandreject-filter no
+@c @set config-bass-filter no
+@c @set config-biquad-filter no
+@c @set config-bs2b-filter no
+@c @set config-channelmap-filter no
+@c @set config-channelsplit-filter no
+@c @set config-chorus-filter no
+@c @set config-compand-filter no
+@c @set config-compensationdelay-filter no
+@c @set config-crossfeed-filter no
+@c @set config-crystalizer-filter no
+@c @set config-dcshift-filter no
+@c @set config-drmeter-filter no
+@c @set config-dynaudnorm-filter no
+@c @set config-earwax-filter no
+@c @set config-ebur128-filter no
+@c @set config-equalizer-filter no
+@c @set config-extrastereo-filter no
+@c @set config-firequalizer-filter no
+@c @set config-flanger-filter no
+@c @set config-haas-filter no
+@c @set config-hdcd-filter no
+@c @set config-headphone-filter no
+@c @set config-highpass-filter no
+@c @set config-highshelf-filter no
+@c @set config-join-filter no
+@c @set config-ladspa-filter no
+@c @set config-loudnorm-filter no
+@c @set config-lowpass-filter no
+@c @set config-lowshelf-filter no
+@c @set config-lv2-filter no
+@c @set config-mcompand-filter no
+@c @set config-pan-filter no
+@c @set config-replaygain-filter no
+@c @set config-resample-filter no
+@c @set config-rubberband-filter no
+@c @set config-sidechaincompress-filter no
+@c @set config-sidechaingate-filter no
+@c @set config-silencedetect-filter no
+@c @set config-silenceremove-filter no
+@c @set config-sofalizer-filter no
+@c @set config-stereotools-filter no
+@c @set config-stereowiden-filter no
+@c @set config-superequalizer-filter no
+@c @set config-surround-filter no
+@c @set config-treble-filter no
+@c @set config-tremolo-filter no
+@c @set config-vibrato-filter no
+@c @set config-volume-filter no
+@c @set config-volumedetect-filter no
+@c @set config-aevalsrc-filter no
+@c @set config-anoisesrc-filter no
+@c @set config-anullsrc-filter no
+@c @set config-flite-filter no
+@c @set config-hilbert-filter no
+@c @set config-sinc-filter no
+@c @set config-sine-filter no
+@c @set config-anullsink-filter no
+@c @set config-alphaextract-filter no
+@c @set config-alphamerge-filter no
+@c @set config-amplify-filter no
+@c @set config-ass-filter no
+@c @set config-atadenoise-filter no
+@c @set config-avgblur-filter no
+@c @set config-avgblur-opencl-filter no
+@c @set config-bbox-filter no
+@c @set config-bench-filter no
+@c @set config-bitplanenoise-filter no
+@c @set config-blackdetect-filter no
+@c @set config-blackframe-filter no
+@c @set config-blend-filter no
+@c @set config-bm3d-filter no
+@c @set config-boxblur-filter no
+@c @set config-boxblur-opencl-filter no
+@c @set config-bwdif-filter no
+@c @set config-chromahold-filter no
+@c @set config-chromakey-filter no
+@c @set config-ciescope-filter no
+@c @set config-codecview-filter no
+@c @set config-colorbalance-filter no
+@c @set config-colorchannelmixer-filter no
+@c @set config-colorkey-filter no
+@c @set config-colorlevels-filter no
+@c @set config-colormatrix-filter no
+@c @set config-colorspace-filter no
+@c @set config-convolution-filter no
+@c @set config-convolution-opencl-filter no
+@c @set config-convolve-filter no
+@c @set config-copy-filter no
+@c @set config-coreimage-filter no
+@c @set config-cover-rect-filter no
+@c @set config-crop-filter no
+@c @set config-cropdetect-filter no
+@c @set config-cue-filter no
+@c @set config-curves-filter no
+@c @set config-datascope-filter no
+@c @set config-dctdnoiz-filter no
+@c @set config-deband-filter no
+@c @set config-deblock-filter no
+@c @set config-decimate-filter no
+@c @set config-deconvolve-filter no
+@c @set config-deflate-filter no
+@c @set config-deflicker-filter no
+@c @set config-deinterlace-qsv-filter no
+@c @set config-deinterlace-vaapi-filter no
+@c @set config-dejudder-filter no
+@c @set config-delogo-filter no
+@c @set config-denoise-vaapi-filter no
+@c @set config-deshake-filter no
+@c @set config-despill-filter no
+@c @set config-detelecine-filter no
+@c @set config-dilation-filter no
+@c @set config-dilation-opencl-filter no
+@c @set config-displace-filter no
+@c @set config-doubleweave-filter no
+@c @set config-drawbox-filter no
+@c @set config-drawgraph-filter no
+@c @set config-drawgrid-filter no
+@c @set config-drawtext-filter no
+@c @set config-edgedetect-filter no
+@c @set config-elbg-filter no
+@c @set config-entropy-filter no
+@c @set config-eq-filter no
+@c @set config-erosion-filter no
+@c @set config-erosion-opencl-filter no
+@c @set config-extractplanes-filter no
+@c @set config-fade-filter no
+@c @set config-fftdnoiz-filter no
+@c @set config-fftfilt-filter no
+@c @set config-field-filter no
+@c @set config-fieldhint-filter no
+@c @set config-fieldmatch-filter no
+@c @set config-fieldorder-filter no
+@c @set config-fillborders-filter no
+@c @set config-find-rect-filter no
+@c @set config-floodfill-filter no
+@c @set config-format-filter no
+@c @set config-fps-filter no
+@c @set config-framepack-filter no
+@c @set config-framerate-filter no
+@c @set config-framestep-filter no
+@c @set config-frei0r-filter no
+@c @set config-fspp-filter no
+@c @set config-gblur-filter no
+@c @set config-geq-filter no
+@c @set config-gradfun-filter no
+@c @set config-graphmonitor-filter no
+@c @set config-greyedge-filter no
+@c @set config-haldclut-filter no
+@c @set config-hflip-filter no
+@c @set config-histeq-filter no
+@c @set config-histogram-filter no
+@c @set config-hqdn3d-filter no
+@c @set config-hqx-filter no
+@c @set config-hstack-filter no
+@c @set config-hue-filter no
+@c @set config-hwdownload-filter no
+@c @set config-hwmap-filter no
+@c @set config-hwupload-filter no
+@c @set config-hwupload-cuda-filter no
+@c @set config-hysteresis-filter no
+@c @set config-idet-filter no
+@c @set config-il-filter no
+@c @set config-inflate-filter no
+@c @set config-interlace-filter no
+@c @set config-interleave-filter no
+@c @set config-kerndeint-filter no
+@c @set config-lenscorrection-filter no
+@c @set config-lensfun-filter no
+@c @set config-libvmaf-filter no
+@c @set config-limiter-filter no
+@c @set config-loop-filter no
+@c @set config-lumakey-filter no
+@c @set config-lut-filter no
+@c @set config-lut1d-filter no
+@c @set config-lut2-filter no
+@c @set config-lut3d-filter no
+@c @set config-lutrgb-filter no
+@c @set config-lutyuv-filter no
+@c @set config-maskedclamp-filter no
+@c @set config-maskedmerge-filter no
+@c @set config-mcdeint-filter no
+@c @set config-mergeplanes-filter no
+@c @set config-mestimate-filter no
+@c @set config-metadata-filter no
+@c @set config-midequalizer-filter no
+@c @set config-minterpolate-filter no
+@c @set config-mix-filter no
+@c @set config-mpdecimate-filter no
+@c @set config-negate-filter no
+@c @set config-nlmeans-filter no
+@c @set config-nnedi-filter no
+@c @set config-noformat-filter no
+@c @set config-noise-filter no
+@c @set config-normalize-filter no
+@c @set config-null-filter no
+@c @set config-ocr-filter no
+@c @set config-ocv-filter no
+@c @set config-oscilloscope-filter no
+@c @set config-overlay-filter no
+@c @set config-overlay-opencl-filter no
+@c @set config-overlay-qsv-filter no
+@c @set config-owdenoise-filter no
+@c @set config-pad-filter no
+@c @set config-palettegen-filter no
+@c @set config-paletteuse-filter no
+@c @set config-perms-filter no
+@c @set config-perspective-filter no
+@c @set config-phase-filter no
+@c @set config-pixdesctest-filter no
+@c @set config-pixscope-filter no
+@c @set config-pp-filter no
+@c @set config-pp7-filter no
+@c @set config-premultiply-filter no
+@c @set config-prewitt-filter no
+@c @set config-prewitt-opencl-filter no
+@c @set config-procamp-vaapi-filter no
+@c @set config-program-opencl-filter no
+@c @set config-pseudocolor-filter no
+@c @set config-psnr-filter no
+@c @set config-pullup-filter no
+@c @set config-qp-filter no
+@c @set config-random-filter no
+@c @set config-readeia608-filter no
+@c @set config-readvitc-filter no
+@c @set config-realtime-filter no
+@c @set config-remap-filter no
+@c @set config-removegrain-filter no
+@c @set config-removelogo-filter no
+@c @set config-repeatfields-filter no
+@c @set config-reverse-filter no
+@c @set config-roberts-filter no
+@c @set config-roberts-opencl-filter no
+@c @set config-rotate-filter no
+@c @set config-sab-filter no
+@c @set config-scale-filter no
+@c @set config-scale-cuda-filter no
+@c @set config-scale-npp-filter no
+@c @set config-scale-qsv-filter no
+@c @set config-scale-vaapi-filter no
+@c @set config-scale2ref-filter no
+@c @set config-select-filter no
+@c @set config-selectivecolor-filter no
+@c @set config-sendcmd-filter no
+@c @set config-separatefields-filter no
+@c @set config-setdar-filter no
+@c @set config-setfield-filter no
+@c @set config-setparams-filter no
+@c @set config-setpts-filter no
+@c @set config-setrange-filter no
+@c @set config-setsar-filter no
+@c @set config-settb-filter no
+@c @set config-sharpness-vaapi-filter no
+@c @set config-showinfo-filter no
+@c @set config-showpalette-filter no
+@c @set config-shuffleframes-filter no
+@c @set config-shuffleplanes-filter no
+@c @set config-sidedata-filter no
+@c @set config-signalstats-filter no
+@c @set config-signature-filter no
+@c @set config-smartblur-filter no
+@c @set config-sobel-filter no
+@c @set config-sobel-opencl-filter no
+@c @set config-split-filter no
+@c @set config-spp-filter no
+@c @set config-sr-filter no
+@c @set config-ssim-filter no
+@c @set config-stereo3d-filter no
+@c @set config-streamselect-filter no
+@c @set config-subtitles-filter no
+@c @set config-super2xsai-filter no
+@c @set config-swaprect-filter no
+@c @set config-swapuv-filter no
+@c @set config-tblend-filter no
+@c @set config-telecine-filter no
+@c @set config-threshold-filter no
+@c @set config-thumbnail-filter no
+@c @set config-thumbnail-cuda-filter no
+@c @set config-tile-filter no
+@c @set config-tinterlace-filter no
+@c @set config-tlut2-filter no
+@c @set config-tmix-filter no
+@c @set config-tonemap-filter no
+@c @set config-tonemap-opencl-filter no
+@c @set config-transpose-filter no
+@c @set config-transpose-npp-filter no
+@c @set config-trim-filter no
+@c @set config-unpremultiply-filter no
+@c @set config-unsharp-filter no
+@c @set config-unsharp-opencl-filter no
+@c @set config-uspp-filter no
+@c @set config-vaguedenoiser-filter no
+@c @set config-vectorscope-filter no
+@c @set config-vflip-filter no
+@c @set config-vfrdet-filter no
+@c @set config-vibrance-filter no
+@c @set config-vidstabdetect-filter no
+@c @set config-vidstabtransform-filter no
+@c @set config-vignette-filter no
+@c @set config-vmafmotion-filter no
+@c @set config-vpp-qsv-filter no
+@c @set config-vstack-filter no
+@c @set config-w3fdif-filter no
+@c @set config-waveform-filter no
+@c @set config-weave-filter no
+@c @set config-xbr-filter no
+@c @set config-xstack-filter no
+@c @set config-yadif-filter no
+@c @set config-yadif-cuda-filter no
+@c @set config-zmq-filter no
+@c @set config-zoompan-filter no
+@c @set config-zscale-filter no
+@c @set config-allrgb-filter no
+@c @set config-allyuv-filter no
+@c @set config-cellauto-filter no
+@c @set config-color-filter no
+@c @set config-coreimagesrc-filter no
+@c @set config-frei0r-src-filter no
+@c @set config-haldclutsrc-filter no
+@c @set config-life-filter no
+@c @set config-mandelbrot-filter no
+@c @set config-mptestsrc-filter no
+@c @set config-nullsrc-filter no
+@c @set config-openclsrc-filter no
+@c @set config-pal75bars-filter no
+@c @set config-pal100bars-filter no
+@c @set config-rgbtestsrc-filter no
+@c @set config-smptebars-filter no
+@c @set config-smptehdbars-filter no
+@c @set config-testsrc-filter no
+@c @set config-testsrc2-filter no
+@c @set config-yuvtestsrc-filter no
+@c @set config-nullsink-filter no
+@c @set config-abitscope-filter no
+@c @set config-adrawgraph-filter no
+@c @set config-agraphmonitor-filter no
+@c @set config-ahistogram-filter no
+@c @set config-aphasemeter-filter no
+@c @set config-avectorscope-filter no
+@c @set config-concat-filter no
+@c @set config-showcqt-filter no
+@c @set config-showfreqs-filter no
+@c @set config-showspectrum-filter no
+@c @set config-showspectrumpic-filter no
+@c @set config-showvolume-filter no
+@c @set config-showwaves-filter no
+@c @set config-showwavespic-filter no
+@c @set config-spectrumsynth-filter no
+@c @set config-amovie-filter no
+@c @set config-movie-filter no
+@c @set config-afifo-filter no
+@c @set config-fifo-filter no
+@c @set config-aa-demuxer no
+@c @set config-aac-demuxer no
+@c @set config-ac3-demuxer no
+@c @set config-acm-demuxer no
+@c @set config-act-demuxer no
+@c @set config-adf-demuxer no
+@c @set config-adp-demuxer no
+@c @set config-ads-demuxer no
+@c @set config-adx-demuxer no
+@c @set config-aea-demuxer no
+@c @set config-afc-demuxer no
+@c @set config-aiff-demuxer no
+@c @set config-aix-demuxer no
+@c @set config-amr-demuxer no
+@c @set config-amrnb-demuxer no
+@c @set config-amrwb-demuxer no
+@c @set config-anm-demuxer no
+@c @set config-apc-demuxer no
+@c @set config-ape-demuxer no
+@c @set config-apng-demuxer no
+@c @set config-aptx-demuxer no
+@c @set config-aptx-hd-demuxer no
+@c @set config-aqtitle-demuxer no
+@c @set config-asf-demuxer no
+@c @set config-asf-o-demuxer no
+@c @set config-ass-demuxer no
+@c @set config-ast-demuxer no
+@c @set config-au-demuxer no
+@c @set config-avi-demuxer no
+@c @set config-avisynth-demuxer no
+@c @set config-avr-demuxer no
+@c @set config-avs-demuxer no
+@c @set config-avs2-demuxer no
+@c @set config-bethsoftvid-demuxer no
+@c @set config-bfi-demuxer no
+@c @set config-bintext-demuxer no
+@c @set config-bink-demuxer no
+@c @set config-bit-demuxer no
+@c @set config-bmv-demuxer no
+@c @set config-bfstm-demuxer no
+@c @set config-brstm-demuxer no
+@c @set config-boa-demuxer no
+@c @set config-c93-demuxer no
+@c @set config-caf-demuxer no
+@c @set config-cavsvideo-demuxer no
+@c @set config-cdg-demuxer no
+@c @set config-cdxl-demuxer no
+@c @set config-cine-demuxer no
+@c @set config-codec2-demuxer no
+@c @set config-codec2raw-demuxer no
+@c @set config-concat-demuxer no
+@c @set config-dash-demuxer no
+@c @set config-data-demuxer no
+@c @set config-daud-demuxer no
+@c @set config-dcstr-demuxer no
+@c @set config-dfa-demuxer no
+@c @set config-dirac-demuxer no
+@c @set config-dnxhd-demuxer no
+@c @set config-dsf-demuxer no
+@c @set config-dsicin-demuxer no
+@c @set config-dss-demuxer no
+@c @set config-dts-demuxer no
+@c @set config-dtshd-demuxer no
+@c @set config-dv-demuxer no
+@c @set config-dvbsub-demuxer no
+@c @set config-dvbtxt-demuxer no
+@c @set config-dxa-demuxer no
+@c @set config-ea-demuxer no
+@c @set config-ea-cdata-demuxer no
+@c @set config-eac3-demuxer no
+@c @set config-epaf-demuxer no
+@c @set config-ffmetadata-demuxer no
+@c @set config-filmstrip-demuxer no
+@c @set config-fits-demuxer no
+@c @set config-flac-demuxer no
+@c @set config-flic-demuxer no
+@c @set config-flv-demuxer no
+@c @set config-live-flv-demuxer no
+@c @set config-fourxm-demuxer no
+@c @set config-frm-demuxer no
+@c @set config-fsb-demuxer no
+@c @set config-g722-demuxer no
+@c @set config-g723-1-demuxer no
+@c @set config-g726-demuxer no
+@c @set config-g726le-demuxer no
+@c @set config-g729-demuxer no
+@c @set config-gdv-demuxer no
+@c @set config-genh-demuxer no
+@c @set config-gif-demuxer no
+@c @set config-gsm-demuxer no
+@c @set config-gxf-demuxer no
+@c @set config-h261-demuxer no
+@c @set config-h263-demuxer no
+@c @set config-h264-demuxer no
+@c @set config-hevc-demuxer no
+@c @set config-hls-demuxer no
+@c @set config-hnm-demuxer no
+@c @set config-ico-demuxer no
+@c @set config-idcin-demuxer no
+@c @set config-idf-demuxer no
+@c @set config-iff-demuxer no
+@c @set config-ilbc-demuxer no
+@c @set config-image2-demuxer no
+@c @set config-image2pipe-demuxer no
+@c @set config-image2-alias-pix-demuxer no
+@c @set config-image2-brender-pix-demuxer no
+@c @set config-ingenient-demuxer no
+@c @set config-ipmovie-demuxer no
+@c @set config-ircam-demuxer no
+@c @set config-iss-demuxer no
+@c @set config-iv8-demuxer no
+@c @set config-ivf-demuxer no
+@c @set config-ivr-demuxer no
+@c @set config-jacosub-demuxer no
+@c @set config-jv-demuxer no
+@c @set config-lmlm4-demuxer no
+@c @set config-loas-demuxer no
+@c @set config-lrc-demuxer no
+@c @set config-lvf-demuxer no
+@c @set config-lxf-demuxer no
+@c @set config-m4v-demuxer no
+@c @set config-matroska-demuxer no
+@c @set config-mgsts-demuxer no
+@c @set config-microdvd-demuxer no
+@c @set config-mjpeg-demuxer no
+@c @set config-mjpeg-2000-demuxer no
+@c @set config-mlp-demuxer no
+@c @set config-mlv-demuxer no
+@c @set config-mm-demuxer no
+@c @set config-mmf-demuxer no
+@set config-mov-demuxer yes
+@c @set config-mp3-demuxer no
+@c @set config-mpc-demuxer no
+@c @set config-mpc8-demuxer no
+@c @set config-mpegps-demuxer no
+@c @set config-mpegts-demuxer no
+@c @set config-mpegtsraw-demuxer no
+@c @set config-mpegvideo-demuxer no
+@c @set config-mpjpeg-demuxer no
+@c @set config-mpl2-demuxer no
+@c @set config-mpsub-demuxer no
+@c @set config-msf-demuxer no
+@c @set config-msnwc-tcp-demuxer no
+@c @set config-mtaf-demuxer no
+@c @set config-mtv-demuxer no
+@c @set config-musx-demuxer no
+@c @set config-mv-demuxer no
+@c @set config-mvi-demuxer no
+@c @set config-mxf-demuxer no
+@c @set config-mxg-demuxer no
+@c @set config-nc-demuxer no
+@c @set config-nistsphere-demuxer no
+@c @set config-nsp-demuxer no
+@c @set config-nsv-demuxer no
+@c @set config-nut-demuxer no
+@c @set config-nuv-demuxer no
+@c @set config-ogg-demuxer no
+@c @set config-oma-demuxer no
+@c @set config-paf-demuxer no
+@c @set config-pcm-alaw-demuxer no
+@c @set config-pcm-mulaw-demuxer no
+@c @set config-pcm-vidc-demuxer no
+@c @set config-pcm-f64be-demuxer no
+@c @set config-pcm-f64le-demuxer no
+@c @set config-pcm-f32be-demuxer no
+@c @set config-pcm-f32le-demuxer no
+@c @set config-pcm-s32be-demuxer no
+@c @set config-pcm-s32le-demuxer no
+@c @set config-pcm-s24be-demuxer no
+@c @set config-pcm-s24le-demuxer no
+@c @set config-pcm-s16be-demuxer no
+@c @set config-pcm-s16le-demuxer no
+@c @set config-pcm-s8-demuxer no
+@c @set config-pcm-u32be-demuxer no
+@c @set config-pcm-u32le-demuxer no
+@c @set config-pcm-u24be-demuxer no
+@c @set config-pcm-u24le-demuxer no
+@c @set config-pcm-u16be-demuxer no
+@c @set config-pcm-u16le-demuxer no
+@c @set config-pcm-u8-demuxer no
+@c @set config-pjs-demuxer no
+@c @set config-pmp-demuxer no
+@c @set config-pva-demuxer no
+@c @set config-pvf-demuxer no
+@c @set config-qcp-demuxer no
+@c @set config-r3d-demuxer no
+@c @set config-rawvideo-demuxer no
+@c @set config-realtext-demuxer no
+@c @set config-redspark-demuxer no
+@c @set config-rl2-demuxer no
+@c @set config-rm-demuxer no
+@c @set config-roq-demuxer no
+@c @set config-rpl-demuxer no
+@c @set config-rsd-demuxer no
+@c @set config-rso-demuxer no
+@c @set config-rtp-demuxer no
+@c @set config-rtsp-demuxer no
+@c @set config-s337m-demuxer no
+@c @set config-sami-demuxer no
+@c @set config-sap-demuxer no
+@c @set config-sbc-demuxer no
+@c @set config-sbg-demuxer no
+@c @set config-scc-demuxer no
+@c @set config-sdp-demuxer no
+@c @set config-sdr2-demuxer no
+@c @set config-sds-demuxer no
+@c @set config-sdx-demuxer no
+@c @set config-segafilm-demuxer no
+@c @set config-ser-demuxer no
+@c @set config-shorten-demuxer no
+@c @set config-siff-demuxer no
+@c @set config-sln-demuxer no
+@c @set config-smacker-demuxer no
+@c @set config-smjpeg-demuxer no
+@c @set config-smush-demuxer no
+@c @set config-sol-demuxer no
+@c @set config-sox-demuxer no
+@c @set config-spdif-demuxer no
+@c @set config-srt-demuxer no
+@c @set config-str-demuxer no
+@c @set config-stl-demuxer no
+@c @set config-subviewer1-demuxer no
+@c @set config-subviewer-demuxer no
+@c @set config-sup-demuxer no
+@c @set config-svag-demuxer no
+@c @set config-swf-demuxer no
+@c @set config-tak-demuxer no
+@c @set config-tedcaptions-demuxer no
+@c @set config-thp-demuxer no
+@c @set config-threedostr-demuxer no
+@c @set config-tiertexseq-demuxer no
+@c @set config-tmv-demuxer no
+@c @set config-truehd-demuxer no
+@c @set config-tta-demuxer no
+@c @set config-txd-demuxer no
+@c @set config-tty-demuxer no
+@c @set config-ty-demuxer no
+@c @set config-v210-demuxer no
+@c @set config-v210x-demuxer no
+@c @set config-vag-demuxer no
+@c @set config-vc1-demuxer no
+@c @set config-vc1t-demuxer no
+@c @set config-vivo-demuxer no
+@c @set config-vmd-demuxer no
+@c @set config-vobsub-demuxer no
+@c @set config-voc-demuxer no
+@c @set config-vpk-demuxer no
+@c @set config-vplayer-demuxer no
+@c @set config-vqf-demuxer no
+@c @set config-w64-demuxer no
+@set config-wav-demuxer yes
+@c @set config-wc3-demuxer no
+@c @set config-webm-dash-manifest-demuxer no
+@c @set config-webvtt-demuxer no
+@c @set config-wsaud-demuxer no
+@c @set config-wsd-demuxer no
+@c @set config-wsvqa-demuxer no
+@c @set config-wtv-demuxer no
+@c @set config-wve-demuxer no
+@c @set config-wv-demuxer no
+@c @set config-xa-demuxer no
+@c @set config-xbin-demuxer no
+@c @set config-xmv-demuxer no
+@c @set config-xvag-demuxer no
+@c @set config-xwma-demuxer no
+@c @set config-yop-demuxer no
+@c @set config-yuv4mpegpipe-demuxer no
+@c @set config-image-bmp-pipe-demuxer no
+@c @set config-image-dds-pipe-demuxer no
+@c @set config-image-dpx-pipe-demuxer no
+@c @set config-image-exr-pipe-demuxer no
+@c @set config-image-j2k-pipe-demuxer no
+@c @set config-image-jpeg-pipe-demuxer no
+@c @set config-image-jpegls-pipe-demuxer no
+@c @set config-image-pam-pipe-demuxer no
+@c @set config-image-pbm-pipe-demuxer no
+@c @set config-image-pcx-pipe-demuxer no
+@c @set config-image-pgmyuv-pipe-demuxer no
+@c @set config-image-pgm-pipe-demuxer no
+@c @set config-image-pictor-pipe-demuxer no
+@c @set config-image-png-pipe-demuxer no
+@c @set config-image-ppm-pipe-demuxer no
+@c @set config-image-psd-pipe-demuxer no
+@c @set config-image-qdraw-pipe-demuxer no
+@c @set config-image-sgi-pipe-demuxer no
+@c @set config-image-svg-pipe-demuxer no
+@c @set config-image-sunrast-pipe-demuxer no
+@c @set config-image-tiff-pipe-demuxer no
+@c @set config-image-webp-pipe-demuxer no
+@c @set config-image-xpm-pipe-demuxer no
+@c @set config-image-xwd-pipe-demuxer no
+@c @set config-libgme-demuxer no
+@c @set config-libmodplug-demuxer no
+@c @set config-libopenmpt-demuxer no
+@c @set config-vapoursynth-demuxer no
+@c @set config-a64-muxer no
+@c @set config-ac3-muxer no
+@c @set config-adts-muxer no
+@c @set config-adx-muxer no
+@c @set config-aiff-muxer no
+@c @set config-amr-muxer no
+@c @set config-apng-muxer no
+@c @set config-aptx-muxer no
+@c @set config-aptx-hd-muxer no
+@c @set config-asf-muxer no
+@c @set config-ass-muxer no
+@c @set config-ast-muxer no
+@c @set config-asf-stream-muxer no
+@c @set config-au-muxer no
+@c @set config-avi-muxer no
+@c @set config-avm2-muxer no
+@c @set config-avs2-muxer no
+@c @set config-bit-muxer no
+@c @set config-caf-muxer no
+@c @set config-cavsvideo-muxer no
+@c @set config-codec2-muxer no
+@c @set config-codec2raw-muxer no
+@c @set config-crc-muxer no
+@c @set config-dash-muxer no
+@c @set config-data-muxer no
+@c @set config-daud-muxer no
+@c @set config-dirac-muxer no
+@c @set config-dnxhd-muxer no
+@c @set config-dts-muxer no
+@c @set config-dv-muxer no
+@c @set config-eac3-muxer no
+@c @set config-f4v-muxer no
+@c @set config-ffmetadata-muxer no
+@c @set config-fifo-muxer no
+@c @set config-fifo-test-muxer no
+@c @set config-filmstrip-muxer no
+@c @set config-fits-muxer no
+@c @set config-flac-muxer no
+@c @set config-flv-muxer no
+@c @set config-framecrc-muxer no
+@c @set config-framehash-muxer no
+@c @set config-framemd5-muxer no
+@c @set config-g722-muxer no
+@c @set config-g723-1-muxer no
+@c @set config-g726-muxer no
+@c @set config-g726le-muxer no
+@c @set config-gif-muxer no
+@c @set config-gsm-muxer no
+@c @set config-gxf-muxer no
+@c @set config-h261-muxer no
+@c @set config-h263-muxer no
+@c @set config-h264-muxer no
+@c @set config-hash-muxer no
+@c @set config-hds-muxer no
+@c @set config-hevc-muxer no
+@c @set config-hls-muxer no
+@c @set config-ico-muxer no
+@c @set config-ilbc-muxer no
+@c @set config-image2-muxer no
+@c @set config-image2pipe-muxer no
+@c @set config-ipod-muxer no
+@c @set config-ircam-muxer no
+@c @set config-ismv-muxer no
+@c @set config-ivf-muxer no
+@c @set config-jacosub-muxer no
+@c @set config-latm-muxer no
+@c @set config-lrc-muxer no
+@c @set config-m4v-muxer no
+@c @set config-md5-muxer no
+@c @set config-matroska-muxer no
+@c @set config-matroska-audio-muxer no
+@c @set config-microdvd-muxer no
+@c @set config-mjpeg-muxer no
+@c @set config-mlp-muxer no
+@c @set config-mmf-muxer no
+@c @set config-mov-muxer no
+@c @set config-mp2-muxer no
+@c @set config-mp3-muxer no
+@c @set config-mp4-muxer no
+@c @set config-mpeg1system-muxer no
+@c @set config-mpeg1vcd-muxer no
+@c @set config-mpeg1video-muxer no
+@c @set config-mpeg2dvd-muxer no
+@c @set config-mpeg2svcd-muxer no
+@c @set config-mpeg2video-muxer no
+@c @set config-mpeg2vob-muxer no
+@c @set config-mpegts-muxer no
+@c @set config-mpjpeg-muxer no
+@c @set config-mxf-muxer no
+@c @set config-mxf-d10-muxer no
+@c @set config-mxf-opatom-muxer no
+@c @set config-null-muxer no
+@c @set config-nut-muxer no
+@c @set config-oga-muxer no
+@c @set config-ogg-muxer no
+@c @set config-ogv-muxer no
+@c @set config-oma-muxer no
+@c @set config-opus-muxer no
+@c @set config-pcm-alaw-muxer no
+@c @set config-pcm-mulaw-muxer no
+@c @set config-pcm-vidc-muxer no
+@c @set config-pcm-f64be-muxer no
+@c @set config-pcm-f64le-muxer no
+@c @set config-pcm-f32be-muxer no
+@c @set config-pcm-f32le-muxer no
+@c @set config-pcm-s32be-muxer no
+@c @set config-pcm-s32le-muxer no
+@c @set config-pcm-s24be-muxer no
+@c @set config-pcm-s24le-muxer no
+@c @set config-pcm-s16be-muxer no
+@c @set config-pcm-s16le-muxer no
+@c @set config-pcm-s8-muxer no
+@c @set config-pcm-u32be-muxer no
+@c @set config-pcm-u32le-muxer no
+@c @set config-pcm-u24be-muxer no
+@c @set config-pcm-u24le-muxer no
+@c @set config-pcm-u16be-muxer no
+@c @set config-pcm-u16le-muxer no
+@c @set config-pcm-u8-muxer no
+@c @set config-psp-muxer no
+@c @set config-rawvideo-muxer no
+@c @set config-rm-muxer no
+@c @set config-roq-muxer no
+@c @set config-rso-muxer no
+@c @set config-rtp-muxer no
+@c @set config-rtp-mpegts-muxer no
+@c @set config-rtsp-muxer no
+@c @set config-sap-muxer no
+@c @set config-sbc-muxer no
+@c @set config-scc-muxer no
+@c @set config-segafilm-muxer no
+@c @set config-segment-muxer no
+@c @set config-stream-segment-muxer no
+@c @set config-singlejpeg-muxer no
+@c @set config-smjpeg-muxer no
+@c @set config-smoothstreaming-muxer no
+@c @set config-sox-muxer no
+@c @set config-spx-muxer no
+@c @set config-spdif-muxer no
+@c @set config-srt-muxer no
+@c @set config-sup-muxer no
+@c @set config-swf-muxer no
+@c @set config-tee-muxer no
+@c @set config-tg2-muxer no
+@c @set config-tgp-muxer no
+@c @set config-mkvtimestamp-v2-muxer no
+@c @set config-truehd-muxer no
+@c @set config-tta-muxer no
+@c @set config-uncodedframecrc-muxer no
+@c @set config-vc1-muxer no
+@c @set config-vc1t-muxer no
+@c @set config-voc-muxer no
+@c @set config-w64-muxer no
+@c @set config-wav-muxer no
+@c @set config-webm-muxer no
+@c @set config-webm-dash-manifest-muxer no
+@c @set config-webm-chunk-muxer no
+@c @set config-webp-muxer no
+@c @set config-webvtt-muxer no
+@c @set config-wtv-muxer no
+@c @set config-wv-muxer no
+@c @set config-yuv4mpegpipe-muxer no
+@c @set config-chromaprint-muxer no
+@c @set config-async-protocol no
+@c @set config-bluray-protocol no
+@c @set config-cache-protocol no
+@c @set config-concat-protocol no
+@c @set config-crypto-protocol no
+@c @set config-data-protocol no
+@c @set config-ffrtmpcrypt-protocol no
+@c @set config-ffrtmphttp-protocol no
+@set config-file-protocol yes
+@c @set config-ftp-protocol no
+@c @set config-gopher-protocol no
+@c @set config-hls-protocol no
+@c @set config-http-protocol no
+@c @set config-httpproxy-protocol no
+@c @set config-https-protocol no
+@c @set config-icecast-protocol no
+@c @set config-mmsh-protocol no
+@c @set config-mmst-protocol no
+@c @set config-md5-protocol no
+@c @set config-pipe-protocol no
+@c @set config-prompeg-protocol no
+@c @set config-rtmp-protocol no
+@c @set config-rtmpe-protocol no
+@c @set config-rtmps-protocol no
+@c @set config-rtmpt-protocol no
+@c @set config-rtmpte-protocol no
+@c @set config-rtmpts-protocol no
+@c @set config-rtp-protocol no
+@c @set config-sctp-protocol no
+@c @set config-srtp-protocol no
+@c @set config-subfile-protocol no
+@c @set config-tee-protocol no
+@c @set config-tcp-protocol no
+@c @set config-tls-protocol no
+@c @set config-udp-protocol no
+@c @set config-udplite-protocol no
+@c @set config-unix-protocol no
+@c @set config-librtmp-protocol no
+@c @set config-librtmpe-protocol no
+@c @set config-librtmps-protocol no
+@c @set config-librtmpt-protocol no
+@c @set config-librtmpte-protocol no
+@c @set config-libsrt-protocol no
+@c @set config-libssh-protocol no
+@c @set config-libsmbclient-protocol no
diff -uparN ffmpeg-4.1/doc/examples/pc-uninstalled/libavcodec-uninstalled.pc ffmpeg-y/doc/examples/pc-uninstalled/libavcodec-uninstalled.pc
--- ffmpeg-4.1/doc/examples/pc-uninstalled/libavcodec-uninstalled.pc	1970-01-01 08:00:00.000000000 +0800
+++ ffmpeg-y/doc/examples/pc-uninstalled/libavcodec-uninstalled.pc	2019-06-29 11:49:36.861017668 +0800
@@ -0,0 +1,12 @@
+prefix=
+exec_prefix=
+libdir=${pcfiledir}/../../../libavcodec
+includedir=.
+
+Name: libavcodec
+Description: FFmpeg codec library
+Version: 58.35.100
+Requires:  libavutil >= 56.22.100
+Conflicts:
+Libs: -L${libdir} -Wl,-rpath,${libdir} -lavcodec -lm
+Cflags: -I${includedir}
diff -uparN ffmpeg-4.1/doc/examples/pc-uninstalled/libavformat-uninstalled.pc ffmpeg-y/doc/examples/pc-uninstalled/libavformat-uninstalled.pc
--- ffmpeg-4.1/doc/examples/pc-uninstalled/libavformat-uninstalled.pc	1970-01-01 08:00:00.000000000 +0800
+++ ffmpeg-y/doc/examples/pc-uninstalled/libavformat-uninstalled.pc	2019-06-29 11:49:36.861017668 +0800
@@ -0,0 +1,12 @@
+prefix=
+exec_prefix=
+libdir=${pcfiledir}/../../../libavformat
+includedir=.
+
+Name: libavformat
+Description: FFmpeg container format library
+Version: 58.20.100
+Requires:  libavcodec >= 58.35.100,  libavutil >= 56.22.100
+Conflicts:
+Libs: -L${libdir} -Wl,-rpath,${libdir} -lavformat -lm
+Cflags: -I${includedir}
diff -uparN ffmpeg-4.1/doc/examples/pc-uninstalled/libavutil-uninstalled.pc ffmpeg-y/doc/examples/pc-uninstalled/libavutil-uninstalled.pc
--- ffmpeg-4.1/doc/examples/pc-uninstalled/libavutil-uninstalled.pc	1970-01-01 08:00:00.000000000 +0800
+++ ffmpeg-y/doc/examples/pc-uninstalled/libavutil-uninstalled.pc	2019-06-29 11:49:36.861017668 +0800
@@ -0,0 +1,12 @@
+prefix=
+exec_prefix=
+libdir=${pcfiledir}/../../../libavutil
+includedir=.
+
+Name: libavutil
+Description: FFmpeg utility library
+Version: 56.22.100
+Requires: 
+Conflicts:
+Libs: -L${libdir} -Wl,-rpath,${libdir} -lavutil -lm
+Cflags: -I${includedir}
diff -uparN ffmpeg-4.1/enable_decoder_config.sh ffmpeg-y/enable_decoder_config.sh
--- ffmpeg-4.1/enable_decoder_config.sh	1970-01-01 08:00:00.000000000 +0800
+++ ffmpeg-y/enable_decoder_config.sh	2019-06-29 11:49:36.857017668 +0800
@@ -0,0 +1,58 @@
+echo `pwd`
+CUR_DIR=$(pwd)
+HIBERRY_DIR=$(pwd)/../../..
+source ${HIBERRY_DIR}/cfg.mak
+
+configure_attr=" --prefix=./install \
+	--enable-small \
+	--disable-armv5te \
+	--disable-armv6 \
+	--disable-armv6t2 \
+	--disable-linux-perf\
+    --enable-static \
+    --disable-encoders \
+    --disable-muxers \
+    --disable-avfilter \
+    --disable-swscale \
+    --disable-demuxers \
+    --enable-demuxer=mov \
+	--disable-protocols \
+    --enable-protocol=file \
+	--disable-bsfs \
+    --enable-bsf=h264_mp4toannexb \
+    --enable-bsf=hevc_mp4toannexb \
+	--disable-parsers \
+    --disable-iconv \
+    --disable-avdevice \
+    --disable-network \
+    --disable-decoders \
+    --enable-decoder=h264 \
+    --enable-decoder=hevc \
+    --enable-decoder=mjpeg \
+    --enable-asm \
+    --disable-filters \
+    --enable-inline-asm \
+    --enable-neon \
+    --disable-shared \
+    --disable-debug \
+    --disable-doc \
+    --disable-swresample \
+    --disable-programs \
+    --disable-symver \
+    --target-os=linux \
+    --enable-cross-compile \
+    --cross-prefix=${CFG_SDK_TOOLCHAIN} "
+
+if [ ${CFG_CHIP_TYPE} == "hi3518ev300" ]; then
+echo "hi3518ev300 =? ${CFG_CHIP_TYPE}"
+configure_attr+=" --arch=arm --cpu=cortex-a7 "
+echo ${configure_attr} --extra-cflags="-mfloat-abi=softfp -mfpu=neon-vfpv4"
+./configure  ${configure_attr} --extra-cflags="-mfloat-abi=softfp -mfpu=neon-vfpv4 -fPIC"
+fi
+
+if [ ${CFG_CHIP_TYPE} == "hi3516cv300" ]; then
+echo "hi3516cv300 =? ${CFG_CHIP_TYPE}"
+configure_attr+=" --arch=arm --cpu=arm926ej-s --cross-prefix=${CFG_SDK_TOOLCHAIN} "
+./configure  ${configure_attr} --extra-cflags="-nostdlib -nostdinc -fPIC -mlong-calls" 
+fi
+
diff -uparN ffmpeg-4.1/ffbuild/config.fate ffmpeg-y/ffbuild/config.fate
--- ffmpeg-4.1/ffbuild/config.fate	1970-01-01 08:00:00.000000000 +0800
+++ ffmpeg-y/ffbuild/config.fate	2019-06-29 11:49:36.833017667 +0800
@@ -0,0 +1 @@
+config:c:armv7a:cortex-a7:linux:gcc 6.3.0 (HC&C V100R002C00B032_20190114):--prefix=./install --enable-cross-compile --disable-doc --disable-htmlpages --target-os=linux --enable-static --disable-shared --disable-debug --disable-iconv --enable-small --disable-network --disable-filters --disable-devices --disable-programs --disable-swresample --disable-swscale --disable-avdevice --disable-postproc --disable-avfilter --disable-protocols --disable-pthreads --disable-runtime-cpudetect --disable-everything --enable-pic --enable-protocol=file --disable-muxers --enable-demuxer=mov --disable-neon --disable-inline-asm --disable-asm --disable-armv6 --disable-armv6t2 --disable-armv5te --disable-vfp --disable-hardcoded-tables --disable-mediacodec --enable-bsf=h264_mp4toannexb --enable-bsf=hevc_mp4toannexb --disable-pixelutils --enable-demuxer=wav --disable-gpl --cpu=cortex-a7 --arch=armv7-a --cross-prefix=arm-himix100-linux-
diff -uparN ffmpeg-4.1/ffbuild/config.log ffmpeg-y/ffbuild/config.log
--- ffmpeg-4.1/ffbuild/config.log	1970-01-01 08:00:00.000000000 +0800
+++ ffmpeg-y/ffbuild/config.log	2019-06-29 11:49:36.833017667 +0800
@@ -0,0 +1,14583 @@
+# ./configure --prefix=./install --enable-cross-compile --disable-doc --disable-htmlpages --target-os=linux --enable-static --disable-shared --disable-debug --disable-iconv --enable-small --disable-network --disable-filters --disable-devices --disable-programs --disable-swresample --disable-swscale --disable-avdevice --disable-postproc --disable-avfilter --disable-protocols --disable-pthreads --disable-runtime-cpudetect --disable-everything --enable-pic --enable-protocol=file --disable-muxers --enable-demuxer=mov --disable-neon --disable-inline-asm --disable-asm --disable-armv6 --disable-armv6t2 --disable-armv5te --disable-vfp --disable-hardcoded-tables --disable-mediacodec --enable-bsf=h264_mp4toannexb --enable-bsf=hevc_mp4toannexb --disable-pixelutils --enable-demuxer=wav --disable-gpl --cpu=cortex-a7 --arch=armv7-a --cross-prefix=arm-himix100-linux-
+ALL_COMPONENTS='
+    
+    aac_adtstoasc_bsf
+av1_metadata_bsf
+chomp_bsf
+dump_extradata_bsf
+dca_core_bsf
+eac3_core_bsf
+extract_extradata_bsf
+filter_units_bsf
+h264_metadata_bsf
+h264_mp4toannexb_bsf
+h264_redundant_pps_bsf
+hapqa_extract_bsf
+hevc_metadata_bsf
+hevc_mp4toannexb_bsf
+imx_dump_header_bsf
+mjpeg2jpeg_bsf
+mjpega_dump_header_bsf
+mp3_header_decompress_bsf
+mpeg2_metadata_bsf
+mpeg4_unpack_bframes_bsf
+mov2textsub_bsf
+noise_bsf
+null_bsf
+remove_extradata_bsf
+text2movsub_bsf
+trace_headers_bsf
+vp9_metadata_bsf
+vp9_raw_reorder_bsf
+vp9_superframe_bsf
+vp9_superframe_split_bsf
+    aasc_decoder
+aic_decoder
+alias_pix_decoder
+amv_decoder
+anm_decoder
+ansi_decoder
+apng_decoder
+asv1_decoder
+asv2_decoder
+aura_decoder
+aura2_decoder
+avrp_decoder
+avrn_decoder
+avs_decoder
+avui_decoder
+ayuv_decoder
+bethsoftvid_decoder
+bfi_decoder
+bink_decoder
+bitpacked_decoder
+bmp_decoder
+bmv_video_decoder
+brender_pix_decoder
+c93_decoder
+cavs_decoder
+cdgraphics_decoder
+cdxl_decoder
+cfhd_decoder
+cinepak_decoder
+clearvideo_decoder
+cljr_decoder
+cllc_decoder
+comfortnoise_decoder
+cpia_decoder
+cscd_decoder
+cyuv_decoder
+dds_decoder
+dfa_decoder
+dirac_decoder
+dnxhd_decoder
+dpx_decoder
+dsicinvideo_decoder
+dvaudio_decoder
+dvvideo_decoder
+dxa_decoder
+dxtory_decoder
+dxv_decoder
+eacmv_decoder
+eamad_decoder
+eatgq_decoder
+eatgv_decoder
+eatqi_decoder
+eightbps_decoder
+eightsvx_exp_decoder
+eightsvx_fib_decoder
+escape124_decoder
+escape130_decoder
+exr_decoder
+ffv1_decoder
+ffvhuff_decoder
+fic_decoder
+fits_decoder
+flashsv_decoder
+flashsv2_decoder
+flic_decoder
+flv_decoder
+fmvc_decoder
+fourxm_decoder
+fraps_decoder
+frwu_decoder
+g2m_decoder
+gdv_decoder
+gif_decoder
+h261_decoder
+h263_decoder
+h263i_decoder
+h263p_decoder
+h263_v4l2m2m_decoder
+h264_decoder
+h264_crystalhd_decoder
+h264_v4l2m2m_decoder
+h264_mediacodec_decoder
+h264_mmal_decoder
+h264_qsv_decoder
+h264_rkmpp_decoder
+hap_decoder
+hevc_decoder
+hevc_qsv_decoder
+hevc_rkmpp_decoder
+hevc_v4l2m2m_decoder
+hnm4_video_decoder
+hq_hqa_decoder
+hqx_decoder
+huffyuv_decoder
+idcin_decoder
+iff_ilbm_decoder
+imm4_decoder
+indeo2_decoder
+indeo3_decoder
+indeo4_decoder
+indeo5_decoder
+interplay_video_decoder
+jpeg2000_decoder
+jpegls_decoder
+jv_decoder
+kgv1_decoder
+kmvc_decoder
+lagarith_decoder
+loco_decoder
+m101_decoder
+magicyuv_decoder
+mdec_decoder
+mimic_decoder
+mjpeg_decoder
+mjpegb_decoder
+mmvideo_decoder
+motionpixels_decoder
+mpeg1video_decoder
+mpeg2video_decoder
+mpeg4_decoder
+mpeg4_crystalhd_decoder
+mpeg4_v4l2m2m_decoder
+mpeg4_mmal_decoder
+mpegvideo_decoder
+mpeg1_v4l2m2m_decoder
+mpeg2_mmal_decoder
+mpeg2_crystalhd_decoder
+mpeg2_v4l2m2m_decoder
+mpeg2_qsv_decoder
+mpeg2_mediacodec_decoder
+msa1_decoder
+mscc_decoder
+msmpeg4v1_decoder
+msmpeg4v2_decoder
+msmpeg4v3_decoder
+msmpeg4_crystalhd_decoder
+msrle_decoder
+mss1_decoder
+mss2_decoder
+msvideo1_decoder
+mszh_decoder
+mts2_decoder
+mvc1_decoder
+mvc2_decoder
+mwsc_decoder
+mxpeg_decoder
+nuv_decoder
+paf_video_decoder
+pam_decoder
+pbm_decoder
+pcx_decoder
+pgm_decoder
+pgmyuv_decoder
+pictor_decoder
+pixlet_decoder
+png_decoder
+ppm_decoder
+prores_decoder
+prosumer_decoder
+psd_decoder
+ptx_decoder
+qdraw_decoder
+qpeg_decoder
+qtrle_decoder
+r10k_decoder
+r210_decoder
+rasc_decoder
+rawvideo_decoder
+rl2_decoder
+roq_decoder
+rpza_decoder
+rscc_decoder
+rv10_decoder
+rv20_decoder
+rv30_decoder
+rv40_decoder
+s302m_decoder
+sanm_decoder
+scpr_decoder
+screenpresso_decoder
+sdx2_dpcm_decoder
+sgi_decoder
+sgirle_decoder
+sheervideo_decoder
+smacker_decoder
+smc_decoder
+smvjpeg_decoder
+snow_decoder
+sp5x_decoder
+speedhq_decoder
+srgc_decoder
+sunrast_decoder
+svq1_decoder
+svq3_decoder
+targa_decoder
+targa_y216_decoder
+tdsc_decoder
+theora_decoder
+thp_decoder
+tiertexseqvideo_decoder
+tiff_decoder
+tmv_decoder
+truemotion1_decoder
+truemotion2_decoder
+truemotion2rt_decoder
+tscc_decoder
+tscc2_decoder
+txd_decoder
+ulti_decoder
+utvideo_decoder
+v210_decoder
+v210x_decoder
+v308_decoder
+v408_decoder
+v410_decoder
+vb_decoder
+vble_decoder
+vc1_decoder
+vc1_crystalhd_decoder
+vc1image_decoder
+vc1_mmal_decoder
+vc1_qsv_decoder
+vc1_v4l2m2m_decoder
+vcr1_decoder
+vmdvideo_decoder
+vmnc_decoder
+vp3_decoder
+vp5_decoder
+vp6_decoder
+vp6a_decoder
+vp6f_decoder
+vp7_decoder
+vp8_decoder
+vp8_rkmpp_decoder
+vp8_v4l2m2m_decoder
+vp9_decoder
+vp9_rkmpp_decoder
+vp9_v4l2m2m_decoder
+vqa_decoder
+webp_decoder
+wcmv_decoder
+wrapped_avframe_decoder
+wmv1_decoder
+wmv2_decoder
+wmv3_decoder
+wmv3_crystalhd_decoder
+wmv3image_decoder
+wnv1_decoder
+xan_wc3_decoder
+xan_wc4_decoder
+xbm_decoder
+xface_decoder
+xl_decoder
+xpm_decoder
+xwd_decoder
+y41p_decoder
+ylc_decoder
+yop_decoder
+yuv4_decoder
+zero12v_decoder
+zerocodec_decoder
+zlib_decoder
+zmbv_decoder
+aac_decoder
+aac_fixed_decoder
+aac_latm_decoder
+ac3_decoder
+ac3_fixed_decoder
+alac_decoder
+als_decoder
+amrnb_decoder
+amrwb_decoder
+ape_decoder
+aptx_decoder
+aptx_hd_decoder
+atrac1_decoder
+atrac3_decoder
+atrac3al_decoder
+atrac3p_decoder
+atrac3pal_decoder
+atrac9_decoder
+binkaudio_dct_decoder
+binkaudio_rdft_decoder
+bmv_audio_decoder
+cook_decoder
+dca_decoder
+dolby_e_decoder
+dsd_lsbf_decoder
+dsd_msbf_decoder
+dsd_lsbf_planar_decoder
+dsd_msbf_planar_decoder
+dsicinaudio_decoder
+dss_sp_decoder
+dst_decoder
+eac3_decoder
+evrc_decoder
+ffwavesynth_decoder
+flac_decoder
+g723_1_decoder
+g729_decoder
+gsm_decoder
+gsm_ms_decoder
+iac_decoder
+ilbc_decoder
+imc_decoder
+interplay_acm_decoder
+mace3_decoder
+mace6_decoder
+metasound_decoder
+mlp_decoder
+mp1_decoder
+mp1float_decoder
+mp2_decoder
+mp2float_decoder
+mp3float_decoder
+mp3_decoder
+mp3adufloat_decoder
+mp3adu_decoder
+mp3on4float_decoder
+mp3on4_decoder
+mpc7_decoder
+mpc8_decoder
+nellymoser_decoder
+on2avc_decoder
+opus_decoder
+paf_audio_decoder
+qcelp_decoder
+qdm2_decoder
+qdmc_decoder
+ra_144_decoder
+ra_288_decoder
+ralf_decoder
+sbc_decoder
+shorten_decoder
+sipr_decoder
+smackaud_decoder
+sonic_decoder
+tak_decoder
+truehd_decoder
+truespeech_decoder
+tta_decoder
+twinvq_decoder
+vmdaudio_decoder
+vorbis_decoder
+wavpack_decoder
+wmalossless_decoder
+wmapro_decoder
+wmav1_decoder
+wmav2_decoder
+wmavoice_decoder
+ws_snd1_decoder
+xma1_decoder
+xma2_decoder
+pcm_alaw_decoder
+pcm_bluray_decoder
+pcm_dvd_decoder
+pcm_f16le_decoder
+pcm_f24le_decoder
+pcm_f32be_decoder
+pcm_f32le_decoder
+pcm_f64be_decoder
+pcm_f64le_decoder
+pcm_lxf_decoder
+pcm_mulaw_decoder
+pcm_s8_decoder
+pcm_s8_planar_decoder
+pcm_s16be_decoder
+pcm_s16be_planar_decoder
+pcm_s16le_decoder
+pcm_s16le_planar_decoder
+pcm_s24be_decoder
+pcm_s24daud_decoder
+pcm_s24le_decoder
+pcm_s24le_planar_decoder
+pcm_s32be_decoder
+pcm_s32le_decoder
+pcm_s32le_planar_decoder
+pcm_s64be_decoder
+pcm_s64le_decoder
+pcm_u8_decoder
+pcm_u16be_decoder
+pcm_u16le_decoder
+pcm_u24be_decoder
+pcm_u24le_decoder
+pcm_u32be_decoder
+pcm_u32le_decoder
+pcm_vidc_decoder
+pcm_zork_decoder
+gremlin_dpcm_decoder
+interplay_dpcm_decoder
+roq_dpcm_decoder
+sol_dpcm_decoder
+xan_dpcm_decoder
+adpcm_4xm_decoder
+adpcm_adx_decoder
+adpcm_afc_decoder
+adpcm_aica_decoder
+adpcm_ct_decoder
+adpcm_dtk_decoder
+adpcm_ea_decoder
+adpcm_ea_maxis_xa_decoder
+adpcm_ea_r1_decoder
+adpcm_ea_r2_decoder
+adpcm_ea_r3_decoder
+adpcm_ea_xas_decoder
+adpcm_g722_decoder
+adpcm_g726_decoder
+adpcm_g726le_decoder
+adpcm_ima_amv_decoder
+adpcm_ima_apc_decoder
+adpcm_ima_dat4_decoder
+adpcm_ima_dk3_decoder
+adpcm_ima_dk4_decoder
+adpcm_ima_ea_eacs_decoder
+adpcm_ima_ea_sead_decoder
+adpcm_ima_iss_decoder
+adpcm_ima_oki_decoder
+adpcm_ima_qt_decoder
+adpcm_ima_rad_decoder
+adpcm_ima_smjpeg_decoder
+adpcm_ima_wav_decoder
+adpcm_ima_ws_decoder
+adpcm_ms_decoder
+adpcm_mtaf_decoder
+adpcm_psx_decoder
+adpcm_sbpro_2_decoder
+adpcm_sbpro_3_decoder
+adpcm_sbpro_4_decoder
+adpcm_swf_decoder
+adpcm_thp_decoder
+adpcm_thp_le_decoder
+adpcm_vima_decoder
+adpcm_xa_decoder
+adpcm_yamaha_decoder
+ssa_decoder
+ass_decoder
+ccaption_decoder
+dvbsub_decoder
+dvdsub_decoder
+jacosub_decoder
+microdvd_decoder
+movtext_decoder
+mpl2_decoder
+pgssub_decoder
+pjs_decoder
+realtext_decoder
+sami_decoder
+srt_decoder
+stl_decoder
+subrip_decoder
+subviewer_decoder
+subviewer1_decoder
+text_decoder
+vplayer_decoder
+webvtt_decoder
+xsub_decoder
+aac_at_decoder
+ac3_at_decoder
+adpcm_ima_qt_at_decoder
+alac_at_decoder
+amr_nb_at_decoder
+eac3_at_decoder
+gsm_ms_at_decoder
+ilbc_at_decoder
+mp1_at_decoder
+mp2_at_decoder
+mp3_at_decoder
+pcm_alaw_at_decoder
+pcm_mulaw_at_decoder
+qdmc_at_decoder
+qdm2_at_decoder
+libaom_av1_decoder
+libcelt_decoder
+libcodec2_decoder
+libdavs2_decoder
+libfdk_aac_decoder
+libgsm_decoder
+libgsm_ms_decoder
+libilbc_decoder
+libopencore_amrnb_decoder
+libopencore_amrwb_decoder
+libopenjpeg_decoder
+libopus_decoder
+librsvg_decoder
+libspeex_decoder
+libvorbis_decoder
+libvpx_vp8_decoder
+libvpx_vp9_decoder
+libzvbi_teletext_decoder
+bintext_decoder
+xbin_decoder
+idf_decoder
+libopenh264_decoder
+h264_cuvid_decoder
+hevc_cuvid_decoder
+hevc_mediacodec_decoder
+mjpeg_cuvid_decoder
+mpeg1_cuvid_decoder
+mpeg2_cuvid_decoder
+mpeg4_cuvid_decoder
+mpeg4_mediacodec_decoder
+vc1_cuvid_decoder
+vp8_cuvid_decoder
+vp8_mediacodec_decoder
+vp8_qsv_decoder
+vp9_cuvid_decoder
+vp9_mediacodec_decoder
+    a64multi_encoder
+a64multi5_encoder
+alias_pix_encoder
+amv_encoder
+apng_encoder
+asv1_encoder
+asv2_encoder
+avrp_encoder
+avui_encoder
+ayuv_encoder
+bmp_encoder
+cinepak_encoder
+cljr_encoder
+comfortnoise_encoder
+dnxhd_encoder
+dpx_encoder
+dvvideo_encoder
+ffv1_encoder
+ffvhuff_encoder
+fits_encoder
+flashsv_encoder
+flashsv2_encoder
+flv_encoder
+gif_encoder
+h261_encoder
+h263_encoder
+h263p_encoder
+hap_encoder
+huffyuv_encoder
+jpeg2000_encoder
+jpegls_encoder
+ljpeg_encoder
+magicyuv_encoder
+mjpeg_encoder
+mpeg1video_encoder
+mpeg2video_encoder
+mpeg4_encoder
+msmpeg4v2_encoder
+msmpeg4v3_encoder
+msvideo1_encoder
+pam_encoder
+pbm_encoder
+pcx_encoder
+pgm_encoder
+pgmyuv_encoder
+png_encoder
+ppm_encoder
+prores_encoder
+prores_aw_encoder
+prores_ks_encoder
+qtrle_encoder
+r10k_encoder
+r210_encoder
+rawvideo_encoder
+roq_encoder
+rv10_encoder
+rv20_encoder
+s302m_encoder
+sgi_encoder
+snow_encoder
+sunrast_encoder
+svq1_encoder
+targa_encoder
+tiff_encoder
+utvideo_encoder
+v210_encoder
+v308_encoder
+v408_encoder
+v410_encoder
+vc2_encoder
+wrapped_avframe_encoder
+wmv1_encoder
+wmv2_encoder
+xbm_encoder
+xface_encoder
+xwd_encoder
+y41p_encoder
+yuv4_encoder
+zlib_encoder
+zmbv_encoder
+aac_encoder
+ac3_encoder
+ac3_fixed_encoder
+alac_encoder
+aptx_encoder
+aptx_hd_encoder
+dca_encoder
+eac3_encoder
+flac_encoder
+g723_1_encoder
+mlp_encoder
+mp2_encoder
+mp2fixed_encoder
+nellymoser_encoder
+opus_encoder
+ra_144_encoder
+sbc_encoder
+sonic_encoder
+sonic_ls_encoder
+truehd_encoder
+tta_encoder
+vorbis_encoder
+wavpack_encoder
+wmav1_encoder
+wmav2_encoder
+pcm_alaw_encoder
+pcm_f32be_encoder
+pcm_f32le_encoder
+pcm_f64be_encoder
+pcm_f64le_encoder
+pcm_mulaw_encoder
+pcm_s8_encoder
+pcm_s8_planar_encoder
+pcm_s16be_encoder
+pcm_s16be_planar_encoder
+pcm_s16le_encoder
+pcm_s16le_planar_encoder
+pcm_s24be_encoder
+pcm_s24daud_encoder
+pcm_s24le_encoder
+pcm_s24le_planar_encoder
+pcm_s32be_encoder
+pcm_s32le_encoder
+pcm_s32le_planar_encoder
+pcm_s64be_encoder
+pcm_s64le_encoder
+pcm_u8_encoder
+pcm_u16be_encoder
+pcm_u16le_encoder
+pcm_u24be_encoder
+pcm_u24le_encoder
+pcm_u32be_encoder
+pcm_u32le_encoder
+pcm_vidc_encoder
+roq_dpcm_encoder
+adpcm_adx_encoder
+adpcm_g722_encoder
+adpcm_g726_encoder
+adpcm_g726le_encoder
+adpcm_ima_qt_encoder
+adpcm_ima_wav_encoder
+adpcm_ms_encoder
+adpcm_swf_encoder
+adpcm_yamaha_encoder
+ssa_encoder
+ass_encoder
+dvbsub_encoder
+dvdsub_encoder
+movtext_encoder
+srt_encoder
+subrip_encoder
+text_encoder
+webvtt_encoder
+xsub_encoder
+aac_at_encoder
+alac_at_encoder
+ilbc_at_encoder
+pcm_alaw_at_encoder
+pcm_mulaw_at_encoder
+libaom_av1_encoder
+libcodec2_encoder
+libfdk_aac_encoder
+libgsm_encoder
+libgsm_ms_encoder
+libilbc_encoder
+libmp3lame_encoder
+libopencore_amrnb_encoder
+libopenjpeg_encoder
+libopus_encoder
+libshine_encoder
+libspeex_encoder
+libtheora_encoder
+libtwolame_encoder
+libvo_amrwbenc_encoder
+libvorbis_encoder
+libvpx_vp8_encoder
+libvpx_vp9_encoder
+libwavpack_encoder
+libwebp_anim_encoder
+libwebp_encoder
+libx262_encoder
+libx264_encoder
+libx264rgb_encoder
+libx265_encoder
+libxavs_encoder
+libxavs2_encoder
+libxvid_encoder
+h263_v4l2m2m_encoder
+libopenh264_encoder
+h264_amf_encoder
+h264_nvenc_encoder
+h264_omx_encoder
+h264_qsv_encoder
+h264_v4l2m2m_encoder
+h264_vaapi_encoder
+h264_videotoolbox_encoder
+nvenc_encoder
+nvenc_h264_encoder
+nvenc_hevc_encoder
+hevc_amf_encoder
+hevc_nvenc_encoder
+hevc_qsv_encoder
+hevc_v4l2m2m_encoder
+hevc_vaapi_encoder
+hevc_videotoolbox_encoder
+libkvazaar_encoder
+mjpeg_qsv_encoder
+mjpeg_vaapi_encoder
+mpeg2_qsv_encoder
+mpeg2_vaapi_encoder
+mpeg4_v4l2m2m_encoder
+vp8_v4l2m2m_encoder
+vp8_vaapi_encoder
+vp9_vaapi_encoder
+    h263_vaapi_hwaccel
+h263_videotoolbox_hwaccel
+h264_d3d11va_hwaccel
+h264_d3d11va2_hwaccel
+h264_dxva2_hwaccel
+h264_nvdec_hwaccel
+h264_vaapi_hwaccel
+h264_vdpau_hwaccel
+h264_videotoolbox_hwaccel
+hevc_d3d11va_hwaccel
+hevc_d3d11va2_hwaccel
+hevc_dxva2_hwaccel
+hevc_nvdec_hwaccel
+hevc_vaapi_hwaccel
+hevc_vdpau_hwaccel
+hevc_videotoolbox_hwaccel
+mjpeg_nvdec_hwaccel
+mjpeg_vaapi_hwaccel
+mpeg1_nvdec_hwaccel
+mpeg1_vdpau_hwaccel
+mpeg1_videotoolbox_hwaccel
+mpeg1_xvmc_hwaccel
+mpeg2_d3d11va_hwaccel
+mpeg2_d3d11va2_hwaccel
+mpeg2_nvdec_hwaccel
+mpeg2_dxva2_hwaccel
+mpeg2_vaapi_hwaccel
+mpeg2_vdpau_hwaccel
+mpeg2_videotoolbox_hwaccel
+mpeg2_xvmc_hwaccel
+mpeg4_nvdec_hwaccel
+mpeg4_vaapi_hwaccel
+mpeg4_vdpau_hwaccel
+mpeg4_videotoolbox_hwaccel
+vc1_d3d11va_hwaccel
+vc1_d3d11va2_hwaccel
+vc1_dxva2_hwaccel
+vc1_nvdec_hwaccel
+vc1_vaapi_hwaccel
+vc1_vdpau_hwaccel
+vp8_nvdec_hwaccel
+vp8_vaapi_hwaccel
+vp9_d3d11va_hwaccel
+vp9_d3d11va2_hwaccel
+vp9_dxva2_hwaccel
+vp9_nvdec_hwaccel
+vp9_vaapi_hwaccel
+wmv3_d3d11va_hwaccel
+wmv3_d3d11va2_hwaccel
+wmv3_dxva2_hwaccel
+wmv3_nvdec_hwaccel
+wmv3_vaapi_hwaccel
+wmv3_vdpau_hwaccel
+    aac_parser
+aac_latm_parser
+ac3_parser
+adx_parser
+av1_parser
+avs2_parser
+bmp_parser
+cavsvideo_parser
+cook_parser
+dca_parser
+dirac_parser
+dnxhd_parser
+dpx_parser
+dvaudio_parser
+dvbsub_parser
+dvdsub_parser
+dvd_nav_parser
+flac_parser
+g729_parser
+gsm_parser
+h261_parser
+h263_parser
+h264_parser
+hevc_parser
+mjpeg_parser
+mlp_parser
+mpeg4video_parser
+mpegaudio_parser
+mpegvideo_parser
+opus_parser
+png_parser
+pnm_parser
+rv30_parser
+rv40_parser
+sbc_parser
+sipr_parser
+tak_parser
+vc1_parser
+vorbis_parser
+vp3_parser
+vp8_parser
+vp9_parser
+xma_parser
+
+    
+    alsa_indev
+android_camera_indev
+avfoundation_indev
+bktr_indev
+decklink_indev
+libndi_newtek_indev
+dshow_indev
+fbdev_indev
+gdigrab_indev
+iec61883_indev
+jack_indev
+kmsgrab_indev
+lavfi_indev
+openal_indev
+oss_indev
+pulse_indev
+sndio_indev
+v4l2_indev
+vfwcap_indev
+xcbgrab_indev
+libcdio_indev
+libdc1394_indev
+    alsa_outdev
+caca_outdev
+decklink_outdev
+libndi_newtek_outdev
+fbdev_outdev
+opengl_outdev
+oss_outdev
+pulse_outdev
+sdl2_outdev
+sndio_outdev
+v4l2_outdev
+xv_outdev
+
+    
+    abench_filter
+acompressor_filter
+acontrast_filter
+acopy_filter
+acue_filter
+acrossfade_filter
+acrossover_filter
+acrusher_filter
+adeclick_filter
+adeclip_filter
+adelay_filter
+aderivative_filter
+aecho_filter
+aemphasis_filter
+aeval_filter
+afade_filter
+afftdn_filter
+afftfilt_filter
+afir_filter
+aformat_filter
+agate_filter
+aiir_filter
+aintegral_filter
+ainterleave_filter
+alimiter_filter
+allpass_filter
+aloop_filter
+amerge_filter
+ametadata_filter
+amix_filter
+amultiply_filter
+anequalizer_filter
+anull_filter
+apad_filter
+aperms_filter
+aphaser_filter
+apulsator_filter
+arealtime_filter
+aresample_filter
+areverse_filter
+aselect_filter
+asendcmd_filter
+asetnsamples_filter
+asetpts_filter
+asetrate_filter
+asettb_filter
+ashowinfo_filter
+asidedata_filter
+asplit_filter
+astats_filter
+astreamselect_filter
+atempo_filter
+atrim_filter
+azmq_filter
+bandpass_filter
+bandreject_filter
+bass_filter
+biquad_filter
+bs2b_filter
+channelmap_filter
+channelsplit_filter
+chorus_filter
+compand_filter
+compensationdelay_filter
+crossfeed_filter
+crystalizer_filter
+dcshift_filter
+drmeter_filter
+dynaudnorm_filter
+earwax_filter
+ebur128_filter
+equalizer_filter
+extrastereo_filter
+firequalizer_filter
+flanger_filter
+haas_filter
+hdcd_filter
+headphone_filter
+highpass_filter
+highshelf_filter
+join_filter
+ladspa_filter
+loudnorm_filter
+lowpass_filter
+lowshelf_filter
+lv2_filter
+mcompand_filter
+pan_filter
+replaygain_filter
+resample_filter
+rubberband_filter
+sidechaincompress_filter
+sidechaingate_filter
+silencedetect_filter
+silenceremove_filter
+sofalizer_filter
+stereotools_filter
+stereowiden_filter
+superequalizer_filter
+surround_filter
+treble_filter
+tremolo_filter
+vibrato_filter
+volume_filter
+volumedetect_filter
+aevalsrc_filter
+anoisesrc_filter
+anullsrc_filter
+flite_filter
+hilbert_filter
+sinc_filter
+sine_filter
+anullsink_filter
+alphaextract_filter
+alphamerge_filter
+amplify_filter
+ass_filter
+atadenoise_filter
+avgblur_filter
+avgblur_opencl_filter
+bbox_filter
+bench_filter
+bitplanenoise_filter
+blackdetect_filter
+blackframe_filter
+blend_filter
+bm3d_filter
+boxblur_filter
+boxblur_opencl_filter
+bwdif_filter
+chromahold_filter
+chromakey_filter
+ciescope_filter
+codecview_filter
+colorbalance_filter
+colorchannelmixer_filter
+colorkey_filter
+colorlevels_filter
+colormatrix_filter
+colorspace_filter
+convolution_filter
+convolution_opencl_filter
+convolve_filter
+copy_filter
+coreimage_filter
+cover_rect_filter
+crop_filter
+cropdetect_filter
+cue_filter
+curves_filter
+datascope_filter
+dctdnoiz_filter
+deband_filter
+deblock_filter
+decimate_filter
+deconvolve_filter
+deflate_filter
+deflicker_filter
+deinterlace_qsv_filter
+deinterlace_vaapi_filter
+dejudder_filter
+delogo_filter
+denoise_vaapi_filter
+deshake_filter
+despill_filter
+detelecine_filter
+dilation_filter
+dilation_opencl_filter
+displace_filter
+doubleweave_filter
+drawbox_filter
+drawgraph_filter
+drawgrid_filter
+drawtext_filter
+edgedetect_filter
+elbg_filter
+entropy_filter
+eq_filter
+erosion_filter
+erosion_opencl_filter
+extractplanes_filter
+fade_filter
+fftdnoiz_filter
+fftfilt_filter
+field_filter
+fieldhint_filter
+fieldmatch_filter
+fieldorder_filter
+fillborders_filter
+find_rect_filter
+floodfill_filter
+format_filter
+fps_filter
+framepack_filter
+framerate_filter
+framestep_filter
+frei0r_filter
+fspp_filter
+gblur_filter
+geq_filter
+gradfun_filter
+graphmonitor_filter
+greyedge_filter
+haldclut_filter
+hflip_filter
+histeq_filter
+histogram_filter
+hqdn3d_filter
+hqx_filter
+hstack_filter
+hue_filter
+hwdownload_filter
+hwmap_filter
+hwupload_filter
+hwupload_cuda_filter
+hysteresis_filter
+idet_filter
+il_filter
+inflate_filter
+interlace_filter
+interleave_filter
+kerndeint_filter
+lenscorrection_filter
+lensfun_filter
+libvmaf_filter
+limiter_filter
+loop_filter
+lumakey_filter
+lut_filter
+lut1d_filter
+lut2_filter
+lut3d_filter
+lutrgb_filter
+lutyuv_filter
+maskedclamp_filter
+maskedmerge_filter
+mcdeint_filter
+mergeplanes_filter
+mestimate_filter
+metadata_filter
+midequalizer_filter
+minterpolate_filter
+mix_filter
+mpdecimate_filter
+negate_filter
+nlmeans_filter
+nnedi_filter
+noformat_filter
+noise_filter
+normalize_filter
+null_filter
+ocr_filter
+ocv_filter
+oscilloscope_filter
+overlay_filter
+overlay_opencl_filter
+overlay_qsv_filter
+owdenoise_filter
+pad_filter
+palettegen_filter
+paletteuse_filter
+perms_filter
+perspective_filter
+phase_filter
+pixdesctest_filter
+pixscope_filter
+pp_filter
+pp7_filter
+premultiply_filter
+prewitt_filter
+prewitt_opencl_filter
+procamp_vaapi_filter
+program_opencl_filter
+pseudocolor_filter
+psnr_filter
+pullup_filter
+qp_filter
+random_filter
+readeia608_filter
+readvitc_filter
+realtime_filter
+remap_filter
+removegrain_filter
+removelogo_filter
+repeatfields_filter
+reverse_filter
+roberts_filter
+roberts_opencl_filter
+rotate_filter
+sab_filter
+scale_filter
+scale_cuda_filter
+scale_npp_filter
+scale_qsv_filter
+scale_vaapi_filter
+scale2ref_filter
+select_filter
+selectivecolor_filter
+sendcmd_filter
+separatefields_filter
+setdar_filter
+setfield_filter
+setparams_filter
+setpts_filter
+setrange_filter
+setsar_filter
+settb_filter
+sharpness_vaapi_filter
+showinfo_filter
+showpalette_filter
+shuffleframes_filter
+shuffleplanes_filter
+sidedata_filter
+signalstats_filter
+signature_filter
+smartblur_filter
+sobel_filter
+sobel_opencl_filter
+split_filter
+spp_filter
+sr_filter
+ssim_filter
+stereo3d_filter
+streamselect_filter
+subtitles_filter
+super2xsai_filter
+swaprect_filter
+swapuv_filter
+tblend_filter
+telecine_filter
+threshold_filter
+thumbnail_filter
+thumbnail_cuda_filter
+tile_filter
+tinterlace_filter
+tlut2_filter
+tmix_filter
+tonemap_filter
+tonemap_opencl_filter
+transpose_filter
+transpose_npp_filter
+trim_filter
+unpremultiply_filter
+unsharp_filter
+unsharp_opencl_filter
+uspp_filter
+vaguedenoiser_filter
+vectorscope_filter
+vflip_filter
+vfrdet_filter
+vibrance_filter
+vidstabdetect_filter
+vidstabtransform_filter
+vignette_filter
+vmafmotion_filter
+vpp_qsv_filter
+vstack_filter
+w3fdif_filter
+waveform_filter
+weave_filter
+xbr_filter
+xstack_filter
+yadif_filter
+yadif_cuda_filter
+zmq_filter
+zoompan_filter
+zscale_filter
+allrgb_filter
+allyuv_filter
+cellauto_filter
+color_filter
+coreimagesrc_filter
+frei0r_src_filter
+haldclutsrc_filter
+life_filter
+mandelbrot_filter
+mptestsrc_filter
+nullsrc_filter
+openclsrc_filter
+pal75bars_filter
+pal100bars_filter
+rgbtestsrc_filter
+smptebars_filter
+smptehdbars_filter
+testsrc_filter
+testsrc2_filter
+yuvtestsrc_filter
+nullsink_filter
+abitscope_filter
+adrawgraph_filter
+agraphmonitor_filter
+ahistogram_filter
+aphasemeter_filter
+avectorscope_filter
+concat_filter
+showcqt_filter
+showfreqs_filter
+showspectrum_filter
+showspectrumpic_filter
+showvolume_filter
+showwaves_filter
+showwavespic_filter
+spectrumsynth_filter
+amovie_filter
+movie_filter
+afifo_filter
+fifo_filter
+
+    
+    aa_demuxer
+aac_demuxer
+ac3_demuxer
+acm_demuxer
+act_demuxer
+adf_demuxer
+adp_demuxer
+ads_demuxer
+adx_demuxer
+aea_demuxer
+afc_demuxer
+aiff_demuxer
+aix_demuxer
+amr_demuxer
+amrnb_demuxer
+amrwb_demuxer
+anm_demuxer
+apc_demuxer
+ape_demuxer
+apng_demuxer
+aptx_demuxer
+aptx_hd_demuxer
+aqtitle_demuxer
+asf_demuxer
+asf_o_demuxer
+ass_demuxer
+ast_demuxer
+au_demuxer
+avi_demuxer
+avisynth_demuxer
+avr_demuxer
+avs_demuxer
+avs2_demuxer
+bethsoftvid_demuxer
+bfi_demuxer
+bintext_demuxer
+bink_demuxer
+bit_demuxer
+bmv_demuxer
+bfstm_demuxer
+brstm_demuxer
+boa_demuxer
+c93_demuxer
+caf_demuxer
+cavsvideo_demuxer
+cdg_demuxer
+cdxl_demuxer
+cine_demuxer
+codec2_demuxer
+codec2raw_demuxer
+concat_demuxer
+dash_demuxer
+data_demuxer
+daud_demuxer
+dcstr_demuxer
+dfa_demuxer
+dirac_demuxer
+dnxhd_demuxer
+dsf_demuxer
+dsicin_demuxer
+dss_demuxer
+dts_demuxer
+dtshd_demuxer
+dv_demuxer
+dvbsub_demuxer
+dvbtxt_demuxer
+dxa_demuxer
+ea_demuxer
+ea_cdata_demuxer
+eac3_demuxer
+epaf_demuxer
+ffmetadata_demuxer
+filmstrip_demuxer
+fits_demuxer
+flac_demuxer
+flic_demuxer
+flv_demuxer
+live_flv_demuxer
+fourxm_demuxer
+frm_demuxer
+fsb_demuxer
+g722_demuxer
+g723_1_demuxer
+g726_demuxer
+g726le_demuxer
+g729_demuxer
+gdv_demuxer
+genh_demuxer
+gif_demuxer
+gsm_demuxer
+gxf_demuxer
+h261_demuxer
+h263_demuxer
+h264_demuxer
+hevc_demuxer
+hls_demuxer
+hnm_demuxer
+ico_demuxer
+idcin_demuxer
+idf_demuxer
+iff_demuxer
+ilbc_demuxer
+image2_demuxer
+image2pipe_demuxer
+image2_alias_pix_demuxer
+image2_brender_pix_demuxer
+ingenient_demuxer
+ipmovie_demuxer
+ircam_demuxer
+iss_demuxer
+iv8_demuxer
+ivf_demuxer
+ivr_demuxer
+jacosub_demuxer
+jv_demuxer
+lmlm4_demuxer
+loas_demuxer
+lrc_demuxer
+lvf_demuxer
+lxf_demuxer
+m4v_demuxer
+matroska_demuxer
+mgsts_demuxer
+microdvd_demuxer
+mjpeg_demuxer
+mjpeg_2000_demuxer
+mlp_demuxer
+mlv_demuxer
+mm_demuxer
+mmf_demuxer
+mov_demuxer
+mp3_demuxer
+mpc_demuxer
+mpc8_demuxer
+mpegps_demuxer
+mpegts_demuxer
+mpegtsraw_demuxer
+mpegvideo_demuxer
+mpjpeg_demuxer
+mpl2_demuxer
+mpsub_demuxer
+msf_demuxer
+msnwc_tcp_demuxer
+mtaf_demuxer
+mtv_demuxer
+musx_demuxer
+mv_demuxer
+mvi_demuxer
+mxf_demuxer
+mxg_demuxer
+nc_demuxer
+nistsphere_demuxer
+nsp_demuxer
+nsv_demuxer
+nut_demuxer
+nuv_demuxer
+ogg_demuxer
+oma_demuxer
+paf_demuxer
+pcm_alaw_demuxer
+pcm_mulaw_demuxer
+pcm_vidc_demuxer
+pcm_f64be_demuxer
+pcm_f64le_demuxer
+pcm_f32be_demuxer
+pcm_f32le_demuxer
+pcm_s32be_demuxer
+pcm_s32le_demuxer
+pcm_s24be_demuxer
+pcm_s24le_demuxer
+pcm_s16be_demuxer
+pcm_s16le_demuxer
+pcm_s8_demuxer
+pcm_u32be_demuxer
+pcm_u32le_demuxer
+pcm_u24be_demuxer
+pcm_u24le_demuxer
+pcm_u16be_demuxer
+pcm_u16le_demuxer
+pcm_u8_demuxer
+pjs_demuxer
+pmp_demuxer
+pva_demuxer
+pvf_demuxer
+qcp_demuxer
+r3d_demuxer
+rawvideo_demuxer
+realtext_demuxer
+redspark_demuxer
+rl2_demuxer
+rm_demuxer
+roq_demuxer
+rpl_demuxer
+rsd_demuxer
+rso_demuxer
+rtp_demuxer
+rtsp_demuxer
+s337m_demuxer
+sami_demuxer
+sap_demuxer
+sbc_demuxer
+sbg_demuxer
+scc_demuxer
+sdp_demuxer
+sdr2_demuxer
+sds_demuxer
+sdx_demuxer
+segafilm_demuxer
+ser_demuxer
+shorten_demuxer
+siff_demuxer
+sln_demuxer
+smacker_demuxer
+smjpeg_demuxer
+smush_demuxer
+sol_demuxer
+sox_demuxer
+spdif_demuxer
+srt_demuxer
+str_demuxer
+stl_demuxer
+subviewer1_demuxer
+subviewer_demuxer
+sup_demuxer
+svag_demuxer
+swf_demuxer
+tak_demuxer
+tedcaptions_demuxer
+thp_demuxer
+threedostr_demuxer
+tiertexseq_demuxer
+tmv_demuxer
+truehd_demuxer
+tta_demuxer
+txd_demuxer
+tty_demuxer
+ty_demuxer
+v210_demuxer
+v210x_demuxer
+vag_demuxer
+vc1_demuxer
+vc1t_demuxer
+vivo_demuxer
+vmd_demuxer
+vobsub_demuxer
+voc_demuxer
+vpk_demuxer
+vplayer_demuxer
+vqf_demuxer
+w64_demuxer
+wav_demuxer
+wc3_demuxer
+webm_dash_manifest_demuxer
+webvtt_demuxer
+wsaud_demuxer
+wsd_demuxer
+wsvqa_demuxer
+wtv_demuxer
+wve_demuxer
+wv_demuxer
+xa_demuxer
+xbin_demuxer
+xmv_demuxer
+xvag_demuxer
+xwma_demuxer
+yop_demuxer
+yuv4mpegpipe_demuxer
+image_bmp_pipe_demuxer
+image_dds_pipe_demuxer
+image_dpx_pipe_demuxer
+image_exr_pipe_demuxer
+image_j2k_pipe_demuxer
+image_jpeg_pipe_demuxer
+image_jpegls_pipe_demuxer
+image_pam_pipe_demuxer
+image_pbm_pipe_demuxer
+image_pcx_pipe_demuxer
+image_pgmyuv_pipe_demuxer
+image_pgm_pipe_demuxer
+image_pictor_pipe_demuxer
+image_png_pipe_demuxer
+image_ppm_pipe_demuxer
+image_psd_pipe_demuxer
+image_qdraw_pipe_demuxer
+image_sgi_pipe_demuxer
+image_svg_pipe_demuxer
+image_sunrast_pipe_demuxer
+image_tiff_pipe_demuxer
+image_webp_pipe_demuxer
+image_xpm_pipe_demuxer
+image_xwd_pipe_demuxer
+libgme_demuxer
+libmodplug_demuxer
+libopenmpt_demuxer
+vapoursynth_demuxer
+    a64_muxer
+ac3_muxer
+adts_muxer
+adx_muxer
+aiff_muxer
+amr_muxer
+apng_muxer
+aptx_muxer
+aptx_hd_muxer
+asf_muxer
+ass_muxer
+ast_muxer
+asf_stream_muxer
+au_muxer
+avi_muxer
+avm2_muxer
+avs2_muxer
+bit_muxer
+caf_muxer
+cavsvideo_muxer
+codec2_muxer
+codec2raw_muxer
+crc_muxer
+dash_muxer
+data_muxer
+daud_muxer
+dirac_muxer
+dnxhd_muxer
+dts_muxer
+dv_muxer
+eac3_muxer
+f4v_muxer
+ffmetadata_muxer
+fifo_muxer
+fifo_test_muxer
+filmstrip_muxer
+fits_muxer
+flac_muxer
+flv_muxer
+framecrc_muxer
+framehash_muxer
+framemd5_muxer
+g722_muxer
+g723_1_muxer
+g726_muxer
+g726le_muxer
+gif_muxer
+gsm_muxer
+gxf_muxer
+h261_muxer
+h263_muxer
+h264_muxer
+hash_muxer
+hds_muxer
+hevc_muxer
+hls_muxer
+ico_muxer
+ilbc_muxer
+image2_muxer
+image2pipe_muxer
+ipod_muxer
+ircam_muxer
+ismv_muxer
+ivf_muxer
+jacosub_muxer
+latm_muxer
+lrc_muxer
+m4v_muxer
+md5_muxer
+matroska_muxer
+matroska_audio_muxer
+microdvd_muxer
+mjpeg_muxer
+mlp_muxer
+mmf_muxer
+mov_muxer
+mp2_muxer
+mp3_muxer
+mp4_muxer
+mpeg1system_muxer
+mpeg1vcd_muxer
+mpeg1video_muxer
+mpeg2dvd_muxer
+mpeg2svcd_muxer
+mpeg2video_muxer
+mpeg2vob_muxer
+mpegts_muxer
+mpjpeg_muxer
+mxf_muxer
+mxf_d10_muxer
+mxf_opatom_muxer
+null_muxer
+nut_muxer
+oga_muxer
+ogg_muxer
+ogv_muxer
+oma_muxer
+opus_muxer
+pcm_alaw_muxer
+pcm_mulaw_muxer
+pcm_vidc_muxer
+pcm_f64be_muxer
+pcm_f64le_muxer
+pcm_f32be_muxer
+pcm_f32le_muxer
+pcm_s32be_muxer
+pcm_s32le_muxer
+pcm_s24be_muxer
+pcm_s24le_muxer
+pcm_s16be_muxer
+pcm_s16le_muxer
+pcm_s8_muxer
+pcm_u32be_muxer
+pcm_u32le_muxer
+pcm_u24be_muxer
+pcm_u24le_muxer
+pcm_u16be_muxer
+pcm_u16le_muxer
+pcm_u8_muxer
+psp_muxer
+rawvideo_muxer
+rm_muxer
+roq_muxer
+rso_muxer
+rtp_muxer
+rtp_mpegts_muxer
+rtsp_muxer
+sap_muxer
+sbc_muxer
+scc_muxer
+segafilm_muxer
+segment_muxer
+stream_segment_muxer
+singlejpeg_muxer
+smjpeg_muxer
+smoothstreaming_muxer
+sox_muxer
+spx_muxer
+spdif_muxer
+srt_muxer
+sup_muxer
+swf_muxer
+tee_muxer
+tg2_muxer
+tgp_muxer
+mkvtimestamp_v2_muxer
+truehd_muxer
+tta_muxer
+uncodedframecrc_muxer
+vc1_muxer
+vc1t_muxer
+voc_muxer
+w64_muxer
+wav_muxer
+webm_muxer
+webm_dash_manifest_muxer
+webm_chunk_muxer
+webp_muxer
+webvtt_muxer
+wtv_muxer
+wv_muxer
+yuv4mpegpipe_muxer
+chromaprint_muxer
+    async_protocol
+bluray_protocol
+cache_protocol
+concat_protocol
+crypto_protocol
+data_protocol
+ffrtmpcrypt_protocol
+ffrtmphttp_protocol
+file_protocol
+ftp_protocol
+gopher_protocol
+hls_protocol
+http_protocol
+httpproxy_protocol
+https_protocol
+icecast_protocol
+mmsh_protocol
+mmst_protocol
+md5_protocol
+pipe_protocol
+prompeg_protocol
+rtmp_protocol
+rtmpe_protocol
+rtmps_protocol
+rtmpt_protocol
+rtmpte_protocol
+rtmpts_protocol
+rtp_protocol
+sctp_protocol
+srtp_protocol
+subfile_protocol
+tee_protocol
+tcp_protocol
+tls_protocol
+udp_protocol
+udplite_protocol
+unix_protocol
+librtmp_protocol
+librtmpe_protocol
+librtmps_protocol
+librtmpt_protocol
+librtmpte_protocol
+libsrt_protocol
+libssh_protocol
+libsmbclient_protocol
+
+'
+ALL_OBJS=' ndk media_adpt common thirdparty component sample'
+ANT_HOME=/home/yangyi/ant/apache-ant-1.9.4
+ARCH_EXT_LIST='
+    
+    armv5te
+    armv6
+    armv6t2
+    armv8
+    neon
+    vfp
+    vfpv3
+    setend
+
+    
+    altivec
+    dcbzl
+    ldbrx
+    power8
+    ppc4xx
+    vsx
+
+    
+    
+    aesni
+    amd3dnow
+    amd3dnowext
+    avx
+    avx2
+    avx512
+    fma3
+    fma4
+    mmx
+    mmxext
+    sse
+    sse2
+    sse3
+    sse4
+    sse42
+    ssse3
+    xop
+
+    cpunop
+    i686
+
+    
+    mipsfpu
+    mips32r2
+    mips32r5
+    mips64r2
+    mips32r6
+    mips64r6
+    mipsdsp
+    mipsdspr2
+    msa
+
+    
+    loongson2
+    loongson3
+    mmi
+
+'
+ARCH_EXT_LIST_ARM='
+    armv5te
+    armv6
+    armv6t2
+    armv8
+    neon
+    vfp
+    vfpv3
+    setend
+'
+ARCH_EXT_LIST_LOONGSON='
+    loongson2
+    loongson3
+    mmi
+'
+ARCH_EXT_LIST_MIPS='
+    mipsfpu
+    mips32r2
+    mips32r5
+    mips64r2
+    mips32r6
+    mips64r6
+    mipsdsp
+    mipsdspr2
+    msa
+'
+ARCH_EXT_LIST_PPC='
+    altivec
+    dcbzl
+    ldbrx
+    power8
+    ppc4xx
+    vsx
+'
+ARCH_EXT_LIST_X86='
+    
+    aesni
+    amd3dnow
+    amd3dnowext
+    avx
+    avx2
+    avx512
+    fma3
+    fma4
+    mmx
+    mmxext
+    sse
+    sse2
+    sse3
+    sse4
+    sse42
+    ssse3
+    xop
+
+    cpunop
+    i686
+'
+ARCH_EXT_LIST_X86_SIMD='
+    aesni
+    amd3dnow
+    amd3dnowext
+    avx
+    avx2
+    avx512
+    fma3
+    fma4
+    mmx
+    mmxext
+    sse
+    sse2
+    sse3
+    sse4
+    sse42
+    ssse3
+    xop
+'
+ARCH_FEATURES='
+    aligned_stack
+    fast_64bit
+    fast_clz
+    fast_cmov
+    local_aligned
+    simd_align_16
+    simd_align_32
+    simd_align_64
+'
+ARCH_LIST='
+    aarch64
+    alpha
+    arm
+    avr32
+    avr32_ap
+    avr32_uc
+    bfin
+    ia64
+    m68k
+    mips
+    mips64
+    parisc
+    ppc
+    ppc64
+    s390
+    sh4
+    sparc
+    sparc64
+    tilegx
+    tilepro
+    tomi
+    x86
+    x86_32
+    x86_64
+'
+AS_C=-c
+AS_O='-o $@'
+ATOMICS_LIST='
+    atomics_gcc
+    atomics_suncc
+    atomics_win32
+'
+AUTODETECT_LIBS='
+    
+    alsa
+    appkit
+    avfoundation
+    bzlib
+    coreimage
+    iconv
+    libxcb
+    libxcb_shm
+    libxcb_shape
+    libxcb_xfixes
+    lzma
+    schannel
+    sdl2
+    securetransport
+    sndio
+    xlib
+    zlib
+
+    
+    amf
+    audiotoolbox
+    crystalhd
+    cuda
+    cuvid
+    d3d11va
+    dxva2
+    ffnvcodec
+    nvdec
+    nvenc
+    vaapi
+    vdpau
+    videotoolbox
+    v4l2_m2m
+    xvmc
+
+    
+    pthreads
+    os2threads
+    w32threads
+
+'
+AVCODEC_COMPONENTS='
+    bsfs
+    decoders
+    encoders
+    hwaccels
+    parsers
+'
+AVCODEC_COMPONENTS_LIST='
+    aac_adtstoasc_bsf
+av1_metadata_bsf
+chomp_bsf
+dump_extradata_bsf
+dca_core_bsf
+eac3_core_bsf
+extract_extradata_bsf
+filter_units_bsf
+h264_metadata_bsf
+h264_mp4toannexb_bsf
+h264_redundant_pps_bsf
+hapqa_extract_bsf
+hevc_metadata_bsf
+hevc_mp4toannexb_bsf
+imx_dump_header_bsf
+mjpeg2jpeg_bsf
+mjpega_dump_header_bsf
+mp3_header_decompress_bsf
+mpeg2_metadata_bsf
+mpeg4_unpack_bframes_bsf
+mov2textsub_bsf
+noise_bsf
+null_bsf
+remove_extradata_bsf
+text2movsub_bsf
+trace_headers_bsf
+vp9_metadata_bsf
+vp9_raw_reorder_bsf
+vp9_superframe_bsf
+vp9_superframe_split_bsf
+    aasc_decoder
+aic_decoder
+alias_pix_decoder
+amv_decoder
+anm_decoder
+ansi_decoder
+apng_decoder
+asv1_decoder
+asv2_decoder
+aura_decoder
+aura2_decoder
+avrp_decoder
+avrn_decoder
+avs_decoder
+avui_decoder
+ayuv_decoder
+bethsoftvid_decoder
+bfi_decoder
+bink_decoder
+bitpacked_decoder
+bmp_decoder
+bmv_video_decoder
+brender_pix_decoder
+c93_decoder
+cavs_decoder
+cdgraphics_decoder
+cdxl_decoder
+cfhd_decoder
+cinepak_decoder
+clearvideo_decoder
+cljr_decoder
+cllc_decoder
+comfortnoise_decoder
+cpia_decoder
+cscd_decoder
+cyuv_decoder
+dds_decoder
+dfa_decoder
+dirac_decoder
+dnxhd_decoder
+dpx_decoder
+dsicinvideo_decoder
+dvaudio_decoder
+dvvideo_decoder
+dxa_decoder
+dxtory_decoder
+dxv_decoder
+eacmv_decoder
+eamad_decoder
+eatgq_decoder
+eatgv_decoder
+eatqi_decoder
+eightbps_decoder
+eightsvx_exp_decoder
+eightsvx_fib_decoder
+escape124_decoder
+escape130_decoder
+exr_decoder
+ffv1_decoder
+ffvhuff_decoder
+fic_decoder
+fits_decoder
+flashsv_decoder
+flashsv2_decoder
+flic_decoder
+flv_decoder
+fmvc_decoder
+fourxm_decoder
+fraps_decoder
+frwu_decoder
+g2m_decoder
+gdv_decoder
+gif_decoder
+h261_decoder
+h263_decoder
+h263i_decoder
+h263p_decoder
+h263_v4l2m2m_decoder
+h264_decoder
+h264_crystalhd_decoder
+h264_v4l2m2m_decoder
+h264_mediacodec_decoder
+h264_mmal_decoder
+h264_qsv_decoder
+h264_rkmpp_decoder
+hap_decoder
+hevc_decoder
+hevc_qsv_decoder
+hevc_rkmpp_decoder
+hevc_v4l2m2m_decoder
+hnm4_video_decoder
+hq_hqa_decoder
+hqx_decoder
+huffyuv_decoder
+idcin_decoder
+iff_ilbm_decoder
+imm4_decoder
+indeo2_decoder
+indeo3_decoder
+indeo4_decoder
+indeo5_decoder
+interplay_video_decoder
+jpeg2000_decoder
+jpegls_decoder
+jv_decoder
+kgv1_decoder
+kmvc_decoder
+lagarith_decoder
+loco_decoder
+m101_decoder
+magicyuv_decoder
+mdec_decoder
+mimic_decoder
+mjpeg_decoder
+mjpegb_decoder
+mmvideo_decoder
+motionpixels_decoder
+mpeg1video_decoder
+mpeg2video_decoder
+mpeg4_decoder
+mpeg4_crystalhd_decoder
+mpeg4_v4l2m2m_decoder
+mpeg4_mmal_decoder
+mpegvideo_decoder
+mpeg1_v4l2m2m_decoder
+mpeg2_mmal_decoder
+mpeg2_crystalhd_decoder
+mpeg2_v4l2m2m_decoder
+mpeg2_qsv_decoder
+mpeg2_mediacodec_decoder
+msa1_decoder
+mscc_decoder
+msmpeg4v1_decoder
+msmpeg4v2_decoder
+msmpeg4v3_decoder
+msmpeg4_crystalhd_decoder
+msrle_decoder
+mss1_decoder
+mss2_decoder
+msvideo1_decoder
+mszh_decoder
+mts2_decoder
+mvc1_decoder
+mvc2_decoder
+mwsc_decoder
+mxpeg_decoder
+nuv_decoder
+paf_video_decoder
+pam_decoder
+pbm_decoder
+pcx_decoder
+pgm_decoder
+pgmyuv_decoder
+pictor_decoder
+pixlet_decoder
+png_decoder
+ppm_decoder
+prores_decoder
+prosumer_decoder
+psd_decoder
+ptx_decoder
+qdraw_decoder
+qpeg_decoder
+qtrle_decoder
+r10k_decoder
+r210_decoder
+rasc_decoder
+rawvideo_decoder
+rl2_decoder
+roq_decoder
+rpza_decoder
+rscc_decoder
+rv10_decoder
+rv20_decoder
+rv30_decoder
+rv40_decoder
+s302m_decoder
+sanm_decoder
+scpr_decoder
+screenpresso_decoder
+sdx2_dpcm_decoder
+sgi_decoder
+sgirle_decoder
+sheervideo_decoder
+smacker_decoder
+smc_decoder
+smvjpeg_decoder
+snow_decoder
+sp5x_decoder
+speedhq_decoder
+srgc_decoder
+sunrast_decoder
+svq1_decoder
+svq3_decoder
+targa_decoder
+targa_y216_decoder
+tdsc_decoder
+theora_decoder
+thp_decoder
+tiertexseqvideo_decoder
+tiff_decoder
+tmv_decoder
+truemotion1_decoder
+truemotion2_decoder
+truemotion2rt_decoder
+tscc_decoder
+tscc2_decoder
+txd_decoder
+ulti_decoder
+utvideo_decoder
+v210_decoder
+v210x_decoder
+v308_decoder
+v408_decoder
+v410_decoder
+vb_decoder
+vble_decoder
+vc1_decoder
+vc1_crystalhd_decoder
+vc1image_decoder
+vc1_mmal_decoder
+vc1_qsv_decoder
+vc1_v4l2m2m_decoder
+vcr1_decoder
+vmdvideo_decoder
+vmnc_decoder
+vp3_decoder
+vp5_decoder
+vp6_decoder
+vp6a_decoder
+vp6f_decoder
+vp7_decoder
+vp8_decoder
+vp8_rkmpp_decoder
+vp8_v4l2m2m_decoder
+vp9_decoder
+vp9_rkmpp_decoder
+vp9_v4l2m2m_decoder
+vqa_decoder
+webp_decoder
+wcmv_decoder
+wrapped_avframe_decoder
+wmv1_decoder
+wmv2_decoder
+wmv3_decoder
+wmv3_crystalhd_decoder
+wmv3image_decoder
+wnv1_decoder
+xan_wc3_decoder
+xan_wc4_decoder
+xbm_decoder
+xface_decoder
+xl_decoder
+xpm_decoder
+xwd_decoder
+y41p_decoder
+ylc_decoder
+yop_decoder
+yuv4_decoder
+zero12v_decoder
+zerocodec_decoder
+zlib_decoder
+zmbv_decoder
+aac_decoder
+aac_fixed_decoder
+aac_latm_decoder
+ac3_decoder
+ac3_fixed_decoder
+alac_decoder
+als_decoder
+amrnb_decoder
+amrwb_decoder
+ape_decoder
+aptx_decoder
+aptx_hd_decoder
+atrac1_decoder
+atrac3_decoder
+atrac3al_decoder
+atrac3p_decoder
+atrac3pal_decoder
+atrac9_decoder
+binkaudio_dct_decoder
+binkaudio_rdft_decoder
+bmv_audio_decoder
+cook_decoder
+dca_decoder
+dolby_e_decoder
+dsd_lsbf_decoder
+dsd_msbf_decoder
+dsd_lsbf_planar_decoder
+dsd_msbf_planar_decoder
+dsicinaudio_decoder
+dss_sp_decoder
+dst_decoder
+eac3_decoder
+evrc_decoder
+ffwavesynth_decoder
+flac_decoder
+g723_1_decoder
+g729_decoder
+gsm_decoder
+gsm_ms_decoder
+iac_decoder
+ilbc_decoder
+imc_decoder
+interplay_acm_decoder
+mace3_decoder
+mace6_decoder
+metasound_decoder
+mlp_decoder
+mp1_decoder
+mp1float_decoder
+mp2_decoder
+mp2float_decoder
+mp3float_decoder
+mp3_decoder
+mp3adufloat_decoder
+mp3adu_decoder
+mp3on4float_decoder
+mp3on4_decoder
+mpc7_decoder
+mpc8_decoder
+nellymoser_decoder
+on2avc_decoder
+opus_decoder
+paf_audio_decoder
+qcelp_decoder
+qdm2_decoder
+qdmc_decoder
+ra_144_decoder
+ra_288_decoder
+ralf_decoder
+sbc_decoder
+shorten_decoder
+sipr_decoder
+smackaud_decoder
+sonic_decoder
+tak_decoder
+truehd_decoder
+truespeech_decoder
+tta_decoder
+twinvq_decoder
+vmdaudio_decoder
+vorbis_decoder
+wavpack_decoder
+wmalossless_decoder
+wmapro_decoder
+wmav1_decoder
+wmav2_decoder
+wmavoice_decoder
+ws_snd1_decoder
+xma1_decoder
+xma2_decoder
+pcm_alaw_decoder
+pcm_bluray_decoder
+pcm_dvd_decoder
+pcm_f16le_decoder
+pcm_f24le_decoder
+pcm_f32be_decoder
+pcm_f32le_decoder
+pcm_f64be_decoder
+pcm_f64le_decoder
+pcm_lxf_decoder
+pcm_mulaw_decoder
+pcm_s8_decoder
+pcm_s8_planar_decoder
+pcm_s16be_decoder
+pcm_s16be_planar_decoder
+pcm_s16le_decoder
+pcm_s16le_planar_decoder
+pcm_s24be_decoder
+pcm_s24daud_decoder
+pcm_s24le_decoder
+pcm_s24le_planar_decoder
+pcm_s32be_decoder
+pcm_s32le_decoder
+pcm_s32le_planar_decoder
+pcm_s64be_decoder
+pcm_s64le_decoder
+pcm_u8_decoder
+pcm_u16be_decoder
+pcm_u16le_decoder
+pcm_u24be_decoder
+pcm_u24le_decoder
+pcm_u32be_decoder
+pcm_u32le_decoder
+pcm_vidc_decoder
+pcm_zork_decoder
+gremlin_dpcm_decoder
+interplay_dpcm_decoder
+roq_dpcm_decoder
+sol_dpcm_decoder
+xan_dpcm_decoder
+adpcm_4xm_decoder
+adpcm_adx_decoder
+adpcm_afc_decoder
+adpcm_aica_decoder
+adpcm_ct_decoder
+adpcm_dtk_decoder
+adpcm_ea_decoder
+adpcm_ea_maxis_xa_decoder
+adpcm_ea_r1_decoder
+adpcm_ea_r2_decoder
+adpcm_ea_r3_decoder
+adpcm_ea_xas_decoder
+adpcm_g722_decoder
+adpcm_g726_decoder
+adpcm_g726le_decoder
+adpcm_ima_amv_decoder
+adpcm_ima_apc_decoder
+adpcm_ima_dat4_decoder
+adpcm_ima_dk3_decoder
+adpcm_ima_dk4_decoder
+adpcm_ima_ea_eacs_decoder
+adpcm_ima_ea_sead_decoder
+adpcm_ima_iss_decoder
+adpcm_ima_oki_decoder
+adpcm_ima_qt_decoder
+adpcm_ima_rad_decoder
+adpcm_ima_smjpeg_decoder
+adpcm_ima_wav_decoder
+adpcm_ima_ws_decoder
+adpcm_ms_decoder
+adpcm_mtaf_decoder
+adpcm_psx_decoder
+adpcm_sbpro_2_decoder
+adpcm_sbpro_3_decoder
+adpcm_sbpro_4_decoder
+adpcm_swf_decoder
+adpcm_thp_decoder
+adpcm_thp_le_decoder
+adpcm_vima_decoder
+adpcm_xa_decoder
+adpcm_yamaha_decoder
+ssa_decoder
+ass_decoder
+ccaption_decoder
+dvbsub_decoder
+dvdsub_decoder
+jacosub_decoder
+microdvd_decoder
+movtext_decoder
+mpl2_decoder
+pgssub_decoder
+pjs_decoder
+realtext_decoder
+sami_decoder
+srt_decoder
+stl_decoder
+subrip_decoder
+subviewer_decoder
+subviewer1_decoder
+text_decoder
+vplayer_decoder
+webvtt_decoder
+xsub_decoder
+aac_at_decoder
+ac3_at_decoder
+adpcm_ima_qt_at_decoder
+alac_at_decoder
+amr_nb_at_decoder
+eac3_at_decoder
+gsm_ms_at_decoder
+ilbc_at_decoder
+mp1_at_decoder
+mp2_at_decoder
+mp3_at_decoder
+pcm_alaw_at_decoder
+pcm_mulaw_at_decoder
+qdmc_at_decoder
+qdm2_at_decoder
+libaom_av1_decoder
+libcelt_decoder
+libcodec2_decoder
+libdavs2_decoder
+libfdk_aac_decoder
+libgsm_decoder
+libgsm_ms_decoder
+libilbc_decoder
+libopencore_amrnb_decoder
+libopencore_amrwb_decoder
+libopenjpeg_decoder
+libopus_decoder
+librsvg_decoder
+libspeex_decoder
+libvorbis_decoder
+libvpx_vp8_decoder
+libvpx_vp9_decoder
+libzvbi_teletext_decoder
+bintext_decoder
+xbin_decoder
+idf_decoder
+libopenh264_decoder
+h264_cuvid_decoder
+hevc_cuvid_decoder
+hevc_mediacodec_decoder
+mjpeg_cuvid_decoder
+mpeg1_cuvid_decoder
+mpeg2_cuvid_decoder
+mpeg4_cuvid_decoder
+mpeg4_mediacodec_decoder
+vc1_cuvid_decoder
+vp8_cuvid_decoder
+vp8_mediacodec_decoder
+vp8_qsv_decoder
+vp9_cuvid_decoder
+vp9_mediacodec_decoder
+    a64multi_encoder
+a64multi5_encoder
+alias_pix_encoder
+amv_encoder
+apng_encoder
+asv1_encoder
+asv2_encoder
+avrp_encoder
+avui_encoder
+ayuv_encoder
+bmp_encoder
+cinepak_encoder
+cljr_encoder
+comfortnoise_encoder
+dnxhd_encoder
+dpx_encoder
+dvvideo_encoder
+ffv1_encoder
+ffvhuff_encoder
+fits_encoder
+flashsv_encoder
+flashsv2_encoder
+flv_encoder
+gif_encoder
+h261_encoder
+h263_encoder
+h263p_encoder
+hap_encoder
+huffyuv_encoder
+jpeg2000_encoder
+jpegls_encoder
+ljpeg_encoder
+magicyuv_encoder
+mjpeg_encoder
+mpeg1video_encoder
+mpeg2video_encoder
+mpeg4_encoder
+msmpeg4v2_encoder
+msmpeg4v3_encoder
+msvideo1_encoder
+pam_encoder
+pbm_encoder
+pcx_encoder
+pgm_encoder
+pgmyuv_encoder
+png_encoder
+ppm_encoder
+prores_encoder
+prores_aw_encoder
+prores_ks_encoder
+qtrle_encoder
+r10k_encoder
+r210_encoder
+rawvideo_encoder
+roq_encoder
+rv10_encoder
+rv20_encoder
+s302m_encoder
+sgi_encoder
+snow_encoder
+sunrast_encoder
+svq1_encoder
+targa_encoder
+tiff_encoder
+utvideo_encoder
+v210_encoder
+v308_encoder
+v408_encoder
+v410_encoder
+vc2_encoder
+wrapped_avframe_encoder
+wmv1_encoder
+wmv2_encoder
+xbm_encoder
+xface_encoder
+xwd_encoder
+y41p_encoder
+yuv4_encoder
+zlib_encoder
+zmbv_encoder
+aac_encoder
+ac3_encoder
+ac3_fixed_encoder
+alac_encoder
+aptx_encoder
+aptx_hd_encoder
+dca_encoder
+eac3_encoder
+flac_encoder
+g723_1_encoder
+mlp_encoder
+mp2_encoder
+mp2fixed_encoder
+nellymoser_encoder
+opus_encoder
+ra_144_encoder
+sbc_encoder
+sonic_encoder
+sonic_ls_encoder
+truehd_encoder
+tta_encoder
+vorbis_encoder
+wavpack_encoder
+wmav1_encoder
+wmav2_encoder
+pcm_alaw_encoder
+pcm_f32be_encoder
+pcm_f32le_encoder
+pcm_f64be_encoder
+pcm_f64le_encoder
+pcm_mulaw_encoder
+pcm_s8_encoder
+pcm_s8_planar_encoder
+pcm_s16be_encoder
+pcm_s16be_planar_encoder
+pcm_s16le_encoder
+pcm_s16le_planar_encoder
+pcm_s24be_encoder
+pcm_s24daud_encoder
+pcm_s24le_encoder
+pcm_s24le_planar_encoder
+pcm_s32be_encoder
+pcm_s32le_encoder
+pcm_s32le_planar_encoder
+pcm_s64be_encoder
+pcm_s64le_encoder
+pcm_u8_encoder
+pcm_u16be_encoder
+pcm_u16le_encoder
+pcm_u24be_encoder
+pcm_u24le_encoder
+pcm_u32be_encoder
+pcm_u32le_encoder
+pcm_vidc_encoder
+roq_dpcm_encoder
+adpcm_adx_encoder
+adpcm_g722_encoder
+adpcm_g726_encoder
+adpcm_g726le_encoder
+adpcm_ima_qt_encoder
+adpcm_ima_wav_encoder
+adpcm_ms_encoder
+adpcm_swf_encoder
+adpcm_yamaha_encoder
+ssa_encoder
+ass_encoder
+dvbsub_encoder
+dvdsub_encoder
+movtext_encoder
+srt_encoder
+subrip_encoder
+text_encoder
+webvtt_encoder
+xsub_encoder
+aac_at_encoder
+alac_at_encoder
+ilbc_at_encoder
+pcm_alaw_at_encoder
+pcm_mulaw_at_encoder
+libaom_av1_encoder
+libcodec2_encoder
+libfdk_aac_encoder
+libgsm_encoder
+libgsm_ms_encoder
+libilbc_encoder
+libmp3lame_encoder
+libopencore_amrnb_encoder
+libopenjpeg_encoder
+libopus_encoder
+libshine_encoder
+libspeex_encoder
+libtheora_encoder
+libtwolame_encoder
+libvo_amrwbenc_encoder
+libvorbis_encoder
+libvpx_vp8_encoder
+libvpx_vp9_encoder
+libwavpack_encoder
+libwebp_anim_encoder
+libwebp_encoder
+libx262_encoder
+libx264_encoder
+libx264rgb_encoder
+libx265_encoder
+libxavs_encoder
+libxavs2_encoder
+libxvid_encoder
+h263_v4l2m2m_encoder
+libopenh264_encoder
+h264_amf_encoder
+h264_nvenc_encoder
+h264_omx_encoder
+h264_qsv_encoder
+h264_v4l2m2m_encoder
+h264_vaapi_encoder
+h264_videotoolbox_encoder
+nvenc_encoder
+nvenc_h264_encoder
+nvenc_hevc_encoder
+hevc_amf_encoder
+hevc_nvenc_encoder
+hevc_qsv_encoder
+hevc_v4l2m2m_encoder
+hevc_vaapi_encoder
+hevc_videotoolbox_encoder
+libkvazaar_encoder
+mjpeg_qsv_encoder
+mjpeg_vaapi_encoder
+mpeg2_qsv_encoder
+mpeg2_vaapi_encoder
+mpeg4_v4l2m2m_encoder
+vp8_v4l2m2m_encoder
+vp8_vaapi_encoder
+vp9_vaapi_encoder
+    h263_vaapi_hwaccel
+h263_videotoolbox_hwaccel
+h264_d3d11va_hwaccel
+h264_d3d11va2_hwaccel
+h264_dxva2_hwaccel
+h264_nvdec_hwaccel
+h264_vaapi_hwaccel
+h264_vdpau_hwaccel
+h264_videotoolbox_hwaccel
+hevc_d3d11va_hwaccel
+hevc_d3d11va2_hwaccel
+hevc_dxva2_hwaccel
+hevc_nvdec_hwaccel
+hevc_vaapi_hwaccel
+hevc_vdpau_hwaccel
+hevc_videotoolbox_hwaccel
+mjpeg_nvdec_hwaccel
+mjpeg_vaapi_hwaccel
+mpeg1_nvdec_hwaccel
+mpeg1_vdpau_hwaccel
+mpeg1_videotoolbox_hwaccel
+mpeg1_xvmc_hwaccel
+mpeg2_d3d11va_hwaccel
+mpeg2_d3d11va2_hwaccel
+mpeg2_nvdec_hwaccel
+mpeg2_dxva2_hwaccel
+mpeg2_vaapi_hwaccel
+mpeg2_vdpau_hwaccel
+mpeg2_videotoolbox_hwaccel
+mpeg2_xvmc_hwaccel
+mpeg4_nvdec_hwaccel
+mpeg4_vaapi_hwaccel
+mpeg4_vdpau_hwaccel
+mpeg4_videotoolbox_hwaccel
+vc1_d3d11va_hwaccel
+vc1_d3d11va2_hwaccel
+vc1_dxva2_hwaccel
+vc1_nvdec_hwaccel
+vc1_vaapi_hwaccel
+vc1_vdpau_hwaccel
+vp8_nvdec_hwaccel
+vp8_vaapi_hwaccel
+vp9_d3d11va_hwaccel
+vp9_d3d11va2_hwaccel
+vp9_dxva2_hwaccel
+vp9_nvdec_hwaccel
+vp9_vaapi_hwaccel
+wmv3_d3d11va_hwaccel
+wmv3_d3d11va2_hwaccel
+wmv3_dxva2_hwaccel
+wmv3_nvdec_hwaccel
+wmv3_vaapi_hwaccel
+wmv3_vdpau_hwaccel
+    aac_parser
+aac_latm_parser
+ac3_parser
+adx_parser
+av1_parser
+avs2_parser
+bmp_parser
+cavsvideo_parser
+cook_parser
+dca_parser
+dirac_parser
+dnxhd_parser
+dpx_parser
+dvaudio_parser
+dvbsub_parser
+dvdsub_parser
+dvd_nav_parser
+flac_parser
+g729_parser
+gsm_parser
+h261_parser
+h263_parser
+h264_parser
+hevc_parser
+mjpeg_parser
+mlp_parser
+mpeg4video_parser
+mpegaudio_parser
+mpegvideo_parser
+opus_parser
+png_parser
+pnm_parser
+rv30_parser
+rv40_parser
+sbc_parser
+sipr_parser
+tak_parser
+vc1_parser
+vorbis_parser
+vp3_parser
+vp8_parser
+vp9_parser
+xma_parser
+'
+AVDEVICE_COMPONENTS='
+    indevs
+    outdevs
+'
+AVDEVICE_COMPONENTS_LIST='
+    alsa_indev
+android_camera_indev
+avfoundation_indev
+bktr_indev
+decklink_indev
+libndi_newtek_indev
+dshow_indev
+fbdev_indev
+gdigrab_indev
+iec61883_indev
+jack_indev
+kmsgrab_indev
+lavfi_indev
+openal_indev
+oss_indev
+pulse_indev
+sndio_indev
+v4l2_indev
+vfwcap_indev
+xcbgrab_indev
+libcdio_indev
+libdc1394_indev
+    alsa_outdev
+caca_outdev
+decklink_outdev
+libndi_newtek_outdev
+fbdev_outdev
+opengl_outdev
+oss_outdev
+pulse_outdev
+sdl2_outdev
+sndio_outdev
+v4l2_outdev
+xv_outdev
+'
+AVFILTER_COMPONENTS='
+    filters
+'
+AVFILTER_COMPONENTS_LIST='
+    abench_filter
+acompressor_filter
+acontrast_filter
+acopy_filter
+acue_filter
+acrossfade_filter
+acrossover_filter
+acrusher_filter
+adeclick_filter
+adeclip_filter
+adelay_filter
+aderivative_filter
+aecho_filter
+aemphasis_filter
+aeval_filter
+afade_filter
+afftdn_filter
+afftfilt_filter
+afir_filter
+aformat_filter
+agate_filter
+aiir_filter
+aintegral_filter
+ainterleave_filter
+alimiter_filter
+allpass_filter
+aloop_filter
+amerge_filter
+ametadata_filter
+amix_filter
+amultiply_filter
+anequalizer_filter
+anull_filter
+apad_filter
+aperms_filter
+aphaser_filter
+apulsator_filter
+arealtime_filter
+aresample_filter
+areverse_filter
+aselect_filter
+asendcmd_filter
+asetnsamples_filter
+asetpts_filter
+asetrate_filter
+asettb_filter
+ashowinfo_filter
+asidedata_filter
+asplit_filter
+astats_filter
+astreamselect_filter
+atempo_filter
+atrim_filter
+azmq_filter
+bandpass_filter
+bandreject_filter
+bass_filter
+biquad_filter
+bs2b_filter
+channelmap_filter
+channelsplit_filter
+chorus_filter
+compand_filter
+compensationdelay_filter
+crossfeed_filter
+crystalizer_filter
+dcshift_filter
+drmeter_filter
+dynaudnorm_filter
+earwax_filter
+ebur128_filter
+equalizer_filter
+extrastereo_filter
+firequalizer_filter
+flanger_filter
+haas_filter
+hdcd_filter
+headphone_filter
+highpass_filter
+highshelf_filter
+join_filter
+ladspa_filter
+loudnorm_filter
+lowpass_filter
+lowshelf_filter
+lv2_filter
+mcompand_filter
+pan_filter
+replaygain_filter
+resample_filter
+rubberband_filter
+sidechaincompress_filter
+sidechaingate_filter
+silencedetect_filter
+silenceremove_filter
+sofalizer_filter
+stereotools_filter
+stereowiden_filter
+superequalizer_filter
+surround_filter
+treble_filter
+tremolo_filter
+vibrato_filter
+volume_filter
+volumedetect_filter
+aevalsrc_filter
+anoisesrc_filter
+anullsrc_filter
+flite_filter
+hilbert_filter
+sinc_filter
+sine_filter
+anullsink_filter
+alphaextract_filter
+alphamerge_filter
+amplify_filter
+ass_filter
+atadenoise_filter
+avgblur_filter
+avgblur_opencl_filter
+bbox_filter
+bench_filter
+bitplanenoise_filter
+blackdetect_filter
+blackframe_filter
+blend_filter
+bm3d_filter
+boxblur_filter
+boxblur_opencl_filter
+bwdif_filter
+chromahold_filter
+chromakey_filter
+ciescope_filter
+codecview_filter
+colorbalance_filter
+colorchannelmixer_filter
+colorkey_filter
+colorlevels_filter
+colormatrix_filter
+colorspace_filter
+convolution_filter
+convolution_opencl_filter
+convolve_filter
+copy_filter
+coreimage_filter
+cover_rect_filter
+crop_filter
+cropdetect_filter
+cue_filter
+curves_filter
+datascope_filter
+dctdnoiz_filter
+deband_filter
+deblock_filter
+decimate_filter
+deconvolve_filter
+deflate_filter
+deflicker_filter
+deinterlace_qsv_filter
+deinterlace_vaapi_filter
+dejudder_filter
+delogo_filter
+denoise_vaapi_filter
+deshake_filter
+despill_filter
+detelecine_filter
+dilation_filter
+dilation_opencl_filter
+displace_filter
+doubleweave_filter
+drawbox_filter
+drawgraph_filter
+drawgrid_filter
+drawtext_filter
+edgedetect_filter
+elbg_filter
+entropy_filter
+eq_filter
+erosion_filter
+erosion_opencl_filter
+extractplanes_filter
+fade_filter
+fftdnoiz_filter
+fftfilt_filter
+field_filter
+fieldhint_filter
+fieldmatch_filter
+fieldorder_filter
+fillborders_filter
+find_rect_filter
+floodfill_filter
+format_filter
+fps_filter
+framepack_filter
+framerate_filter
+framestep_filter
+frei0r_filter
+fspp_filter
+gblur_filter
+geq_filter
+gradfun_filter
+graphmonitor_filter
+greyedge_filter
+haldclut_filter
+hflip_filter
+histeq_filter
+histogram_filter
+hqdn3d_filter
+hqx_filter
+hstack_filter
+hue_filter
+hwdownload_filter
+hwmap_filter
+hwupload_filter
+hwupload_cuda_filter
+hysteresis_filter
+idet_filter
+il_filter
+inflate_filter
+interlace_filter
+interleave_filter
+kerndeint_filter
+lenscorrection_filter
+lensfun_filter
+libvmaf_filter
+limiter_filter
+loop_filter
+lumakey_filter
+lut_filter
+lut1d_filter
+lut2_filter
+lut3d_filter
+lutrgb_filter
+lutyuv_filter
+maskedclamp_filter
+maskedmerge_filter
+mcdeint_filter
+mergeplanes_filter
+mestimate_filter
+metadata_filter
+midequalizer_filter
+minterpolate_filter
+mix_filter
+mpdecimate_filter
+negate_filter
+nlmeans_filter
+nnedi_filter
+noformat_filter
+noise_filter
+normalize_filter
+null_filter
+ocr_filter
+ocv_filter
+oscilloscope_filter
+overlay_filter
+overlay_opencl_filter
+overlay_qsv_filter
+owdenoise_filter
+pad_filter
+palettegen_filter
+paletteuse_filter
+perms_filter
+perspective_filter
+phase_filter
+pixdesctest_filter
+pixscope_filter
+pp_filter
+pp7_filter
+premultiply_filter
+prewitt_filter
+prewitt_opencl_filter
+procamp_vaapi_filter
+program_opencl_filter
+pseudocolor_filter
+psnr_filter
+pullup_filter
+qp_filter
+random_filter
+readeia608_filter
+readvitc_filter
+realtime_filter
+remap_filter
+removegrain_filter
+removelogo_filter
+repeatfields_filter
+reverse_filter
+roberts_filter
+roberts_opencl_filter
+rotate_filter
+sab_filter
+scale_filter
+scale_cuda_filter
+scale_npp_filter
+scale_qsv_filter
+scale_vaapi_filter
+scale2ref_filter
+select_filter
+selectivecolor_filter
+sendcmd_filter
+separatefields_filter
+setdar_filter
+setfield_filter
+setparams_filter
+setpts_filter
+setrange_filter
+setsar_filter
+settb_filter
+sharpness_vaapi_filter
+showinfo_filter
+showpalette_filter
+shuffleframes_filter
+shuffleplanes_filter
+sidedata_filter
+signalstats_filter
+signature_filter
+smartblur_filter
+sobel_filter
+sobel_opencl_filter
+split_filter
+spp_filter
+sr_filter
+ssim_filter
+stereo3d_filter
+streamselect_filter
+subtitles_filter
+super2xsai_filter
+swaprect_filter
+swapuv_filter
+tblend_filter
+telecine_filter
+threshold_filter
+thumbnail_filter
+thumbnail_cuda_filter
+tile_filter
+tinterlace_filter
+tlut2_filter
+tmix_filter
+tonemap_filter
+tonemap_opencl_filter
+transpose_filter
+transpose_npp_filter
+trim_filter
+unpremultiply_filter
+unsharp_filter
+unsharp_opencl_filter
+uspp_filter
+vaguedenoiser_filter
+vectorscope_filter
+vflip_filter
+vfrdet_filter
+vibrance_filter
+vidstabdetect_filter
+vidstabtransform_filter
+vignette_filter
+vmafmotion_filter
+vpp_qsv_filter
+vstack_filter
+w3fdif_filter
+waveform_filter
+weave_filter
+xbr_filter
+xstack_filter
+yadif_filter
+yadif_cuda_filter
+zmq_filter
+zoompan_filter
+zscale_filter
+allrgb_filter
+allyuv_filter
+cellauto_filter
+color_filter
+coreimagesrc_filter
+frei0r_src_filter
+haldclutsrc_filter
+life_filter
+mandelbrot_filter
+mptestsrc_filter
+nullsrc_filter
+openclsrc_filter
+pal75bars_filter
+pal100bars_filter
+rgbtestsrc_filter
+smptebars_filter
+smptehdbars_filter
+testsrc_filter
+testsrc2_filter
+yuvtestsrc_filter
+nullsink_filter
+abitscope_filter
+adrawgraph_filter
+agraphmonitor_filter
+ahistogram_filter
+aphasemeter_filter
+avectorscope_filter
+concat_filter
+showcqt_filter
+showfreqs_filter
+showspectrum_filter
+showspectrumpic_filter
+showvolume_filter
+showwaves_filter
+showwavespic_filter
+spectrumsynth_filter
+amovie_filter
+movie_filter
+afifo_filter
+fifo_filter
+'
+AVFORMAT_COMPONENTS='
+    demuxers
+    muxers
+    protocols
+'
+AVFORMAT_COMPONENTS_LIST='
+    aa_demuxer
+aac_demuxer
+ac3_demuxer
+acm_demuxer
+act_demuxer
+adf_demuxer
+adp_demuxer
+ads_demuxer
+adx_demuxer
+aea_demuxer
+afc_demuxer
+aiff_demuxer
+aix_demuxer
+amr_demuxer
+amrnb_demuxer
+amrwb_demuxer
+anm_demuxer
+apc_demuxer
+ape_demuxer
+apng_demuxer
+aptx_demuxer
+aptx_hd_demuxer
+aqtitle_demuxer
+asf_demuxer
+asf_o_demuxer
+ass_demuxer
+ast_demuxer
+au_demuxer
+avi_demuxer
+avisynth_demuxer
+avr_demuxer
+avs_demuxer
+avs2_demuxer
+bethsoftvid_demuxer
+bfi_demuxer
+bintext_demuxer
+bink_demuxer
+bit_demuxer
+bmv_demuxer
+bfstm_demuxer
+brstm_demuxer
+boa_demuxer
+c93_demuxer
+caf_demuxer
+cavsvideo_demuxer
+cdg_demuxer
+cdxl_demuxer
+cine_demuxer
+codec2_demuxer
+codec2raw_demuxer
+concat_demuxer
+dash_demuxer
+data_demuxer
+daud_demuxer
+dcstr_demuxer
+dfa_demuxer
+dirac_demuxer
+dnxhd_demuxer
+dsf_demuxer
+dsicin_demuxer
+dss_demuxer
+dts_demuxer
+dtshd_demuxer
+dv_demuxer
+dvbsub_demuxer
+dvbtxt_demuxer
+dxa_demuxer
+ea_demuxer
+ea_cdata_demuxer
+eac3_demuxer
+epaf_demuxer
+ffmetadata_demuxer
+filmstrip_demuxer
+fits_demuxer
+flac_demuxer
+flic_demuxer
+flv_demuxer
+live_flv_demuxer
+fourxm_demuxer
+frm_demuxer
+fsb_demuxer
+g722_demuxer
+g723_1_demuxer
+g726_demuxer
+g726le_demuxer
+g729_demuxer
+gdv_demuxer
+genh_demuxer
+gif_demuxer
+gsm_demuxer
+gxf_demuxer
+h261_demuxer
+h263_demuxer
+h264_demuxer
+hevc_demuxer
+hls_demuxer
+hnm_demuxer
+ico_demuxer
+idcin_demuxer
+idf_demuxer
+iff_demuxer
+ilbc_demuxer
+image2_demuxer
+image2pipe_demuxer
+image2_alias_pix_demuxer
+image2_brender_pix_demuxer
+ingenient_demuxer
+ipmovie_demuxer
+ircam_demuxer
+iss_demuxer
+iv8_demuxer
+ivf_demuxer
+ivr_demuxer
+jacosub_demuxer
+jv_demuxer
+lmlm4_demuxer
+loas_demuxer
+lrc_demuxer
+lvf_demuxer
+lxf_demuxer
+m4v_demuxer
+matroska_demuxer
+mgsts_demuxer
+microdvd_demuxer
+mjpeg_demuxer
+mjpeg_2000_demuxer
+mlp_demuxer
+mlv_demuxer
+mm_demuxer
+mmf_demuxer
+mov_demuxer
+mp3_demuxer
+mpc_demuxer
+mpc8_demuxer
+mpegps_demuxer
+mpegts_demuxer
+mpegtsraw_demuxer
+mpegvideo_demuxer
+mpjpeg_demuxer
+mpl2_demuxer
+mpsub_demuxer
+msf_demuxer
+msnwc_tcp_demuxer
+mtaf_demuxer
+mtv_demuxer
+musx_demuxer
+mv_demuxer
+mvi_demuxer
+mxf_demuxer
+mxg_demuxer
+nc_demuxer
+nistsphere_demuxer
+nsp_demuxer
+nsv_demuxer
+nut_demuxer
+nuv_demuxer
+ogg_demuxer
+oma_demuxer
+paf_demuxer
+pcm_alaw_demuxer
+pcm_mulaw_demuxer
+pcm_vidc_demuxer
+pcm_f64be_demuxer
+pcm_f64le_demuxer
+pcm_f32be_demuxer
+pcm_f32le_demuxer
+pcm_s32be_demuxer
+pcm_s32le_demuxer
+pcm_s24be_demuxer
+pcm_s24le_demuxer
+pcm_s16be_demuxer
+pcm_s16le_demuxer
+pcm_s8_demuxer
+pcm_u32be_demuxer
+pcm_u32le_demuxer
+pcm_u24be_demuxer
+pcm_u24le_demuxer
+pcm_u16be_demuxer
+pcm_u16le_demuxer
+pcm_u8_demuxer
+pjs_demuxer
+pmp_demuxer
+pva_demuxer
+pvf_demuxer
+qcp_demuxer
+r3d_demuxer
+rawvideo_demuxer
+realtext_demuxer
+redspark_demuxer
+rl2_demuxer
+rm_demuxer
+roq_demuxer
+rpl_demuxer
+rsd_demuxer
+rso_demuxer
+rtp_demuxer
+rtsp_demuxer
+s337m_demuxer
+sami_demuxer
+sap_demuxer
+sbc_demuxer
+sbg_demuxer
+scc_demuxer
+sdp_demuxer
+sdr2_demuxer
+sds_demuxer
+sdx_demuxer
+segafilm_demuxer
+ser_demuxer
+shorten_demuxer
+siff_demuxer
+sln_demuxer
+smacker_demuxer
+smjpeg_demuxer
+smush_demuxer
+sol_demuxer
+sox_demuxer
+spdif_demuxer
+srt_demuxer
+str_demuxer
+stl_demuxer
+subviewer1_demuxer
+subviewer_demuxer
+sup_demuxer
+svag_demuxer
+swf_demuxer
+tak_demuxer
+tedcaptions_demuxer
+thp_demuxer
+threedostr_demuxer
+tiertexseq_demuxer
+tmv_demuxer
+truehd_demuxer
+tta_demuxer
+txd_demuxer
+tty_demuxer
+ty_demuxer
+v210_demuxer
+v210x_demuxer
+vag_demuxer
+vc1_demuxer
+vc1t_demuxer
+vivo_demuxer
+vmd_demuxer
+vobsub_demuxer
+voc_demuxer
+vpk_demuxer
+vplayer_demuxer
+vqf_demuxer
+w64_demuxer
+wav_demuxer
+wc3_demuxer
+webm_dash_manifest_demuxer
+webvtt_demuxer
+wsaud_demuxer
+wsd_demuxer
+wsvqa_demuxer
+wtv_demuxer
+wve_demuxer
+wv_demuxer
+xa_demuxer
+xbin_demuxer
+xmv_demuxer
+xvag_demuxer
+xwma_demuxer
+yop_demuxer
+yuv4mpegpipe_demuxer
+image_bmp_pipe_demuxer
+image_dds_pipe_demuxer
+image_dpx_pipe_demuxer
+image_exr_pipe_demuxer
+image_j2k_pipe_demuxer
+image_jpeg_pipe_demuxer
+image_jpegls_pipe_demuxer
+image_pam_pipe_demuxer
+image_pbm_pipe_demuxer
+image_pcx_pipe_demuxer
+image_pgmyuv_pipe_demuxer
+image_pgm_pipe_demuxer
+image_pictor_pipe_demuxer
+image_png_pipe_demuxer
+image_ppm_pipe_demuxer
+image_psd_pipe_demuxer
+image_qdraw_pipe_demuxer
+image_sgi_pipe_demuxer
+image_svg_pipe_demuxer
+image_sunrast_pipe_demuxer
+image_tiff_pipe_demuxer
+image_webp_pipe_demuxer
+image_xpm_pipe_demuxer
+image_xwd_pipe_demuxer
+libgme_demuxer
+libmodplug_demuxer
+libopenmpt_demuxer
+vapoursynth_demuxer
+    a64_muxer
+ac3_muxer
+adts_muxer
+adx_muxer
+aiff_muxer
+amr_muxer
+apng_muxer
+aptx_muxer
+aptx_hd_muxer
+asf_muxer
+ass_muxer
+ast_muxer
+asf_stream_muxer
+au_muxer
+avi_muxer
+avm2_muxer
+avs2_muxer
+bit_muxer
+caf_muxer
+cavsvideo_muxer
+codec2_muxer
+codec2raw_muxer
+crc_muxer
+dash_muxer
+data_muxer
+daud_muxer
+dirac_muxer
+dnxhd_muxer
+dts_muxer
+dv_muxer
+eac3_muxer
+f4v_muxer
+ffmetadata_muxer
+fifo_muxer
+fifo_test_muxer
+filmstrip_muxer
+fits_muxer
+flac_muxer
+flv_muxer
+framecrc_muxer
+framehash_muxer
+framemd5_muxer
+g722_muxer
+g723_1_muxer
+g726_muxer
+g726le_muxer
+gif_muxer
+gsm_muxer
+gxf_muxer
+h261_muxer
+h263_muxer
+h264_muxer
+hash_muxer
+hds_muxer
+hevc_muxer
+hls_muxer
+ico_muxer
+ilbc_muxer
+image2_muxer
+image2pipe_muxer
+ipod_muxer
+ircam_muxer
+ismv_muxer
+ivf_muxer
+jacosub_muxer
+latm_muxer
+lrc_muxer
+m4v_muxer
+md5_muxer
+matroska_muxer
+matroska_audio_muxer
+microdvd_muxer
+mjpeg_muxer
+mlp_muxer
+mmf_muxer
+mov_muxer
+mp2_muxer
+mp3_muxer
+mp4_muxer
+mpeg1system_muxer
+mpeg1vcd_muxer
+mpeg1video_muxer
+mpeg2dvd_muxer
+mpeg2svcd_muxer
+mpeg2video_muxer
+mpeg2vob_muxer
+mpegts_muxer
+mpjpeg_muxer
+mxf_muxer
+mxf_d10_muxer
+mxf_opatom_muxer
+null_muxer
+nut_muxer
+oga_muxer
+ogg_muxer
+ogv_muxer
+oma_muxer
+opus_muxer
+pcm_alaw_muxer
+pcm_mulaw_muxer
+pcm_vidc_muxer
+pcm_f64be_muxer
+pcm_f64le_muxer
+pcm_f32be_muxer
+pcm_f32le_muxer
+pcm_s32be_muxer
+pcm_s32le_muxer
+pcm_s24be_muxer
+pcm_s24le_muxer
+pcm_s16be_muxer
+pcm_s16le_muxer
+pcm_s8_muxer
+pcm_u32be_muxer
+pcm_u32le_muxer
+pcm_u24be_muxer
+pcm_u24le_muxer
+pcm_u16be_muxer
+pcm_u16le_muxer
+pcm_u8_muxer
+psp_muxer
+rawvideo_muxer
+rm_muxer
+roq_muxer
+rso_muxer
+rtp_muxer
+rtp_mpegts_muxer
+rtsp_muxer
+sap_muxer
+sbc_muxer
+scc_muxer
+segafilm_muxer
+segment_muxer
+stream_segment_muxer
+singlejpeg_muxer
+smjpeg_muxer
+smoothstreaming_muxer
+sox_muxer
+spx_muxer
+spdif_muxer
+srt_muxer
+sup_muxer
+swf_muxer
+tee_muxer
+tg2_muxer
+tgp_muxer
+mkvtimestamp_v2_muxer
+truehd_muxer
+tta_muxer
+uncodedframecrc_muxer
+vc1_muxer
+vc1t_muxer
+voc_muxer
+w64_muxer
+wav_muxer
+webm_muxer
+webm_dash_manifest_muxer
+webm_chunk_muxer
+webp_muxer
+webvtt_muxer
+wtv_muxer
+wv_muxer
+yuv4mpegpipe_muxer
+chromaprint_muxer
+    async_protocol
+bluray_protocol
+cache_protocol
+concat_protocol
+crypto_protocol
+data_protocol
+ffrtmpcrypt_protocol
+ffrtmphttp_protocol
+file_protocol
+ftp_protocol
+gopher_protocol
+hls_protocol
+http_protocol
+httpproxy_protocol
+https_protocol
+icecast_protocol
+mmsh_protocol
+mmst_protocol
+md5_protocol
+pipe_protocol
+prompeg_protocol
+rtmp_protocol
+rtmpe_protocol
+rtmps_protocol
+rtmpt_protocol
+rtmpte_protocol
+rtmpts_protocol
+rtp_protocol
+sctp_protocol
+srtp_protocol
+subfile_protocol
+tee_protocol
+tcp_protocol
+tls_protocol
+udp_protocol
+udplite_protocol
+unix_protocol
+librtmp_protocol
+librtmpe_protocol
+librtmps_protocol
+librtmpt_protocol
+librtmpte_protocol
+libsrt_protocol
+libssh_protocol
+libsmbclient_protocol
+'
+BASH=/bin/sh
+BASHOPTS=cmdhist:complete_fullquote:extquote:force_fignore:hostcomplete:interactive_comments:progcomp:promptvars:sourcepath
+BASH_ALIASES=()
+BASH_ARGC=([0]="44")
+BASH_ARGV=([0]="--cross-prefix=arm-himix100-linux-" [1]="--arch=armv7-a" [2]="--cpu=cortex-a7" [3]="--disable-gpl" [4]="--enable-demuxer=wav" [5]="--disable-pixelutils" [6]="--enable-bsf=hevc_mp4toannexb" [7]="--enable-bsf=h264_mp4toannexb" [8]="--disable-mediacodec" [9]="--disable-hardcoded-tables" [10]="--disable-vfp" [11]="--disable-armv5te" [12]="--disable-armv6t2" [13]="--disable-armv6" [14]="--disable-asm" [15]="--disable-inline-asm" [16]="--disable-neon" [17]="--enable-demuxer=mov" [18]="--disable-muxers" [19]="--enable-protocol=file" [20]="--enable-pic" [21]="--disable-everything" [22]="--disable-runtime-cpudetect" [23]="--disable-pthreads" [24]="--disable-protocols" [25]="--disable-avfilter" [26]="--disable-postproc" [27]="--disable-avdevice" [28]="--disable-swscale" [29]="--disable-swresample" [30]="--disable-programs" [31]="--disable-devices" [32]="--disable-filters" [33]="--disable-network" [34]="--enable-small" [35]="--disable-iconv" [36]="--disable-debug" [37]="--disable-shared" [38]="--enable-static" [39]="--target-os=linux" [40]="--disable-htmlpages" [41]="--disable-doc" [42]="--enable-cross-compile" [43]="--prefix=./install")
+BASH_CMDS=()
+BASH_LINENO=([0]="0")
+BASH_SOURCE=([0]="./configure")
+BASH_VERSINFO=([0]="4" [1]="3" [2]="11" [3]="1" [4]="release" [5]="x86_64-pc-linux-gnu")
+BASH_VERSION='4.3.11(1)-release'
+BSF_LIST='aac_adtstoasc_bsf
+av1_metadata_bsf
+chomp_bsf
+dump_extradata_bsf
+dca_core_bsf
+eac3_core_bsf
+extract_extradata_bsf
+filter_units_bsf
+h264_metadata_bsf
+h264_mp4toannexb_bsf
+h264_redundant_pps_bsf
+hapqa_extract_bsf
+hevc_metadata_bsf
+hevc_mp4toannexb_bsf
+imx_dump_header_bsf
+mjpeg2jpeg_bsf
+mjpega_dump_header_bsf
+mp3_header_decompress_bsf
+mpeg2_metadata_bsf
+mpeg4_unpack_bframes_bsf
+mov2textsub_bsf
+noise_bsf
+null_bsf
+remove_extradata_bsf
+text2movsub_bsf
+trace_headers_bsf
+vp9_metadata_bsf
+vp9_raw_reorder_bsf
+vp9_superframe_bsf
+vp9_superframe_split_bsf'
+BUILTIN_LIST='
+    atomic_cas_ptr
+    machine_rw_barrier
+    MemoryBarrier
+    mm_empty
+    rdtsc
+    sem_timedwait
+    sync_val_compare_and_swap
+'
+CC_C=-c
+CC_E='-E -o $@'
+CC_O='-o $@'
+CFG_CHIP_TYPE=hi3559v200
+CFG_CHIP_TYPE_HI3559V200=y
+CFG_ENABLE_AUDIO=y
+CFG_ENABLE_FALLOCATE=y
+CFG_ENABLE_NDK=y
+CFG_HI_EXPORT_FLAG=y
+CFG_MW_VERSION=2.0.1.0
+CFG_OS_TYPE=linux_liteos
+CFG_OS_TYPE_LINUX_LITEOS=y
+CFG_REL_FOR_PDT=y
+CFG_SDK_TOOLCHAIN=arm-himix100-linux-
+CFG_SDK_TOOLCHAIN_ARM_HIMIX100=y
+CFG_SERVER_TOOLCHAIN=arm-himix100-linux-
+CFG_SERVER_TOOLCHAIN_ARM_HIMIX100=y
+CFG_TURNON_LOG=y
+CFG_TURNON_PROC=y
+CLASSPATH=/usr/lib/jvm/java/jdk1.6.0_31/lib:
+CLEAN_OBJS=' sample_clean component_clean common_clean thirdparty_clean media_adpt_clean ndk_clean'
+CMDLINE_APPEND='
+    extra_cflags
+    extra_cxxflags
+    extra_objcflags
+    host_cppflags
+'
+CMDLINE_SELECT='
+    
+    
+    armv5te
+    armv6
+    armv6t2
+    armv8
+    neon
+    vfp
+    vfpv3
+    setend
+
+    
+    altivec
+    dcbzl
+    ldbrx
+    power8
+    ppc4xx
+    vsx
+
+    
+    
+    aesni
+    amd3dnow
+    amd3dnowext
+    avx
+    avx2
+    avx512
+    fma3
+    fma4
+    mmx
+    mmxext
+    sse
+    sse2
+    sse3
+    sse4
+    sse42
+    ssse3
+    xop
+
+    cpunop
+    i686
+
+    
+    mipsfpu
+    mips32r2
+    mips32r5
+    mips64r2
+    mips32r6
+    mips64r6
+    mipsdsp
+    mipsdspr2
+    msa
+
+    
+    loongson2
+    loongson3
+    mmi
+
+
+    
+    
+    doc
+    htmlpages
+    manpages
+    podpages
+    txtpages
+
+    
+    avio_dir_cmd_example
+    avio_reading_example
+    decode_audio_example
+    decode_video_example
+    demuxing_decoding_example
+    encode_audio_example
+    encode_video_example
+    extract_mvs_example
+    filter_audio_example
+    filtering_audio_example
+    filtering_video_example
+    http_multiclient_example
+    hw_decode_example
+    metadata_example
+    muxing_example
+    qsvdec_example
+    remuxing_example
+    resampling_audio_example
+    scaling_video_example
+    transcode_aac_example
+    transcoding_example
+    vaapi_encode_example
+    vaapi_transcode_example
+
+    
+    
+    avisynth
+    frei0r
+    libcdio
+    libdavs2
+    librubberband
+    libvidstab
+    libx264
+    libx265
+    libxavs
+    libxavs2
+    libxvid
+
+    
+    decklink
+    libndi_newtek
+    libfdk_aac
+    openssl
+    libtls
+
+    
+    gmp
+    liblensfun
+    libopencore_amrnb
+    libopencore_amrwb
+    libvmaf
+    libvo_amrwbenc
+    mbedtls
+    rkmpp
+
+    
+    libsmbclient
+
+    chromaprint
+    gcrypt
+    gnutls
+    jni
+    ladspa
+    libaom
+    libass
+    libbluray
+    libbs2b
+    libcaca
+    libcelt
+    libcodec2
+    libdc1394
+    libdrm
+    libflite
+    libfontconfig
+    libfreetype
+    libfribidi
+    libgme
+    libgsm
+    libiec61883
+    libilbc
+    libjack
+    libklvanc
+    libkvazaar
+    libmodplug
+    libmp3lame
+    libmysofa
+    libopencv
+    libopenh264
+    libopenjpeg
+    libopenmpt
+    libopus
+    libpulse
+    librsvg
+    librtmp
+    libshine
+    libsmbclient
+    libsnappy
+    libsoxr
+    libspeex
+    libsrt
+    libssh
+    libtensorflow
+    libtesseract
+    libtheora
+    libtwolame
+    libv4l2
+    libvorbis
+    libvpx
+    libwavpack
+    libwebp
+    libxml2
+    libzimg
+    libzmq
+    libzvbi
+    lv2
+    mediacodec
+    openal
+    opengl
+    vapoursynth
+
+    
+    alsa
+    appkit
+    avfoundation
+    bzlib
+    coreimage
+    iconv
+    libxcb
+    libxcb_shm
+    libxcb_shape
+    libxcb_xfixes
+    lzma
+    schannel
+    sdl2
+    securetransport
+    sndio
+    xlib
+    zlib
+
+    
+    
+    cuda_sdk
+    libnpp
+
+    libmfx
+    mmal
+    omx
+    opencl
+
+    
+    amf
+    audiotoolbox
+    crystalhd
+    cuda
+    cuvid
+    d3d11va
+    dxva2
+    ffnvcodec
+    nvdec
+    nvenc
+    vaapi
+    vdpau
+    videotoolbox
+    v4l2_m2m
+    xvmc
+
+    
+    ftrapv
+    gray
+    hardcoded_tables
+    omx_rpi
+    runtime_cpudetect
+    safe_bitstream_reader
+    shared
+    small
+    static
+    swscale_alpha
+
+    
+    gpl
+    nonfree
+    version3
+
+    
+    avdevice
+    avfilter
+    swscale
+    postproc
+    avformat
+    avcodec
+    swresample
+    avresample
+    avutil
+
+    
+    ffplay
+    ffprobe
+    ffmpeg
+
+    
+    dct
+    dwt
+    error_resilience
+    faan
+    fast_unaligned
+    fft
+    lsp
+    lzo
+    mdct
+    pixelutils
+    network
+    rdft
+
+    autodetect
+    fontconfig
+    linux_perf
+    memory_poisoning
+    neon_clobber_test
+    ossfuzz
+    pic
+    thumb
+    valgrind_backtrace
+    xmm_clobber_test
+    
+    
+    bsfs
+    decoders
+    encoders
+    hwaccels
+    parsers
+
+    
+    indevs
+    outdevs
+
+    
+    filters
+
+    
+    demuxers
+    muxers
+    protocols
+
+
+
+    
+    inline_asm
+    symver
+    x86asm
+
+    
+    pthreads
+    os2threads
+    w32threads
+
+    asm
+    cross_compile
+    debug
+    extra_warnings
+    logging
+    lto
+    optimizations
+    rpath
+    stripping
+'
+CMDLINE_SET='
+    
+    bindir
+    datadir
+    docdir
+    incdir
+    libdir
+    mandir
+    pkgconfigdir
+    prefix
+    shlibdir
+    install_name_dir
+
+    ar
+    arch
+    as
+    assert_level
+    build_suffix
+    cc
+    objcc
+    cpu
+    cross_prefix
+    custom_allocator
+    cxx
+    dep_cc
+    doxygen
+    env
+    extra_version
+    gas
+    host_cc
+    host_cflags
+    host_extralibs
+    host_ld
+    host_ldflags
+    host_os
+    ignore_tests
+    install
+    ld
+    ln_s
+    logfile
+    malloc_prefix
+    nm
+    optflags
+    nvcc
+    nvccflags
+    pkg_config
+    pkg_config_flags
+    progs_suffix
+    random_seed
+    ranlib
+    samples
+    strip
+    sws_max_filter_size
+    sysinclude
+    sysroot
+    target_exec
+    target_os
+    target_path
+    target_samples
+    tempprefix
+    toolchain
+    valgrind
+    x86asmexe
+'
+CODEC_LIST='
+    a64multi_encoder
+a64multi5_encoder
+alias_pix_encoder
+amv_encoder
+apng_encoder
+asv1_encoder
+asv2_encoder
+avrp_encoder
+avui_encoder
+ayuv_encoder
+bmp_encoder
+cinepak_encoder
+cljr_encoder
+comfortnoise_encoder
+dnxhd_encoder
+dpx_encoder
+dvvideo_encoder
+ffv1_encoder
+ffvhuff_encoder
+fits_encoder
+flashsv_encoder
+flashsv2_encoder
+flv_encoder
+gif_encoder
+h261_encoder
+h263_encoder
+h263p_encoder
+hap_encoder
+huffyuv_encoder
+jpeg2000_encoder
+jpegls_encoder
+ljpeg_encoder
+magicyuv_encoder
+mjpeg_encoder
+mpeg1video_encoder
+mpeg2video_encoder
+mpeg4_encoder
+msmpeg4v2_encoder
+msmpeg4v3_encoder
+msvideo1_encoder
+pam_encoder
+pbm_encoder
+pcx_encoder
+pgm_encoder
+pgmyuv_encoder
+png_encoder
+ppm_encoder
+prores_encoder
+prores_aw_encoder
+prores_ks_encoder
+qtrle_encoder
+r10k_encoder
+r210_encoder
+rawvideo_encoder
+roq_encoder
+rv10_encoder
+rv20_encoder
+s302m_encoder
+sgi_encoder
+snow_encoder
+sunrast_encoder
+svq1_encoder
+targa_encoder
+tiff_encoder
+utvideo_encoder
+v210_encoder
+v308_encoder
+v408_encoder
+v410_encoder
+vc2_encoder
+wrapped_avframe_encoder
+wmv1_encoder
+wmv2_encoder
+xbm_encoder
+xface_encoder
+xwd_encoder
+y41p_encoder
+yuv4_encoder
+zlib_encoder
+zmbv_encoder
+aac_encoder
+ac3_encoder
+ac3_fixed_encoder
+alac_encoder
+aptx_encoder
+aptx_hd_encoder
+dca_encoder
+eac3_encoder
+flac_encoder
+g723_1_encoder
+mlp_encoder
+mp2_encoder
+mp2fixed_encoder
+nellymoser_encoder
+opus_encoder
+ra_144_encoder
+sbc_encoder
+sonic_encoder
+sonic_ls_encoder
+truehd_encoder
+tta_encoder
+vorbis_encoder
+wavpack_encoder
+wmav1_encoder
+wmav2_encoder
+pcm_alaw_encoder
+pcm_f32be_encoder
+pcm_f32le_encoder
+pcm_f64be_encoder
+pcm_f64le_encoder
+pcm_mulaw_encoder
+pcm_s8_encoder
+pcm_s8_planar_encoder
+pcm_s16be_encoder
+pcm_s16be_planar_encoder
+pcm_s16le_encoder
+pcm_s16le_planar_encoder
+pcm_s24be_encoder
+pcm_s24daud_encoder
+pcm_s24le_encoder
+pcm_s24le_planar_encoder
+pcm_s32be_encoder
+pcm_s32le_encoder
+pcm_s32le_planar_encoder
+pcm_s64be_encoder
+pcm_s64le_encoder
+pcm_u8_encoder
+pcm_u16be_encoder
+pcm_u16le_encoder
+pcm_u24be_encoder
+pcm_u24le_encoder
+pcm_u32be_encoder
+pcm_u32le_encoder
+pcm_vidc_encoder
+roq_dpcm_encoder
+adpcm_adx_encoder
+adpcm_g722_encoder
+adpcm_g726_encoder
+adpcm_g726le_encoder
+adpcm_ima_qt_encoder
+adpcm_ima_wav_encoder
+adpcm_ms_encoder
+adpcm_swf_encoder
+adpcm_yamaha_encoder
+ssa_encoder
+ass_encoder
+dvbsub_encoder
+dvdsub_encoder
+movtext_encoder
+srt_encoder
+subrip_encoder
+text_encoder
+webvtt_encoder
+xsub_encoder
+aac_at_encoder
+alac_at_encoder
+ilbc_at_encoder
+pcm_alaw_at_encoder
+pcm_mulaw_at_encoder
+libaom_av1_encoder
+libcodec2_encoder
+libfdk_aac_encoder
+libgsm_encoder
+libgsm_ms_encoder
+libilbc_encoder
+libmp3lame_encoder
+libopencore_amrnb_encoder
+libopenjpeg_encoder
+libopus_encoder
+libshine_encoder
+libspeex_encoder
+libtheora_encoder
+libtwolame_encoder
+libvo_amrwbenc_encoder
+libvorbis_encoder
+libvpx_vp8_encoder
+libvpx_vp9_encoder
+libwavpack_encoder
+libwebp_anim_encoder
+libwebp_encoder
+libx262_encoder
+libx264_encoder
+libx264rgb_encoder
+libx265_encoder
+libxavs_encoder
+libxavs2_encoder
+libxvid_encoder
+h263_v4l2m2m_encoder
+libopenh264_encoder
+h264_amf_encoder
+h264_nvenc_encoder
+h264_omx_encoder
+h264_qsv_encoder
+h264_v4l2m2m_encoder
+h264_vaapi_encoder
+h264_videotoolbox_encoder
+nvenc_encoder
+nvenc_h264_encoder
+nvenc_hevc_encoder
+hevc_amf_encoder
+hevc_nvenc_encoder
+hevc_qsv_encoder
+hevc_v4l2m2m_encoder
+hevc_vaapi_encoder
+hevc_videotoolbox_encoder
+libkvazaar_encoder
+mjpeg_qsv_encoder
+mjpeg_vaapi_encoder
+mpeg2_qsv_encoder
+mpeg2_vaapi_encoder
+mpeg4_v4l2m2m_encoder
+vp8_v4l2m2m_encoder
+vp8_vaapi_encoder
+vp9_vaapi_encoder
+    aasc_decoder
+aic_decoder
+alias_pix_decoder
+amv_decoder
+anm_decoder
+ansi_decoder
+apng_decoder
+asv1_decoder
+asv2_decoder
+aura_decoder
+aura2_decoder
+avrp_decoder
+avrn_decoder
+avs_decoder
+avui_decoder
+ayuv_decoder
+bethsoftvid_decoder
+bfi_decoder
+bink_decoder
+bitpacked_decoder
+bmp_decoder
+bmv_video_decoder
+brender_pix_decoder
+c93_decoder
+cavs_decoder
+cdgraphics_decoder
+cdxl_decoder
+cfhd_decoder
+cinepak_decoder
+clearvideo_decoder
+cljr_decoder
+cllc_decoder
+comfortnoise_decoder
+cpia_decoder
+cscd_decoder
+cyuv_decoder
+dds_decoder
+dfa_decoder
+dirac_decoder
+dnxhd_decoder
+dpx_decoder
+dsicinvideo_decoder
+dvaudio_decoder
+dvvideo_decoder
+dxa_decoder
+dxtory_decoder
+dxv_decoder
+eacmv_decoder
+eamad_decoder
+eatgq_decoder
+eatgv_decoder
+eatqi_decoder
+eightbps_decoder
+eightsvx_exp_decoder
+eightsvx_fib_decoder
+escape124_decoder
+escape130_decoder
+exr_decoder
+ffv1_decoder
+ffvhuff_decoder
+fic_decoder
+fits_decoder
+flashsv_decoder
+flashsv2_decoder
+flic_decoder
+flv_decoder
+fmvc_decoder
+fourxm_decoder
+fraps_decoder
+frwu_decoder
+g2m_decoder
+gdv_decoder
+gif_decoder
+h261_decoder
+h263_decoder
+h263i_decoder
+h263p_decoder
+h263_v4l2m2m_decoder
+h264_decoder
+h264_crystalhd_decoder
+h264_v4l2m2m_decoder
+h264_mediacodec_decoder
+h264_mmal_decoder
+h264_qsv_decoder
+h264_rkmpp_decoder
+hap_decoder
+hevc_decoder
+hevc_qsv_decoder
+hevc_rkmpp_decoder
+hevc_v4l2m2m_decoder
+hnm4_video_decoder
+hq_hqa_decoder
+hqx_decoder
+huffyuv_decoder
+idcin_decoder
+iff_ilbm_decoder
+imm4_decoder
+indeo2_decoder
+indeo3_decoder
+indeo4_decoder
+indeo5_decoder
+interplay_video_decoder
+jpeg2000_decoder
+jpegls_decoder
+jv_decoder
+kgv1_decoder
+kmvc_decoder
+lagarith_decoder
+loco_decoder
+m101_decoder
+magicyuv_decoder
+mdec_decoder
+mimic_decoder
+mjpeg_decoder
+mjpegb_decoder
+mmvideo_decoder
+motionpixels_decoder
+mpeg1video_decoder
+mpeg2video_decoder
+mpeg4_decoder
+mpeg4_crystalhd_decoder
+mpeg4_v4l2m2m_decoder
+mpeg4_mmal_decoder
+mpegvideo_decoder
+mpeg1_v4l2m2m_decoder
+mpeg2_mmal_decoder
+mpeg2_crystalhd_decoder
+mpeg2_v4l2m2m_decoder
+mpeg2_qsv_decoder
+mpeg2_mediacodec_decoder
+msa1_decoder
+mscc_decoder
+msmpeg4v1_decoder
+msmpeg4v2_decoder
+msmpeg4v3_decoder
+msmpeg4_crystalhd_decoder
+msrle_decoder
+mss1_decoder
+mss2_decoder
+msvideo1_decoder
+mszh_decoder
+mts2_decoder
+mvc1_decoder
+mvc2_decoder
+mwsc_decoder
+mxpeg_decoder
+nuv_decoder
+paf_video_decoder
+pam_decoder
+pbm_decoder
+pcx_decoder
+pgm_decoder
+pgmyuv_decoder
+pictor_decoder
+pixlet_decoder
+png_decoder
+ppm_decoder
+prores_decoder
+prosumer_decoder
+psd_decoder
+ptx_decoder
+qdraw_decoder
+qpeg_decoder
+qtrle_decoder
+r10k_decoder
+r210_decoder
+rasc_decoder
+rawvideo_decoder
+rl2_decoder
+roq_decoder
+rpza_decoder
+rscc_decoder
+rv10_decoder
+rv20_decoder
+rv30_decoder
+rv40_decoder
+s302m_decoder
+sanm_decoder
+scpr_decoder
+screenpresso_decoder
+sdx2_dpcm_decoder
+sgi_decoder
+sgirle_decoder
+sheervideo_decoder
+smacker_decoder
+smc_decoder
+smvjpeg_decoder
+snow_decoder
+sp5x_decoder
+speedhq_decoder
+srgc_decoder
+sunrast_decoder
+svq1_decoder
+svq3_decoder
+targa_decoder
+targa_y216_decoder
+tdsc_decoder
+theora_decoder
+thp_decoder
+tiertexseqvideo_decoder
+tiff_decoder
+tmv_decoder
+truemotion1_decoder
+truemotion2_decoder
+truemotion2rt_decoder
+tscc_decoder
+tscc2_decoder
+txd_decoder
+ulti_decoder
+utvideo_decoder
+v210_decoder
+v210x_decoder
+v308_decoder
+v408_decoder
+v410_decoder
+vb_decoder
+vble_decoder
+vc1_decoder
+vc1_crystalhd_decoder
+vc1image_decoder
+vc1_mmal_decoder
+vc1_qsv_decoder
+vc1_v4l2m2m_decoder
+vcr1_decoder
+vmdvideo_decoder
+vmnc_decoder
+vp3_decoder
+vp5_decoder
+vp6_decoder
+vp6a_decoder
+vp6f_decoder
+vp7_decoder
+vp8_decoder
+vp8_rkmpp_decoder
+vp8_v4l2m2m_decoder
+vp9_decoder
+vp9_rkmpp_decoder
+vp9_v4l2m2m_decoder
+vqa_decoder
+webp_decoder
+wcmv_decoder
+wrapped_avframe_decoder
+wmv1_decoder
+wmv2_decoder
+wmv3_decoder
+wmv3_crystalhd_decoder
+wmv3image_decoder
+wnv1_decoder
+xan_wc3_decoder
+xan_wc4_decoder
+xbm_decoder
+xface_decoder
+xl_decoder
+xpm_decoder
+xwd_decoder
+y41p_decoder
+ylc_decoder
+yop_decoder
+yuv4_decoder
+zero12v_decoder
+zerocodec_decoder
+zlib_decoder
+zmbv_decoder
+aac_decoder
+aac_fixed_decoder
+aac_latm_decoder
+ac3_decoder
+ac3_fixed_decoder
+alac_decoder
+als_decoder
+amrnb_decoder
+amrwb_decoder
+ape_decoder
+aptx_decoder
+aptx_hd_decoder
+atrac1_decoder
+atrac3_decoder
+atrac3al_decoder
+atrac3p_decoder
+atrac3pal_decoder
+atrac9_decoder
+binkaudio_dct_decoder
+binkaudio_rdft_decoder
+bmv_audio_decoder
+cook_decoder
+dca_decoder
+dolby_e_decoder
+dsd_lsbf_decoder
+dsd_msbf_decoder
+dsd_lsbf_planar_decoder
+dsd_msbf_planar_decoder
+dsicinaudio_decoder
+dss_sp_decoder
+dst_decoder
+eac3_decoder
+evrc_decoder
+ffwavesynth_decoder
+flac_decoder
+g723_1_decoder
+g729_decoder
+gsm_decoder
+gsm_ms_decoder
+iac_decoder
+ilbc_decoder
+imc_decoder
+interplay_acm_decoder
+mace3_decoder
+mace6_decoder
+metasound_decoder
+mlp_decoder
+mp1_decoder
+mp1float_decoder
+mp2_decoder
+mp2float_decoder
+mp3float_decoder
+mp3_decoder
+mp3adufloat_decoder
+mp3adu_decoder
+mp3on4float_decoder
+mp3on4_decoder
+mpc7_decoder
+mpc8_decoder
+nellymoser_decoder
+on2avc_decoder
+opus_decoder
+paf_audio_decoder
+qcelp_decoder
+qdm2_decoder
+qdmc_decoder
+ra_144_decoder
+ra_288_decoder
+ralf_decoder
+sbc_decoder
+shorten_decoder
+sipr_decoder
+smackaud_decoder
+sonic_decoder
+tak_decoder
+truehd_decoder
+truespeech_decoder
+tta_decoder
+twinvq_decoder
+vmdaudio_decoder
+vorbis_decoder
+wavpack_decoder
+wmalossless_decoder
+wmapro_decoder
+wmav1_decoder
+wmav2_decoder
+wmavoice_decoder
+ws_snd1_decoder
+xma1_decoder
+xma2_decoder
+pcm_alaw_decoder
+pcm_bluray_decoder
+pcm_dvd_decoder
+pcm_f16le_decoder
+pcm_f24le_decoder
+pcm_f32be_decoder
+pcm_f32le_decoder
+pcm_f64be_decoder
+pcm_f64le_decoder
+pcm_lxf_decoder
+pcm_mulaw_decoder
+pcm_s8_decoder
+pcm_s8_planar_decoder
+pcm_s16be_decoder
+pcm_s16be_planar_decoder
+pcm_s16le_decoder
+pcm_s16le_planar_decoder
+pcm_s24be_decoder
+pcm_s24daud_decoder
+pcm_s24le_decoder
+pcm_s24le_planar_decoder
+pcm_s32be_decoder
+pcm_s32le_decoder
+pcm_s32le_planar_decoder
+pcm_s64be_decoder
+pcm_s64le_decoder
+pcm_u8_decoder
+pcm_u16be_decoder
+pcm_u16le_decoder
+pcm_u24be_decoder
+pcm_u24le_decoder
+pcm_u32be_decoder
+pcm_u32le_decoder
+pcm_vidc_decoder
+pcm_zork_decoder
+gremlin_dpcm_decoder
+interplay_dpcm_decoder
+roq_dpcm_decoder
+sol_dpcm_decoder
+xan_dpcm_decoder
+adpcm_4xm_decoder
+adpcm_adx_decoder
+adpcm_afc_decoder
+adpcm_aica_decoder
+adpcm_ct_decoder
+adpcm_dtk_decoder
+adpcm_ea_decoder
+adpcm_ea_maxis_xa_decoder
+adpcm_ea_r1_decoder
+adpcm_ea_r2_decoder
+adpcm_ea_r3_decoder
+adpcm_ea_xas_decoder
+adpcm_g722_decoder
+adpcm_g726_decoder
+adpcm_g726le_decoder
+adpcm_ima_amv_decoder
+adpcm_ima_apc_decoder
+adpcm_ima_dat4_decoder
+adpcm_ima_dk3_decoder
+adpcm_ima_dk4_decoder
+adpcm_ima_ea_eacs_decoder
+adpcm_ima_ea_sead_decoder
+adpcm_ima_iss_decoder
+adpcm_ima_oki_decoder
+adpcm_ima_qt_decoder
+adpcm_ima_rad_decoder
+adpcm_ima_smjpeg_decoder
+adpcm_ima_wav_decoder
+adpcm_ima_ws_decoder
+adpcm_ms_decoder
+adpcm_mtaf_decoder
+adpcm_psx_decoder
+adpcm_sbpro_2_decoder
+adpcm_sbpro_3_decoder
+adpcm_sbpro_4_decoder
+adpcm_swf_decoder
+adpcm_thp_decoder
+adpcm_thp_le_decoder
+adpcm_vima_decoder
+adpcm_xa_decoder
+adpcm_yamaha_decoder
+ssa_decoder
+ass_decoder
+ccaption_decoder
+dvbsub_decoder
+dvdsub_decoder
+jacosub_decoder
+microdvd_decoder
+movtext_decoder
+mpl2_decoder
+pgssub_decoder
+pjs_decoder
+realtext_decoder
+sami_decoder
+srt_decoder
+stl_decoder
+subrip_decoder
+subviewer_decoder
+subviewer1_decoder
+text_decoder
+vplayer_decoder
+webvtt_decoder
+xsub_decoder
+aac_at_decoder
+ac3_at_decoder
+adpcm_ima_qt_at_decoder
+alac_at_decoder
+amr_nb_at_decoder
+eac3_at_decoder
+gsm_ms_at_decoder
+ilbc_at_decoder
+mp1_at_decoder
+mp2_at_decoder
+mp3_at_decoder
+pcm_alaw_at_decoder
+pcm_mulaw_at_decoder
+qdmc_at_decoder
+qdm2_at_decoder
+libaom_av1_decoder
+libcelt_decoder
+libcodec2_decoder
+libdavs2_decoder
+libfdk_aac_decoder
+libgsm_decoder
+libgsm_ms_decoder
+libilbc_decoder
+libopencore_amrnb_decoder
+libopencore_amrwb_decoder
+libopenjpeg_decoder
+libopus_decoder
+librsvg_decoder
+libspeex_decoder
+libvorbis_decoder
+libvpx_vp8_decoder
+libvpx_vp9_decoder
+libzvbi_teletext_decoder
+bintext_decoder
+xbin_decoder
+idf_decoder
+libopenh264_decoder
+h264_cuvid_decoder
+hevc_cuvid_decoder
+hevc_mediacodec_decoder
+mjpeg_cuvid_decoder
+mpeg1_cuvid_decoder
+mpeg2_cuvid_decoder
+mpeg4_cuvid_decoder
+mpeg4_mediacodec_decoder
+vc1_cuvid_decoder
+vp8_cuvid_decoder
+vp8_mediacodec_decoder
+vp8_qsv_decoder
+vp9_cuvid_decoder
+vp9_mediacodec_decoder
+'
+COMPLEX_FUNCS='
+    cabs
+    cexp
+'
+COMPONENT_LIST='
+    
+    bsfs
+    decoders
+    encoders
+    hwaccels
+    parsers
+
+    
+    indevs
+    outdevs
+
+    
+    filters
+
+    
+    demuxers
+    muxers
+    protocols
+
+'
+CONFIG_EXTRA='
+    aandcttables
+    ac3dsp
+    adts_header
+    audio_frame_queue
+    audiodsp
+    blockdsp
+    bswapdsp
+    cabac
+    cbs
+    cbs_av1
+    cbs_h264
+    cbs_h265
+    cbs_jpeg
+    cbs_mpeg2
+    cbs_vp9
+    dirac_parse
+    dnn
+    dvprofile
+    exif
+    faandct
+    faanidct
+    fdctdsp
+    flacdsp
+    fmtconvert
+    frame_thread_encoder
+    g722dsp
+    golomb
+    gplv3
+    h263dsp
+    h264chroma
+    h264dsp
+    h264parse
+    h264pred
+    h264qpel
+    hevcparse
+    hpeldsp
+    huffman
+    huffyuvdsp
+    huffyuvencdsp
+    idctdsp
+    iirfilter
+    mdct15
+    intrax8
+    iso_media
+    ividsp
+    jpegtables
+    lgplv3
+    libx262
+    llauddsp
+    llviddsp
+    llvidencdsp
+    lpc
+    lzf
+    me_cmp
+    mpeg_er
+    mpegaudio
+    mpegaudiodsp
+    mpegaudioheader
+    mpegvideo
+    mpegvideoenc
+    mss34dsp
+    pixblockdsp
+    qpeldsp
+    qsv
+    qsvdec
+    qsvenc
+    qsvvpp
+    rangecoder
+    riffdec
+    riffenc
+    rtpdec
+    rtpenc_chain
+    rv34dsp
+    sinewin
+    snappy
+    srtp
+    startcode
+    texturedsp
+    texturedspenc
+    tpeldsp
+    vaapi_1
+    vaapi_encode
+    vc1dsp
+    videodsp
+    vp3dsp
+    vp56dsp
+    vp8dsp
+    wma_freqs
+    wmv2dsp
+'
+CONFIG_LIST='
+    
+    doc
+    htmlpages
+    manpages
+    podpages
+    txtpages
+
+    
+    avio_dir_cmd_example
+    avio_reading_example
+    decode_audio_example
+    decode_video_example
+    demuxing_decoding_example
+    encode_audio_example
+    encode_video_example
+    extract_mvs_example
+    filter_audio_example
+    filtering_audio_example
+    filtering_video_example
+    http_multiclient_example
+    hw_decode_example
+    metadata_example
+    muxing_example
+    qsvdec_example
+    remuxing_example
+    resampling_audio_example
+    scaling_video_example
+    transcode_aac_example
+    transcoding_example
+    vaapi_encode_example
+    vaapi_transcode_example
+
+    
+    
+    avisynth
+    frei0r
+    libcdio
+    libdavs2
+    librubberband
+    libvidstab
+    libx264
+    libx265
+    libxavs
+    libxavs2
+    libxvid
+
+    
+    decklink
+    libndi_newtek
+    libfdk_aac
+    openssl
+    libtls
+
+    
+    gmp
+    liblensfun
+    libopencore_amrnb
+    libopencore_amrwb
+    libvmaf
+    libvo_amrwbenc
+    mbedtls
+    rkmpp
+
+    
+    libsmbclient
+
+    chromaprint
+    gcrypt
+    gnutls
+    jni
+    ladspa
+    libaom
+    libass
+    libbluray
+    libbs2b
+    libcaca
+    libcelt
+    libcodec2
+    libdc1394
+    libdrm
+    libflite
+    libfontconfig
+    libfreetype
+    libfribidi
+    libgme
+    libgsm
+    libiec61883
+    libilbc
+    libjack
+    libklvanc
+    libkvazaar
+    libmodplug
+    libmp3lame
+    libmysofa
+    libopencv
+    libopenh264
+    libopenjpeg
+    libopenmpt
+    libopus
+    libpulse
+    librsvg
+    librtmp
+    libshine
+    libsmbclient
+    libsnappy
+    libsoxr
+    libspeex
+    libsrt
+    libssh
+    libtensorflow
+    libtesseract
+    libtheora
+    libtwolame
+    libv4l2
+    libvorbis
+    libvpx
+    libwavpack
+    libwebp
+    libxml2
+    libzimg
+    libzmq
+    libzvbi
+    lv2
+    mediacodec
+    openal
+    opengl
+    vapoursynth
+
+    
+    alsa
+    appkit
+    avfoundation
+    bzlib
+    coreimage
+    iconv
+    libxcb
+    libxcb_shm
+    libxcb_shape
+    libxcb_xfixes
+    lzma
+    schannel
+    sdl2
+    securetransport
+    sndio
+    xlib
+    zlib
+
+    
+    
+    cuda_sdk
+    libnpp
+
+    libmfx
+    mmal
+    omx
+    opencl
+
+    
+    amf
+    audiotoolbox
+    crystalhd
+    cuda
+    cuvid
+    d3d11va
+    dxva2
+    ffnvcodec
+    nvdec
+    nvenc
+    vaapi
+    vdpau
+    videotoolbox
+    v4l2_m2m
+    xvmc
+
+    
+    ftrapv
+    gray
+    hardcoded_tables
+    omx_rpi
+    runtime_cpudetect
+    safe_bitstream_reader
+    shared
+    small
+    static
+    swscale_alpha
+
+    
+    gpl
+    nonfree
+    version3
+
+    
+    avdevice
+    avfilter
+    swscale
+    postproc
+    avformat
+    avcodec
+    swresample
+    avresample
+    avutil
+
+    
+    ffplay
+    ffprobe
+    ffmpeg
+
+    
+    dct
+    dwt
+    error_resilience
+    faan
+    fast_unaligned
+    fft
+    lsp
+    lzo
+    mdct
+    pixelutils
+    network
+    rdft
+
+    autodetect
+    fontconfig
+    linux_perf
+    memory_poisoning
+    neon_clobber_test
+    ossfuzz
+    pic
+    thumb
+    valgrind_backtrace
+    xmm_clobber_test
+    
+    
+    bsfs
+    decoders
+    encoders
+    hwaccels
+    parsers
+
+    
+    indevs
+    outdevs
+
+    
+    filters
+
+    
+    demuxers
+    muxers
+    protocols
+
+
+'
+CURDIR=/home/yangyi/develop/hiberryV1R2/hiberry_dev_20190629_1115/source/thirdparty/ffmpeg
+CXX_C=-c
+CXX_O='-o $@'
+DECODER_LIST='aasc_decoder
+aic_decoder
+alias_pix_decoder
+amv_decoder
+anm_decoder
+ansi_decoder
+apng_decoder
+asv1_decoder
+asv2_decoder
+aura_decoder
+aura2_decoder
+avrp_decoder
+avrn_decoder
+avs_decoder
+avui_decoder
+ayuv_decoder
+bethsoftvid_decoder
+bfi_decoder
+bink_decoder
+bitpacked_decoder
+bmp_decoder
+bmv_video_decoder
+brender_pix_decoder
+c93_decoder
+cavs_decoder
+cdgraphics_decoder
+cdxl_decoder
+cfhd_decoder
+cinepak_decoder
+clearvideo_decoder
+cljr_decoder
+cllc_decoder
+comfortnoise_decoder
+cpia_decoder
+cscd_decoder
+cyuv_decoder
+dds_decoder
+dfa_decoder
+dirac_decoder
+dnxhd_decoder
+dpx_decoder
+dsicinvideo_decoder
+dvaudio_decoder
+dvvideo_decoder
+dxa_decoder
+dxtory_decoder
+dxv_decoder
+eacmv_decoder
+eamad_decoder
+eatgq_decoder
+eatgv_decoder
+eatqi_decoder
+eightbps_decoder
+eightsvx_exp_decoder
+eightsvx_fib_decoder
+escape124_decoder
+escape130_decoder
+exr_decoder
+ffv1_decoder
+ffvhuff_decoder
+fic_decoder
+fits_decoder
+flashsv_decoder
+flashsv2_decoder
+flic_decoder
+flv_decoder
+fmvc_decoder
+fourxm_decoder
+fraps_decoder
+frwu_decoder
+g2m_decoder
+gdv_decoder
+gif_decoder
+h261_decoder
+h263_decoder
+h263i_decoder
+h263p_decoder
+h263_v4l2m2m_decoder
+h264_decoder
+h264_crystalhd_decoder
+h264_v4l2m2m_decoder
+h264_mediacodec_decoder
+h264_mmal_decoder
+h264_qsv_decoder
+h264_rkmpp_decoder
+hap_decoder
+hevc_decoder
+hevc_qsv_decoder
+hevc_rkmpp_decoder
+hevc_v4l2m2m_decoder
+hnm4_video_decoder
+hq_hqa_decoder
+hqx_decoder
+huffyuv_decoder
+idcin_decoder
+iff_ilbm_decoder
+imm4_decoder
+indeo2_decoder
+indeo3_decoder
+indeo4_decoder
+indeo5_decoder
+interplay_video_decoder
+jpeg2000_decoder
+jpegls_decoder
+jv_decoder
+kgv1_decoder
+kmvc_decoder
+lagarith_decoder
+loco_decoder
+m101_decoder
+magicyuv_decoder
+mdec_decoder
+mimic_decoder
+mjpeg_decoder
+mjpegb_decoder
+mmvideo_decoder
+motionpixels_decoder
+mpeg1video_decoder
+mpeg2video_decoder
+mpeg4_decoder
+mpeg4_crystalhd_decoder
+mpeg4_v4l2m2m_decoder
+mpeg4_mmal_decoder
+mpegvideo_decoder
+mpeg1_v4l2m2m_decoder
+mpeg2_mmal_decoder
+mpeg2_crystalhd_decoder
+mpeg2_v4l2m2m_decoder
+mpeg2_qsv_decoder
+mpeg2_mediacodec_decoder
+msa1_decoder
+mscc_decoder
+msmpeg4v1_decoder
+msmpeg4v2_decoder
+msmpeg4v3_decoder
+msmpeg4_crystalhd_decoder
+msrle_decoder
+mss1_decoder
+mss2_decoder
+msvideo1_decoder
+mszh_decoder
+mts2_decoder
+mvc1_decoder
+mvc2_decoder
+mwsc_decoder
+mxpeg_decoder
+nuv_decoder
+paf_video_decoder
+pam_decoder
+pbm_decoder
+pcx_decoder
+pgm_decoder
+pgmyuv_decoder
+pictor_decoder
+pixlet_decoder
+png_decoder
+ppm_decoder
+prores_decoder
+prosumer_decoder
+psd_decoder
+ptx_decoder
+qdraw_decoder
+qpeg_decoder
+qtrle_decoder
+r10k_decoder
+r210_decoder
+rasc_decoder
+rawvideo_decoder
+rl2_decoder
+roq_decoder
+rpza_decoder
+rscc_decoder
+rv10_decoder
+rv20_decoder
+rv30_decoder
+rv40_decoder
+s302m_decoder
+sanm_decoder
+scpr_decoder
+screenpresso_decoder
+sdx2_dpcm_decoder
+sgi_decoder
+sgirle_decoder
+sheervideo_decoder
+smacker_decoder
+smc_decoder
+smvjpeg_decoder
+snow_decoder
+sp5x_decoder
+speedhq_decoder
+srgc_decoder
+sunrast_decoder
+svq1_decoder
+svq3_decoder
+targa_decoder
+targa_y216_decoder
+tdsc_decoder
+theora_decoder
+thp_decoder
+tiertexseqvideo_decoder
+tiff_decoder
+tmv_decoder
+truemotion1_decoder
+truemotion2_decoder
+truemotion2rt_decoder
+tscc_decoder
+tscc2_decoder
+txd_decoder
+ulti_decoder
+utvideo_decoder
+v210_decoder
+v210x_decoder
+v308_decoder
+v408_decoder
+v410_decoder
+vb_decoder
+vble_decoder
+vc1_decoder
+vc1_crystalhd_decoder
+vc1image_decoder
+vc1_mmal_decoder
+vc1_qsv_decoder
+vc1_v4l2m2m_decoder
+vcr1_decoder
+vmdvideo_decoder
+vmnc_decoder
+vp3_decoder
+vp5_decoder
+vp6_decoder
+vp6a_decoder
+vp6f_decoder
+vp7_decoder
+vp8_decoder
+vp8_rkmpp_decoder
+vp8_v4l2m2m_decoder
+vp9_decoder
+vp9_rkmpp_decoder
+vp9_v4l2m2m_decoder
+vqa_decoder
+webp_decoder
+wcmv_decoder
+wrapped_avframe_decoder
+wmv1_decoder
+wmv2_decoder
+wmv3_decoder
+wmv3_crystalhd_decoder
+wmv3image_decoder
+wnv1_decoder
+xan_wc3_decoder
+xan_wc4_decoder
+xbm_decoder
+xface_decoder
+xl_decoder
+xpm_decoder
+xwd_decoder
+y41p_decoder
+ylc_decoder
+yop_decoder
+yuv4_decoder
+zero12v_decoder
+zerocodec_decoder
+zlib_decoder
+zmbv_decoder
+aac_decoder
+aac_fixed_decoder
+aac_latm_decoder
+ac3_decoder
+ac3_fixed_decoder
+alac_decoder
+als_decoder
+amrnb_decoder
+amrwb_decoder
+ape_decoder
+aptx_decoder
+aptx_hd_decoder
+atrac1_decoder
+atrac3_decoder
+atrac3al_decoder
+atrac3p_decoder
+atrac3pal_decoder
+atrac9_decoder
+binkaudio_dct_decoder
+binkaudio_rdft_decoder
+bmv_audio_decoder
+cook_decoder
+dca_decoder
+dolby_e_decoder
+dsd_lsbf_decoder
+dsd_msbf_decoder
+dsd_lsbf_planar_decoder
+dsd_msbf_planar_decoder
+dsicinaudio_decoder
+dss_sp_decoder
+dst_decoder
+eac3_decoder
+evrc_decoder
+ffwavesynth_decoder
+flac_decoder
+g723_1_decoder
+g729_decoder
+gsm_decoder
+gsm_ms_decoder
+iac_decoder
+ilbc_decoder
+imc_decoder
+interplay_acm_decoder
+mace3_decoder
+mace6_decoder
+metasound_decoder
+mlp_decoder
+mp1_decoder
+mp1float_decoder
+mp2_decoder
+mp2float_decoder
+mp3float_decoder
+mp3_decoder
+mp3adufloat_decoder
+mp3adu_decoder
+mp3on4float_decoder
+mp3on4_decoder
+mpc7_decoder
+mpc8_decoder
+nellymoser_decoder
+on2avc_decoder
+opus_decoder
+paf_audio_decoder
+qcelp_decoder
+qdm2_decoder
+qdmc_decoder
+ra_144_decoder
+ra_288_decoder
+ralf_decoder
+sbc_decoder
+shorten_decoder
+sipr_decoder
+smackaud_decoder
+sonic_decoder
+tak_decoder
+truehd_decoder
+truespeech_decoder
+tta_decoder
+twinvq_decoder
+vmdaudio_decoder
+vorbis_decoder
+wavpack_decoder
+wmalossless_decoder
+wmapro_decoder
+wmav1_decoder
+wmav2_decoder
+wmavoice_decoder
+ws_snd1_decoder
+xma1_decoder
+xma2_decoder
+pcm_alaw_decoder
+pcm_bluray_decoder
+pcm_dvd_decoder
+pcm_f16le_decoder
+pcm_f24le_decoder
+pcm_f32be_decoder
+pcm_f32le_decoder
+pcm_f64be_decoder
+pcm_f64le_decoder
+pcm_lxf_decoder
+pcm_mulaw_decoder
+pcm_s8_decoder
+pcm_s8_planar_decoder
+pcm_s16be_decoder
+pcm_s16be_planar_decoder
+pcm_s16le_decoder
+pcm_s16le_planar_decoder
+pcm_s24be_decoder
+pcm_s24daud_decoder
+pcm_s24le_decoder
+pcm_s24le_planar_decoder
+pcm_s32be_decoder
+pcm_s32le_decoder
+pcm_s32le_planar_decoder
+pcm_s64be_decoder
+pcm_s64le_decoder
+pcm_u8_decoder
+pcm_u16be_decoder
+pcm_u16le_decoder
+pcm_u24be_decoder
+pcm_u24le_decoder
+pcm_u32be_decoder
+pcm_u32le_decoder
+pcm_vidc_decoder
+pcm_zork_decoder
+gremlin_dpcm_decoder
+interplay_dpcm_decoder
+roq_dpcm_decoder
+sol_dpcm_decoder
+xan_dpcm_decoder
+adpcm_4xm_decoder
+adpcm_adx_decoder
+adpcm_afc_decoder
+adpcm_aica_decoder
+adpcm_ct_decoder
+adpcm_dtk_decoder
+adpcm_ea_decoder
+adpcm_ea_maxis_xa_decoder
+adpcm_ea_r1_decoder
+adpcm_ea_r2_decoder
+adpcm_ea_r3_decoder
+adpcm_ea_xas_decoder
+adpcm_g722_decoder
+adpcm_g726_decoder
+adpcm_g726le_decoder
+adpcm_ima_amv_decoder
+adpcm_ima_apc_decoder
+adpcm_ima_dat4_decoder
+adpcm_ima_dk3_decoder
+adpcm_ima_dk4_decoder
+adpcm_ima_ea_eacs_decoder
+adpcm_ima_ea_sead_decoder
+adpcm_ima_iss_decoder
+adpcm_ima_oki_decoder
+adpcm_ima_qt_decoder
+adpcm_ima_rad_decoder
+adpcm_ima_smjpeg_decoder
+adpcm_ima_wav_decoder
+adpcm_ima_ws_decoder
+adpcm_ms_decoder
+adpcm_mtaf_decoder
+adpcm_psx_decoder
+adpcm_sbpro_2_decoder
+adpcm_sbpro_3_decoder
+adpcm_sbpro_4_decoder
+adpcm_swf_decoder
+adpcm_thp_decoder
+adpcm_thp_le_decoder
+adpcm_vima_decoder
+adpcm_xa_decoder
+adpcm_yamaha_decoder
+ssa_decoder
+ass_decoder
+ccaption_decoder
+dvbsub_decoder
+dvdsub_decoder
+jacosub_decoder
+microdvd_decoder
+movtext_decoder
+mpl2_decoder
+pgssub_decoder
+pjs_decoder
+realtext_decoder
+sami_decoder
+srt_decoder
+stl_decoder
+subrip_decoder
+subviewer_decoder
+subviewer1_decoder
+text_decoder
+vplayer_decoder
+webvtt_decoder
+xsub_decoder
+aac_at_decoder
+ac3_at_decoder
+adpcm_ima_qt_at_decoder
+alac_at_decoder
+amr_nb_at_decoder
+eac3_at_decoder
+gsm_ms_at_decoder
+ilbc_at_decoder
+mp1_at_decoder
+mp2_at_decoder
+mp3_at_decoder
+pcm_alaw_at_decoder
+pcm_mulaw_at_decoder
+qdmc_at_decoder
+qdm2_at_decoder
+libaom_av1_decoder
+libcelt_decoder
+libcodec2_decoder
+libdavs2_decoder
+libfdk_aac_decoder
+libgsm_decoder
+libgsm_ms_decoder
+libilbc_decoder
+libopencore_amrnb_decoder
+libopencore_amrwb_decoder
+libopenjpeg_decoder
+libopus_decoder
+librsvg_decoder
+libspeex_decoder
+libvorbis_decoder
+libvpx_vp8_decoder
+libvpx_vp9_decoder
+libzvbi_teletext_decoder
+bintext_decoder
+xbin_decoder
+idf_decoder
+libopenh264_decoder
+h264_cuvid_decoder
+hevc_cuvid_decoder
+hevc_mediacodec_decoder
+mjpeg_cuvid_decoder
+mpeg1_cuvid_decoder
+mpeg2_cuvid_decoder
+mpeg4_cuvid_decoder
+mpeg4_mediacodec_decoder
+vc1_cuvid_decoder
+vp8_cuvid_decoder
+vp8_mediacodec_decoder
+vp8_qsv_decoder
+vp9_cuvid_decoder
+vp9_mediacodec_decoder'
+DEMUXER_LIST='aa_demuxer
+aac_demuxer
+ac3_demuxer
+acm_demuxer
+act_demuxer
+adf_demuxer
+adp_demuxer
+ads_demuxer
+adx_demuxer
+aea_demuxer
+afc_demuxer
+aiff_demuxer
+aix_demuxer
+amr_demuxer
+amrnb_demuxer
+amrwb_demuxer
+anm_demuxer
+apc_demuxer
+ape_demuxer
+apng_demuxer
+aptx_demuxer
+aptx_hd_demuxer
+aqtitle_demuxer
+asf_demuxer
+asf_o_demuxer
+ass_demuxer
+ast_demuxer
+au_demuxer
+avi_demuxer
+avisynth_demuxer
+avr_demuxer
+avs_demuxer
+avs2_demuxer
+bethsoftvid_demuxer
+bfi_demuxer
+bintext_demuxer
+bink_demuxer
+bit_demuxer
+bmv_demuxer
+bfstm_demuxer
+brstm_demuxer
+boa_demuxer
+c93_demuxer
+caf_demuxer
+cavsvideo_demuxer
+cdg_demuxer
+cdxl_demuxer
+cine_demuxer
+codec2_demuxer
+codec2raw_demuxer
+concat_demuxer
+dash_demuxer
+data_demuxer
+daud_demuxer
+dcstr_demuxer
+dfa_demuxer
+dirac_demuxer
+dnxhd_demuxer
+dsf_demuxer
+dsicin_demuxer
+dss_demuxer
+dts_demuxer
+dtshd_demuxer
+dv_demuxer
+dvbsub_demuxer
+dvbtxt_demuxer
+dxa_demuxer
+ea_demuxer
+ea_cdata_demuxer
+eac3_demuxer
+epaf_demuxer
+ffmetadata_demuxer
+filmstrip_demuxer
+fits_demuxer
+flac_demuxer
+flic_demuxer
+flv_demuxer
+live_flv_demuxer
+fourxm_demuxer
+frm_demuxer
+fsb_demuxer
+g722_demuxer
+g723_1_demuxer
+g726_demuxer
+g726le_demuxer
+g729_demuxer
+gdv_demuxer
+genh_demuxer
+gif_demuxer
+gsm_demuxer
+gxf_demuxer
+h261_demuxer
+h263_demuxer
+h264_demuxer
+hevc_demuxer
+hls_demuxer
+hnm_demuxer
+ico_demuxer
+idcin_demuxer
+idf_demuxer
+iff_demuxer
+ilbc_demuxer
+image2_demuxer
+image2pipe_demuxer
+image2_alias_pix_demuxer
+image2_brender_pix_demuxer
+ingenient_demuxer
+ipmovie_demuxer
+ircam_demuxer
+iss_demuxer
+iv8_demuxer
+ivf_demuxer
+ivr_demuxer
+jacosub_demuxer
+jv_demuxer
+lmlm4_demuxer
+loas_demuxer
+lrc_demuxer
+lvf_demuxer
+lxf_demuxer
+m4v_demuxer
+matroska_demuxer
+mgsts_demuxer
+microdvd_demuxer
+mjpeg_demuxer
+mjpeg_2000_demuxer
+mlp_demuxer
+mlv_demuxer
+mm_demuxer
+mmf_demuxer
+mov_demuxer
+mp3_demuxer
+mpc_demuxer
+mpc8_demuxer
+mpegps_demuxer
+mpegts_demuxer
+mpegtsraw_demuxer
+mpegvideo_demuxer
+mpjpeg_demuxer
+mpl2_demuxer
+mpsub_demuxer
+msf_demuxer
+msnwc_tcp_demuxer
+mtaf_demuxer
+mtv_demuxer
+musx_demuxer
+mv_demuxer
+mvi_demuxer
+mxf_demuxer
+mxg_demuxer
+nc_demuxer
+nistsphere_demuxer
+nsp_demuxer
+nsv_demuxer
+nut_demuxer
+nuv_demuxer
+ogg_demuxer
+oma_demuxer
+paf_demuxer
+pcm_alaw_demuxer
+pcm_mulaw_demuxer
+pcm_vidc_demuxer
+pcm_f64be_demuxer
+pcm_f64le_demuxer
+pcm_f32be_demuxer
+pcm_f32le_demuxer
+pcm_s32be_demuxer
+pcm_s32le_demuxer
+pcm_s24be_demuxer
+pcm_s24le_demuxer
+pcm_s16be_demuxer
+pcm_s16le_demuxer
+pcm_s8_demuxer
+pcm_u32be_demuxer
+pcm_u32le_demuxer
+pcm_u24be_demuxer
+pcm_u24le_demuxer
+pcm_u16be_demuxer
+pcm_u16le_demuxer
+pcm_u8_demuxer
+pjs_demuxer
+pmp_demuxer
+pva_demuxer
+pvf_demuxer
+qcp_demuxer
+r3d_demuxer
+rawvideo_demuxer
+realtext_demuxer
+redspark_demuxer
+rl2_demuxer
+rm_demuxer
+roq_demuxer
+rpl_demuxer
+rsd_demuxer
+rso_demuxer
+rtp_demuxer
+rtsp_demuxer
+s337m_demuxer
+sami_demuxer
+sap_demuxer
+sbc_demuxer
+sbg_demuxer
+scc_demuxer
+sdp_demuxer
+sdr2_demuxer
+sds_demuxer
+sdx_demuxer
+segafilm_demuxer
+ser_demuxer
+shorten_demuxer
+siff_demuxer
+sln_demuxer
+smacker_demuxer
+smjpeg_demuxer
+smush_demuxer
+sol_demuxer
+sox_demuxer
+spdif_demuxer
+srt_demuxer
+str_demuxer
+stl_demuxer
+subviewer1_demuxer
+subviewer_demuxer
+sup_demuxer
+svag_demuxer
+swf_demuxer
+tak_demuxer
+tedcaptions_demuxer
+thp_demuxer
+threedostr_demuxer
+tiertexseq_demuxer
+tmv_demuxer
+truehd_demuxer
+tta_demuxer
+txd_demuxer
+tty_demuxer
+ty_demuxer
+v210_demuxer
+v210x_demuxer
+vag_demuxer
+vc1_demuxer
+vc1t_demuxer
+vivo_demuxer
+vmd_demuxer
+vobsub_demuxer
+voc_demuxer
+vpk_demuxer
+vplayer_demuxer
+vqf_demuxer
+w64_demuxer
+wav_demuxer
+wc3_demuxer
+webm_dash_manifest_demuxer
+webvtt_demuxer
+wsaud_demuxer
+wsd_demuxer
+wsvqa_demuxer
+wtv_demuxer
+wve_demuxer
+wv_demuxer
+xa_demuxer
+xbin_demuxer
+xmv_demuxer
+xvag_demuxer
+xwma_demuxer
+yop_demuxer
+yuv4mpegpipe_demuxer
+image_bmp_pipe_demuxer
+image_dds_pipe_demuxer
+image_dpx_pipe_demuxer
+image_exr_pipe_demuxer
+image_j2k_pipe_demuxer
+image_jpeg_pipe_demuxer
+image_jpegls_pipe_demuxer
+image_pam_pipe_demuxer
+image_pbm_pipe_demuxer
+image_pcx_pipe_demuxer
+image_pgmyuv_pipe_demuxer
+image_pgm_pipe_demuxer
+image_pictor_pipe_demuxer
+image_png_pipe_demuxer
+image_ppm_pipe_demuxer
+image_psd_pipe_demuxer
+image_qdraw_pipe_demuxer
+image_sgi_pipe_demuxer
+image_svg_pipe_demuxer
+image_sunrast_pipe_demuxer
+image_tiff_pipe_demuxer
+image_webp_pipe_demuxer
+image_xpm_pipe_demuxer
+image_xwd_pipe_demuxer
+libgme_demuxer
+libmodplug_demuxer
+libopenmpt_demuxer
+vapoursynth_demuxer'
+DEPCMD='$(DEP$(1)) $(DEP$(1)FLAGS) $($(1)DEP_FLAGS) $< 2>/dev/null | sed -e "/^\#.*/d" -e "s,^[[:space:]]*$(@F),$(@D)/$(@F)," > $(@:.o=.d)'
+DEPFLAGS=-MM
+DIRSTACK=()
+DISPLAY=localhost:32.0
+DOCUMENT_LIST='
+    doc
+    htmlpages
+    manpages
+    podpages
+    txtpages
+'
+E1=0
+E2=1
+EDITOR=vim
+ENABLE_MW_COMP_OPT_1=y
+ENABLE_MW_COMP_OPT_10=y
+ENABLE_MW_COMP_OPT_11=y
+ENABLE_MW_COMP_OPT_2=y
+ENABLE_MW_COMP_OPT_3=y
+ENABLE_MW_COMP_OPT_4=y
+ENABLE_MW_COMP_OPT_5=y
+ENABLE_MW_COMP_OPT_6=y
+ENABLE_MW_COMP_OPT_7=y
+ENABLE_MW_COMP_OPT_8=y
+ENABLE_MW_COMP_OPT_9=y
+ENCODER_LIST='a64multi_encoder
+a64multi5_encoder
+alias_pix_encoder
+amv_encoder
+apng_encoder
+asv1_encoder
+asv2_encoder
+avrp_encoder
+avui_encoder
+ayuv_encoder
+bmp_encoder
+cinepak_encoder
+cljr_encoder
+comfortnoise_encoder
+dnxhd_encoder
+dpx_encoder
+dvvideo_encoder
+ffv1_encoder
+ffvhuff_encoder
+fits_encoder
+flashsv_encoder
+flashsv2_encoder
+flv_encoder
+gif_encoder
+h261_encoder
+h263_encoder
+h263p_encoder
+hap_encoder
+huffyuv_encoder
+jpeg2000_encoder
+jpegls_encoder
+ljpeg_encoder
+magicyuv_encoder
+mjpeg_encoder
+mpeg1video_encoder
+mpeg2video_encoder
+mpeg4_encoder
+msmpeg4v2_encoder
+msmpeg4v3_encoder
+msvideo1_encoder
+pam_encoder
+pbm_encoder
+pcx_encoder
+pgm_encoder
+pgmyuv_encoder
+png_encoder
+ppm_encoder
+prores_encoder
+prores_aw_encoder
+prores_ks_encoder
+qtrle_encoder
+r10k_encoder
+r210_encoder
+rawvideo_encoder
+roq_encoder
+rv10_encoder
+rv20_encoder
+s302m_encoder
+sgi_encoder
+snow_encoder
+sunrast_encoder
+svq1_encoder
+targa_encoder
+tiff_encoder
+utvideo_encoder
+v210_encoder
+v308_encoder
+v408_encoder
+v410_encoder
+vc2_encoder
+wrapped_avframe_encoder
+wmv1_encoder
+wmv2_encoder
+xbm_encoder
+xface_encoder
+xwd_encoder
+y41p_encoder
+yuv4_encoder
+zlib_encoder
+zmbv_encoder
+aac_encoder
+ac3_encoder
+ac3_fixed_encoder
+alac_encoder
+aptx_encoder
+aptx_hd_encoder
+dca_encoder
+eac3_encoder
+flac_encoder
+g723_1_encoder
+mlp_encoder
+mp2_encoder
+mp2fixed_encoder
+nellymoser_encoder
+opus_encoder
+ra_144_encoder
+sbc_encoder
+sonic_encoder
+sonic_ls_encoder
+truehd_encoder
+tta_encoder
+vorbis_encoder
+wavpack_encoder
+wmav1_encoder
+wmav2_encoder
+pcm_alaw_encoder
+pcm_f32be_encoder
+pcm_f32le_encoder
+pcm_f64be_encoder
+pcm_f64le_encoder
+pcm_mulaw_encoder
+pcm_s8_encoder
+pcm_s8_planar_encoder
+pcm_s16be_encoder
+pcm_s16be_planar_encoder
+pcm_s16le_encoder
+pcm_s16le_planar_encoder
+pcm_s24be_encoder
+pcm_s24daud_encoder
+pcm_s24le_encoder
+pcm_s24le_planar_encoder
+pcm_s32be_encoder
+pcm_s32le_encoder
+pcm_s32le_planar_encoder
+pcm_s64be_encoder
+pcm_s64le_encoder
+pcm_u8_encoder
+pcm_u16be_encoder
+pcm_u16le_encoder
+pcm_u24be_encoder
+pcm_u24le_encoder
+pcm_u32be_encoder
+pcm_u32le_encoder
+pcm_vidc_encoder
+roq_dpcm_encoder
+adpcm_adx_encoder
+adpcm_g722_encoder
+adpcm_g726_encoder
+adpcm_g726le_encoder
+adpcm_ima_qt_encoder
+adpcm_ima_wav_encoder
+adpcm_ms_encoder
+adpcm_swf_encoder
+adpcm_yamaha_encoder
+ssa_encoder
+ass_encoder
+dvbsub_encoder
+dvdsub_encoder
+movtext_encoder
+srt_encoder
+subrip_encoder
+text_encoder
+webvtt_encoder
+xsub_encoder
+aac_at_encoder
+alac_at_encoder
+ilbc_at_encoder
+pcm_alaw_at_encoder
+pcm_mulaw_at_encoder
+libaom_av1_encoder
+libcodec2_encoder
+libfdk_aac_encoder
+libgsm_encoder
+libgsm_ms_encoder
+libilbc_encoder
+libmp3lame_encoder
+libopencore_amrnb_encoder
+libopenjpeg_encoder
+libopus_encoder
+libshine_encoder
+libspeex_encoder
+libtheora_encoder
+libtwolame_encoder
+libvo_amrwbenc_encoder
+libvorbis_encoder
+libvpx_vp8_encoder
+libvpx_vp9_encoder
+libwavpack_encoder
+libwebp_anim_encoder
+libwebp_encoder
+libx262_encoder
+libx264_encoder
+libx264rgb_encoder
+libx265_encoder
+libxavs_encoder
+libxavs2_encoder
+libxvid_encoder
+h263_v4l2m2m_encoder
+libopenh264_encoder
+h264_amf_encoder
+h264_nvenc_encoder
+h264_omx_encoder
+h264_qsv_encoder
+h264_v4l2m2m_encoder
+h264_vaapi_encoder
+h264_videotoolbox_encoder
+nvenc_encoder
+nvenc_h264_encoder
+nvenc_hevc_encoder
+hevc_amf_encoder
+hevc_nvenc_encoder
+hevc_qsv_encoder
+hevc_v4l2m2m_encoder
+hevc_vaapi_encoder
+hevc_videotoolbox_encoder
+libkvazaar_encoder
+mjpeg_qsv_encoder
+mjpeg_vaapi_encoder
+mpeg2_qsv_encoder
+mpeg2_vaapi_encoder
+mpeg4_v4l2m2m_encoder
+vp8_v4l2m2m_encoder
+vp8_vaapi_encoder
+vp9_vaapi_encoder'
+EUID=1036
+EXAMPLE_LIST='
+    avio_dir_cmd_example
+    avio_reading_example
+    decode_audio_example
+    decode_video_example
+    demuxing_decoding_example
+    encode_audio_example
+    encode_video_example
+    extract_mvs_example
+    filter_audio_example
+    filtering_audio_example
+    filtering_video_example
+    http_multiclient_example
+    hw_decode_example
+    metadata_example
+    muxing_example
+    qsvdec_example
+    remuxing_example
+    resampling_audio_example
+    scaling_video_example
+    transcode_aac_example
+    transcoding_example
+    vaapi_encode_example
+    vaapi_transcode_example
+'
+EXPAT_DEST_DIR=/home/yangyi/develop/hiberryV1R2/hiberry_dev_20190629_1115/source/thirdparty/xmp-toolkit-sdk
+EXPAT_DEST_DIR_REL=/home/yangyi/develop/hiberryV1R2/hiberry_dev_20190629_1115/middleware/thirdparty/xmp-toolkit-sdk
+EXPAT_ORG_TAR=libexpat-R_2_2_6.tar.gz
+EXTERNAL_AUTODETECT_LIBRARY_LIST='
+    alsa
+    appkit
+    avfoundation
+    bzlib
+    coreimage
+    iconv
+    libxcb
+    libxcb_shm
+    libxcb_shape
+    libxcb_xfixes
+    lzma
+    schannel
+    sdl2
+    securetransport
+    sndio
+    xlib
+    zlib
+'
+EXTERNAL_LIBRARY_GPLV3_LIST='
+    libsmbclient
+'
+EXTERNAL_LIBRARY_GPL_LIST='
+    avisynth
+    frei0r
+    libcdio
+    libdavs2
+    librubberband
+    libvidstab
+    libx264
+    libx265
+    libxavs
+    libxavs2
+    libxvid
+'
+EXTERNAL_LIBRARY_LIST='
+    
+    avisynth
+    frei0r
+    libcdio
+    libdavs2
+    librubberband
+    libvidstab
+    libx264
+    libx265
+    libxavs
+    libxavs2
+    libxvid
+
+    
+    decklink
+    libndi_newtek
+    libfdk_aac
+    openssl
+    libtls
+
+    
+    gmp
+    liblensfun
+    libopencore_amrnb
+    libopencore_amrwb
+    libvmaf
+    libvo_amrwbenc
+    mbedtls
+    rkmpp
+
+    
+    libsmbclient
+
+    chromaprint
+    gcrypt
+    gnutls
+    jni
+    ladspa
+    libaom
+    libass
+    libbluray
+    libbs2b
+    libcaca
+    libcelt
+    libcodec2
+    libdc1394
+    libdrm
+    libflite
+    libfontconfig
+    libfreetype
+    libfribidi
+    libgme
+    libgsm
+    libiec61883
+    libilbc
+    libjack
+    libklvanc
+    libkvazaar
+    libmodplug
+    libmp3lame
+    libmysofa
+    libopencv
+    libopenh264
+    libopenjpeg
+    libopenmpt
+    libopus
+    libpulse
+    librsvg
+    librtmp
+    libshine
+    libsmbclient
+    libsnappy
+    libsoxr
+    libspeex
+    libsrt
+    libssh
+    libtensorflow
+    libtesseract
+    libtheora
+    libtwolame
+    libv4l2
+    libvorbis
+    libvpx
+    libwavpack
+    libwebp
+    libxml2
+    libzimg
+    libzmq
+    libzvbi
+    lv2
+    mediacodec
+    openal
+    opengl
+    vapoursynth
+'
+EXTERNAL_LIBRARY_NONFREE_LIST='
+    decklink
+    libndi_newtek
+    libfdk_aac
+    openssl
+    libtls
+'
+EXTERNAL_LIBRARY_VERSION3_LIST='
+    gmp
+    liblensfun
+    libopencore_amrnb
+    libopencore_amrwb
+    libvmaf
+    libvo_amrwbenc
+    mbedtls
+    rkmpp
+'
+EXTRALIBS_LIST='
+    cpu_init
+    cws2fws
+'
+FEATURE_LIST='
+    ftrapv
+    gray
+    hardcoded_tables
+    omx_rpi
+    runtime_cpudetect
+    safe_bitstream_reader
+    shared
+    small
+    static
+    swscale_alpha
+'
+FFMEG_DEST_DIR=/home/yangyi/develop/hiberryV1R2/hiberry_dev_20190629_1115/source/thirdparty/ffmpeg
+FFMEG_DEST_DIR_REL=/home/yangyi/develop/hiberryV1R2/hiberry_dev_20190629_1115/middleware/thirdparty/ffmpeg
+FFMPEG_CONFIGURATION='--prefix=./install --enable-cross-compile --disable-doc --disable-htmlpages --target-os=linux --enable-static --disable-shared --disable-debug --disable-iconv --enable-small --disable-network --disable-filters --disable-devices --disable-programs --disable-swresample --disable-swscale --disable-avdevice --disable-postproc --disable-avfilter --disable-protocols --disable-pthreads --disable-runtime-cpudetect --disable-everything --enable-pic --enable-protocol=file --disable-muxers --enable-demuxer=mov --disable-neon --disable-inline-asm --disable-asm --disable-armv6 --disable-armv6t2 --disable-armv5te --disable-vfp --disable-hardcoded-tables --disable-mediacodec --enable-bsf=h264_mp4toannexb --enable-bsf=hevc_mp4toannexb --disable-pixelutils --enable-demuxer=wav --disable-gpl --cpu=cortex-a7 --arch=armv7-a --cross-prefix=arm-himix100-linux-'
+FFMPEG_ORG_TAR=ffmpeg-4.1.tar.gz
+FFMPEG_ORIGIN_VER=ffmpeg-4.1
+FILTER_LIST='abench_filter
+acompressor_filter
+acontrast_filter
+acopy_filter
+acue_filter
+acrossfade_filter
+acrossover_filter
+acrusher_filter
+adeclick_filter
+adeclip_filter
+adelay_filter
+aderivative_filter
+aecho_filter
+aemphasis_filter
+aeval_filter
+afade_filter
+afftdn_filter
+afftfilt_filter
+afir_filter
+aformat_filter
+agate_filter
+aiir_filter
+aintegral_filter
+ainterleave_filter
+alimiter_filter
+allpass_filter
+aloop_filter
+amerge_filter
+ametadata_filter
+amix_filter
+amultiply_filter
+anequalizer_filter
+anull_filter
+apad_filter
+aperms_filter
+aphaser_filter
+apulsator_filter
+arealtime_filter
+aresample_filter
+areverse_filter
+aselect_filter
+asendcmd_filter
+asetnsamples_filter
+asetpts_filter
+asetrate_filter
+asettb_filter
+ashowinfo_filter
+asidedata_filter
+asplit_filter
+astats_filter
+astreamselect_filter
+atempo_filter
+atrim_filter
+azmq_filter
+bandpass_filter
+bandreject_filter
+bass_filter
+biquad_filter
+bs2b_filter
+channelmap_filter
+channelsplit_filter
+chorus_filter
+compand_filter
+compensationdelay_filter
+crossfeed_filter
+crystalizer_filter
+dcshift_filter
+drmeter_filter
+dynaudnorm_filter
+earwax_filter
+ebur128_filter
+equalizer_filter
+extrastereo_filter
+firequalizer_filter
+flanger_filter
+haas_filter
+hdcd_filter
+headphone_filter
+highpass_filter
+highshelf_filter
+join_filter
+ladspa_filter
+loudnorm_filter
+lowpass_filter
+lowshelf_filter
+lv2_filter
+mcompand_filter
+pan_filter
+replaygain_filter
+resample_filter
+rubberband_filter
+sidechaincompress_filter
+sidechaingate_filter
+silencedetect_filter
+silenceremove_filter
+sofalizer_filter
+stereotools_filter
+stereowiden_filter
+superequalizer_filter
+surround_filter
+treble_filter
+tremolo_filter
+vibrato_filter
+volume_filter
+volumedetect_filter
+aevalsrc_filter
+anoisesrc_filter
+anullsrc_filter
+flite_filter
+hilbert_filter
+sinc_filter
+sine_filter
+anullsink_filter
+alphaextract_filter
+alphamerge_filter
+amplify_filter
+ass_filter
+atadenoise_filter
+avgblur_filter
+avgblur_opencl_filter
+bbox_filter
+bench_filter
+bitplanenoise_filter
+blackdetect_filter
+blackframe_filter
+blend_filter
+bm3d_filter
+boxblur_filter
+boxblur_opencl_filter
+bwdif_filter
+chromahold_filter
+chromakey_filter
+ciescope_filter
+codecview_filter
+colorbalance_filter
+colorchannelmixer_filter
+colorkey_filter
+colorlevels_filter
+colormatrix_filter
+colorspace_filter
+convolution_filter
+convolution_opencl_filter
+convolve_filter
+copy_filter
+coreimage_filter
+cover_rect_filter
+crop_filter
+cropdetect_filter
+cue_filter
+curves_filter
+datascope_filter
+dctdnoiz_filter
+deband_filter
+deblock_filter
+decimate_filter
+deconvolve_filter
+deflate_filter
+deflicker_filter
+deinterlace_qsv_filter
+deinterlace_vaapi_filter
+dejudder_filter
+delogo_filter
+denoise_vaapi_filter
+deshake_filter
+despill_filter
+detelecine_filter
+dilation_filter
+dilation_opencl_filter
+displace_filter
+doubleweave_filter
+drawbox_filter
+drawgraph_filter
+drawgrid_filter
+drawtext_filter
+edgedetect_filter
+elbg_filter
+entropy_filter
+eq_filter
+erosion_filter
+erosion_opencl_filter
+extractplanes_filter
+fade_filter
+fftdnoiz_filter
+fftfilt_filter
+field_filter
+fieldhint_filter
+fieldmatch_filter
+fieldorder_filter
+fillborders_filter
+find_rect_filter
+floodfill_filter
+format_filter
+fps_filter
+framepack_filter
+framerate_filter
+framestep_filter
+frei0r_filter
+fspp_filter
+gblur_filter
+geq_filter
+gradfun_filter
+graphmonitor_filter
+greyedge_filter
+haldclut_filter
+hflip_filter
+histeq_filter
+histogram_filter
+hqdn3d_filter
+hqx_filter
+hstack_filter
+hue_filter
+hwdownload_filter
+hwmap_filter
+hwupload_filter
+hwupload_cuda_filter
+hysteresis_filter
+idet_filter
+il_filter
+inflate_filter
+interlace_filter
+interleave_filter
+kerndeint_filter
+lenscorrection_filter
+lensfun_filter
+libvmaf_filter
+limiter_filter
+loop_filter
+lumakey_filter
+lut_filter
+lut1d_filter
+lut2_filter
+lut3d_filter
+lutrgb_filter
+lutyuv_filter
+maskedclamp_filter
+maskedmerge_filter
+mcdeint_filter
+mergeplanes_filter
+mestimate_filter
+metadata_filter
+midequalizer_filter
+minterpolate_filter
+mix_filter
+mpdecimate_filter
+negate_filter
+nlmeans_filter
+nnedi_filter
+noformat_filter
+noise_filter
+normalize_filter
+null_filter
+ocr_filter
+ocv_filter
+oscilloscope_filter
+overlay_filter
+overlay_opencl_filter
+overlay_qsv_filter
+owdenoise_filter
+pad_filter
+palettegen_filter
+paletteuse_filter
+perms_filter
+perspective_filter
+phase_filter
+pixdesctest_filter
+pixscope_filter
+pp_filter
+pp7_filter
+premultiply_filter
+prewitt_filter
+prewitt_opencl_filter
+procamp_vaapi_filter
+program_opencl_filter
+pseudocolor_filter
+psnr_filter
+pullup_filter
+qp_filter
+random_filter
+readeia608_filter
+readvitc_filter
+realtime_filter
+remap_filter
+removegrain_filter
+removelogo_filter
+repeatfields_filter
+reverse_filter
+roberts_filter
+roberts_opencl_filter
+rotate_filter
+sab_filter
+scale_filter
+scale_cuda_filter
+scale_npp_filter
+scale_qsv_filter
+scale_vaapi_filter
+scale2ref_filter
+select_filter
+selectivecolor_filter
+sendcmd_filter
+separatefields_filter
+setdar_filter
+setfield_filter
+setparams_filter
+setpts_filter
+setrange_filter
+setsar_filter
+settb_filter
+sharpness_vaapi_filter
+showinfo_filter
+showpalette_filter
+shuffleframes_filter
+shuffleplanes_filter
+sidedata_filter
+signalstats_filter
+signature_filter
+smartblur_filter
+sobel_filter
+sobel_opencl_filter
+split_filter
+spp_filter
+sr_filter
+ssim_filter
+stereo3d_filter
+streamselect_filter
+subtitles_filter
+super2xsai_filter
+swaprect_filter
+swapuv_filter
+tblend_filter
+telecine_filter
+threshold_filter
+thumbnail_filter
+thumbnail_cuda_filter
+tile_filter
+tinterlace_filter
+tlut2_filter
+tmix_filter
+tonemap_filter
+tonemap_opencl_filter
+transpose_filter
+transpose_npp_filter
+trim_filter
+unpremultiply_filter
+unsharp_filter
+unsharp_opencl_filter
+uspp_filter
+vaguedenoiser_filter
+vectorscope_filter
+vflip_filter
+vfrdet_filter
+vibrance_filter
+vidstabdetect_filter
+vidstabtransform_filter
+vignette_filter
+vmafmotion_filter
+vpp_qsv_filter
+vstack_filter
+w3fdif_filter
+waveform_filter
+weave_filter
+xbr_filter
+xstack_filter
+yadif_filter
+yadif_cuda_filter
+zmq_filter
+zoompan_filter
+zscale_filter
+allrgb_filter
+allyuv_filter
+cellauto_filter
+color_filter
+coreimagesrc_filter
+frei0r_src_filter
+haldclutsrc_filter
+life_filter
+mandelbrot_filter
+mptestsrc_filter
+nullsrc_filter
+openclsrc_filter
+pal75bars_filter
+pal100bars_filter
+rgbtestsrc_filter
+smptebars_filter
+smptehdbars_filter
+testsrc_filter
+testsrc2_filter
+yuvtestsrc_filter
+nullsink_filter
+abitscope_filter
+adrawgraph_filter
+agraphmonitor_filter
+ahistogram_filter
+aphasemeter_filter
+avectorscope_filter
+concat_filter
+showcqt_filter
+showfreqs_filter
+showspectrum_filter
+showspectrumpic_filter
+showvolume_filter
+showwaves_filter
+showwavespic_filter
+spectrumsynth_filter
+amovie_filter
+movie_filter
+afifo_filter
+fifo_filter'
+FULLNAME='$(NAME)$(BUILDSUF)'
+GREEN='"\e[32;1m"'
+GROUPS=()
+HAVE_LIST='
+    
+    
+    armv5te
+    armv6
+    armv6t2
+    armv8
+    neon
+    vfp
+    vfpv3
+    setend
+
+    
+    altivec
+    dcbzl
+    ldbrx
+    power8
+    ppc4xx
+    vsx
+
+    
+    
+    aesni
+    amd3dnow
+    amd3dnowext
+    avx
+    avx2
+    avx512
+    fma3
+    fma4
+    mmx
+    mmxext
+    sse
+    sse2
+    sse3
+    sse4
+    sse42
+    ssse3
+    xop
+
+    cpunop
+    i686
+
+    
+    mipsfpu
+    mips32r2
+    mips32r5
+    mips64r2
+    mips32r6
+    mips64r6
+    mipsdsp
+    mipsdspr2
+    msa
+
+    
+    loongson2
+    loongson3
+    mmi
+
+
+    armv5te_external
+armv6_external
+armv6t2_external
+armv8_external
+neon_external
+vfp_external
+vfpv3_external
+setend_external
+altivec_external
+dcbzl_external
+ldbrx_external
+power8_external
+ppc4xx_external
+vsx_external
+aesni_external
+amd3dnow_external
+amd3dnowext_external
+avx_external
+avx2_external
+avx512_external
+fma3_external
+fma4_external
+mmx_external
+mmxext_external
+sse_external
+sse2_external
+sse3_external
+sse4_external
+sse42_external
+ssse3_external
+xop_external
+cpunop_external
+i686_external
+mipsfpu_external
+mips32r2_external
+mips32r5_external
+mips64r2_external
+mips32r6_external
+mips64r6_external
+mipsdsp_external
+mipsdspr2_external
+msa_external
+loongson2_external
+loongson3_external
+mmi_external
+    armv5te_inline
+armv6_inline
+armv6t2_inline
+armv8_inline
+neon_inline
+vfp_inline
+vfpv3_inline
+setend_inline
+altivec_inline
+dcbzl_inline
+ldbrx_inline
+power8_inline
+ppc4xx_inline
+vsx_inline
+aesni_inline
+amd3dnow_inline
+amd3dnowext_inline
+avx_inline
+avx2_inline
+avx512_inline
+fma3_inline
+fma4_inline
+mmx_inline
+mmxext_inline
+sse_inline
+sse2_inline
+sse3_inline
+sse4_inline
+sse42_inline
+ssse3_inline
+xop_inline
+cpunop_inline
+i686_inline
+mipsfpu_inline
+mips32r2_inline
+mips32r5_inline
+mips64r2_inline
+mips32r6_inline
+mips64r6_inline
+mipsdsp_inline
+mipsdspr2_inline
+msa_inline
+loongson2_inline
+loongson3_inline
+mmi_inline
+    
+    aligned_stack
+    fast_64bit
+    fast_clz
+    fast_cmov
+    local_aligned
+    simd_align_16
+    simd_align_32
+    simd_align_64
+
+    
+    atomic_cas_ptr
+    machine_rw_barrier
+    MemoryBarrier
+    mm_empty
+    rdtsc
+    sem_timedwait
+    sync_val_compare_and_swap
+
+    
+    cabs
+    cexp
+
+    
+    inline_asm
+    symver
+    x86asm
+
+    
+    bigendian
+    fast_unaligned
+
+    
+    arpa_inet_h
+    asm_types_h
+    cdio_paranoia_h
+    cdio_paranoia_paranoia_h
+    cuda_h
+    dispatch_dispatch_h
+    dev_bktr_ioctl_bt848_h
+    dev_bktr_ioctl_meteor_h
+    dev_ic_bt8xx_h
+    dev_video_bktr_ioctl_bt848_h
+    dev_video_meteor_ioctl_meteor_h
+    direct_h
+    dirent_h
+    dxgidebug_h
+    dxva_h
+    ES2_gl_h
+    gsm_h
+    io_h
+    linux_perf_event_h
+    machine_ioctl_bt848_h
+    machine_ioctl_meteor_h
+    malloc_h
+    opencv2_core_core_c_h
+    OpenGL_gl3_h
+    poll_h
+    sys_param_h
+    sys_resource_h
+    sys_select_h
+    sys_soundcard_h
+    sys_time_h
+    sys_un_h
+    sys_videoio_h
+    termios_h
+    udplite_h
+    unistd_h
+    valgrind_valgrind_h
+    windows_h
+    winsock2_h
+
+    
+    intrinsics_neon
+
+    
+    atanf
+    atan2f
+    cbrt
+    cbrtf
+    copysign
+    cosf
+    erf
+    exp2
+    exp2f
+    expf
+    hypot
+    isfinite
+    isinf
+    isnan
+    ldexpf
+    llrint
+    llrintf
+    log2
+    log2f
+    log10f
+    lrint
+    lrintf
+    powf
+    rint
+    round
+    roundf
+    sinf
+    trunc
+    truncf
+
+    
+    dos_paths
+    libc_msvcrt
+    MMAL_PARAMETER_VIDEO_MAX_NUM_CALLBACKS
+    section_data_rel_ro
+    threads
+    uwp
+    winrt
+
+    
+    access
+    aligned_malloc
+    arc4random
+    clock_gettime
+    closesocket
+    CommandLineToArgvW
+    fcntl
+    getaddrinfo
+    gethrtime
+    getopt
+    GetProcessAffinityMask
+    GetProcessMemoryInfo
+    GetProcessTimes
+    getrusage
+    GetSystemTimeAsFileTime
+    gettimeofday
+    glob
+    glXGetProcAddress
+    gmtime_r
+    inet_aton
+    isatty
+    kbhit
+    localtime_r
+    lstat
+    lzo1x_999_compress
+    mach_absolute_time
+    MapViewOfFile
+    memalign
+    mkstemp
+    mmap
+    mprotect
+    nanosleep
+    PeekNamedPipe
+    posix_memalign
+    pthread_cancel
+    sched_getaffinity
+    SecItemImport
+    SetConsoleTextAttribute
+    SetConsoleCtrlHandler
+    setmode
+    setrlimit
+    Sleep
+    strerror_r
+    sysconf
+    sysctl
+    usleep
+    UTGetOSTypeFromString
+    VirtualAlloc
+    wglGetProcAddress
+
+    
+    bcrypt
+    vaapi_drm
+    vaapi_x11
+    vdpau_x11
+
+    
+    pthreads
+    os2threads
+    w32threads
+
+    
+    as_arch_directive
+    as_dn_directive
+    as_fpu_directive
+    as_func
+    as_object_arch
+    asm_mod_q
+    blocks_extension
+    ebp_available
+    ebx_available
+    gnu_as
+    gnu_windres
+    ibm_asm
+    inline_asm_direct_symbol_refs
+    inline_asm_labels
+    inline_asm_nonlocal_labels
+    pragma_deprecated
+    rsync_contimeout
+    symver_asm_label
+    symver_gnu_asm
+    vfp_args
+    xform_asm
+    xmm_clobbers
+
+    
+    kCMVideoCodecType_HEVC
+    socklen_t
+    struct_addrinfo
+    struct_group_source_req
+    struct_ip_mreq_source
+    struct_ipv6_mreq
+    struct_msghdr_msg_flags
+    struct_pollfd
+    struct_rusage_ru_maxrss
+    struct_sctp_event_subscribe
+    struct_sockaddr_in6
+    struct_sockaddr_sa_len
+    struct_sockaddr_storage
+    struct_stat_st_mtim_tv_nsec
+    struct_v4l2_frmivalenum_discrete
+
+    makeinfo
+    makeinfo_html
+    opencl_d3d11
+    opencl_drm_arm
+    opencl_drm_beignet
+    opencl_dxva2
+    opencl_vaapi_beignet
+    opencl_vaapi_intel_media
+    perl
+    pod2man
+    texi2html
+'
+HAVE_LIST_CMDLINE='
+    inline_asm
+    symver
+    x86asm
+'
+HAVE_LIST_PUB='
+    bigendian
+    fast_unaligned
+'
+HEADERS_LIST='
+    arpa_inet_h
+    asm_types_h
+    cdio_paranoia_h
+    cdio_paranoia_paranoia_h
+    cuda_h
+    dispatch_dispatch_h
+    dev_bktr_ioctl_bt848_h
+    dev_bktr_ioctl_meteor_h
+    dev_ic_bt8xx_h
+    dev_video_bktr_ioctl_bt848_h
+    dev_video_meteor_ioctl_meteor_h
+    direct_h
+    dirent_h
+    dxgidebug_h
+    dxva_h
+    ES2_gl_h
+    gsm_h
+    io_h
+    linux_perf_event_h
+    machine_ioctl_bt848_h
+    machine_ioctl_meteor_h
+    malloc_h
+    opencv2_core_core_c_h
+    OpenGL_gl3_h
+    poll_h
+    sys_param_h
+    sys_resource_h
+    sys_select_h
+    sys_soundcard_h
+    sys_time_h
+    sys_un_h
+    sys_videoio_h
+    termios_h
+    udplite_h
+    unistd_h
+    valgrind_valgrind_h
+    windows_h
+    winsock2_h
+'
+HI3516CV300_SDK_PATH=HiBerry_SDK_Version/hi3516cv300
+HI3516CV300_SDK_VERSION=Hi3516CV300_SDK_V1.0.2.0
+HI3516CV500_LINUX_VERSION=linux-4.9.37
+HI3516CV500_SDK_PATH=HiBerry_SDK_Version/Hi3516CV500
+HI3516CV500_SDK_VERSION=Hi3516CV500_SDK_V2.0.1.1
+HI3518EV300_LINUX_VERSION=linux-4.9.37
+HI3518EV300_SDK_PATH=HiBerry_SDK_Version/Hi3518EV300
+HI3518EV300_SDK_VERSION=Hi3516EV200_SDK_V1.0.0.2
+HI3519AV100_LINUX_VERSION=linux-4.9.37
+HI3519AV100_OS_MEM_CFG=osdrv_mem_cfg.sh
+HI3519AV100_SDK_PATH=HiBerry_SDK_Version/Hi3519AV100
+HI3519AV100_SDK_VERSION=Hi3519AV100_SDK_V8.0.0.2_B010
+HI3519AV100_UPROC=userproc
+HI3556AV100_LINUX_VERSION=linux-4.9.37
+HI3556AV100_OS_MEM_CFG=osdrv_mem_cfg.sh
+HI3556AV100_REF_SDK_PATH=http://szxsvn04-rd:6801/svn/HI_BVT_SVN/HiMobileCam%20V100R001/trunk/04.SW/01.CI/1.6.CODE/trunk/hi3556av100_glibc
+HI3556AV100_REF_SDK_VERSION=hi3556av100_glibc
+HI3556AV100_SDK_PATH=HiBerry_SDK_Version/Hi3556AV100
+HI3556AV100_SDK_VERSION=Hi3556AV100_SDK_V2.0.1.1_B010
+HI3556AV100_UPROC=userproc
+HI3559AES_SDK_PATH=HiBerry_SDK_Version/Hi3559AV100
+HI3559AES_SDK_VERSION=Hi3559AV100_SDK_V2.0.0.1
+HI3559AV100_LINUX_VERSION=linux-4.9.37
+HI3559AV100_REF_SDK_DIR=hi3559av100_glibc
+HI3559AV100_REF_SDK_PATH=http://szxsvn04-rd:6801/svn/HI_BVT_SVN/HiMobileCam%20V100R001/trunk/04.SW/01.CI/1.6.CODE/trunk/hi3559av100_glibc
+HI3559AV100_SDK_PATH=HiBerry_SDK_Version/Hi3559AV100
+HI3559AV100_SDK_VERSION=Hi3559AV100_SDK_V2.0.1.0
+HI3559V200_LINUX_VERSION=linux-4.9.37
+HI3559V200_SDK_PATH=HiBerry_SDK_Version/Hi3559V200
+HI3559V200_SDK_VERSION=Hi3559V200_SDK_V1.0.1.3_B010
+HI3559V200_UPROC=userproc
+HI3559_LINUX_SHARE_DIR=HiBerry_SDK_Version/Hi3559V100
+HI3559_LINUX_VERSION=linux-3.18.20
+HI3559_SDK_DIR=glibc
+HI3559_SDK_PATH=http://szxsvn04-rd:6801/svn/HI_BVT_SVN/Hi3559V100R003/nonkey/trunk/04.SW/01.CI/1.6.CODE/trunk/glibc
+HI3559_SDK_VERSION=hi3559_glibc
+HIARCH=hi3559v200
+HIBERRY_CFGFILE=/home/yangyi/develop/hiberryV1R2/hiberry_dev_20190629_1115/source/cfg.mak
+HIBERRY_DIR=/home/yangyi/develop/hiberryV1R2/hiberry_dev_20190629_1115/source/thirdparty/ffmpeg/ffmpeg-y/../../..
+HOME=/home/yangyi
+HOSTCC_C=-c
+HOSTCC_E='-E -o $@'
+HOSTCC_O='-o $@'
+HOSTLD_O='-o $@'
+HOSTNAME=2013
+HOSTTYPE=x86_64
+HWACCEL_AUTODETECT_LIBRARY_LIST='
+    amf
+    audiotoolbox
+    crystalhd
+    cuda
+    cuvid
+    d3d11va
+    dxva2
+    ffnvcodec
+    nvdec
+    nvenc
+    vaapi
+    vdpau
+    videotoolbox
+    v4l2_m2m
+    xvmc
+'
+HWACCEL_LIBRARY_LIST='
+    
+    cuda_sdk
+    libnpp
+
+    libmfx
+    mmal
+    omx
+    opencl
+'
+HWACCEL_LIBRARY_NONFREE_LIST='
+    cuda_sdk
+    libnpp
+'
+HWACCEL_LIST='h263_vaapi_hwaccel
+h263_videotoolbox_hwaccel
+h264_d3d11va_hwaccel
+h264_d3d11va2_hwaccel
+h264_dxva2_hwaccel
+h264_nvdec_hwaccel
+h264_vaapi_hwaccel
+h264_vdpau_hwaccel
+h264_videotoolbox_hwaccel
+hevc_d3d11va_hwaccel
+hevc_d3d11va2_hwaccel
+hevc_dxva2_hwaccel
+hevc_nvdec_hwaccel
+hevc_vaapi_hwaccel
+hevc_vdpau_hwaccel
+hevc_videotoolbox_hwaccel
+mjpeg_nvdec_hwaccel
+mjpeg_vaapi_hwaccel
+mpeg1_nvdec_hwaccel
+mpeg1_vdpau_hwaccel
+mpeg1_videotoolbox_hwaccel
+mpeg1_xvmc_hwaccel
+mpeg2_d3d11va_hwaccel
+mpeg2_d3d11va2_hwaccel
+mpeg2_nvdec_hwaccel
+mpeg2_dxva2_hwaccel
+mpeg2_vaapi_hwaccel
+mpeg2_vdpau_hwaccel
+mpeg2_videotoolbox_hwaccel
+mpeg2_xvmc_hwaccel
+mpeg4_nvdec_hwaccel
+mpeg4_vaapi_hwaccel
+mpeg4_vdpau_hwaccel
+mpeg4_videotoolbox_hwaccel
+vc1_d3d11va_hwaccel
+vc1_d3d11va2_hwaccel
+vc1_dxva2_hwaccel
+vc1_nvdec_hwaccel
+vc1_vaapi_hwaccel
+vc1_vdpau_hwaccel
+vp8_nvdec_hwaccel
+vp8_vaapi_hwaccel
+vp9_d3d11va_hwaccel
+vp9_d3d11va2_hwaccel
+vp9_dxva2_hwaccel
+vp9_nvdec_hwaccel
+vp9_vaapi_hwaccel
+wmv3_d3d11va_hwaccel
+wmv3_d3d11va2_hwaccel
+wmv3_dxva2_hwaccel
+wmv3_nvdec_hwaccel
+wmv3_vaapi_hwaccel
+wmv3_vdpau_hwaccel'
+IFS=' 	
+'
+INDEV_LIST='alsa_indev
+android_camera_indev
+avfoundation_indev
+bktr_indev
+decklink_indev
+libndi_newtek_indev
+dshow_indev
+fbdev_indev
+gdigrab_indev
+iec61883_indev
+jack_indev
+kmsgrab_indev
+lavfi_indev
+openal_indev
+oss_indev
+pulse_indev
+sndio_indev
+v4l2_indev
+vfwcap_indev
+xcbgrab_indev
+libcdio_indev
+libdc1394_indev'
+INSTALL_OBJS=' ndk_install media_adpt_install common_install thirdparty_install component_install sample_install'
+INTRINSICS_LIST='
+    intrinsics_neon
+'
+JAVA_HOME=/usr/lib/jvm/java/jdk1.6.0_31
+KCONFIG_CFG=build/scripts/kconfig/mainKconfig
+KCONFIG_DIR=/home/yangyi/develop/hiberryV1R2/hiberry_dev_20190629_1115/out/tools/kconfig
+KCONFIG_EXE=/home/yangyi/develop/hiberryV1R2/hiberry_dev_20190629_1115/out/tools/kconfig/himconf
+LANG=en_US.UTF-8
+LC_ALL=C
+LD_LIB=-l%
+LD_O='-o $@'
+LD_PATH=-L
+LESSCLOSE='/usr/bin/lesspipe %s %s'
+LESSOPEN='| /usr/bin/lesspipe %s'
+LIBNAME='$(LIBPREF)$(FULLNAME)$(LIBSUF)'
+LIBPREF=lib
+LIBRARY_LIST='
+    avdevice
+    avfilter
+    swscale
+    postproc
+    avformat
+    avcodec
+    swresample
+    avresample
+    avutil
+'
+LIBSUF=.a
+LIB_INSTALL_EXTRA_CMD='$$(RANLIB) "$(LIBDIR)/$(LIBNAME)"'
+LICENSE_LIST='
+    gpl
+    nonfree
+    version3
+'
+LITEOSTOPDIR=/home/yangyi/develop/hiberryV1R2/hiberry_dev_20190629_1115/source/thirdparty/ffmpeg/../../../osdrv/platform/liteos
+LITEOS_ROOT=/home/yangyi/develop/hiberryV1R2/hiberry_dev_20190629_1115/source/thirdparty/ffmpeg/../../../osdrv/platform/liteos
+LOGNAME=yangyi
+LS_COLORS='rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arj=01;31:*.taz=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lz=01;31:*.xz=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.jpg=01;35:*.jpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.axv=01;35:*.anx=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.axa=00;36:*.oga=00;36:*.spx=00;36:*.xspf=00;36:'
+MACHTYPE=x86_64-pc-linux-gnu
+MAIL=/var/mail/yangyi
+MAKEFILE_LIST=' Makefile /home/yangyi/develop/hiberryV1R2/hiberry_dev_20190629_1115/source/thirdparty/ffmpeg/../../base.mak /home/yangyi/develop/hiberryV1R2/hiberry_dev_20190629_1115/source/thirdparty/ffmpeg/../../cfg.mak /home/yangyi/develop/hiberryV1R2/hiberry_dev_20190629_1115/source/thirdparty/ffmpeg/../../ndk/build/config_hi3559v200.mak /home/yangyi/develop/hiberryV1R2/hiberry_dev_20190629_1115/source/thirdparty/ffmpeg/../../hichip_base_in.mak /home/yangyi/develop/hiberryV1R2/hiberry_dev_20190629_1115/source/thirdparty/ffmpeg/../../os_base_in.mak /home/yangyi/develop/hiberryV1R2/hiberry_dev_20190629_1115/source/thirdparty/ffmpeg/../../../osdrv/platform/liteos/config.mk /home/yangyi/develop/hiberryV1R2/hiberry_dev_20190629_1115/source/thirdparty/ffmpeg/../../../osdrv/platform/liteos/build/mk/los_config.mk /home/yangyi/develop/hiberryV1R2/hiberry_dev_20190629_1115/source/thirdparty/ffmpeg/../../../osdrv/platform/liteos/.config /home/yangyi/develop/hiberryV1R2/hiberry_dev_20190629_1115/source/thirdparty/ffmpeg/../../../osdrv/platform/liteos/build/mk/dynload_ld.mk /home/yangyi/develop/hiberryV1R2/hiberry_dev_20190629_1115/source/thirdparty/ffmpeg/../../../osdrv/platform/liteos/platform/bsp/bsp.mk /home/yangyi/develop/hiberryV1R2/hiberry_dev_20190629_1115/source/thirdparty/ffmpeg/../../../osdrv/platform/liteos/build/mk/liteos_tables_ldflags.mk'
+MAKEFLAGS='w --jobserver-fds=3,4 -j'
+MAKELEVEL=4
+MAPI_CPU_TYPE=dual
+MATH_FUNCS='
+    atanf
+    atan2f
+    cbrt
+    cbrtf
+    copysign
+    cosf
+    erf
+    exp2
+    exp2f
+    expf
+    hypot
+    isfinite
+    isinf
+    isnan
+    ldexpf
+    llrint
+    llrintf
+    log2
+    log2f
+    log10f
+    lrint
+    lrintf
+    powf
+    rint
+    round
+    roundf
+    sinf
+    trunc
+    truncf
+'
+MFLAGS='-w --jobserver-fds=3,4 -j'
+MIDDLEWARE_REL_PACK=middleware
+MUXER_LIST='a64_muxer
+ac3_muxer
+adts_muxer
+adx_muxer
+aiff_muxer
+amr_muxer
+apng_muxer
+aptx_muxer
+aptx_hd_muxer
+asf_muxer
+ass_muxer
+ast_muxer
+asf_stream_muxer
+au_muxer
+avi_muxer
+avm2_muxer
+avs2_muxer
+bit_muxer
+caf_muxer
+cavsvideo_muxer
+codec2_muxer
+codec2raw_muxer
+crc_muxer
+dash_muxer
+data_muxer
+daud_muxer
+dirac_muxer
+dnxhd_muxer
+dts_muxer
+dv_muxer
+eac3_muxer
+f4v_muxer
+ffmetadata_muxer
+fifo_muxer
+fifo_test_muxer
+filmstrip_muxer
+fits_muxer
+flac_muxer
+flv_muxer
+framecrc_muxer
+framehash_muxer
+framemd5_muxer
+g722_muxer
+g723_1_muxer
+g726_muxer
+g726le_muxer
+gif_muxer
+gsm_muxer
+gxf_muxer
+h261_muxer
+h263_muxer
+h264_muxer
+hash_muxer
+hds_muxer
+hevc_muxer
+hls_muxer
+ico_muxer
+ilbc_muxer
+image2_muxer
+image2pipe_muxer
+ipod_muxer
+ircam_muxer
+ismv_muxer
+ivf_muxer
+jacosub_muxer
+latm_muxer
+lrc_muxer
+m4v_muxer
+md5_muxer
+matroska_muxer
+matroska_audio_muxer
+microdvd_muxer
+mjpeg_muxer
+mlp_muxer
+mmf_muxer
+mov_muxer
+mp2_muxer
+mp3_muxer
+mp4_muxer
+mpeg1system_muxer
+mpeg1vcd_muxer
+mpeg1video_muxer
+mpeg2dvd_muxer
+mpeg2svcd_muxer
+mpeg2video_muxer
+mpeg2vob_muxer
+mpegts_muxer
+mpjpeg_muxer
+mxf_muxer
+mxf_d10_muxer
+mxf_opatom_muxer
+null_muxer
+nut_muxer
+oga_muxer
+ogg_muxer
+ogv_muxer
+oma_muxer
+opus_muxer
+pcm_alaw_muxer
+pcm_mulaw_muxer
+pcm_vidc_muxer
+pcm_f64be_muxer
+pcm_f64le_muxer
+pcm_f32be_muxer
+pcm_f32le_muxer
+pcm_s32be_muxer
+pcm_s32le_muxer
+pcm_s24be_muxer
+pcm_s24le_muxer
+pcm_s16be_muxer
+pcm_s16le_muxer
+pcm_s8_muxer
+pcm_u32be_muxer
+pcm_u32le_muxer
+pcm_u24be_muxer
+pcm_u24le_muxer
+pcm_u16be_muxer
+pcm_u16le_muxer
+pcm_u8_muxer
+psp_muxer
+rawvideo_muxer
+rm_muxer
+roq_muxer
+rso_muxer
+rtp_muxer
+rtp_mpegts_muxer
+rtsp_muxer
+sap_muxer
+sbc_muxer
+scc_muxer
+segafilm_muxer
+segment_muxer
+stream_segment_muxer
+singlejpeg_muxer
+smjpeg_muxer
+smoothstreaming_muxer
+sox_muxer
+spx_muxer
+spdif_muxer
+srt_muxer
+sup_muxer
+swf_muxer
+tee_muxer
+tg2_muxer
+tgp_muxer
+mkvtimestamp_v2_muxer
+truehd_muxer
+tta_muxer
+uncodedframecrc_muxer
+vc1_muxer
+vc1t_muxer
+voc_muxer
+w64_muxer
+wav_muxer
+webm_muxer
+webm_dash_manifest_muxer
+webm_chunk_muxer
+webp_muxer
+webvtt_muxer
+wtv_muxer
+wv_muxer
+yuv4mpegpipe_muxer
+chromaprint_muxer'
+MW_COMP_OPT_1='SENSOR_CABLE_TYPE=LANE_DIVIDE_MODE_0 CFG_SENSOR_TYPE0=IMX307 CFG_SENSOR_TYPE1=AHD CFG_SENSOR_TYPE2=UNUSED CFG_SENSOR_TYPE3=UNUSED CFG_SENSOR_TYPE4=UNUSED CFG_SENSOR_TYPE5=UNUSED CFG_SENSOR_TYPE6=UNUSED CFG_SENSOR_TYPE7=UNUSED CFG_SUPPORT_PHOTO_POST_PROCESS=n CFG_SUPPORT_STITCH=n CFG_SUPPORT_HDMI=n CFG_ENABLE_MINI=y CFG_SUPPORT_RECORDVQE=n CFG_SUPPORT_TALKVQE=n'
+MW_COMP_OPT_10='SENSOR_CABLE_TYPE=LANE_DIVIDE_MODE_0 CFG_SENSOR_TYPE0=IMX377 CFG_SENSOR_TYPE1=UNUSED CFG_SENSOR_TYPE2=UNUSED CFG_SENSOR_TYPE3=UNUSED CFG_SENSOR_TYPE4=UNUSED CFG_SENSOR_TYPE5=UNUSED CFG_SENSOR_TYPE6=UNUSED CFG_SENSOR_TYPE7=UNUSED CFG_SUPPORT_PHOTO_POST_PROCESS=n CFG_SUPPORT_STITCH=n CFG_SUPPORT_HDMI=n CFG_ENABLE_MINI=y CFG_SUPPORT_IVE=n'
+MW_COMP_OPT_11='SENSOR_CABLE_TYPE=LANE_DIVIDE_MODE_0 CFG_SENSOR_TYPE0=AHD CFG_SENSOR_TYPE1=IMX307 CFG_SENSOR_TYPE2=UNUSED CFG_SENSOR_TYPE3=UNUSED CFG_SENSOR_TYPE4=UNUSED CFG_SENSOR_TYPE5=UNUSED CFG_SENSOR_TYPE6=UNUSED CFG_SENSOR_TYPE7=UNUSED CFG_SUPPORT_PHOTO_POST_PROCESS=n CFG_SUPPORT_STITCH=n CFG_SUPPORT_HDMI=n CFG_ENABLE_MINI=y CFG_SUPPORT_IVE=y CFG_SUPPORT_SERDES=y'
+MW_COMP_OPT_2='ENSOR_CABLE_TYPE=LANE_DIVIDE_MODE_0 CFG_SENSOR_TYPE0=IMX123 CFG_SENSOR_TYPE1=UNUSED CFG_SENSOR_TYPE2=UNUSED CFG_SENSOR_TYPE3=UNUSED CFG_SENSOR_TYPE4=UNUSED CFG_SENSOR_TYPE5=UNUSED CFG_SENSOR_TYPE6=UNUSED CFG_SENSOR_TYPE7=UNUSED CFG_SUPPORT_PHOTO_POST_PROCESS=n CFG_SUPPORT_STITCH=n CFG_SUPPORT_HDMI=n CFG_ENABLE_MINI=y CFG_SUPPORT_RECORDVQE=n CFG_SUPPORT_TALKVQE=y'
+MW_COMP_OPT_3='SENSOR_CABLE_TYPE=LANE_DIVIDE_MODE_1 CFG_SENSOR_TYPE0=IMX307 CFG_SENSOR_TYPE1=AHD CFG_SENSOR_TYPE2=UNUSED CFG_SENSOR_TYPE3=UNUSED CFG_SENSOR_TYPE4=UNUSED CFG_SENSOR_TYPE5=UNUSED CFG_SENSOR_TYPE6=UNUSED CFG_SENSOR_TYPE7=UNUSED CFG_SUPPORT_PHOTO_POST_PROCESS=n CFG_SUPPORT_STITCH=n CFG_SUPPORT_HDMI=n CFG_ENABLE_MINI=y CFG_SUPPORT_RECORDVQE=y CFG_SUPPORT_TALKVQE=n'
+MW_COMP_OPT_4='SENSOR_CABLE_TYPE=LANE_DIVIDE_MODE_0 CFG_SENSOR_TYPE0=AHD CFG_SENSOR_TYPE1=UNUSED CFG_SENSOR_TYPE2=UNUSED CFG_SENSOR_TYPE3=UNUSED CFG_SENSOR_TYPE4=UNUSED CFG_SENSOR_TYPE5=UNUSED CFG_SENSOR_TYPE6=UNUSED CFG_SENSOR_TYPE7=UNUSED CFG_SUPPORT_PHOTO_POST_PROCESS=n CFG_SUPPORT_STITCH=n CFG_SUPPORT_HDMI=n CFG_ENABLE_MINI=y CFG_SUPPORT_RECORDVQE=y CFG_SUPPORT_TALKVQE=y'
+MW_COMP_OPT_5='SENSOR_CABLE_TYPE=LANE_DIVIDE_MODE_0 CFG_SENSOR_TYPE0=IMX458 CFG_SENSOR_TYPE1=UNUSED CFG_SENSOR_TYPE2=UNUSED CFG_SENSOR_TYPE3=UNUSED CFG_SENSOR_TYPE4=UNUSED CFG_SENSOR_TYPE5=UNUSED CFG_SENSOR_TYPE6=UNUSED CFG_SENSOR_TYPE7=UNUSED CFG_SUPPORT_PHOTO_POST_PROCESS=n CFG_SUPPORT_STITCH=n CFG_ENABLE_MINI=y'
+MW_COMP_OPT_6='SENSOR_CABLE_TYPE=LANE_DIVIDE_MODE_0 CFG_SENSOR_TYPE0=IMX458 CFG_SENSOR_TYPE1=UNUSED CFG_SENSOR_TYPE2=UNUSED CFG_SENSOR_TYPE3=UNUSED CFG_SENSOR_TYPE4=UNUSED CFG_SENSOR_TYPE5=UNUSED CFG_SENSOR_TYPE6=UNUSED CFG_SENSOR_TYPE7=UNUSED CFG_SUPPORT_PHOTO_POST_PROCESS=n CFG_SUPPORT_STITCH=n CFG_SUPPORT_GYRO=y CFG_ENABLE_MINI=y'
+MW_COMP_OPT_7='SENSOR_CABLE_TYPE=LANE_DIVIDE_MODE_1 CFG_SENSOR_TYPE0=IMX307 CFG_SENSOR_TYPE1=IMX307 CFG_SENSOR_TYPE2=UNUSED CFG_SENSOR_TYPE3=UNUSED CFG_SENSOR_TYPE4=UNUSED CFG_SENSOR_TYPE5=UNUSED CFG_SENSOR_TYPE6=UNUSED CFG_SENSOR_TYPE7=UNUSED CFG_SUPPORT_PHOTO_POST_PROCESS=n CFG_SUPPORT_STITCH=n CFG_SUPPORT_HDMI=n CFG_ENABLE_MINI=y'
+MW_COMP_OPT_8='SENSOR_CABLE_TYPE=LANE_DIVIDE_MODE_0 CFG_SENSOR_TYPE0=IMX123 CFG_SENSOR_TYPE1=AHD CFG_SENSOR_TYPE2=UNUSED CFG_SENSOR_TYPE3=UNUSED CFG_SENSOR_TYPE4=UNUSED CFG_SENSOR_TYPE5=UNUSED CFG_SENSOR_TYPE6=UNUSED CFG_SENSOR_TYPE7=UNUSED CFG_SUPPORT_PHOTO_POST_PROCESS=n CFG_SUPPORT_STITCH=n CFG_SUPPORT_HDMI=n CFG_ENABLE_MINI=y'
+MW_COMP_OPT_9='SENSOR_CABLE_TYPE=LANE_DIVIDE_MODE_0 CFG_SENSOR_TYPE0=IMX377 CFG_SENSOR_TYPE1=AHD CFG_SENSOR_TYPE2=UNUSED CFG_SENSOR_TYPE3=UNUSED CFG_SENSOR_TYPE4=UNUSED CFG_SENSOR_TYPE5=UNUSED CFG_SENSOR_TYPE6=UNUSED CFG_SENSOR_TYPE7=UNUSED CFG_SUPPORT_PHOTO_POST_PROCESS=n CFG_SUPPORT_STITCH=n CFG_SUPPORT_HDMI=n CFG_ENABLE_MINI=y'
+NDK_HOME=/opt/android-ndk-r7b
+NORMAL='"\e[39m"'
+NVCC_C=-c
+NVCC_O='-o $@'
+OBJCC_C=-c
+OBJCC_E='-E -o $@'
+OBJCC_O='-o $@'
+OPEN_SOURCE_SHARE_DIR=HiBerry_SDK_Version/opensource
+OPTERR=1
+OPTIND=1
+OSTYPE=linux-gnu
+OUTDEV_LIST='alsa_outdev
+caca_outdev
+decklink_outdev
+libndi_newtek_outdev
+fbdev_outdev
+opengl_outdev
+oss_outdev
+pulse_outdev
+sdl2_outdev
+sndio_outdev
+v4l2_outdev
+xv_outdev'
+PARSER_LIST='aac_parser
+aac_latm_parser
+ac3_parser
+adx_parser
+av1_parser
+avs2_parser
+bmp_parser
+cavsvideo_parser
+cook_parser
+dca_parser
+dirac_parser
+dnxhd_parser
+dpx_parser
+dvaudio_parser
+dvbsub_parser
+dvdsub_parser
+dvd_nav_parser
+flac_parser
+g729_parser
+gsm_parser
+h261_parser
+h263_parser
+h264_parser
+hevc_parser
+mjpeg_parser
+mlp_parser
+mpeg4video_parser
+mpegaudio_parser
+mpegvideo_parser
+opus_parser
+png_parser
+pnm_parser
+rv30_parser
+rv40_parser
+sbc_parser
+sipr_parser
+tak_parser
+vc1_parser
+vorbis_parser
+vp3_parser
+vp8_parser
+vp9_parser
+xma_parser'
+PATH=/home/yangyi/bin:/home/yangyi/bin:/home/yangyi/bin:/home/yangyi/bin:/home/yangyi/bin:/home/yangyi/bin:/home/yangyi/bin:/home/yangyi/bin:/home/yangyi/bin:/home/yangyi/bin:/home/yangyi/bin:/home/yangyi/bin:/home/yangyi/bin:/home/yangyi/ant/apache-ant-1.9.4/bin:/home/yangyi/bin:/home/yangyi/bin:/home/yangyi/bin:/home/yangyi/bin:/home/yangyi/bin:/home/yangyi/bin:/home/yangyi/bin:/opt/hisi-linux/x86-arm/arm-himix200-linux/bin:/opt/hisi-linux/x86-arm/arm-himix100-linux/bin:/opt/hisi-linux/x86-arm/arm-hisiv500-linux/target/bin:/opt/hisi-linux/x86-arm/arm-hisiv600-linux/target/bin:/opt/hisi-linux/x86-arm/gcc-arm-none-eabi-4_9-2015q3/bin:/opt/hisi-linux/x86-arm/aarch64-himix100-linux/bin:/opt/hisi-linux/x86-arm/arm-hisiv510-linux/target/bin:/opt/hisi-linux/x86-arm/arm-huaweiliteos-linux-androideabi/bin:/opt/hisi-linux/x86-arm/aarch64-hisiv610-linux/target/bin:/opt/hisi-linux/x86-arm/arm-histbv300-linux/bin:/opt/hisi-linux/x86-arm/arm-hisiv400-linux/target/bin:/opt/hisi-linux/x86-arm/arm-histbv310-linux/bin:/opt/hisi-linux-nptl/arm-hisiv100-linux/target/bin:/opt/hisi-linux/arm64/aarch64-linux-gnu/target/bin:/opt/hisi-linux/x86-arm/arm-hisiv200-linux/target/bin:/opt/hisi-linux/x86-arm/arm-hisiv300-linux/target/bin:/opt/hisi-linux/x86-arm/arm-hisiv110-linux/target/bin:/opt/hisi-linux/x86-arm/gcc-3.4.3-uClibc-0.9.28/usr/bin:/opt/hisi-linux/x86-arm/gcc-3.4.3-csl-uClibc-0.9.30-softvfp/bin:/usr/lib/jvm/java/jdk1.6.0_31/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/opt/android-ndk-r7b
+PATHS_LIST='
+    bindir
+    datadir
+    docdir
+    incdir
+    libdir
+    mandir
+    pkgconfigdir
+    prefix
+    shlibdir
+    install_name_dir
+'
+PHONE_REL_PACK=PHONE
+PIPESTATUS=([0]="0")
+POSIXLY_CORRECT=y
+PPID=3921
+PROGRAM_LIST='
+    ffplay
+    ffprobe
+    ffmpeg
+'
+PROTOCOL_LIST='async_protocol
+bluray_protocol
+cache_protocol
+concat_protocol
+crypto_protocol
+data_protocol
+ffrtmpcrypt_protocol
+ffrtmphttp_protocol
+file_protocol
+ftp_protocol
+gopher_protocol
+hls_protocol
+http_protocol
+httpproxy_protocol
+https_protocol
+icecast_protocol
+mmsh_protocol
+mmst_protocol
+md5_protocol
+pipe_protocol
+prompeg_protocol
+rtmp_protocol
+rtmpe_protocol
+rtmps_protocol
+rtmpt_protocol
+rtmpte_protocol
+rtmpts_protocol
+rtp_protocol
+sctp_protocol
+srtp_protocol
+subfile_protocol
+tee_protocol
+tcp_protocol
+tls_protocol
+udp_protocol
+udplite_protocol
+unix_protocol
+librtmp_protocol
+librtmpe_protocol
+librtmps_protocol
+librtmpt_protocol
+librtmpte_protocol
+libsrt_protocol
+libssh_protocol
+libsmbclient_protocol'
+PS4='+ '
+PWD=/home/yangyi/develop/hiberryV1R2/hiberry_dev_20190629_1115/source/thirdparty/ffmpeg/ffmpeg-y
+RAPIDJSON_DEST_DIR=/home/yangyi/develop/hiberryV1R2/hiberry_dev_20190629_1115/source/thirdparty/rapidjson
+RAPIDJSON_DEST_DIR_REL=/home/yangyi/develop/hiberryV1R2/hiberry_dev_20190629_1115/middleware/thirdparty/rapidjson
+RAPIDJSON_ORG_TAR=rapidjson-1.1.0.tar.gz
+RED='"\e[31m"'
+REFERENCES_DIR=/home/yangyi/develop/hiberryV1R2/hiberry_dev_20190629_1115/references
+SDK_LINK_PATH=sdk
+SHELL=/bin/bash
+SHELLOPTS=braceexpand:hashall:interactive-comments:posix
+SHFLAGS='-shared -Wl,-soname,$$(@F)'
+SHLVL=8
+SLIBNAME='$(SLIBPREF)$(FULLNAME)$(SLIBSUF)'
+SLIBNAME_WITH_MAJOR='$(SLIBNAME).$(LIBMAJOR)'
+SLIBNAME_WITH_VERSION='$(SLIBNAME).$(LIBVERSION)'
+SLIBPREF=lib
+SLIBSUF=.so
+SLIB_INSTALL_LINKS='$(SLIBNAME_WITH_MAJOR) $(SLIBNAME)'
+SLIB_INSTALL_NAME='$(SLIBNAME_WITH_VERSION)'
+SOURCE_DIR=/home/yangyi/develop/hiberryV1R2/hiberry_dev_20190629_1115/source
+SOURCE_DIR_REL=/home/yangyi/develop/hiberryV1R2/hiberry_dev_20190629_1115/middleware
+SSH_CLIENT='10.163.164.52 10547 22'
+SSH_CONNECTION='10.163.164.52 10547 10.121.138.30 22'
+SSH_TTY=/dev/pts/26
+SUBSYSTEM_LIST='
+    dct
+    dwt
+    error_resilience
+    faan
+    fast_unaligned
+    fft
+    lsp
+    lzo
+    mdct
+    pixelutils
+    network
+    rdft
+'
+SYSTEM_FEATURES='
+    dos_paths
+    libc_msvcrt
+    MMAL_PARAMETER_VIDEO_MAX_NUM_CALLBACKS
+    section_data_rel_ro
+    threads
+    uwp
+    winrt
+'
+SYSTEM_FUNCS='
+    access
+    aligned_malloc
+    arc4random
+    clock_gettime
+    closesocket
+    CommandLineToArgvW
+    fcntl
+    getaddrinfo
+    gethrtime
+    getopt
+    GetProcessAffinityMask
+    GetProcessMemoryInfo
+    GetProcessTimes
+    getrusage
+    GetSystemTimeAsFileTime
+    gettimeofday
+    glob
+    glXGetProcAddress
+    gmtime_r
+    inet_aton
+    isatty
+    kbhit
+    localtime_r
+    lstat
+    lzo1x_999_compress
+    mach_absolute_time
+    MapViewOfFile
+    memalign
+    mkstemp
+    mmap
+    mprotect
+    nanosleep
+    PeekNamedPipe
+    posix_memalign
+    pthread_cancel
+    sched_getaffinity
+    SecItemImport
+    SetConsoleTextAttribute
+    SetConsoleCtrlHandler
+    setmode
+    setrlimit
+    Sleep
+    strerror_r
+    sysconf
+    sysctl
+    usleep
+    UTGetOSTypeFromString
+    VirtualAlloc
+    wglGetProcAddress
+'
+SYSTEM_LIBRARIES='
+    bcrypt
+    vaapi_drm
+    vaapi_x11
+    vdpau_x11
+'
+TERM=xterm
+TEST_DIR=/home/yangyi/develop/hiberryV1R2/hiberry_dev_20190629_1115/test
+THREADS_LIST='
+    pthreads
+    os2threads
+    w32threads
+'
+TIFF_DEST_DIR=/home/yangyi/develop/hiberryV1R2/hiberry_dev_20190629_1115/source/thirdparty/tiff
+TIFF_DEST_DIR_REL=/home/yangyi/develop/hiberryV1R2/hiberry_dev_20190629_1115/middleware/thirdparty/tiff
+TIFF_ORG_TAR=tiff-4.0.9.tar.gz
+TIFF_ORIGIN_VER=tiff-4.0.9
+TOOLCHAIN_FEATURES='
+    as_arch_directive
+    as_dn_directive
+    as_fpu_directive
+    as_func
+    as_object_arch
+    asm_mod_q
+    blocks_extension
+    ebp_available
+    ebx_available
+    gnu_as
+    gnu_windres
+    ibm_asm
+    inline_asm_direct_symbol_refs
+    inline_asm_labels
+    inline_asm_nonlocal_labels
+    pragma_deprecated
+    rsync_contimeout
+    symver_asm_label
+    symver_gnu_asm
+    vfp_args
+    xform_asm
+    xmm_clobbers
+'
+TOOLS_BUILD_DIR=/home/yangyi/develop/hiberryV1R2/hiberry_dev_20190629_1115/out/tools
+TOOLS_SRC_DIR=/home/yangyi/develop/hiberryV1R2/hiberry_dev_20190629_1115/build/tools
+TOOL_DIR=/home/yangyi/develop/hiberryV1R2/hiberry_dev_20190629_1115/tool
+TYPES_LIST='
+    kCMVideoCodecType_HEVC
+    socklen_t
+    struct_addrinfo
+    struct_group_source_req
+    struct_ip_mreq_source
+    struct_ipv6_mreq
+    struct_msghdr_msg_flags
+    struct_pollfd
+    struct_rusage_ru_maxrss
+    struct_sctp_event_subscribe
+    struct_sockaddr_in6
+    struct_sockaddr_sa_len
+    struct_sockaddr_storage
+    struct_stat_st_mtim_tv_nsec
+    struct_v4l2_frmivalenum_discrete
+'
+UID=1036
+UNINSTALL_OBJS=' sample_uninstall component_uninstall common_uninstall thirdparty_uninstall media_adpt_uninstall ndk_uninstall'
+USER=yangyi
+VERSION_SCRIPT_POSTPROCESS_CMD=cat
+X86ASM_O='-o $@'
+XDG_RUNTIME_DIR=/run/user/1036
+XDG_SESSION_ID=242
+XMP_DEST_DIR=/home/yangyi/develop/hiberryV1R2/hiberry_dev_20190629_1115/source/thirdparty/xmp-toolkit-sdk
+XMP_DEST_DIR_REL=/home/yangyi/develop/hiberryV1R2/hiberry_dev_20190629_1115/middleware/thirdparty/xmp-toolkit-sdk
+XMP_TOOLKIT_SDK_TAR=XMP-Toolkit-SDK-CC201607.zip
+_='# ./configure --prefix=./install --enable-cross-compile --disable-doc --disable-htmlpages --target-os=linux --enable-static --disable-shared --disable-debug --disable-iconv --enable-small --disable-network --disable-filters --disable-devices --disable-programs --disable-swresample --disable-swscale --disable-avdevice --disable-postproc --disable-avfilter --disable-protocols --disable-pthreads --disable-runtime-cpudetect --disable-everything --enable-pic --enable-protocol=file --disable-muxers --enable-demuxer=mov --disable-neon --disable-inline-asm --disable-asm --disable-armv6 --disable-armv6t2 --disable-armv5te --disable-vfp --disable-hardcoded-tables --disable-mediacodec --enable-bsf=h264_mp4toannexb --enable-bsf=hevc_mp4toannexb --disable-pixelutils --enable-demuxer=wav --disable-gpl --cpu=cortex-a7 --arch=armv7-a --cross-prefix=arm-himix100-linux-'
+aac_adtstoasc_bsf_select=adts_header
+aac_at_decoder_deps=audiotoolbox
+aac_at_decoder_select=aac_adtstoasc_bsf
+aac_at_encoder_deps=audiotoolbox
+aac_at_encoder_select=audio_frame_queue
+aac_decoder_select='adts_header mdct15 mdct sinewin'
+aac_encoder_select='audio_frame_queue iirfilter lpc mdct sinewin'
+aac_fixed_decoder_select='adts_header mdct sinewin'
+aac_latm_decoder_select='aac_decoder aac_latm_parser'
+aac_parser_select=adts_header
+abench_filter=no
+abitscope_filter=no
+ac3_at_decoder_deps=audiotoolbox
+ac3_at_decoder_select=ac3_parser
+ac3_decoder_select='ac3_parser ac3dsp bswapdsp fmtconvert mdct'
+ac3_demuxer_select=ac3_parser
+ac3_encoder_select='ac3dsp audiodsp mdct me_cmp'
+ac3_fixed_decoder_select='ac3_parser ac3dsp bswapdsp mdct'
+ac3_fixed_encoder_select='ac3dsp audiodsp mdct me_cmp'
+acompressor_filter=no
+acontrast_filter=no
+acopy_filter=no
+acrossfade_filter=no
+acrossover_filter=no
+acrusher_filter=no
+action=disable
+acue_filter=no
+adeclick_filter=no
+adeclip_filter=no
+adelay_filter=no
+aderivative_filter=no
+adpcm_g722_decoder_select=g722dsp
+adpcm_g722_encoder_select=g722dsp
+adpcm_ima_qt_at_decoder_deps=audiotoolbox
+adrawgraph_filter=no
+aecho_filter=no
+aemphasis_filter=no
+aesni=yes
+aesni_deps=sse42
+aesni_external_deps=sse42_external
+aesni_inline_deps=sse42_inline
+aesni_suggest='aesni_external aesni_inline'
+aeval_filter=no
+aevalsrc_filter=no
+afade_filter=no
+afftdn_filter=no
+afftdn_filter_deps=avcodec
+afftdn_filter_select=fft
+afftfilt_filter=no
+afftfilt_filter_deps=avcodec
+afftfilt_filter_select=fft
+afifo_filter=no
+afir_filter=no
+afir_filter_deps=avcodec
+afir_filter_select=fft
+aformat_filter=no
+agate_filter=no
+agraphmonitor_filter=no
+ahistogram_filter=no
+aic_decoder_select='golomb idctdsp'
+aiff_muxer_select=iso_media
+aiir_filter=no
+aintegral_filter=no
+ainterleave_filter=no
+alac_at_decoder_deps=audiotoolbox
+alac_at_encoder_deps=audiotoolbox
+alac_at_encoder_select=audio_frame_queue
+alac_encoder_select=lpc
+aligned_stack_if_any='aarch64 ppc x86'
+alimiter_filter=no
+allpass_filter=no
+allrgb_filter=no
+allyuv_filter=no
+aloop_filter=no
+alphaextract_filter=no
+alphamerge_filter=no
+als_decoder_select=bswapdsp
+alsa=yes
+alsa_indev=no
+alsa_indev_deps=alsa
+alsa_outdev=no
+alsa_outdev_deps=alsa
+altivec=yes
+altivec_deps=ppc
+amd3dnow=yes
+amd3dnow_deps=mmx
+amd3dnow_external_deps=mmx_external
+amd3dnow_inline_deps=mmx_inline
+amd3dnow_suggest='amd3dnow_external amd3dnow_inline'
+amd3dnowext=yes
+amd3dnowext_deps=amd3dnow
+amd3dnowext_external_deps=amd3dnow_external
+amd3dnowext_inline_deps=amd3dnow_inline
+amd3dnowext_suggest='amd3dnowext_external amd3dnowext_inline'
+amerge_filter=no
+ametadata_filter=no
+amf=yes
+amf_deps_any='libdl LoadLibrary'
+amix_filter=no
+amovie_filter=no
+amovie_filter_deps='avcodec avformat'
+amplify_filter=no
+amr_nb_at_decoder_deps=audiotoolbox
+amrnb_decoder_select=lsp
+amrwb_decoder_select=lsp
+amultiply_filter=no
+amv_decoder_select='sp5x_decoder exif'
+amv_encoder_select='jpegtables mpegvideoenc'
+android_camera_indev=no
+android_camera_indev_deps='android camera2ndk mediandk pthreads'
+android_camera_indev_extralibs='-landroid -lcamera2ndk -lmediandk'
+anequalizer_filter=no
+anoisesrc_filter=no
+anull_filter=no
+anullsink_filter=no
+anullsrc_filter=no
+apad_filter=no
+ape_decoder_select='bswapdsp llauddsp'
+aperms_filter=no
+aphasemeter_filter=no
+aphaser_filter=no
+apng_decoder_deps=zlib
+apng_encoder_deps=zlib
+apng_encoder_select=llvidencdsp
+appkit=yes
+aptx_decoder_select=audio_frame_queue
+aptx_encoder_select=audio_frame_queue
+aptx_hd_decoder_select=audio_frame_queue
+aptx_hd_encoder_select=audio_frame_queue
+apulsator_filter=no
+ar_default=ar
+arch=armv7-a
+arch_default=x86_64
+arealtime_filter=no
+aresample_filter=no
+aresample_filter_deps=swresample
+areverse_filter=no
+armv5te=no
+armv5te_deps=arm
+armv5te_inline_deps=inline_asm
+armv6=no
+armv6_deps=arm
+armv6_inline_deps=inline_asm
+armv6t2=no
+armv6t2_deps=arm
+armv6t2_inline_deps=inline_asm
+armv8=yes
+armv8_deps=aarch64
+armv8_inline_deps=inline_asm
+aselect_filter=no
+asendcmd_filter=no
+asetnsamples_filter=no
+asetpts_filter=no
+asetrate_filter=no
+asettb_filter=no
+asf_demuxer_select=riffdec
+asf_muxer_select=riffenc
+asf_o_demuxer_select=riffdec
+asf_stream_muxer_select=asf_muxer
+asflags_filter=echo
+ashowinfo_filter=no
+asidedata_filter=no
+asm=no
+asplit_filter=no
+ass_filter=no
+ass_filter_deps=libass
+astats_filter=no
+astreamselect_filter=no
+asv1_decoder_select='blockdsp bswapdsp idctdsp'
+asv1_encoder_select='aandcttables bswapdsp fdctdsp pixblockdsp'
+asv2_decoder_select='blockdsp bswapdsp idctdsp'
+asv2_encoder_select='aandcttables bswapdsp fdctdsp pixblockdsp'
+async_protocol_deps=threads
+atadenoise_filter=no
+atempo_filter=no
+atempo_filter_deps=avcodec
+atempo_filter_select=rdft
+atomics_gcc_if=sync_val_compare_and_swap
+atomics_native_if_any='
+    atomics_gcc
+    atomics_suncc
+    atomics_win32
+'
+atomics_suncc_if='atomic_cas_ptr machine_rw_barrier'
+atomics_win32_if=MemoryBarrier
+atrac1_decoder_select='mdct sinewin'
+atrac3_decoder_select=mdct
+atrac3p_decoder_select='mdct sinewin'
+atrac9_decoder_select=mdct
+atrim_filter=no
+audiotoolbox=yes
+av1_metadata_bsf_select=cbs_av1
+av1_parser_select=cbs_av1
+avcodec=yes
+avcodec_deps=avutil
+avcodec_extralibs='pthreads_extralibs iconv_extralibs'
+avcodec_select=null_bsf
+avcodec_suggest=libm
+avdevice=no
+avdevice_deps='avformat avcodec avutil'
+avdevice_suggest=libm
+avectorscope_filter=no
+avfilter=no
+avfilter_deps=avutil
+avfilter_extralibs=pthreads_extralibs
+avfilter_suggest=libm
+avformat=yes
+avformat_deps='avcodec avutil'
+avformat_suggest='libm network zlib'
+avfoundation=yes
+avfoundation_indev=no
+avfoundation_indev_deps='avfoundation corevideo coremedia pthreads'
+avfoundation_indev_extralibs='-framework Foundation'
+avfoundation_indev_suggest='coregraphics applicationservices'
+avgblur_filter=no
+avgblur_opencl_filter=no
+avgblur_opencl_filter_deps=opencl
+avi_demuxer_select='iso_media riffdec exif'
+avi_muxer_select=riffenc
+avio_dir_cmd_deps='avformat avutil'
+avio_dir_cmd_example=yes
+avio_reading_deps='avformat avcodec avutil'
+avio_reading_example=yes
+avisynth=no
+avisynth_demuxer_deps=avisynth
+avisynth_demuxer_select=riffdec
+avisynth_deps_any='libdl LoadLibrary'
+avresample_deps=avutil
+avresample_suggest=libm
+avrn_decoder_select='exif jpegtables'
+avutil=yes
+avutil_extralibs='d3d11va_extralibs nanosleep_extralibs pthreads_extralibs vaapi_drm_extralibs vaapi_x11_extralibs vdpau_x11_extralibs'
+avutil_suggest='clock_gettime ffnvcodec libm libdrm libmfx opencl user32 vaapi videotoolbox corefoundation corevideo coremedia bcrypt'
+avx=yes
+avx2=yes
+avx2_deps=avx
+avx2_external_deps=avx_external
+avx2_inline_deps=avx_inline
+avx2_suggest='avx2_external avx2_inline'
+avx512=yes
+avx512_deps=avx2
+avx512_external_deps=avx2_external
+avx512_inline_deps=avx2_inline
+avx512_suggest='avx512_external avx512_inline'
+avx_deps=sse42
+avx_external_deps=sse42_external
+avx_inline_deps=sse42_inline
+avx_suggest='avx_external avx_inline'
+azmq_filter=no
+azmq_filter_deps=libzmq
+bandpass_filter=no
+bandreject_filter=no
+bass_filter=no
+bbox_filter=no
+bench_filter=no
+bindir_default='${prefix}/bin'
+bink_decoder_select='blockdsp hpeldsp'
+binkaudio_dct_decoder_select='mdct rdft dct sinewin wma_freqs'
+binkaudio_rdft_decoder_select='mdct rdft sinewin wma_freqs'
+biquad_filter=no
+bitplanenoise_filter=no
+bktr_indev=no
+bktr_indev_deps_any='dev_bktr_ioctl_bt848_h machine_ioctl_bt848_h dev_video_bktr_ioctl_bt848_h dev_ic_bt8xx_h'
+blackdetect_filter=no
+blackframe_filter=no
+blackframe_filter_deps=gpl
+blend_filter=no
+bluray_protocol_deps=libbluray
+bm3d_filter=no
+bm3d_filter_deps=avcodec
+bm3d_filter_select=dct
+bold_color='[1m'
+boxblur_filter=no
+boxblur_filter_deps=gpl
+boxblur_opencl_filter=no
+boxblur_opencl_filter_deps='opencl gpl'
+bs2b_filter=no
+bs2b_filter_deps=libbs2b
+bsfs_if_any='aac_adtstoasc_bsf
+av1_metadata_bsf
+chomp_bsf
+dump_extradata_bsf
+dca_core_bsf
+eac3_core_bsf
+extract_extradata_bsf
+filter_units_bsf
+h264_metadata_bsf
+h264_mp4toannexb_bsf
+h264_redundant_pps_bsf
+hapqa_extract_bsf
+hevc_metadata_bsf
+hevc_mp4toannexb_bsf
+imx_dump_header_bsf
+mjpeg2jpeg_bsf
+mjpega_dump_header_bsf
+mp3_header_decompress_bsf
+mpeg2_metadata_bsf
+mpeg4_unpack_bframes_bsf
+mov2textsub_bsf
+noise_bsf
+null_bsf
+remove_extradata_bsf
+text2movsub_bsf
+trace_headers_bsf
+vp9_metadata_bsf
+vp9_raw_reorder_bsf
+vp9_superframe_bsf
+vp9_superframe_split_bsf'
+bwdif_filter=no
+bzlib=yes
+caca_outdev=no
+caca_outdev_deps=libcaca
+caf_demuxer_select='iso_media riffdec'
+caf_muxer_select=iso_media
+cavs_decoder_select='blockdsp golomb h264chroma idctdsp qpeldsp videodsp'
+cbs_av1_select=cbs
+cbs_h264_select='cbs golomb'
+cbs_h265_select='cbs golomb'
+cbs_jpeg_select=cbs
+cbs_mpeg2_select=cbs
+cbs_vp9_select=cbs
+cc_default=gcc
+cellauto_filter=no
+cflags_filter=echo
+channelmap_filter=no
+channelsplit_filter=no
+chorus_filter=no
+chromahold_filter=no
+chromakey_filter=no
+chromaprint=no
+chromaprint_muxer_deps=chromaprint
+ciescope_filter=no
+clearvideo_decoder_select=idctdsp
+cllc_decoder_select=bswapdsp
+codecview_filter=no
+color_filter=no
+colorbalance_filter=no
+colorchannelmixer_filter=no
+colorkey_filter=no
+colorlevels_filter=no
+colormatrix_filter=no
+colormatrix_filter_deps=gpl
+colorspace_filter=no
+comfortnoise_encoder_select=lpc
+compand_filter=no
+compensationdelay_filter=no
+concat_filter=no
+convolution_filter=no
+convolution_opencl_filter=no
+convolution_opencl_filter_deps=opencl
+convolve_filter=no
+convolve_filter_deps=avcodec
+convolve_filter_select=fft
+cook_decoder_select='audiodsp mdct sinewin'
+copy_filter=no
+coreimage=yes
+coreimage_filter=no
+coreimage_filter_deps='coreimage appkit'
+coreimage_filter_extralibs='-framework OpenGL'
+coreimagesrc_filter=no
+coreimagesrc_filter_deps='coreimage appkit'
+coreimagesrc_filter_extralibs='-framework OpenGL'
+cover_rect_filter=no
+cover_rect_filter_deps='avcodec avformat gpl'
+cpu=cortex-a7
+cpu_init=yes
+cpu_init_extralibs=pthreads_extralibs
+cpunop=yes
+cpunop_deps=i686
+crop_filter=no
+cropdetect_filter=no
+cropdetect_filter_deps=gpl
+cross_compile=yes
+cross_prefix=arm-himix100-linux-
+crossfeed_filter=no
+crystalhd=yes
+crystalhd_deps=libcrystalhd_libcrystalhd_if_h
+crystalizer_filter=no
+cscd_decoder_select=lzo
+cscd_decoder_suggest=zlib
+cuda=yes
+cuda_deps=ffnvcodec
+cuda_sdk=no
+cue_filter=no
+curves_filter=no
+cuvid=yes
+cuvid_deps=ffnvcodec
+cws2fws=yes
+cws2fws_extralibs=zlib_extralibs
+cxx_default=g++
+d3d11va=yes
+d3d11va_deps='dxva_h ID3D11VideoDecoder ID3D11VideoContext'
+dash_demuxer_deps=libxml2
+dash_muxer_select=mp4_muxer
+datadir_default='${prefix}/share/ffmpeg'
+datascope_filter=no
+dca_decoder_select=mdct
+dcbzl=yes
+dcbzl_deps=ppc
+dcshift_filter=no
+dct_select=rdft
+dctdnoiz_filter=no
+dds_decoder_select=texturedsp
+deband_filter=no
+deblock_filter=no
+debug=no
+decimate_filter=no
+decklink=no
+decklink_deps_any='libdl LoadLibrary'
+decklink_indev=no
+decklink_indev_deps='decklink threads'
+decklink_indev_extralibs=-lstdc++
+decklink_outdev=no
+decklink_outdev_deps='decklink threads'
+decklink_outdev_extralibs=-lstdc++
+decklink_outdev_suggest=libklvanc
+decode_audio_example=yes
+decode_audio_example_deps='avcodec avutil'
+decode_video_example=yes
+decode_video_example_deps='avcodec avutil'
+decoders_if_any='aasc_decoder
+aic_decoder
+alias_pix_decoder
+amv_decoder
+anm_decoder
+ansi_decoder
+apng_decoder
+asv1_decoder
+asv2_decoder
+aura_decoder
+aura2_decoder
+avrp_decoder
+avrn_decoder
+avs_decoder
+avui_decoder
+ayuv_decoder
+bethsoftvid_decoder
+bfi_decoder
+bink_decoder
+bitpacked_decoder
+bmp_decoder
+bmv_video_decoder
+brender_pix_decoder
+c93_decoder
+cavs_decoder
+cdgraphics_decoder
+cdxl_decoder
+cfhd_decoder
+cinepak_decoder
+clearvideo_decoder
+cljr_decoder
+cllc_decoder
+comfortnoise_decoder
+cpia_decoder
+cscd_decoder
+cyuv_decoder
+dds_decoder
+dfa_decoder
+dirac_decoder
+dnxhd_decoder
+dpx_decoder
+dsicinvideo_decoder
+dvaudio_decoder
+dvvideo_decoder
+dxa_decoder
+dxtory_decoder
+dxv_decoder
+eacmv_decoder
+eamad_decoder
+eatgq_decoder
+eatgv_decoder
+eatqi_decoder
+eightbps_decoder
+eightsvx_exp_decoder
+eightsvx_fib_decoder
+escape124_decoder
+escape130_decoder
+exr_decoder
+ffv1_decoder
+ffvhuff_decoder
+fic_decoder
+fits_decoder
+flashsv_decoder
+flashsv2_decoder
+flic_decoder
+flv_decoder
+fmvc_decoder
+fourxm_decoder
+fraps_decoder
+frwu_decoder
+g2m_decoder
+gdv_decoder
+gif_decoder
+h261_decoder
+h263_decoder
+h263i_decoder
+h263p_decoder
+h263_v4l2m2m_decoder
+h264_decoder
+h264_crystalhd_decoder
+h264_v4l2m2m_decoder
+h264_mediacodec_decoder
+h264_mmal_decoder
+h264_qsv_decoder
+h264_rkmpp_decoder
+hap_decoder
+hevc_decoder
+hevc_qsv_decoder
+hevc_rkmpp_decoder
+hevc_v4l2m2m_decoder
+hnm4_video_decoder
+hq_hqa_decoder
+hqx_decoder
+huffyuv_decoder
+idcin_decoder
+iff_ilbm_decoder
+imm4_decoder
+indeo2_decoder
+indeo3_decoder
+indeo4_decoder
+indeo5_decoder
+interplay_video_decoder
+jpeg2000_decoder
+jpegls_decoder
+jv_decoder
+kgv1_decoder
+kmvc_decoder
+lagarith_decoder
+loco_decoder
+m101_decoder
+magicyuv_decoder
+mdec_decoder
+mimic_decoder
+mjpeg_decoder
+mjpegb_decoder
+mmvideo_decoder
+motionpixels_decoder
+mpeg1video_decoder
+mpeg2video_decoder
+mpeg4_decoder
+mpeg4_crystalhd_decoder
+mpeg4_v4l2m2m_decoder
+mpeg4_mmal_decoder
+mpegvideo_decoder
+mpeg1_v4l2m2m_decoder
+mpeg2_mmal_decoder
+mpeg2_crystalhd_decoder
+mpeg2_v4l2m2m_decoder
+mpeg2_qsv_decoder
+mpeg2_mediacodec_decoder
+msa1_decoder
+mscc_decoder
+msmpeg4v1_decoder
+msmpeg4v2_decoder
+msmpeg4v3_decoder
+msmpeg4_crystalhd_decoder
+msrle_decoder
+mss1_decoder
+mss2_decoder
+msvideo1_decoder
+mszh_decoder
+mts2_decoder
+mvc1_decoder
+mvc2_decoder
+mwsc_decoder
+mxpeg_decoder
+nuv_decoder
+paf_video_decoder
+pam_decoder
+pbm_decoder
+pcx_decoder
+pgm_decoder
+pgmyuv_decoder
+pictor_decoder
+pixlet_decoder
+png_decoder
+ppm_decoder
+prores_decoder
+prosumer_decoder
+psd_decoder
+ptx_decoder
+qdraw_decoder
+qpeg_decoder
+qtrle_decoder
+r10k_decoder
+r210_decoder
+rasc_decoder
+rawvideo_decoder
+rl2_decoder
+roq_decoder
+rpza_decoder
+rscc_decoder
+rv10_decoder
+rv20_decoder
+rv30_decoder
+rv40_decoder
+s302m_decoder
+sanm_decoder
+scpr_decoder
+screenpresso_decoder
+sdx2_dpcm_decoder
+sgi_decoder
+sgirle_decoder
+sheervideo_decoder
+smacker_decoder
+smc_decoder
+smvjpeg_decoder
+snow_decoder
+sp5x_decoder
+speedhq_decoder
+srgc_decoder
+sunrast_decoder
+svq1_decoder
+svq3_decoder
+targa_decoder
+targa_y216_decoder
+tdsc_decoder
+theora_decoder
+thp_decoder
+tiertexseqvideo_decoder
+tiff_decoder
+tmv_decoder
+truemotion1_decoder
+truemotion2_decoder
+truemotion2rt_decoder
+tscc_decoder
+tscc2_decoder
+txd_decoder
+ulti_decoder
+utvideo_decoder
+v210_decoder
+v210x_decoder
+v308_decoder
+v408_decoder
+v410_decoder
+vb_decoder
+vble_decoder
+vc1_decoder
+vc1_crystalhd_decoder
+vc1image_decoder
+vc1_mmal_decoder
+vc1_qsv_decoder
+vc1_v4l2m2m_decoder
+vcr1_decoder
+vmdvideo_decoder
+vmnc_decoder
+vp3_decoder
+vp5_decoder
+vp6_decoder
+vp6a_decoder
+vp6f_decoder
+vp7_decoder
+vp8_decoder
+vp8_rkmpp_decoder
+vp8_v4l2m2m_decoder
+vp9_decoder
+vp9_rkmpp_decoder
+vp9_v4l2m2m_decoder
+vqa_decoder
+webp_decoder
+wcmv_decoder
+wrapped_avframe_decoder
+wmv1_decoder
+wmv2_decoder
+wmv3_decoder
+wmv3_crystalhd_decoder
+wmv3image_decoder
+wnv1_decoder
+xan_wc3_decoder
+xan_wc4_decoder
+xbm_decoder
+xface_decoder
+xl_decoder
+xpm_decoder
+xwd_decoder
+y41p_decoder
+ylc_decoder
+yop_decoder
+yuv4_decoder
+zero12v_decoder
+zerocodec_decoder
+zlib_decoder
+zmbv_decoder
+aac_decoder
+aac_fixed_decoder
+aac_latm_decoder
+ac3_decoder
+ac3_fixed_decoder
+alac_decoder
+als_decoder
+amrnb_decoder
+amrwb_decoder
+ape_decoder
+aptx_decoder
+aptx_hd_decoder
+atrac1_decoder
+atrac3_decoder
+atrac3al_decoder
+atrac3p_decoder
+atrac3pal_decoder
+atrac9_decoder
+binkaudio_dct_decoder
+binkaudio_rdft_decoder
+bmv_audio_decoder
+cook_decoder
+dca_decoder
+dolby_e_decoder
+dsd_lsbf_decoder
+dsd_msbf_decoder
+dsd_lsbf_planar_decoder
+dsd_msbf_planar_decoder
+dsicinaudio_decoder
+dss_sp_decoder
+dst_decoder
+eac3_decoder
+evrc_decoder
+ffwavesynth_decoder
+flac_decoder
+g723_1_decoder
+g729_decoder
+gsm_decoder
+gsm_ms_decoder
+iac_decoder
+ilbc_decoder
+imc_decoder
+interplay_acm_decoder
+mace3_decoder
+mace6_decoder
+metasound_decoder
+mlp_decoder
+mp1_decoder
+mp1float_decoder
+mp2_decoder
+mp2float_decoder
+mp3float_decoder
+mp3_decoder
+mp3adufloat_decoder
+mp3adu_decoder
+mp3on4float_decoder
+mp3on4_decoder
+mpc7_decoder
+mpc8_decoder
+nellymoser_decoder
+on2avc_decoder
+opus_decoder
+paf_audio_decoder
+qcelp_decoder
+qdm2_decoder
+qdmc_decoder
+ra_144_decoder
+ra_288_decoder
+ralf_decoder
+sbc_decoder
+shorten_decoder
+sipr_decoder
+smackaud_decoder
+sonic_decoder
+tak_decoder
+truehd_decoder
+truespeech_decoder
+tta_decoder
+twinvq_decoder
+vmdaudio_decoder
+vorbis_decoder
+wavpack_decoder
+wmalossless_decoder
+wmapro_decoder
+wmav1_decoder
+wmav2_decoder
+wmavoice_decoder
+ws_snd1_decoder
+xma1_decoder
+xma2_decoder
+pcm_alaw_decoder
+pcm_bluray_decoder
+pcm_dvd_decoder
+pcm_f16le_decoder
+pcm_f24le_decoder
+pcm_f32be_decoder
+pcm_f32le_decoder
+pcm_f64be_decoder
+pcm_f64le_decoder
+pcm_lxf_decoder
+pcm_mulaw_decoder
+pcm_s8_decoder
+pcm_s8_planar_decoder
+pcm_s16be_decoder
+pcm_s16be_planar_decoder
+pcm_s16le_decoder
+pcm_s16le_planar_decoder
+pcm_s24be_decoder
+pcm_s24daud_decoder
+pcm_s24le_decoder
+pcm_s24le_planar_decoder
+pcm_s32be_decoder
+pcm_s32le_decoder
+pcm_s32le_planar_decoder
+pcm_s64be_decoder
+pcm_s64le_decoder
+pcm_u8_decoder
+pcm_u16be_decoder
+pcm_u16le_decoder
+pcm_u24be_decoder
+pcm_u24le_decoder
+pcm_u32be_decoder
+pcm_u32le_decoder
+pcm_vidc_decoder
+pcm_zork_decoder
+gremlin_dpcm_decoder
+interplay_dpcm_decoder
+roq_dpcm_decoder
+sol_dpcm_decoder
+xan_dpcm_decoder
+adpcm_4xm_decoder
+adpcm_adx_decoder
+adpcm_afc_decoder
+adpcm_aica_decoder
+adpcm_ct_decoder
+adpcm_dtk_decoder
+adpcm_ea_decoder
+adpcm_ea_maxis_xa_decoder
+adpcm_ea_r1_decoder
+adpcm_ea_r2_decoder
+adpcm_ea_r3_decoder
+adpcm_ea_xas_decoder
+adpcm_g722_decoder
+adpcm_g726_decoder
+adpcm_g726le_decoder
+adpcm_ima_amv_decoder
+adpcm_ima_apc_decoder
+adpcm_ima_dat4_decoder
+adpcm_ima_dk3_decoder
+adpcm_ima_dk4_decoder
+adpcm_ima_ea_eacs_decoder
+adpcm_ima_ea_sead_decoder
+adpcm_ima_iss_decoder
+adpcm_ima_oki_decoder
+adpcm_ima_qt_decoder
+adpcm_ima_rad_decoder
+adpcm_ima_smjpeg_decoder
+adpcm_ima_wav_decoder
+adpcm_ima_ws_decoder
+adpcm_ms_decoder
+adpcm_mtaf_decoder
+adpcm_psx_decoder
+adpcm_sbpro_2_decoder
+adpcm_sbpro_3_decoder
+adpcm_sbpro_4_decoder
+adpcm_swf_decoder
+adpcm_thp_decoder
+adpcm_thp_le_decoder
+adpcm_vima_decoder
+adpcm_xa_decoder
+adpcm_yamaha_decoder
+ssa_decoder
+ass_decoder
+ccaption_decoder
+dvbsub_decoder
+dvdsub_decoder
+jacosub_decoder
+microdvd_decoder
+movtext_decoder
+mpl2_decoder
+pgssub_decoder
+pjs_decoder
+realtext_decoder
+sami_decoder
+srt_decoder
+stl_decoder
+subrip_decoder
+subviewer_decoder
+subviewer1_decoder
+text_decoder
+vplayer_decoder
+webvtt_decoder
+xsub_decoder
+aac_at_decoder
+ac3_at_decoder
+adpcm_ima_qt_at_decoder
+alac_at_decoder
+amr_nb_at_decoder
+eac3_at_decoder
+gsm_ms_at_decoder
+ilbc_at_decoder
+mp1_at_decoder
+mp2_at_decoder
+mp3_at_decoder
+pcm_alaw_at_decoder
+pcm_mulaw_at_decoder
+qdmc_at_decoder
+qdm2_at_decoder
+libaom_av1_decoder
+libcelt_decoder
+libcodec2_decoder
+libdavs2_decoder
+libfdk_aac_decoder
+libgsm_decoder
+libgsm_ms_decoder
+libilbc_decoder
+libopencore_amrnb_decoder
+libopencore_amrwb_decoder
+libopenjpeg_decoder
+libopus_decoder
+librsvg_decoder
+libspeex_decoder
+libvorbis_decoder
+libvpx_vp8_decoder
+libvpx_vp9_decoder
+libzvbi_teletext_decoder
+bintext_decoder
+xbin_decoder
+idf_decoder
+libopenh264_decoder
+h264_cuvid_decoder
+hevc_cuvid_decoder
+hevc_mediacodec_decoder
+mjpeg_cuvid_decoder
+mpeg1_cuvid_decoder
+mpeg2_cuvid_decoder
+mpeg4_cuvid_decoder
+mpeg4_mediacodec_decoder
+vc1_cuvid_decoder
+vp8_cuvid_decoder
+vp8_mediacodec_decoder
+vp8_qsv_decoder
+vp9_cuvid_decoder
+vp9_mediacodec_decoder'
+deconvolve_filter=no
+deconvolve_filter_deps=avcodec
+deconvolve_filter_select=fft
+deflate_filter=no
+deflicker_filter=no
+deinterlace_qsv_filter=no
+deinterlace_qsv_filter_deps=libmfx
+deinterlace_vaapi_filter=no
+deinterlace_vaapi_filter_deps=vaapi
+dejudder_filter=no
+delogo_filter=no
+delogo_filter_deps=gpl
+demuxers_if_any='aa_demuxer
+aac_demuxer
+ac3_demuxer
+acm_demuxer
+act_demuxer
+adf_demuxer
+adp_demuxer
+ads_demuxer
+adx_demuxer
+aea_demuxer
+afc_demuxer
+aiff_demuxer
+aix_demuxer
+amr_demuxer
+amrnb_demuxer
+amrwb_demuxer
+anm_demuxer
+apc_demuxer
+ape_demuxer
+apng_demuxer
+aptx_demuxer
+aptx_hd_demuxer
+aqtitle_demuxer
+asf_demuxer
+asf_o_demuxer
+ass_demuxer
+ast_demuxer
+au_demuxer
+avi_demuxer
+avisynth_demuxer
+avr_demuxer
+avs_demuxer
+avs2_demuxer
+bethsoftvid_demuxer
+bfi_demuxer
+bintext_demuxer
+bink_demuxer
+bit_demuxer
+bmv_demuxer
+bfstm_demuxer
+brstm_demuxer
+boa_demuxer
+c93_demuxer
+caf_demuxer
+cavsvideo_demuxer
+cdg_demuxer
+cdxl_demuxer
+cine_demuxer
+codec2_demuxer
+codec2raw_demuxer
+concat_demuxer
+dash_demuxer
+data_demuxer
+daud_demuxer
+dcstr_demuxer
+dfa_demuxer
+dirac_demuxer
+dnxhd_demuxer
+dsf_demuxer
+dsicin_demuxer
+dss_demuxer
+dts_demuxer
+dtshd_demuxer
+dv_demuxer
+dvbsub_demuxer
+dvbtxt_demuxer
+dxa_demuxer
+ea_demuxer
+ea_cdata_demuxer
+eac3_demuxer
+epaf_demuxer
+ffmetadata_demuxer
+filmstrip_demuxer
+fits_demuxer
+flac_demuxer
+flic_demuxer
+flv_demuxer
+live_flv_demuxer
+fourxm_demuxer
+frm_demuxer
+fsb_demuxer
+g722_demuxer
+g723_1_demuxer
+g726_demuxer
+g726le_demuxer
+g729_demuxer
+gdv_demuxer
+genh_demuxer
+gif_demuxer
+gsm_demuxer
+gxf_demuxer
+h261_demuxer
+h263_demuxer
+h264_demuxer
+hevc_demuxer
+hls_demuxer
+hnm_demuxer
+ico_demuxer
+idcin_demuxer
+idf_demuxer
+iff_demuxer
+ilbc_demuxer
+image2_demuxer
+image2pipe_demuxer
+image2_alias_pix_demuxer
+image2_brender_pix_demuxer
+ingenient_demuxer
+ipmovie_demuxer
+ircam_demuxer
+iss_demuxer
+iv8_demuxer
+ivf_demuxer
+ivr_demuxer
+jacosub_demuxer
+jv_demuxer
+lmlm4_demuxer
+loas_demuxer
+lrc_demuxer
+lvf_demuxer
+lxf_demuxer
+m4v_demuxer
+matroska_demuxer
+mgsts_demuxer
+microdvd_demuxer
+mjpeg_demuxer
+mjpeg_2000_demuxer
+mlp_demuxer
+mlv_demuxer
+mm_demuxer
+mmf_demuxer
+mov_demuxer
+mp3_demuxer
+mpc_demuxer
+mpc8_demuxer
+mpegps_demuxer
+mpegts_demuxer
+mpegtsraw_demuxer
+mpegvideo_demuxer
+mpjpeg_demuxer
+mpl2_demuxer
+mpsub_demuxer
+msf_demuxer
+msnwc_tcp_demuxer
+mtaf_demuxer
+mtv_demuxer
+musx_demuxer
+mv_demuxer
+mvi_demuxer
+mxf_demuxer
+mxg_demuxer
+nc_demuxer
+nistsphere_demuxer
+nsp_demuxer
+nsv_demuxer
+nut_demuxer
+nuv_demuxer
+ogg_demuxer
+oma_demuxer
+paf_demuxer
+pcm_alaw_demuxer
+pcm_mulaw_demuxer
+pcm_vidc_demuxer
+pcm_f64be_demuxer
+pcm_f64le_demuxer
+pcm_f32be_demuxer
+pcm_f32le_demuxer
+pcm_s32be_demuxer
+pcm_s32le_demuxer
+pcm_s24be_demuxer
+pcm_s24le_demuxer
+pcm_s16be_demuxer
+pcm_s16le_demuxer
+pcm_s8_demuxer
+pcm_u32be_demuxer
+pcm_u32le_demuxer
+pcm_u24be_demuxer
+pcm_u24le_demuxer
+pcm_u16be_demuxer
+pcm_u16le_demuxer
+pcm_u8_demuxer
+pjs_demuxer
+pmp_demuxer
+pva_demuxer
+pvf_demuxer
+qcp_demuxer
+r3d_demuxer
+rawvideo_demuxer
+realtext_demuxer
+redspark_demuxer
+rl2_demuxer
+rm_demuxer
+roq_demuxer
+rpl_demuxer
+rsd_demuxer
+rso_demuxer
+rtp_demuxer
+rtsp_demuxer
+s337m_demuxer
+sami_demuxer
+sap_demuxer
+sbc_demuxer
+sbg_demuxer
+scc_demuxer
+sdp_demuxer
+sdr2_demuxer
+sds_demuxer
+sdx_demuxer
+segafilm_demuxer
+ser_demuxer
+shorten_demuxer
+siff_demuxer
+sln_demuxer
+smacker_demuxer
+smjpeg_demuxer
+smush_demuxer
+sol_demuxer
+sox_demuxer
+spdif_demuxer
+srt_demuxer
+str_demuxer
+stl_demuxer
+subviewer1_demuxer
+subviewer_demuxer
+sup_demuxer
+svag_demuxer
+swf_demuxer
+tak_demuxer
+tedcaptions_demuxer
+thp_demuxer
+threedostr_demuxer
+tiertexseq_demuxer
+tmv_demuxer
+truehd_demuxer
+tta_demuxer
+txd_demuxer
+tty_demuxer
+ty_demuxer
+v210_demuxer
+v210x_demuxer
+vag_demuxer
+vc1_demuxer
+vc1t_demuxer
+vivo_demuxer
+vmd_demuxer
+vobsub_demuxer
+voc_demuxer
+vpk_demuxer
+vplayer_demuxer
+vqf_demuxer
+w64_demuxer
+wav_demuxer
+wc3_demuxer
+webm_dash_manifest_demuxer
+webvtt_demuxer
+wsaud_demuxer
+wsd_demuxer
+wsvqa_demuxer
+wtv_demuxer
+wve_demuxer
+wv_demuxer
+xa_demuxer
+xbin_demuxer
+xmv_demuxer
+xvag_demuxer
+xwma_demuxer
+yop_demuxer
+yuv4mpegpipe_demuxer
+image_bmp_pipe_demuxer
+image_dds_pipe_demuxer
+image_dpx_pipe_demuxer
+image_exr_pipe_demuxer
+image_j2k_pipe_demuxer
+image_jpeg_pipe_demuxer
+image_jpegls_pipe_demuxer
+image_pam_pipe_demuxer
+image_pbm_pipe_demuxer
+image_pcx_pipe_demuxer
+image_pgmyuv_pipe_demuxer
+image_pgm_pipe_demuxer
+image_pictor_pipe_demuxer
+image_png_pipe_demuxer
+image_ppm_pipe_demuxer
+image_psd_pipe_demuxer
+image_qdraw_pipe_demuxer
+image_sgi_pipe_demuxer
+image_svg_pipe_demuxer
+image_sunrast_pipe_demuxer
+image_tiff_pipe_demuxer
+image_webp_pipe_demuxer
+image_xpm_pipe_demuxer
+image_xwd_pipe_demuxer
+libgme_demuxer
+libmodplug_demuxer
+libopenmpt_demuxer
+vapoursynth_demuxer'
+demuxing_decoding_example=yes
+demuxing_decoding_example_deps='avcodec avformat avutil'
+denoise_vaapi_filter=no
+denoise_vaapi_filter_deps=vaapi
+dep=avx
+deshake_filter=no
+deshake_filter_select=pixelutils
+despill_filter=no
+detelecine_filter=no
+dilation_filter=no
+dilation_opencl_filter=no
+dilation_opencl_filter_deps=opencl
+dirac_decoder_select='dirac_parse dwt golomb videodsp mpegvideoenc'
+dirac_demuxer_select=dirac_parser
+dirac_parse_select=golomb
+displace_filter=no
+dnn_suggest=libtensorflow
+dnxhd_decoder_select='blockdsp idctdsp'
+dnxhd_encoder_select='blockdsp fdctdsp idctdsp mpegvideoenc pixblockdsp'
+doc=no
+doc_deps_any='manpages htmlpages podpages txtpages'
+docdir_default='${prefix}/share/doc/ffmpeg'
+dolby_e_decoder_select=mdct
+doubleweave_filter=no
+doxygen_default=doxygen
+drawbox_filter=no
+drawgraph_filter=no
+drawgrid_filter=no
+drawtext_filter=no
+drawtext_filter_deps=libfreetype
+drawtext_filter_suggest='libfontconfig libfribidi'
+drmeter_filter=no
+dshow_indev=no
+dshow_indev_deps=IBaseFilter
+dshow_indev_extralibs='-lpsapi -lole32 -lstrmiids -luuid -loleaut32 -lshlwapi'
+dts_demuxer_select=dca_parser
+dtshd_demuxer_select=dca_parser
+dv_demuxer_select=dvprofile
+dv_muxer_select=dvprofile
+dvvideo_decoder_select='dvprofile idctdsp'
+dvvideo_encoder_select='dvprofile fdctdsp me_cmp pixblockdsp'
+dxa_decoder_deps=zlib
+dxa_demuxer_select=riffdec
+dxv_decoder_select='lzf texturedsp'
+dxva2=yes
+dxva2_deps='dxva2api_h DXVA2_ConfigPictureDecode ole32 user32'
+dynaudnorm_filter=no
+eac3_at_decoder_deps=audiotoolbox
+eac3_at_decoder_select=ac3_parser
+eac3_core_bsf_select=ac3_parser
+eac3_decoder_select=ac3_decoder
+eac3_demuxer_select=ac3_parser
+eac3_encoder_select=ac3_encoder
+eamad_decoder_select='aandcttables blockdsp bswapdsp idctdsp mpegvideo'
+earwax_filter=no
+eatgq_decoder_select=aandcttables
+eatqi_decoder_select='aandcttables blockdsp bswapdsp idctdsp'
+ebur128_filter=no
+edgedetect_filter=no
+elbg_filter=no
+elbg_filter_deps=avcodec
+encode_audio_example=yes
+encode_audio_example_deps='avcodec avutil'
+encode_video_example=yes
+encode_video_example_deps='avcodec avutil'
+encoders_if_any='a64multi_encoder
+a64multi5_encoder
+alias_pix_encoder
+amv_encoder
+apng_encoder
+asv1_encoder
+asv2_encoder
+avrp_encoder
+avui_encoder
+ayuv_encoder
+bmp_encoder
+cinepak_encoder
+cljr_encoder
+comfortnoise_encoder
+dnxhd_encoder
+dpx_encoder
+dvvideo_encoder
+ffv1_encoder
+ffvhuff_encoder
+fits_encoder
+flashsv_encoder
+flashsv2_encoder
+flv_encoder
+gif_encoder
+h261_encoder
+h263_encoder
+h263p_encoder
+hap_encoder
+huffyuv_encoder
+jpeg2000_encoder
+jpegls_encoder
+ljpeg_encoder
+magicyuv_encoder
+mjpeg_encoder
+mpeg1video_encoder
+mpeg2video_encoder
+mpeg4_encoder
+msmpeg4v2_encoder
+msmpeg4v3_encoder
+msvideo1_encoder
+pam_encoder
+pbm_encoder
+pcx_encoder
+pgm_encoder
+pgmyuv_encoder
+png_encoder
+ppm_encoder
+prores_encoder
+prores_aw_encoder
+prores_ks_encoder
+qtrle_encoder
+r10k_encoder
+r210_encoder
+rawvideo_encoder
+roq_encoder
+rv10_encoder
+rv20_encoder
+s302m_encoder
+sgi_encoder
+snow_encoder
+sunrast_encoder
+svq1_encoder
+targa_encoder
+tiff_encoder
+utvideo_encoder
+v210_encoder
+v308_encoder
+v408_encoder
+v410_encoder
+vc2_encoder
+wrapped_avframe_encoder
+wmv1_encoder
+wmv2_encoder
+xbm_encoder
+xface_encoder
+xwd_encoder
+y41p_encoder
+yuv4_encoder
+zlib_encoder
+zmbv_encoder
+aac_encoder
+ac3_encoder
+ac3_fixed_encoder
+alac_encoder
+aptx_encoder
+aptx_hd_encoder
+dca_encoder
+eac3_encoder
+flac_encoder
+g723_1_encoder
+mlp_encoder
+mp2_encoder
+mp2fixed_encoder
+nellymoser_encoder
+opus_encoder
+ra_144_encoder
+sbc_encoder
+sonic_encoder
+sonic_ls_encoder
+truehd_encoder
+tta_encoder
+vorbis_encoder
+wavpack_encoder
+wmav1_encoder
+wmav2_encoder
+pcm_alaw_encoder
+pcm_f32be_encoder
+pcm_f32le_encoder
+pcm_f64be_encoder
+pcm_f64le_encoder
+pcm_mulaw_encoder
+pcm_s8_encoder
+pcm_s8_planar_encoder
+pcm_s16be_encoder
+pcm_s16be_planar_encoder
+pcm_s16le_encoder
+pcm_s16le_planar_encoder
+pcm_s24be_encoder
+pcm_s24daud_encoder
+pcm_s24le_encoder
+pcm_s24le_planar_encoder
+pcm_s32be_encoder
+pcm_s32le_encoder
+pcm_s32le_planar_encoder
+pcm_s64be_encoder
+pcm_s64le_encoder
+pcm_u8_encoder
+pcm_u16be_encoder
+pcm_u16le_encoder
+pcm_u24be_encoder
+pcm_u24le_encoder
+pcm_u32be_encoder
+pcm_u32le_encoder
+pcm_vidc_encoder
+roq_dpcm_encoder
+adpcm_adx_encoder
+adpcm_g722_encoder
+adpcm_g726_encoder
+adpcm_g726le_encoder
+adpcm_ima_qt_encoder
+adpcm_ima_wav_encoder
+adpcm_ms_encoder
+adpcm_swf_encoder
+adpcm_yamaha_encoder
+ssa_encoder
+ass_encoder
+dvbsub_encoder
+dvdsub_encoder
+movtext_encoder
+srt_encoder
+subrip_encoder
+text_encoder
+webvtt_encoder
+xsub_encoder
+aac_at_encoder
+alac_at_encoder
+ilbc_at_encoder
+pcm_alaw_at_encoder
+pcm_mulaw_at_encoder
+libaom_av1_encoder
+libcodec2_encoder
+libfdk_aac_encoder
+libgsm_encoder
+libgsm_ms_encoder
+libilbc_encoder
+libmp3lame_encoder
+libopencore_amrnb_encoder
+libopenjpeg_encoder
+libopus_encoder
+libshine_encoder
+libspeex_encoder
+libtheora_encoder
+libtwolame_encoder
+libvo_amrwbenc_encoder
+libvorbis_encoder
+libvpx_vp8_encoder
+libvpx_vp9_encoder
+libwavpack_encoder
+libwebp_anim_encoder
+libwebp_encoder
+libx262_encoder
+libx264_encoder
+libx264rgb_encoder
+libx265_encoder
+libxavs_encoder
+libxavs2_encoder
+libxvid_encoder
+h263_v4l2m2m_encoder
+libopenh264_encoder
+h264_amf_encoder
+h264_nvenc_encoder
+h264_omx_encoder
+h264_qsv_encoder
+h264_v4l2m2m_encoder
+h264_vaapi_encoder
+h264_videotoolbox_encoder
+nvenc_encoder
+nvenc_h264_encoder
+nvenc_hevc_encoder
+hevc_amf_encoder
+hevc_nvenc_encoder
+hevc_qsv_encoder
+hevc_v4l2m2m_encoder
+hevc_vaapi_encoder
+hevc_videotoolbox_encoder
+libkvazaar_encoder
+mjpeg_qsv_encoder
+mjpeg_vaapi_encoder
+mpeg2_qsv_encoder
+mpeg2_vaapi_encoder
+mpeg4_v4l2m2m_encoder
+vp8_v4l2m2m_encoder
+vp8_vaapi_encoder
+vp9_vaapi_encoder'
+entropy_filter=no
+eq_filter=no
+eq_filter_deps=gpl
+equalizer_filter=no
+erosion_filter=no
+erosion_opencl_filter=no
+erosion_opencl_filter_deps=opencl
+error_color='[31m'
+error_resilience_select=me_cmp
+exr_decoder_deps=zlib
+ext=xop
+extract_mvs_example=yes
+extract_mvs_example_deps='avcodec avformat avutil'
+extractplanes_filter=no
+extrastereo_filter=no
+f4v_muxer_select=mov_muxer
+faan=yes
+faandct=yes
+faandct_deps=faan
+faandct_select=fdctdsp
+faanidct=yes
+faanidct_deps=faan
+faanidct_select=idctdsp
+fade_filter=no
+fast_64bit_if_any='aarch64 alpha ia64 mips64 parisc64 ppc64 sparc64 x86_64'
+fast_clz_if_any='aarch64 alpha avr32 mips ppc x86'
+fast_unaligned_if_any='aarch64 ppc x86'
+fbdev_indev=no
+fbdev_indev_deps=linux_fb_h
+fbdev_outdev=no
+fbdev_outdev_deps=linux_fb_h
+ffmpeg=no
+ffmpeg_deps='avcodec avfilter avformat'
+ffmpeg_select='aformat_filter anull_filter atrim_filter format_filter
+               null_filter
+               trim_filter'
+ffmpeg_suggest='ole32 psapi shell32'
+ffnvcodec=yes
+ffnvcodec_deps_any='libdl LoadLibrary'
+ffplay=no
+ffplay_deps='avcodec avformat swscale swresample sdl2'
+ffplay_select='rdft crop_filter transpose_filter hflip_filter vflip_filter rotate_filter'
+ffplay_suggest=shell32
+ffprobe=no
+ffprobe_deps='avcodec avformat'
+ffprobe_suggest=shell32
+ffrtmpcrypt_protocol_conflict=librtmp_protocol
+ffrtmpcrypt_protocol_deps_any='gcrypt gmp openssl mbedtls'
+ffrtmpcrypt_protocol_select=tcp_protocol
+ffrtmphttp_protocol_conflict=librtmp_protocol
+ffrtmphttp_protocol_select=http_protocol
+fftdnoiz_filter=no
+fftdnoiz_filter_deps=avcodec
+fftdnoiz_filter_select=fft
+fftfilt_filter=no
+fftfilt_filter_deps=avcodec
+fftfilt_filter_select=rdft
+ffv1_decoder_select=rangecoder
+ffv1_encoder_select=rangecoder
+ffvhuff_decoder_select=huffyuv_decoder
+ffvhuff_encoder_select=huffyuv_encoder
+fic_decoder_select=golomb
+field_filter=no
+fieldhint_filter=no
+fieldmatch_filter=no
+fieldorder_filter=no
+fifo_filter=no
+fifo_muxer_deps=threads
+file_protocol=yes
+fillborders_filter=no
+filter_audio_example=yes
+filter_audio_example_deps='avfilter avutil'
+filter_units_bsf_select=cbs
+filtering_audio_example=yes
+filtering_audio_example_deps='avfilter avcodec avformat avutil'
+filtering_video_example=yes
+filtering_video_example_deps='avfilter avcodec avformat avutil'
+filters_if_any='abench_filter
+acompressor_filter
+acontrast_filter
+acopy_filter
+acue_filter
+acrossfade_filter
+acrossover_filter
+acrusher_filter
+adeclick_filter
+adeclip_filter
+adelay_filter
+aderivative_filter
+aecho_filter
+aemphasis_filter
+aeval_filter
+afade_filter
+afftdn_filter
+afftfilt_filter
+afir_filter
+aformat_filter
+agate_filter
+aiir_filter
+aintegral_filter
+ainterleave_filter
+alimiter_filter
+allpass_filter
+aloop_filter
+amerge_filter
+ametadata_filter
+amix_filter
+amultiply_filter
+anequalizer_filter
+anull_filter
+apad_filter
+aperms_filter
+aphaser_filter
+apulsator_filter
+arealtime_filter
+aresample_filter
+areverse_filter
+aselect_filter
+asendcmd_filter
+asetnsamples_filter
+asetpts_filter
+asetrate_filter
+asettb_filter
+ashowinfo_filter
+asidedata_filter
+asplit_filter
+astats_filter
+astreamselect_filter
+atempo_filter
+atrim_filter
+azmq_filter
+bandpass_filter
+bandreject_filter
+bass_filter
+biquad_filter
+bs2b_filter
+channelmap_filter
+channelsplit_filter
+chorus_filter
+compand_filter
+compensationdelay_filter
+crossfeed_filter
+crystalizer_filter
+dcshift_filter
+drmeter_filter
+dynaudnorm_filter
+earwax_filter
+ebur128_filter
+equalizer_filter
+extrastereo_filter
+firequalizer_filter
+flanger_filter
+haas_filter
+hdcd_filter
+headphone_filter
+highpass_filter
+highshelf_filter
+join_filter
+ladspa_filter
+loudnorm_filter
+lowpass_filter
+lowshelf_filter
+lv2_filter
+mcompand_filter
+pan_filter
+replaygain_filter
+resample_filter
+rubberband_filter
+sidechaincompress_filter
+sidechaingate_filter
+silencedetect_filter
+silenceremove_filter
+sofalizer_filter
+stereotools_filter
+stereowiden_filter
+superequalizer_filter
+surround_filter
+treble_filter
+tremolo_filter
+vibrato_filter
+volume_filter
+volumedetect_filter
+aevalsrc_filter
+anoisesrc_filter
+anullsrc_filter
+flite_filter
+hilbert_filter
+sinc_filter
+sine_filter
+anullsink_filter
+alphaextract_filter
+alphamerge_filter
+amplify_filter
+ass_filter
+atadenoise_filter
+avgblur_filter
+avgblur_opencl_filter
+bbox_filter
+bench_filter
+bitplanenoise_filter
+blackdetect_filter
+blackframe_filter
+blend_filter
+bm3d_filter
+boxblur_filter
+boxblur_opencl_filter
+bwdif_filter
+chromahold_filter
+chromakey_filter
+ciescope_filter
+codecview_filter
+colorbalance_filter
+colorchannelmixer_filter
+colorkey_filter
+colorlevels_filter
+colormatrix_filter
+colorspace_filter
+convolution_filter
+convolution_opencl_filter
+convolve_filter
+copy_filter
+coreimage_filter
+cover_rect_filter
+crop_filter
+cropdetect_filter
+cue_filter
+curves_filter
+datascope_filter
+dctdnoiz_filter
+deband_filter
+deblock_filter
+decimate_filter
+deconvolve_filter
+deflate_filter
+deflicker_filter
+deinterlace_qsv_filter
+deinterlace_vaapi_filter
+dejudder_filter
+delogo_filter
+denoise_vaapi_filter
+deshake_filter
+despill_filter
+detelecine_filter
+dilation_filter
+dilation_opencl_filter
+displace_filter
+doubleweave_filter
+drawbox_filter
+drawgraph_filter
+drawgrid_filter
+drawtext_filter
+edgedetect_filter
+elbg_filter
+entropy_filter
+eq_filter
+erosion_filter
+erosion_opencl_filter
+extractplanes_filter
+fade_filter
+fftdnoiz_filter
+fftfilt_filter
+field_filter
+fieldhint_filter
+fieldmatch_filter
+fieldorder_filter
+fillborders_filter
+find_rect_filter
+floodfill_filter
+format_filter
+fps_filter
+framepack_filter
+framerate_filter
+framestep_filter
+frei0r_filter
+fspp_filter
+gblur_filter
+geq_filter
+gradfun_filter
+graphmonitor_filter
+greyedge_filter
+haldclut_filter
+hflip_filter
+histeq_filter
+histogram_filter
+hqdn3d_filter
+hqx_filter
+hstack_filter
+hue_filter
+hwdownload_filter
+hwmap_filter
+hwupload_filter
+hwupload_cuda_filter
+hysteresis_filter
+idet_filter
+il_filter
+inflate_filter
+interlace_filter
+interleave_filter
+kerndeint_filter
+lenscorrection_filter
+lensfun_filter
+libvmaf_filter
+limiter_filter
+loop_filter
+lumakey_filter
+lut_filter
+lut1d_filter
+lut2_filter
+lut3d_filter
+lutrgb_filter
+lutyuv_filter
+maskedclamp_filter
+maskedmerge_filter
+mcdeint_filter
+mergeplanes_filter
+mestimate_filter
+metadata_filter
+midequalizer_filter
+minterpolate_filter
+mix_filter
+mpdecimate_filter
+negate_filter
+nlmeans_filter
+nnedi_filter
+noformat_filter
+noise_filter
+normalize_filter
+null_filter
+ocr_filter
+ocv_filter
+oscilloscope_filter
+overlay_filter
+overlay_opencl_filter
+overlay_qsv_filter
+owdenoise_filter
+pad_filter
+palettegen_filter
+paletteuse_filter
+perms_filter
+perspective_filter
+phase_filter
+pixdesctest_filter
+pixscope_filter
+pp_filter
+pp7_filter
+premultiply_filter
+prewitt_filter
+prewitt_opencl_filter
+procamp_vaapi_filter
+program_opencl_filter
+pseudocolor_filter
+psnr_filter
+pullup_filter
+qp_filter
+random_filter
+readeia608_filter
+readvitc_filter
+realtime_filter
+remap_filter
+removegrain_filter
+removelogo_filter
+repeatfields_filter
+reverse_filter
+roberts_filter
+roberts_opencl_filter
+rotate_filter
+sab_filter
+scale_filter
+scale_cuda_filter
+scale_npp_filter
+scale_qsv_filter
+scale_vaapi_filter
+scale2ref_filter
+select_filter
+selectivecolor_filter
+sendcmd_filter
+separatefields_filter
+setdar_filter
+setfield_filter
+setparams_filter
+setpts_filter
+setrange_filter
+setsar_filter
+settb_filter
+sharpness_vaapi_filter
+showinfo_filter
+showpalette_filter
+shuffleframes_filter
+shuffleplanes_filter
+sidedata_filter
+signalstats_filter
+signature_filter
+smartblur_filter
+sobel_filter
+sobel_opencl_filter
+split_filter
+spp_filter
+sr_filter
+ssim_filter
+stereo3d_filter
+streamselect_filter
+subtitles_filter
+super2xsai_filter
+swaprect_filter
+swapuv_filter
+tblend_filter
+telecine_filter
+threshold_filter
+thumbnail_filter
+thumbnail_cuda_filter
+tile_filter
+tinterlace_filter
+tlut2_filter
+tmix_filter
+tonemap_filter
+tonemap_opencl_filter
+transpose_filter
+transpose_npp_filter
+trim_filter
+unpremultiply_filter
+unsharp_filter
+unsharp_opencl_filter
+uspp_filter
+vaguedenoiser_filter
+vectorscope_filter
+vflip_filter
+vfrdet_filter
+vibrance_filter
+vidstabdetect_filter
+vidstabtransform_filter
+vignette_filter
+vmafmotion_filter
+vpp_qsv_filter
+vstack_filter
+w3fdif_filter
+waveform_filter
+weave_filter
+xbr_filter
+xstack_filter
+yadif_filter
+yadif_cuda_filter
+zmq_filter
+zoompan_filter
+zscale_filter
+allrgb_filter
+allyuv_filter
+cellauto_filter
+color_filter
+coreimagesrc_filter
+frei0r_src_filter
+haldclutsrc_filter
+life_filter
+mandelbrot_filter
+mptestsrc_filter
+nullsrc_filter
+openclsrc_filter
+pal75bars_filter
+pal100bars_filter
+rgbtestsrc_filter
+smptebars_filter
+smptehdbars_filter
+testsrc_filter
+testsrc2_filter
+yuvtestsrc_filter
+nullsink_filter
+abitscope_filter
+adrawgraph_filter
+agraphmonitor_filter
+ahistogram_filter
+aphasemeter_filter
+avectorscope_filter
+concat_filter
+showcqt_filter
+showfreqs_filter
+showspectrum_filter
+showspectrumpic_filter
+showvolume_filter
+showwaves_filter
+showwavespic_filter
+spectrumsynth_filter
+amovie_filter
+movie_filter
+afifo_filter
+fifo_filter'
+find_rect_filter=no
+find_rect_filter_deps='avcodec avformat gpl'
+firequalizer_filter=no
+firequalizer_filter_deps=avcodec
+firequalizer_filter_select=rdft
+flac_decoder_select=flacdsp
+flac_demuxer_select=flac_parser
+flac_encoder_select='bswapdsp flacdsp lpc'
+flanger_filter=no
+flashsv2_decoder_deps=zlib
+flashsv2_encoder_deps=zlib
+flashsv_decoder_deps=zlib
+flashsv_encoder_deps=zlib
+flite_filter=no
+flite_filter_deps=libflite
+floodfill_filter=no
+flv_decoder_select=h263_decoder
+flv_encoder_select=h263_encoder
+fma3=yes
+fma3_deps=avx
+fma3_external_deps=avx_external
+fma3_inline_deps=avx_inline
+fma3_suggest='fma3_external fma3_inline'
+fma4=yes
+fma4_deps=avx
+fma4_external_deps=avx_external
+fma4_inline_deps=avx_inline
+fma4_suggest='fma4_external fma4_inline'
+format_filter=no
+fourxm_decoder_select='blockdsp bswapdsp'
+fps_filter=no
+frame_thread_encoder_deps='encoders threads'
+framepack_filter=no
+framerate_filter=no
+framerate_filter_select=pixelutils
+framestep_filter=no
+fraps_decoder_select='bswapdsp huffman'
+frei0r=no
+frei0r_filter=no
+frei0r_filter_deps='frei0r libdl'
+frei0r_src_filter=no
+frei0r_src_filter_deps='frei0r libdl'
+fspp_filter=no
+fspp_filter_deps=gpl
+ftp_protocol_select=tcp_protocol
+g2m_decoder_deps=zlib
+g2m_decoder_select='blockdsp idctdsp jpegtables'
+g729_decoder_select=audiodsp
+gblur_filter=no
+gcrypt=no
+gdigrab_indev=no
+gdigrab_indev_deps=CreateDIBSection
+gdigrab_indev_extralibs=-lgdi32
+gdigrab_indev_select=bmp_decoder
+geq_filter=no
+geq_filter_deps=gpl
+gmp=no
+gnutls=no
+gopher_protocol_select=network
+gpl=no
+gradfun_filter=no
+graphmonitor_filter=no
+greyedge_filter=no
+gsm_ms_at_decoder_deps=audiotoolbox
+h261_decoder_select=mpegvideo
+h261_encoder_select=mpegvideoenc
+h263_decoder_select='h263_parser h263dsp mpegvideo qpeldsp'
+h263_encoder_select='h263dsp mpegvideoenc'
+h263_v4l2m2m_decoder_deps='v4l2_m2m h263_v4l2_m2m'
+h263_v4l2m2m_encoder_deps='v4l2_m2m h263_v4l2_m2m'
+h263_vaapi_hwaccel_deps=vaapi
+h263_vaapi_hwaccel_select=h263_decoder
+h263_videotoolbox_hwaccel_deps=videotoolbox
+h263_videotoolbox_hwaccel_select=h263_decoder
+h263i_decoder_select=h263_decoder
+h263p_decoder_select=h263_decoder
+h263p_encoder_select=h263_encoder
+h264_amf_encoder_deps=amf
+h264_crystalhd_decoder_select='crystalhd h264_mp4toannexb_bsf h264_parser'
+h264_cuvid_decoder_deps=cuvid
+h264_cuvid_decoder_select=h264_mp4toannexb_bsf
+h264_d3d11va2_hwaccel_deps=d3d11va
+h264_d3d11va2_hwaccel_select=h264_decoder
+h264_d3d11va_hwaccel_deps=d3d11va
+h264_d3d11va_hwaccel_select=h264_decoder
+h264_decoder_select='cabac golomb h264chroma h264dsp h264parse h264pred h264qpel videodsp'
+h264_decoder_suggest=error_resilience
+h264_dxva2_hwaccel_deps=dxva2
+h264_dxva2_hwaccel_select=h264_decoder
+h264_mediacodec_decoder_deps=mediacodec
+h264_mediacodec_decoder_select='h264_mp4toannexb_bsf h264_parser'
+h264_metadata_bsf_deps=const_nan
+h264_metadata_bsf_select=cbs_h264
+h264_mmal_decoder_deps=mmal
+h264_mp4toannexb_bsf=yes
+h264_nvdec_hwaccel_deps=nvdec
+h264_nvdec_hwaccel_select=h264_decoder
+h264_nvenc_encoder_deps=nvenc
+h264_omx_encoder_deps=omx
+h264_parser_select='golomb h264dsp h264parse'
+h264_qsv_decoder_select='h264_mp4toannexb_bsf h264_parser qsvdec'
+h264_qsv_encoder_select=qsvenc
+h264_redundant_pps_bsf_select=cbs_h264
+h264_rkmpp_decoder_deps=rkmpp
+h264_rkmpp_decoder_select=h264_mp4toannexb_bsf
+h264_v4l2m2m_decoder_deps='v4l2_m2m h264_v4l2_m2m'
+h264_v4l2m2m_decoder_select=h264_mp4toannexb_bsf
+h264_v4l2m2m_encoder_deps='v4l2_m2m h264_v4l2_m2m'
+h264_vaapi_encoder_select='cbs_h264 vaapi_encode'
+h264_vaapi_hwaccel_deps=vaapi
+h264_vaapi_hwaccel_select=h264_decoder
+h264_vdpau_hwaccel_deps=vdpau
+h264_vdpau_hwaccel_select=h264_decoder
+h264_videotoolbox_encoder_deps=pthreads
+h264_videotoolbox_encoder_select=videotoolbox_encoder
+h264_videotoolbox_hwaccel_deps=videotoolbox
+h264_videotoolbox_hwaccel_select=h264_decoder
+h264dsp_select=startcode
+haas_filter=no
+haldclut_filter=no
+haldclutsrc_filter=no
+hap_decoder_select='snappy texturedsp'
+hap_encoder_deps=libsnappy
+hap_encoder_select=texturedspenc
+hardcoded_tables=no
+hdcd_filter=no
+hds_muxer_select=flv_muxer
+headphone_filter=no
+hevc_amf_encoder_deps=amf
+hevc_cuvid_decoder_deps=cuvid
+hevc_cuvid_decoder_select=hevc_mp4toannexb_bsf
+hevc_d3d11va2_hwaccel_deps='d3d11va DXVA_PicParams_HEVC'
+hevc_d3d11va2_hwaccel_select=hevc_decoder
+hevc_d3d11va_hwaccel_deps='d3d11va DXVA_PicParams_HEVC'
+hevc_d3d11va_hwaccel_select=hevc_decoder
+hevc_decoder_select='bswapdsp cabac golomb hevcparse videodsp'
+hevc_dxva2_hwaccel_deps='dxva2 DXVA_PicParams_HEVC'
+hevc_dxva2_hwaccel_select=hevc_decoder
+hevc_mediacodec_decoder_deps=mediacodec
+hevc_mediacodec_decoder_select='hevc_mp4toannexb_bsf hevc_parser'
+hevc_metadata_bsf_select=cbs_h265
+hevc_mp4toannexb_bsf=yes
+hevc_nvdec_hwaccel_deps=nvdec
+hevc_nvdec_hwaccel_select=hevc_decoder
+hevc_nvenc_encoder_deps=nvenc
+hevc_parser_select=hevcparse
+hevc_qsv_decoder_select='hevc_mp4toannexb_bsf hevc_parser qsvdec'
+hevc_qsv_encoder_select='hevcparse qsvenc'
+hevc_rkmpp_decoder_deps=rkmpp
+hevc_rkmpp_decoder_select=hevc_mp4toannexb_bsf
+hevc_v4l2m2m_decoder_deps='v4l2_m2m hevc_v4l2_m2m'
+hevc_v4l2m2m_decoder_select=hevc_mp4toannexb_bsf
+hevc_v4l2m2m_encoder_deps='v4l2_m2m hevc_v4l2_m2m'
+hevc_vaapi_encoder_deps=VAEncPictureParameterBufferHEVC
+hevc_vaapi_encoder_select='cbs_h265 vaapi_encode'
+hevc_vaapi_hwaccel_deps='vaapi VAPictureParameterBufferHEVC'
+hevc_vaapi_hwaccel_select=hevc_decoder
+hevc_vdpau_hwaccel_deps='vdpau VdpPictureInfoHEVC'
+hevc_vdpau_hwaccel_select=hevc_decoder
+hevc_videotoolbox_encoder_deps=pthreads
+hevc_videotoolbox_encoder_select=videotoolbox_encoder
+hevc_videotoolbox_hwaccel_deps=videotoolbox
+hevc_videotoolbox_hwaccel_select=hevc_decoder
+hevcparse_select=golomb
+hflip_filter=no
+highpass_filter=no
+highshelf_filter=no
+hilbert_filter=no
+histeq_filter=no
+histeq_filter_deps=gpl
+histogram_filter=no
+hls_muxer_select=mpegts_muxer
+hls_muxer_suggest='gcrypt openssl'
+host_cc_default=gcc
+host_cflags_filter=echo
+host_extralibs=-lm
+host_ldflags_filter=echo
+host_os=linux
+hqdn3d_filter=no
+hqdn3d_filter_deps=gpl
+hqx_filter=no
+hstack_filter=no
+htmlpages=no
+htmlpages_deps=perl
+htmlpages_deps_any='makeinfo_html texi2html'
+http_multiclient_example=yes
+http_multiclient_example_deps='avformat avutil fork'
+http_protocol_select=tcp_protocol
+http_protocol_suggest=zlib
+http_proxy=http://y00227505:1%403%24qwer@proxyhk.huawei.com:8080
+httpproxy_protocol_select=tcp_protocol
+httpproxy_protocol_suggest=zlib
+https_protocol_select=tls_protocol
+https_protocol_suggest=zlib
+https_proxy=https://y00227505:1%403%24qwer@proxyhk.huawei.com:8080
+hue_filter=no
+huffyuv_decoder_select='bswapdsp huffyuvdsp llviddsp'
+huffyuv_encoder_select='bswapdsp huffman huffyuvencdsp llvidencdsp'
+hw_decode_example=yes
+hw_decode_example_deps='avcodec avformat avutil'
+hwaccels_if_any='h263_vaapi_hwaccel
+h263_videotoolbox_hwaccel
+h264_d3d11va_hwaccel
+h264_d3d11va2_hwaccel
+h264_dxva2_hwaccel
+h264_nvdec_hwaccel
+h264_vaapi_hwaccel
+h264_vdpau_hwaccel
+h264_videotoolbox_hwaccel
+hevc_d3d11va_hwaccel
+hevc_d3d11va2_hwaccel
+hevc_dxva2_hwaccel
+hevc_nvdec_hwaccel
+hevc_vaapi_hwaccel
+hevc_vdpau_hwaccel
+hevc_videotoolbox_hwaccel
+mjpeg_nvdec_hwaccel
+mjpeg_vaapi_hwaccel
+mpeg1_nvdec_hwaccel
+mpeg1_vdpau_hwaccel
+mpeg1_videotoolbox_hwaccel
+mpeg1_xvmc_hwaccel
+mpeg2_d3d11va_hwaccel
+mpeg2_d3d11va2_hwaccel
+mpeg2_nvdec_hwaccel
+mpeg2_dxva2_hwaccel
+mpeg2_vaapi_hwaccel
+mpeg2_vdpau_hwaccel
+mpeg2_videotoolbox_hwaccel
+mpeg2_xvmc_hwaccel
+mpeg4_nvdec_hwaccel
+mpeg4_vaapi_hwaccel
+mpeg4_vdpau_hwaccel
+mpeg4_videotoolbox_hwaccel
+vc1_d3d11va_hwaccel
+vc1_d3d11va2_hwaccel
+vc1_dxva2_hwaccel
+vc1_nvdec_hwaccel
+vc1_vaapi_hwaccel
+vc1_vdpau_hwaccel
+vp8_nvdec_hwaccel
+vp8_vaapi_hwaccel
+vp9_d3d11va_hwaccel
+vp9_d3d11va2_hwaccel
+vp9_dxva2_hwaccel
+vp9_nvdec_hwaccel
+vp9_vaapi_hwaccel
+wmv3_d3d11va_hwaccel
+wmv3_d3d11va2_hwaccel
+wmv3_dxva2_hwaccel
+wmv3_nvdec_hwaccel
+wmv3_vaapi_hwaccel
+wmv3_vdpau_hwaccel'
+hwdownload_filter=no
+hwmap_filter=no
+hwupload_cuda_filter=no
+hwupload_cuda_filter_deps=ffnvcodec
+hwupload_filter=no
+hysteresis_filter=no
+i686=yes
+i686_deps=x86
+iac_decoder_select=imc_decoder
+icecast_protocol_select=http_protocol
+iconv=no
+idet_filter=no
+iec61883_indev=no
+iec61883_indev_deps=libiec61883
+il_filter=no
+ilbc_at_decoder_deps=audiotoolbox
+ilbc_at_encoder_deps=audiotoolbox
+ilbc_at_encoder_select=audio_frame_queue
+image2_alias_pix_demuxer_select=image2_demuxer
+image2_brender_pix_demuxer_select=image2_demuxer
+imc_decoder_select='bswapdsp fft mdct sinewin'
+incdir_default='${prefix}/include'
+indeo3_decoder_select=hpeldsp
+indeo4_decoder_select=ividsp
+indeo5_decoder_select=ividsp
+indevs_if_any='alsa_indev
+android_camera_indev
+avfoundation_indev
+bktr_indev
+decklink_indev
+libndi_newtek_indev
+dshow_indev
+fbdev_indev
+gdigrab_indev
+iec61883_indev
+jack_indev
+kmsgrab_indev
+lavfi_indev
+openal_indev
+oss_indev
+pulse_indev
+sndio_indev
+v4l2_indev
+vfwcap_indev
+xcbgrab_indev
+libcdio_indev
+libdc1394_indev'
+inflate_filter=no
+inline_asm=no
+install=install
+interlace_filter=no
+interlace_filter_deps=gpl
+interleave_filter=no
+interplay_video_decoder_select=hpeldsp
+intrax8_select='blockdsp idctdsp'
+intrinsics=none
+intrinsics_neon_deps=neon
+ipod_muxer_select=mov_muxer
+ismv_muxer_select=mov_muxer
+ivf_muxer_select='av1_metadata_bsf vp9_superframe_bsf'
+jack_indev=no
+jack_indev_deps=libjack
+jack_indev_deps_any='sem_timedwait dispatch_dispatch_h'
+jni=no
+join_filter=no
+jpegls_decoder_select=mjpeg_decoder
+jv_decoder_select=blockdsp
+kerndeint_filter=no
+kerndeint_filter_deps=gpl
+kmsgrab_indev=no
+kmsgrab_indev_deps=libdrm
+l=--cross-prefix=
+ladspa=no
+ladspa_filter=no
+ladspa_filter_deps='ladspa libdl'
+lagarith_decoder_select=llviddsp
+lavfi_indev=no
+lavfi_indev_deps=avfilter
+ldbrx=yes
+ldbrx_deps=ppc
+ldflags_filter=echo
+lenscorrection_filter=no
+lensfun_filter=no
+lensfun_filter_deps='liblensfun version3'
+lib=w32threads
+libaom=no
+libaom_av1_decoder_deps=libaom
+libaom_av1_encoder_deps=libaom
+libaom_av1_encoder_select=extract_extradata_bsf
+libass=no
+libbluray=no
+libbs2b=no
+libcaca=no
+libcdio=no
+libcdio_indev=no
+libcdio_indev_deps=libcdio
+libcelt=no
+libcelt_decoder_deps=libcelt
+libcodec2=no
+libcodec2_decoder_deps=libcodec2
+libcodec2_encoder_deps=libcodec2
+libdavs2=no
+libdavs2_decoder_deps=libdavs2
+libdc1394=no
+libdc1394_indev=no
+libdc1394_indev_deps=libdc1394
+libdir_default='${prefix}/lib'
+libdrm=no
+libfdk_aac=no
+libfdk_aac_decoder_deps=libfdk_aac
+libfdk_aac_encoder_deps=libfdk_aac
+libfdk_aac_encoder_select=audio_frame_queue
+libflite=no
+libfontconfig=no
+libfreetype=no
+libfribidi=no
+libgme=no
+libgme_demuxer_deps=libgme
+libgsm=no
+libgsm_decoder_deps=libgsm
+libgsm_encoder_deps=libgsm
+libgsm_ms_decoder_deps=libgsm
+libgsm_ms_encoder_deps=libgsm
+libiec61883=no
+libilbc=no
+libilbc_decoder_deps=libilbc
+libilbc_encoder_deps=libilbc
+libjack=no
+libklvanc=no
+libkvazaar=no
+libkvazaar_encoder_deps=libkvazaar
+liblensfun=no
+libmfx=no
+libmodplug=no
+libmodplug_demuxer_deps=libmodplug
+libmp3lame=no
+libmp3lame_encoder_deps=libmp3lame
+libmp3lame_encoder_select='audio_frame_queue mpegaudioheader'
+libmysofa=no
+libndi_newtek=no
+libndi_newtek_indev=no
+libndi_newtek_indev_deps=libndi_newtek
+libndi_newtek_indev_extralibs=-lndi
+libndi_newtek_outdev=no
+libndi_newtek_outdev_deps=libndi_newtek
+libndi_newtek_outdev_extralibs=-lndi
+libnpp=no
+libopencore_amrnb=no
+libopencore_amrnb_decoder_deps=libopencore_amrnb
+libopencore_amrnb_encoder_deps=libopencore_amrnb
+libopencore_amrnb_encoder_select=audio_frame_queue
+libopencore_amrwb=no
+libopencore_amrwb_decoder_deps=libopencore_amrwb
+libopencv=no
+libopenh264=no
+libopenh264_decoder_deps=libopenh264
+libopenh264_decoder_select=h264_mp4toannexb_bsf
+libopenh264_encoder_deps=libopenh264
+libopenjpeg=no
+libopenjpeg_decoder_deps=libopenjpeg
+libopenjpeg_encoder_deps=libopenjpeg
+libopenmpt=no
+libopenmpt_demuxer_deps=libopenmpt
+libopus=no
+libopus_decoder_deps=libopus
+libopus_encoder_deps=libopus
+libopus_encoder_select=audio_frame_queue
+libpulse=no
+librsvg=no
+librsvg_decoder_deps=librsvg
+librtmp=no
+librtmp_protocol_deps=librtmp
+librtmpe_protocol_deps=librtmp
+librtmps_protocol_deps=librtmp
+librtmpt_protocol_deps=librtmp
+librtmpte_protocol_deps=librtmp
+librubberband=no
+libshine=no
+libshine_encoder_deps=libshine
+libshine_encoder_select=audio_frame_queue
+libsmbclient=no
+libsmbclient_protocol_deps='libsmbclient gplv3'
+libsnappy=no
+libsoxr=no
+libspeex=no
+libspeex_decoder_deps=libspeex
+libspeex_encoder_deps=libspeex
+libspeex_encoder_select=audio_frame_queue
+libsrt=no
+libsrt_protocol_deps=libsrt
+libsrt_protocol_select=network
+libssh=no
+libssh_protocol_deps=libssh
+libtensorflow=no
+libtesseract=no
+libtheora=no
+libtheora_encoder_deps=libtheora
+libtls=no
+libtls_conflict='openssl gnutls mbedtls'
+libtwolame=no
+libtwolame_encoder_deps=libtwolame
+libv4l2=no
+libvidstab=no
+libvmaf=no
+libvmaf_filter=no
+libvmaf_filter_deps='libvmaf pthreads'
+libvo_amrwbenc=no
+libvo_amrwbenc_encoder_deps=libvo_amrwbenc
+libvorbis=no
+libvorbis_decoder_deps=libvorbis
+libvorbis_encoder_deps='libvorbis libvorbisenc'
+libvorbis_encoder_select=audio_frame_queue
+libvpx=no
+libvpx_vp8_decoder_deps=libvpx
+libvpx_vp8_encoder_deps=libvpx
+libvpx_vp9_decoder_deps=libvpx
+libvpx_vp9_encoder_deps=libvpx
+libwavpack=no
+libwavpack_encoder_deps=libwavpack
+libwavpack_encoder_select=audio_frame_queue
+libwebp=no
+libwebp_anim_encoder_deps=libwebp
+libwebp_encoder_deps=libwebp
+libx262_encoder_deps=libx262
+libx264=no
+libx264_encoder_deps=libx264
+libx264rgb_encoder_deps='libx264 x264_csp_bgr'
+libx264rgb_encoder_select=libx264_encoder
+libx265=no
+libx265_encoder_deps=libx265
+libxavs=no
+libxavs2=no
+libxavs2_encoder_deps=libxavs2
+libxavs_encoder_deps=libxavs
+libxcb=yes
+libxcb_shape=yes
+libxcb_shm=yes
+libxcb_xfixes=yes
+libxml2=no
+libxvid=no
+libxvid_encoder_deps=libxvid
+libzimg=no
+libzmq=no
+libzvbi=no
+libzvbi_teletext_decoder_deps=libzvbi
+license='LGPL version 2.1 or later'
+life_filter=no
+limiter_filter=no
+linux_perf_deps=linux_perf_event_h
+list='wav_demuxer '
+ljpeg_encoder_select='idctdsp jpegtables mpegvideoenc'
+ln_s_default='ln -s -f'
+logfile=ffbuild/config.log
+loongson2=yes
+loongson2_deps=mips
+loongson3=yes
+loongson3_deps=mips
+loop_filter=no
+loudnorm_filter=no
+lowpass_filter=no
+lowshelf_filter=no
+lumakey_filter=no
+lut1d_filter=no
+lut2_filter=no
+lut3d_filter=no
+lut_filter=no
+lutrgb_filter=no
+lutyuv_filter=no
+lv2=no
+lv2_filter=no
+lv2_filter_deps=lv2
+lzma=yes
+m='disable_components $v'
+magicyuv_decoder_select=llviddsp
+magicyuv_encoder_select=llvidencdsp
+mandelbrot_filter=no
+mandir_default='${prefix}/share/man'
+manpages=yes
+manpages_deps='perl pod2man'
+maskedclamp_filter=no
+maskedmerge_filter=no
+matroska_audio_muxer_select=matroska_muxer
+matroska_demuxer_select='iso_media riffdec'
+matroska_demuxer_suggest='bzlib lzo zlib'
+matroska_muxer_select='iso_media riffenc'
+mbedtls=no
+mcdeint_filter=no
+mcdeint_filter_deps='avcodec gpl'
+mcompand_filter=no
+mdct15_select=fft
+mdct_select=fft
+mdec_decoder_select='blockdsp idctdsp mpegvideo'
+me_cmp_select='fdctdsp idctdsp pixblockdsp'
+mediacodec=no
+mergeplanes_filter=no
+mestimate_filter=no
+metadata_example=yes
+metadata_example_deps='avformat avutil'
+metadata_filter=no
+metasound_decoder_select='lsp mdct sinewin'
+midequalizer_filter=no
+mimic_decoder_select='blockdsp bswapdsp hpeldsp idctdsp'
+minterpolate_filter=no
+mips32r2=yes
+mips32r2_deps=mips
+mips32r5=yes
+mips32r5_deps=mips
+mips32r6=yes
+mips32r6_deps=mips
+mips64r2=yes
+mips64r2_deps=mips
+mips64r6=yes
+mips64r6_deps=mips
+mipsdsp=yes
+mipsdsp_deps=mips
+mipsdspr2=yes
+mipsdspr2_deps=mips
+mipsfpu=yes
+mipsfpu_deps=mips
+mix_filter=no
+mjpeg2jpeg_bsf_select=jpegtables
+mjpeg_cuvid_decoder_deps=cuvid
+mjpeg_decoder_select='blockdsp hpeldsp exif idctdsp jpegtables'
+mjpeg_encoder_select='jpegtables mpegvideoenc'
+mjpeg_nvdec_hwaccel_deps=nvdec
+mjpeg_nvdec_hwaccel_select=mjpeg_decoder
+mjpeg_qsv_encoder_deps=libmfx
+mjpeg_qsv_encoder_select=qsvenc
+mjpeg_vaapi_encoder_deps=VAEncPictureParameterBufferJPEG
+mjpeg_vaapi_encoder_select='cbs_jpeg jpegtables vaapi_encode'
+mjpeg_vaapi_hwaccel_deps=vaapi
+mjpeg_vaapi_hwaccel_select=mjpeg_decoder
+mjpegb_decoder_select=mjpeg_decoder
+mlp_decoder_select=mlp_parser
+mlp_encoder_select=lpc
+mmal=no
+mmf_muxer_select=riffenc
+mmi=yes
+mmi_deps=mips
+mmsh_protocol_select=http_protocol
+mmst_protocol_select=network
+mmx=yes
+mmx_deps=x86
+mmx_external_deps=x86asm
+mmx_inline_deps='inline_asm x86'
+mmx_suggest='mmx_external mmx_inline'
+mmxext=yes
+mmxext_deps=mmx
+mmxext_external_deps=mmx_external
+mmxext_inline_deps=mmx_inline
+mmxext_suggest='mmxext_external mmxext_inline'
+motionpixels_decoder_select=bswapdsp
+mov_demuxer=yes
+mov_demuxer_select='iso_media riffdec'
+mov_demuxer_suggest=zlib
+mov_muxer_select='iso_media riffenc rtpenc_chain'
+movie_filter=no
+movie_filter_deps='avcodec avformat'
+mp1_at_decoder_deps=audiotoolbox
+mp1_at_decoder_select=mpegaudioheader
+mp1_decoder_select=mpegaudio
+mp1float_decoder_select=mpegaudio
+mp2_at_decoder_deps=audiotoolbox
+mp2_at_decoder_select=mpegaudioheader
+mp2_decoder_select=mpegaudio
+mp2float_decoder_select=mpegaudio
+mp3_at_decoder_deps=audiotoolbox
+mp3_at_decoder_select=mpegaudioheader
+mp3_decoder_select=mpegaudio
+mp3_demuxer_select=mpegaudio_parser
+mp3_muxer_select=mpegaudioheader
+mp3adu_decoder_select=mpegaudio
+mp3adufloat_decoder_select=mpegaudio
+mp3float_decoder_select=mpegaudio
+mp3on4_decoder_select=mpegaudio
+mp3on4float_decoder_select=mpegaudio
+mp4_muxer_select=mov_muxer
+mpc7_decoder_select='bswapdsp mpegaudiodsp'
+mpc8_decoder_select=mpegaudiodsp
+mpdecimate_filter=no
+mpdecimate_filter_deps=gpl
+mpdecimate_filter_select=pixelutils
+mpeg1_cuvid_decoder_deps=cuvid
+mpeg1_nvdec_hwaccel_deps=nvdec
+mpeg1_nvdec_hwaccel_select=mpeg1video_decoder
+mpeg1_v4l2m2m_decoder_deps='v4l2_m2m mpeg1_v4l2_m2m'
+mpeg1_vdpau_hwaccel_deps=vdpau
+mpeg1_vdpau_hwaccel_select=mpeg1video_decoder
+mpeg1_videotoolbox_hwaccel_deps=videotoolbox
+mpeg1_videotoolbox_hwaccel_select=mpeg1video_decoder
+mpeg1_xvmc_hwaccel_deps=xvmc
+mpeg1_xvmc_hwaccel_select=mpeg1video_decoder
+mpeg1video_decoder_select=mpegvideo
+mpeg1video_encoder_select='mpegvideoenc h263dsp'
+mpeg2_crystalhd_decoder_select=crystalhd
+mpeg2_cuvid_decoder_deps=cuvid
+mpeg2_d3d11va2_hwaccel_deps=d3d11va
+mpeg2_d3d11va2_hwaccel_select=mpeg2video_decoder
+mpeg2_d3d11va_hwaccel_deps=d3d11va
+mpeg2_d3d11va_hwaccel_select=mpeg2video_decoder
+mpeg2_dxva2_hwaccel_deps=dxva2
+mpeg2_dxva2_hwaccel_select=mpeg2video_decoder
+mpeg2_mediacodec_decoder_deps=mediacodec
+mpeg2_metadata_bsf_select=cbs_mpeg2
+mpeg2_mmal_decoder_deps=mmal
+mpeg2_nvdec_hwaccel_deps=nvdec
+mpeg2_nvdec_hwaccel_select=mpeg2video_decoder
+mpeg2_qsv_decoder_select='qsvdec mpegvideo_parser'
+mpeg2_qsv_encoder_select=qsvenc
+mpeg2_v4l2m2m_decoder_deps='v4l2_m2m mpeg2_v4l2_m2m'
+mpeg2_vaapi_encoder_select='cbs_mpeg2 vaapi_encode'
+mpeg2_vaapi_hwaccel_deps=vaapi
+mpeg2_vaapi_hwaccel_select=mpeg2video_decoder
+mpeg2_vdpau_hwaccel_deps=vdpau
+mpeg2_vdpau_hwaccel_select=mpeg2video_decoder
+mpeg2_videotoolbox_hwaccel_deps=videotoolbox
+mpeg2_videotoolbox_hwaccel_select=mpeg2video_decoder
+mpeg2_xvmc_hwaccel_deps=xvmc
+mpeg2_xvmc_hwaccel_select=mpeg2video_decoder
+mpeg2video_decoder_select=mpegvideo
+mpeg2video_encoder_select='mpegvideoenc h263dsp'
+mpeg4_crystalhd_decoder_select=crystalhd
+mpeg4_cuvid_decoder_deps=cuvid
+mpeg4_decoder_select='h263_decoder mpeg4video_parser'
+mpeg4_encoder_select=h263_encoder
+mpeg4_mediacodec_decoder_deps=mediacodec
+mpeg4_mmal_decoder_deps=mmal
+mpeg4_nvdec_hwaccel_deps=nvdec
+mpeg4_nvdec_hwaccel_select=mpeg4_decoder
+mpeg4_omx_encoder_deps=omx
+mpeg4_v4l2m2m_decoder_deps='v4l2_m2m mpeg4_v4l2_m2m'
+mpeg4_v4l2m2m_encoder_deps='v4l2_m2m mpeg4_v4l2_m2m'
+mpeg4_vaapi_hwaccel_deps=vaapi
+mpeg4_vaapi_hwaccel_select=mpeg4_decoder
+mpeg4_vdpau_hwaccel_deps=vdpau
+mpeg4_vdpau_hwaccel_select=mpeg4_decoder
+mpeg4_videotoolbox_hwaccel_deps=videotoolbox
+mpeg4_videotoolbox_hwaccel_select=mpeg4_decoder
+mpeg4video_parser_select='h263dsp mpegvideo qpeldsp'
+mpeg_er_select=error_resilience
+mpeg_xvmc_hwaccel_deps=xvmc
+mpeg_xvmc_hwaccel_select=mpeg2video_decoder
+mpegaudio_parser_select=mpegaudioheader
+mpegaudio_select='mpegaudiodsp mpegaudioheader'
+mpegaudiodsp_select=dct
+mpegts_demuxer_select=iso_media
+mpegts_muxer_select='adts_muxer latm_muxer'
+mpegtsraw_demuxer_select=mpegts_demuxer
+mpegvideo_decoder_select=mpegvideo
+mpegvideo_parser_select=mpegvideo
+mpegvideo_select='blockdsp h264chroma hpeldsp idctdsp me_cmp mpeg_er videodsp'
+mpegvideoenc_select='aandcttables me_cmp mpegvideo pixblockdsp qpeldsp'
+mptestsrc_filter=no
+mptestsrc_filter_deps=gpl
+msa=yes
+msa1_decoder_select=mss34dsp
+msa_deps=mipsfpu
+mscc_decoder_deps=zlib
+msmpeg4_crystalhd_decoder_select=crystalhd
+msmpeg4v1_decoder_select=h263_decoder
+msmpeg4v2_decoder_select=h263_decoder
+msmpeg4v2_encoder_select=h263_encoder
+msmpeg4v3_decoder_select=h263_decoder
+msmpeg4v3_encoder_select=h263_encoder
+mss2_decoder_select='mpegvideo qpeldsp vc1_decoder'
+mts2_decoder_select=mss34dsp
+muxers_if_any='a64_muxer
+ac3_muxer
+adts_muxer
+adx_muxer
+aiff_muxer
+amr_muxer
+apng_muxer
+aptx_muxer
+aptx_hd_muxer
+asf_muxer
+ass_muxer
+ast_muxer
+asf_stream_muxer
+au_muxer
+avi_muxer
+avm2_muxer
+avs2_muxer
+bit_muxer
+caf_muxer
+cavsvideo_muxer
+codec2_muxer
+codec2raw_muxer
+crc_muxer
+dash_muxer
+data_muxer
+daud_muxer
+dirac_muxer
+dnxhd_muxer
+dts_muxer
+dv_muxer
+eac3_muxer
+f4v_muxer
+ffmetadata_muxer
+fifo_muxer
+fifo_test_muxer
+filmstrip_muxer
+fits_muxer
+flac_muxer
+flv_muxer
+framecrc_muxer
+framehash_muxer
+framemd5_muxer
+g722_muxer
+g723_1_muxer
+g726_muxer
+g726le_muxer
+gif_muxer
+gsm_muxer
+gxf_muxer
+h261_muxer
+h263_muxer
+h264_muxer
+hash_muxer
+hds_muxer
+hevc_muxer
+hls_muxer
+ico_muxer
+ilbc_muxer
+image2_muxer
+image2pipe_muxer
+ipod_muxer
+ircam_muxer
+ismv_muxer
+ivf_muxer
+jacosub_muxer
+latm_muxer
+lrc_muxer
+m4v_muxer
+md5_muxer
+matroska_muxer
+matroska_audio_muxer
+microdvd_muxer
+mjpeg_muxer
+mlp_muxer
+mmf_muxer
+mov_muxer
+mp2_muxer
+mp3_muxer
+mp4_muxer
+mpeg1system_muxer
+mpeg1vcd_muxer
+mpeg1video_muxer
+mpeg2dvd_muxer
+mpeg2svcd_muxer
+mpeg2video_muxer
+mpeg2vob_muxer
+mpegts_muxer
+mpjpeg_muxer
+mxf_muxer
+mxf_d10_muxer
+mxf_opatom_muxer
+null_muxer
+nut_muxer
+oga_muxer
+ogg_muxer
+ogv_muxer
+oma_muxer
+opus_muxer
+pcm_alaw_muxer
+pcm_mulaw_muxer
+pcm_vidc_muxer
+pcm_f64be_muxer
+pcm_f64le_muxer
+pcm_f32be_muxer
+pcm_f32le_muxer
+pcm_s32be_muxer
+pcm_s32le_muxer
+pcm_s24be_muxer
+pcm_s24le_muxer
+pcm_s16be_muxer
+pcm_s16le_muxer
+pcm_s8_muxer
+pcm_u32be_muxer
+pcm_u32le_muxer
+pcm_u24be_muxer
+pcm_u24le_muxer
+pcm_u16be_muxer
+pcm_u16le_muxer
+pcm_u8_muxer
+psp_muxer
+rawvideo_muxer
+rm_muxer
+roq_muxer
+rso_muxer
+rtp_muxer
+rtp_mpegts_muxer
+rtsp_muxer
+sap_muxer
+sbc_muxer
+scc_muxer
+segafilm_muxer
+segment_muxer
+stream_segment_muxer
+singlejpeg_muxer
+smjpeg_muxer
+smoothstreaming_muxer
+sox_muxer
+spx_muxer
+spdif_muxer
+srt_muxer
+sup_muxer
+swf_muxer
+tee_muxer
+tg2_muxer
+tgp_muxer
+mkvtimestamp_v2_muxer
+truehd_muxer
+tta_muxer
+uncodedframecrc_muxer
+vc1_muxer
+vc1t_muxer
+voc_muxer
+w64_muxer
+wav_muxer
+webm_muxer
+webm_dash_manifest_muxer
+webm_chunk_muxer
+webp_muxer
+webvtt_muxer
+wtv_muxer
+wv_muxer
+yuv4mpegpipe_muxer
+chromaprint_muxer'
+muxing_example=yes
+muxing_example_deps='avcodec avformat avutil swscale'
+mwsc_decoder_deps=zlib
+mxf_d10_muxer_select=mxf_muxer
+mxf_opatom_muxer_select=mxf_muxer
+mxpeg_decoder_select=mjpeg_decoder
+n=protocols
+name=wav_demuxer
+ncolors=8
+ncols=208
+negate_filter=no
+negate_filter_deps=lut_filter
+nellymoser_decoder_select='mdct sinewin'
+nellymoser_encoder_select='audio_frame_queue mdct sinewin'
+neon=no
+neon_deps_any='aarch64 arm'
+neon_inline_deps=inline_asm
+network=no
+nlmeans_filter=no
+nm_default='nm -g'
+nnedi_filter=no
+nnedi_filter_deps=gpl
+no_proxy=szxgit10-rd.huawei.com,pekgit01-rd.huawei.com,10.121.138.30,
+noformat_filter=no
+noise_filter=no
+normalize_filter=no
+null_filter=no
+nullsink_filter=no
+nullsrc_filter=no
+nut_muxer_select=riffenc
+nuv_decoder_select='idctdsp lzo'
+nuv_demuxer_select=riffdec
+nvcc_default=nvcc
+nvccflags_default='-gencode arch=compute_30,code=sm_30 -O2'
+nvdec=yes
+nvdec_deps=ffnvcodec
+nvenc=yes
+nvenc_deps=ffnvcodec
+nvenc_deps_any='libdl LoadLibrary'
+nvenc_encoder_deps=nvenc
+nvenc_h264_encoder_select=h264_nvenc_encoder
+nvenc_hevc_encoder_select=hevc_nvenc_encoder
+objformat=elf32
+ocr_filter=no
+ocr_filter_deps=libtesseract
+ocv_filter=no
+ocv_filter_deps=libopencv
+oga_muxer_select=ogg_muxer
+ogg_demuxer_select=dirac_parse
+ogv_muxer_select=ogg_muxer
+omx=no
+omx_deps='libdl pthreads'
+omx_rpi_select=omx
+on2avc_decoder_select=mdct
+op==
+openal=no
+openal_indev=no
+openal_indev_deps=openal
+opencl=no
+openclsrc_filter=no
+openclsrc_filter_deps=opencl
+opengl=no
+opengl_outdev=no
+opengl_outdev_deps=opengl
+opengl_outdev_suggest=sdl2
+openssl=no
+opt=openssl
+optimizations=yes
+option=gpl
+optname=cross_prefix
+optval=arm-himix100-linux-
+opus_decoder_deps=swresample
+opus_decoder_select=mdct15
+opus_encoder_select='audio_frame_queue mdct15'
+opus_muxer_select=ogg_muxer
+oscilloscope_filter=no
+oss_indev=no
+oss_indev_deps_any=sys_soundcard_h
+oss_outdev=no
+oss_outdev_deps_any=sys_soundcard_h
+outdevs_if_any='alsa_outdev
+caca_outdev
+decklink_outdev
+libndi_newtek_outdev
+fbdev_outdev
+opengl_outdev
+oss_outdev
+pulse_outdev
+sdl2_outdev
+sndio_outdev
+v4l2_outdev
+xv_outdev'
+overlay_filter=no
+overlay_opencl_filter=no
+overlay_opencl_filter_deps=opencl
+overlay_qsv_filter=no
+overlay_qsv_filter_deps=libmfx
+overlay_qsv_filter_select=qsvvpp
+owdenoise_filter=no
+owdenoise_filter_deps=gpl
+pad_filter=no
+pal100bars_filter=no
+pal75bars_filter=no
+palettegen_filter=no
+paletteuse_filter=no
+pan_filter=no
+pan_filter_deps=swresample
+parsers_if_any='aac_parser
+aac_latm_parser
+ac3_parser
+adx_parser
+av1_parser
+avs2_parser
+bmp_parser
+cavsvideo_parser
+cook_parser
+dca_parser
+dirac_parser
+dnxhd_parser
+dpx_parser
+dvaudio_parser
+dvbsub_parser
+dvdsub_parser
+dvd_nav_parser
+flac_parser
+g729_parser
+gsm_parser
+h261_parser
+h263_parser
+h264_parser
+hevc_parser
+mjpeg_parser
+mlp_parser
+mpeg4video_parser
+mpegaudio_parser
+mpegvideo_parser
+opus_parser
+png_parser
+pnm_parser
+rv30_parser
+rv40_parser
+sbc_parser
+sipr_parser
+tak_parser
+vc1_parser
+vorbis_parser
+vp3_parser
+vp8_parser
+vp9_parser
+xma_parser'
+pcm_alaw_at_decoder_deps=audiotoolbox
+pcm_alaw_at_encoder_deps=audiotoolbox
+pcm_alaw_at_encoder_select=audio_frame_queue
+pcm_mulaw_at_decoder_deps=audiotoolbox
+pcm_mulaw_at_encoder_deps=audiotoolbox
+pcm_mulaw_at_encoder_select=audio_frame_queue
+perms_filter=no
+perspective_filter=no
+perspective_filter_deps=gpl
+phase_filter=no
+phase_filter_deps=gpl
+pic=yes
+pixdesctest_filter=no
+pixelutils=no
+pixfmts_super2xsai_test_deps=super2xsai_filter
+pixscope_filter=no
+pkg_config_default=pkg-config
+png_decoder_deps=zlib
+png_encoder_deps=zlib
+png_encoder_select=llvidencdsp
+podpages=yes
+podpages_deps=perl
+postproc=no
+postproc_deps='avutil gpl'
+postproc_suggest=libm
+power8=yes
+power8_deps=vsx
+pp7_filter=no
+pp7_filter_deps=gpl
+pp_filter=no
+pp_filter_deps='gpl postproc'
+ppc4xx=yes
+ppc4xx_deps=ppc
+prefix=./install
+prefix_default=/usr/local
+premultiply_filter=no
+prewitt_filter=no
+prewitt_opencl_filter=no
+prewitt_opencl_filter_deps=opencl
+procamp_vaapi_filter=no
+procamp_vaapi_filter_deps=vaapi
+program_opencl_filter=no
+program_opencl_filter_deps=opencl
+prores_decoder_select='blockdsp idctdsp'
+prores_encoder_select=fdctdsp
+protocols_if_any='async_protocol
+bluray_protocol
+cache_protocol
+concat_protocol
+crypto_protocol
+data_protocol
+ffrtmpcrypt_protocol
+ffrtmphttp_protocol
+file_protocol
+ftp_protocol
+gopher_protocol
+hls_protocol
+http_protocol
+httpproxy_protocol
+https_protocol
+icecast_protocol
+mmsh_protocol
+mmst_protocol
+md5_protocol
+pipe_protocol
+prompeg_protocol
+rtmp_protocol
+rtmpe_protocol
+rtmps_protocol
+rtmpt_protocol
+rtmpte_protocol
+rtmpts_protocol
+rtp_protocol
+sctp_protocol
+srtp_protocol
+subfile_protocol
+tee_protocol
+tcp_protocol
+tls_protocol
+udp_protocol
+udplite_protocol
+unix_protocol
+librtmp_protocol
+librtmpe_protocol
+librtmps_protocol
+librtmpt_protocol
+librtmpte_protocol
+libsrt_protocol
+libssh_protocol
+libsmbclient_protocol'
+pseudocolor_filter=no
+psnr_filter=no
+psp_muxer_select=mov_muxer
+pthreads=no
+pullup_filter=no
+pullup_filter_deps=gpl
+pulse_indev=no
+pulse_indev_deps=libpulse
+pulse_outdev=no
+pulse_outdev_deps=libpulse
+qcelp_decoder_select=lsp
+qdm2_at_decoder_deps=audiotoolbox
+qdm2_decoder_select='mdct rdft mpegaudiodsp'
+qdmc_at_decoder_deps=audiotoolbox
+qp_filter=no
+qsv_deps=libmfx
+qsvdec_example=yes
+qsvdec_example_deps='avcodec avutil libmfx h264_qsv_decoder'
+qsvdec_select=qsv
+qsvenc_select=qsv
+qsvvpp_select=qsv
+r=arm-himix100-linux-
+ra_144_decoder_select=audiodsp
+ra_144_encoder_select='audio_frame_queue lpc audiodsp'
+ralf_decoder_select=golomb
+random_filter=no
+ranlib_default=ranlib
+rasc_decoder_deps=zlib
+rawvideo_decoder_select=bswapdsp
+rdft_select=fft
+readeia608_filter=no
+readvitc_filter=no
+realtime_filter=no
+remap_filter=no
+removegrain_filter=no
+removelogo_filter=no
+removelogo_filter_deps='avcodec avformat swscale'
+remuxing_example=yes
+remuxing_example_deps='avcodec avformat avutil'
+repeatfields_filter=no
+repeatfields_filter_deps=gpl
+replaygain_filter=no
+resample_filter=no
+resample_filter_deps=avresample
+resampling_audio_example=yes
+resampling_audio_example_deps='avutil swresample'
+reset_color='(B[m'
+reverse_filter=no
+rgbtestsrc_filter=no
+rkmpp=no
+roberts_filter=no
+roberts_opencl_filter=no
+roberts_opencl_filter_deps=opencl
+rotate_filter=no
+rscc_decoder_deps=zlib
+rtjpeg_decoder_select=me_cmp
+rtmp_protocol_conflict=librtmp_protocol
+rtmp_protocol_select=tcp_protocol
+rtmp_protocol_suggest=zlib
+rtmpe_protocol_select=ffrtmpcrypt_protocol
+rtmpe_protocol_suggest=zlib
+rtmps_protocol_conflict=librtmp_protocol
+rtmps_protocol_select=tls_protocol
+rtmps_protocol_suggest=zlib
+rtmpt_protocol_select=ffrtmphttp_protocol
+rtmpt_protocol_suggest=zlib
+rtmpte_protocol_select='ffrtmpcrypt_protocol ffrtmphttp_protocol'
+rtmpte_protocol_suggest=zlib
+rtmpts_protocol_select='ffrtmphttp_protocol https_protocol'
+rtmpts_protocol_suggest=zlib
+rtp_demuxer_select=sdp_demuxer
+rtp_muxer_select=golomb
+rtp_protocol_select=udp_protocol
+rtpdec_select='asf_demuxer jpegtables mov_demuxer mpegts_demuxer rm_demuxer rtp_protocol srtp'
+rtsp_demuxer_select='http_protocol rtpdec'
+rtsp_muxer_select='rtp_muxer http_protocol rtp_protocol rtpenc_chain'
+rubberband_filter=no
+rubberband_filter_deps=librubberband
+runtime_cpudetect=no
+rv10_decoder_select=h263_decoder
+rv10_encoder_select=h263_encoder
+rv20_decoder_select=h263_decoder
+rv20_encoder_select=h263_encoder
+rv30_decoder_select='golomb h264pred h264qpel mpegvideo rv34dsp'
+rv40_decoder_select='golomb h264pred h264qpel mpegvideo rv34dsp'
+sab_filter=no
+sab_filter_deps='gpl swscale'
+safe_bitstream_reader=yes
+sap_demuxer_select=sdp_demuxer
+sap_muxer_select='rtp_muxer rtp_protocol rtpenc_chain'
+scale2ref_filter=no
+scale2ref_filter_deps=swscale
+scale_cuda_filter=no
+scale_cuda_filter_deps=cuda_sdk
+scale_filter=no
+scale_filter_deps=swscale
+scale_npp_filter=no
+scale_npp_filter_deps='ffnvcodec libnpp'
+scale_qsv_filter=no
+scale_qsv_filter_deps=libmfx
+scale_vaapi_filter=no
+scale_vaapi_filter_deps=vaapi
+scaling_video_example=yes
+scaling_video_example_deps='avutil swscale'
+schannel=yes
+schannel_conflict='openssl gnutls libtls mbedtls'
+screenpresso_decoder_deps=zlib
+sctp_protocol_deps='struct_sctp_event_subscribe struct_msghdr_msg_flags'
+sctp_protocol_select=network
+sdl2=yes
+sdl2_outdev=no
+sdl2_outdev_deps=sdl2
+sdp_demuxer_select=rtpdec
+securetransport=yes
+securetransport_conflict='openssl gnutls libtls mbedtls'
+select_filter=no
+select_filter_select=pixelutils
+selectivecolor_filter=no
+sendcmd_filter=no
+separatefields_filter=no
+setdar_filter=no
+setend=yes
+setend_deps=arm
+setend_inline_deps=inline_asm
+setfield_filter=no
+setparams_filter=no
+setpts_filter=no
+setrange_filter=no
+setsar_filter=no
+settb_filter=no
+shared=no
+sharpness_vaapi_filter=no
+sharpness_vaapi_filter_deps=vaapi
+shorten_decoder_select=bswapdsp
+showcqt_filter=no
+showcqt_filter_deps='avcodec avformat swscale'
+showcqt_filter_select=fft
+showcqt_filter_suggest='libfontconfig libfreetype'
+showfreqs_filter=no
+showfreqs_filter_deps=avcodec
+showfreqs_filter_select=fft
+showinfo_filter=no
+showpalette_filter=no
+showspectrum_filter=no
+showspectrum_filter_deps=avcodec
+showspectrum_filter_select=fft
+showspectrumpic_filter=no
+showspectrumpic_filter_deps=avcodec
+showspectrumpic_filter_select=fft
+showvolume_filter=no
+showwaves_filter=no
+showwavespic_filter=no
+shuffleframes_filter=no
+shuffleplanes_filter=no
+sidechaincompress_filter=no
+sidechaingate_filter=no
+sidedata_filter=no
+signalstats_filter=no
+signature_filter=no
+signature_filter_deps='gpl avcodec avformat'
+silencedetect_filter=no
+silenceremove_filter=no
+simd_align_16_if_any='altivec neon sse'
+simd_align_32_if_any=avx
+simd_align_64_if_any=avx512
+sinc_filter=no
+sine_filter=no
+sipr_decoder_select=lsp
+small=yes
+smartblur_filter=no
+smartblur_filter_deps='gpl swscale'
+smoothstreaming_muxer_select=ismv_muxer
+smptebars_filter=no
+smptehdbars_filter=no
+sndio=yes
+sndio_indev=no
+sndio_indev_deps=sndio
+sndio_outdev=no
+sndio_outdev_deps=sndio
+snow_decoder_select='dwt h264qpel hpeldsp me_cmp rangecoder videodsp'
+snow_encoder_select='dwt h264qpel hpeldsp me_cmp mpegvideoenc rangecoder'
+sobel_filter=no
+sobel_opencl_filter=no
+sobel_opencl_filter_deps=opencl
+sofalizer_filter=no
+sofalizer_filter_deps='libmysofa avcodec'
+sofalizer_filter_select=fft
+sonic_decoder_select='golomb rangecoder'
+sonic_encoder_select='golomb rangecoder'
+sonic_ls_encoder_select='golomb rangecoder'
+source_path=.
+sp5x_decoder_select=mjpeg_decoder
+spdif_demuxer_select=adts_header
+spdif_muxer_select=adts_header
+spectrumsynth_filter=no
+spectrumsynth_filter_deps=avcodec
+spectrumsynth_filter_select=fft
+speedhq_decoder_select=mpegvideo
+split_filter=no
+spp_filter=no
+spp_filter_deps='gpl avcodec'
+spp_filter_select='fft idctdsp fdctdsp me_cmp pixblockdsp'
+spx_muxer_select=ogg_muxer
+sr_filter=no
+sr_filter_deps='avformat swscale'
+sr_filter_select=dnn
+srgc_decoder_deps=zlib
+srtp_protocol_select='rtp_protocol srtp'
+sse=yes
+sse2=yes
+sse2_deps=sse
+sse2_external_deps=sse_external
+sse2_inline_deps=sse_inline
+sse2_suggest='sse2_external sse2_inline'
+sse3=yes
+sse3_deps=sse2
+sse3_external_deps=sse2_external
+sse3_inline_deps=sse2_inline
+sse3_suggest='sse3_external sse3_inline'
+sse4=yes
+sse42=yes
+sse42_deps=sse4
+sse42_external_deps=sse4_external
+sse42_inline_deps=sse4_inline
+sse42_suggest='sse42_external sse42_inline'
+sse4_deps=ssse3
+sse4_external_deps=ssse3_external
+sse4_inline_deps=ssse3_inline
+sse4_suggest='sse4_external sse4_inline'
+sse_deps=mmxext
+sse_external_deps=mmxext_external
+sse_inline_deps=mmxext_inline
+sse_suggest='sse_external sse_inline'
+ssim_filter=no
+ssse3=yes
+ssse3_deps=sse3
+ssse3_external_deps=sse3_external
+ssse3_inline_deps=sse3_inline
+ssse3_suggest='ssse3_external ssse3_inline'
+static=yes
+stereo3d_filter=no
+stereo3d_filter_deps=gpl
+stereotools_filter=no
+stereowiden_filter=no
+streamselect_filter=no
+strip_default=strip
+stripping=yes
+striptype=direct
+subtitles_filter=no
+subtitles_filter_deps='avformat avcodec libass'
+super2xsai_filter=no
+super2xsai_filter_deps=gpl
+superequalizer_filter=no
+surround_filter=no
+svq1_decoder_select=hpeldsp
+svq1_encoder_select='hpeldsp me_cmp mpegvideoenc'
+svq3_decoder_select='golomb h264dsp h264parse h264pred hpeldsp tpeldsp videodsp'
+svq3_decoder_suggest=zlib
+swaprect_filter=no
+swapuv_filter=no
+swf_demuxer_suggest=zlib
+swresample=no
+swresample_deps=avutil
+swresample_suggest='libm libsoxr'
+sws_max_filter_size=256
+sws_max_filter_size_default=256
+swscale=no
+swscale_alpha=yes
+swscale_deps=avutil
+swscale_suggest=libm
+symver_if_any='symver_asm_label symver_gnu_asm'
+tak_decoder_select=audiodsp
+tak_demuxer_select=tak_parser
+target_os=linux
+target_os_default=linux
+target_path='$(CURDIR)'
+tblend_filter=no
+tcp_protocol_select=network
+tdsc_decoder_deps=zlib
+tdsc_decoder_select=mjpeg_decoder
+telecine_filter=no
+testsrc2_filter=no
+testsrc_filter=no
+tg2_muxer_select=mov_muxer
+tgp_muxer_select=mov_muxer
+theora_decoder_select=vp3_decoder
+thing=demuxer
+thp_decoder_select=mjpeg_decoder
+threads_if_any='
+    pthreads
+    os2threads
+    w32threads
+'
+threshold_filter=no
+thumbnail_cuda_filter=no
+thumbnail_cuda_filter_deps=cuda_sdk
+thumbnail_filter=no
+tiff_decoder_suggest='zlib lzma'
+tiff_encoder_suggest=zlib
+tile_filter=no
+tinterlace_filter=no
+tinterlace_filter_deps=gpl
+tinterlace_merge_test_deps=tinterlace_filter
+tinterlace_pad_test_deps=tinterlace_filter
+tls_protocol_deps_any='gnutls openssl schannel securetransport libtls mbedtls'
+tls_protocol_select=tcp_protocol
+tlut2_filter=no
+tmix_filter=no
+tonemap_filter=no
+tonemap_filter_deps=const_nan
+tonemap_opencl_filter=no
+tonemap_opencl_filter_deps='opencl const_nan'
+trace_headers_bsf_select=cbs
+transcode_aac_example=yes
+transcode_aac_example_deps='avcodec avformat swresample'
+transcoding_example=yes
+transcoding_example_deps='avfilter avcodec avformat avutil'
+transpose_filter=no
+transpose_npp_filter=no
+transpose_npp_filter_deps='ffnvcodec libnpp'
+treble_filter=no
+tremolo_filter=no
+trim_filter=no
+truehd_decoder_select=mlp_parser
+truehd_encoder_select=lpc
+truemotion2_decoder_select=bswapdsp
+truespeech_decoder_select=bswapdsp
+tscc_decoder_deps=zlib
+twinvq_decoder_select='mdct lsp sinewin'
+txd_decoder_select=texturedsp
+txtpages=yes
+txtpages_deps='perl makeinfo'
+udp_protocol_select=network
+udplite_protocol_select=network
+unix_protocol_deps=sys_un_h
+unix_protocol_select=network
+unpremultiply_filter=no
+unsharp_filter=no
+unsharp_opencl_filter=no
+unsharp_opencl_filter_deps=opencl
+uspp_filter=no
+uspp_filter_deps='gpl avcodec'
+utvideo_decoder_select='bswapdsp llviddsp'
+utvideo_encoder_select='bswapdsp huffman llvidencdsp'
+v=avutil
+v4l2_indev=no
+v4l2_indev_deps_any='linux_videodev2_h sys_videoio_h'
+v4l2_indev_suggest=libv4l2
+v4l2_m2m=yes
+v4l2_m2m_deps='linux_videodev2_h sem_timedwait'
+v4l2_outdev=no
+v4l2_outdev_deps_any='linux_videodev2_h sys_videoio_h'
+v4l2_outdev_suggest=libv4l2
+vaapi=yes
+vaapi_encode_deps=vaapi
+vaapi_encode_example=yes
+vaapi_encode_example_deps='avcodec avutil h264_vaapi_encoder'
+vaapi_transcode_example=yes
+vaapi_transcode_example_deps='avcodec avformat avutil h264_vaapi_encoder'
+vaguedenoiser_filter=no
+vaguedenoiser_filter_deps=gpl
+valgrind_backtrace=yes
+valgrind_backtrace_conflict=optimizations
+valgrind_backtrace_deps=valgrind_valgrind_h
+value=no
+vapoursynth=no
+vapoursynth_demuxer_deps=vapoursynth
+var=fifo_filter
+vble_decoder_select=llviddsp
+vc1_crystalhd_decoder_select=crystalhd
+vc1_cuvid_decoder_deps=cuvid
+vc1_d3d11va2_hwaccel_deps=d3d11va
+vc1_d3d11va2_hwaccel_select=vc1_decoder
+vc1_d3d11va_hwaccel_deps=d3d11va
+vc1_d3d11va_hwaccel_select=vc1_decoder
+vc1_decoder_select='blockdsp h263_decoder h264qpel intrax8 mpegvideo vc1dsp'
+vc1_dxva2_hwaccel_deps=dxva2
+vc1_dxva2_hwaccel_select=vc1_decoder
+vc1_mmal_decoder_deps=mmal
+vc1_nvdec_hwaccel_deps=nvdec
+vc1_nvdec_hwaccel_select=vc1_decoder
+vc1_parser_select=vc1dsp
+vc1_qsv_decoder_select='qsvdec vc1_parser'
+vc1_v4l2m2m_decoder_deps='v4l2_m2m vc1_v4l2_m2m'
+vc1_vaapi_hwaccel_deps=vaapi
+vc1_vaapi_hwaccel_select=vc1_decoder
+vc1_vdpau_hwaccel_deps=vdpau
+vc1_vdpau_hwaccel_select=vc1_decoder
+vc1dsp_select='h264chroma qpeldsp startcode'
+vc1image_decoder_select=vc1_decoder
+vdpau=yes
+vectorscope_filter=no
+version_script=--version-script
+vflip_filter=no
+vfp=no
+vfp_deps_any='aarch64 arm'
+vfp_inline_deps=inline_asm
+vfpv3=yes
+vfpv3_deps=vfp
+vfpv3_inline_deps=inline_asm
+vfrdet_filter=no
+vfwcap_indev=no
+vfwcap_indev_deps='vfw32 vfwcap_defines'
+vibrance_filter=no
+vibrato_filter=no
+videotoolbox=yes
+videotoolbox_deps='corefoundation coremedia corevideo'
+videotoolbox_encoder_deps='videotoolbox VTCompressionSessionPrepareToEncodeFrames'
+videotoolbox_hwaccel_deps='videotoolbox pthreads'
+videotoolbox_hwaccel_extralibs='-framework QuartzCore'
+videotoolbox_suggest=coreservices
+vidstabdetect_filter=no
+vidstabdetect_filter_deps=libvidstab
+vidstabtransform_filter=no
+vidstabtransform_filter_deps=libvidstab
+vignette_filter=no
+vmafmotion_filter=no
+vobsub_demuxer_select=mpegps_demuxer
+volume_filter=no
+volumedetect_filter=no
+vorbis_decoder_select=mdct
+vorbis_encoder_select='audio_frame_queue mdct'
+vp3_decoder_select='hpeldsp vp3dsp videodsp'
+vp5_decoder_select='h264chroma hpeldsp videodsp vp3dsp vp56dsp'
+vp6_decoder_select='h264chroma hpeldsp huffman videodsp vp3dsp vp56dsp'
+vp6a_decoder_select=vp6_decoder
+vp6f_decoder_select=vp6_decoder
+vp7_decoder_select='h264pred videodsp vp8dsp'
+vp8_cuvid_decoder_deps=cuvid
+vp8_decoder_select='h264pred videodsp vp8dsp'
+vp8_mediacodec_decoder_deps=mediacodec
+vp8_nvdec_hwaccel_deps=nvdec
+vp8_nvdec_hwaccel_select=vp8_decoder
+vp8_qsv_decoder_select='qsvdec vp8_parser'
+vp8_rkmpp_decoder_deps=rkmpp
+vp8_v4l2m2m_decoder_deps='v4l2_m2m vp8_v4l2_m2m'
+vp8_v4l2m2m_encoder_deps='v4l2_m2m vp8_v4l2_m2m'
+vp8_vaapi_encoder_deps=VAEncPictureParameterBufferVP8
+vp8_vaapi_encoder_select=vaapi_encode
+vp8_vaapi_hwaccel_deps=vaapi
+vp8_vaapi_hwaccel_select=vp8_decoder
+vp9_cuvid_decoder_deps=cuvid
+vp9_d3d11va2_hwaccel_deps='d3d11va DXVA_PicParams_VP9'
+vp9_d3d11va2_hwaccel_select=vp9_decoder
+vp9_d3d11va_hwaccel_deps='d3d11va DXVA_PicParams_VP9'
+vp9_d3d11va_hwaccel_select=vp9_decoder
+vp9_decoder_select='videodsp vp9_parser vp9_superframe_split_bsf'
+vp9_dxva2_hwaccel_deps='dxva2 DXVA_PicParams_VP9'
+vp9_dxva2_hwaccel_select=vp9_decoder
+vp9_mediacodec_decoder_deps=mediacodec
+vp9_metadata_bsf_select=cbs_vp9
+vp9_nvdec_hwaccel_deps=nvdec
+vp9_nvdec_hwaccel_select=vp9_decoder
+vp9_rkmpp_decoder_deps=rkmpp
+vp9_v4l2m2m_decoder_deps='v4l2_m2m vp9_v4l2_m2m'
+vp9_vaapi_encoder_deps=VAEncPictureParameterBufferVP9
+vp9_vaapi_encoder_select=vaapi_encode
+vp9_vaapi_hwaccel_deps='vaapi VADecPictureParameterBufferVP9_bit_depth'
+vp9_vaapi_hwaccel_select=vp9_decoder
+vpp_qsv_filter=no
+vpp_qsv_filter_deps=libmfx
+vpp_qsv_filter_select=qsvvpp
+vstack_filter=no
+vsx=yes
+vsx_deps=altivec
+w32threads_deps=atomics_native
+w3fdif_filter=no
+w64_demuxer_select=wav_demuxer
+w64_muxer_select=wav_muxer
+warn_color='[33m'
+wav_demuxer=yes
+wav_demuxer_select=riffdec
+wav_muxer_select=riffenc
+waveform_filter=no
+wcmv_decoder_deps=zlib
+weave_filter=no
+webm_dash_manifest_demuxer_select=matroska_demuxer
+webm_muxer_select='iso_media riffenc'
+webp_decoder_select='vp8_decoder exif'
+windres_default=windres
+wmalossless_decoder_select=llauddsp
+wmapro_decoder_select='mdct sinewin wma_freqs'
+wmav1_decoder_select='mdct sinewin wma_freqs'
+wmav1_encoder_select='mdct sinewin wma_freqs'
+wmav2_decoder_select='mdct sinewin wma_freqs'
+wmav2_encoder_select='mdct sinewin wma_freqs'
+wmavoice_decoder_select='lsp rdft dct mdct sinewin'
+wmv1_decoder_select=h263_decoder
+wmv1_encoder_select=h263_encoder
+wmv2_decoder_select='blockdsp error_resilience h263_decoder idctdsp intrax8 videodsp wmv2dsp'
+wmv2_encoder_select='h263_encoder wmv2dsp'
+wmv3_crystalhd_decoder_select=crystalhd
+wmv3_d3d11va2_hwaccel_select=vc1_d3d11va2_hwaccel
+wmv3_d3d11va_hwaccel_select=vc1_d3d11va_hwaccel
+wmv3_decoder_select=vc1_decoder
+wmv3_dxva2_hwaccel_select=vc1_dxva2_hwaccel
+wmv3_nvdec_hwaccel_select=vc1_nvdec_hwaccel
+wmv3_vaapi_hwaccel_select=vc1_vaapi_hwaccel
+wmv3_vdpau_hwaccel_select=vc1_vdpau_hwaccel
+wmv3image_decoder_select=wmv3_decoder
+wtv_demuxer_select='mpegts_demuxer riffdec'
+wtv_muxer_select='mpegts_muxer riffenc'
+x86_64_select=i686
+x86_64_suggest=fast_cmov
+x86asmexe_default=nasm
+xbr_filter=no
+xcbgrab_indev=no
+xcbgrab_indev_deps=libxcb
+xcbgrab_indev_suggest='libxcb_shm libxcb_shape libxcb_xfixes'
+xlib=yes
+xma1_decoder_select=wmapro_decoder
+xma2_decoder_select=wmapro_decoder
+xmv_demuxer_select=riffdec
+xop=yes
+xop_deps=avx
+xop_external_deps=avx_external
+xop_inline_deps=avx_inline
+xop_suggest='xop_external xop_inline'
+xstack_filter=no
+xv_outdev=no
+xv_outdev_deps=xlib
+xvmc=yes
+xvmc_deps=X11_extensions_XvMClib_h
+xwma_demuxer_select=riffdec
+yadif_cuda_filter=no
+yadif_cuda_filter_deps=cuda_sdk
+yadif_filter=no
+yuvtestsrc_filter=no
+zerocodec_decoder_deps=zlib
+zlib=yes
+zlib_decoder_deps=zlib
+zlib_encoder_deps=zlib
+zmbv_decoder_deps=zlib
+zmbv_encoder_deps=zlib
+zmq_filter=no
+zmq_filter_deps=libzmq
+zoompan_filter=no
+zoompan_filter_deps=swscale
+zscale_filter=no
+zscale_filter_deps='libzimg const_nan'
+WARNING: arm-himix100-linux-pkg-config not found, library detection may fail.
+mktemp -u XXXXXX
+PeF0LS
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	int main(void){ return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o
+check_cxxflags -std=c++11
+test_cxx -std=c++11
+BEGIN /tmp/ffconf.iiRrWAGJ/test.cpp
+    1	int x;
+END /tmp/ffconf.iiRrWAGJ/test.cpp
+arm-himix100-linux-g++ -D_ISOC99_SOURCE -mcpu=cortex-a7 -D__STDC_CONSTANT_MACROS -std=c++11 -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.cpp
+test_cflags_cc -std=c11 ctype.h __STDC_VERSION__ >= 201112L
+test_cc -std=c11
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <ctype.h>
+    2	#if !(__STDC_VERSION__ >= 201112L)
+    3	#error "unsatisfied condition: __STDC_VERSION__ >= 201112L"
+    4	#endif
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -mcpu=cortex-a7 -std=c11 -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_cppflags -D_FILE_OFFSET_BITS=64
+test_cpp -D_FILE_OFFSET_BITS=64
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <stdlib.h>
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -mcpu=cortex-a7 -std=c11 -D_FILE_OFFSET_BITS=64 -E -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_cppflags -D_LARGEFILE_SOURCE
+test_cpp -D_LARGEFILE_SOURCE
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <stdlib.h>
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -mcpu=cortex-a7 -std=c11 -D_LARGEFILE_SOURCE -E -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_host_cflags -std=c99
+test_host_cc -std=c99
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+gcc -std=c99 -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_host_cflags -Wall
+test_host_cc -Wall
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+gcc -std=c99 -Wall -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_host_cflags -O3
+test_host_cc -O3
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+gcc -std=c99 -Wall -O3 -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+test_cpp_condition features.h defined __UCLIBC__
+test_cpp
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <features.h>
+    2	#if !(defined __UCLIBC__)
+    3	#error "unsatisfied condition: defined __UCLIBC__"
+    4	#endif
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -mcpu=cortex-a7 -std=c11 -E -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <time.h>
+    2	void *v = localtime_r;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -mcpu=cortex-a7 -std=c11 -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+test_host_cpp_condition features.h defined __UCLIBC__
+test_host_cpp
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <features.h>
+    2	#if !(defined __UCLIBC__)
+    3	#error "unsatisfied condition: defined __UCLIBC__"
+    4	#endif
+END /tmp/ffconf.iiRrWAGJ/test.c
+gcc -D_ISOC99_SOURCE -std=c99 -Wall -O3 -E -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:3:2: error: #error "unsatisfied condition: defined __UCLIBC__"
+ #error "unsatisfied condition: defined __UCLIBC__"
+  ^
+test_host_cpp_condition features.h defined __GLIBC__
+test_host_cpp
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <features.h>
+    2	#if !(defined __GLIBC__)
+    3	#error "unsatisfied condition: defined __GLIBC__"
+    4	#endif
+END /tmp/ffconf.iiRrWAGJ/test.c
+gcc -D_ISOC99_SOURCE -std=c99 -Wall -O3 -E -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+test_host_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <time.h>
+    2	void *v = localtime_r;
+END /tmp/ffconf.iiRrWAGJ/test.c
+gcc -std=c99 -Wall -O3 -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:2:11: error: 'localtime_r' undeclared here (not in a function)
+ void *v = localtime_r;
+           ^
+test_host_cc -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <time.h>
+    2	void *v = localtime_r;
+END /tmp/ffconf.iiRrWAGJ/test.c
+gcc -std=c99 -Wall -O3 -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_compile_assert flt_lim float.h limits.h DBL_MAX == (double)DBL_MAX
+test_code cc float.h limits.h char c[2 * !!(DBL_MAX == (double)DBL_MAX) - 1]
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <float.h>
+    2	#include <limits.h>
+    3	int main(void) { char c[2 * !!(DBL_MAX == (double)DBL_MAX) - 1]; return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -mcpu=cortex-a7 -std=c11 -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+test_cpp_condition stdlib.h defined(__PIC__) || defined(__pic__) || defined(PIC)
+test_cpp
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <stdlib.h>
+    2	#if !(defined(__PIC__) || defined(__pic__) || defined(PIC))
+    3	#error "unsatisfied condition: defined(__PIC__) || defined(__pic__) || defined(PIC)"
+    4	#endif
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -mcpu=cortex-a7 -std=c11 -E -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:3:2: error: #error "unsatisfied condition: defined(__PIC__) || defined(__pic__) || defined(PIC)"
+ #error "unsatisfied condition: defined(__PIC__) || defined(__pic__) || defined(PIC)"
+  ^~~~~
+check_cflags -fomit-frame-pointer
+test_cflags -fomit-frame-pointer
+test_cc -fomit-frame-pointer
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	int ff_extern;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+test_code cc  char * restrict p
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	int main(void) { char * restrict p; return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_cc pragma_deprecated  _Pragma("GCC diagnostic ignored \"-Wdeprecated-declarations\"")
+test_code cc  _Pragma("GCC diagnostic ignored \"-Wdeprecated-declarations\"")
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	int main(void) { _Pragma("GCC diagnostic ignored \"-Wdeprecated-declarations\""); return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	unsigned int endian = 'B' << 24 | 'I' << 16 | 'G' << 8 | 'E';
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_cc const_nan math.h struct { double d; } static const bar[] = { { NAN } }
+test_code cc math.h struct { double d; } static const bar[] = { { NAN } }
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <math.h>
+    2	int main(void) { struct { double d; } static const bar[] = { { NAN } }; return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_inline_asm inline_asm_labels "1:\n"
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	void foo(void){ __asm__ volatile("1:\n"); }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_inline_asm inline_asm_nonlocal_labels "Label:\n"
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	void foo(void){ __asm__ volatile("Label:\n"); }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+test_cpp_condition stddef.h defined __thumb__
+test_cpp
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <stddef.h>
+    2	#if !(defined __thumb__)
+    3	#error "unsatisfied condition: defined __thumb__"
+    4	#endif
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -E -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:3:2: error: #error "unsatisfied condition: defined __thumb__"
+ #error "unsatisfied condition: defined __thumb__"
+  ^~~~~
+check_cflags -marm
+test_cflags -marm
+test_cc -marm
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_cpp_condition vfp_args stddef.h defined __ARM_PCS_VFP
+test_cpp_condition stddef.h defined __ARM_PCS_VFP
+test_cpp
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <stddef.h>
+    2	#if !(defined __ARM_PCS_VFP)
+    3	#error "unsatisfied condition: defined __ARM_PCS_VFP"
+    4	#endif
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -E -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:3:2: error: #error "unsatisfied condition: defined __ARM_PCS_VFP"
+ #error "unsatisfied condition: defined __ARM_PCS_VFP"
+  ^~~~~
+check_cpp_condition vfp_args stddef.h defined _M_ARM_FP && _M_ARM_FP >= 30
+test_cpp_condition stddef.h defined _M_ARM_FP && _M_ARM_FP >= 30
+test_cpp
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <stddef.h>
+    2	#if !(defined _M_ARM_FP && _M_ARM_FP >= 30)
+    3	#error "unsatisfied condition: defined _M_ARM_FP && _M_ARM_FP >= 30"
+    4	#endif
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -E -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:3:2: error: #error "unsatisfied condition: defined _M_ARM_FP && _M_ARM_FP >= 30"
+ #error "unsatisfied condition: defined _M_ARM_FP && _M_ARM_FP >= 30"
+  ^~~~~
+test_cpp_condition stddef.h defined __ARM_PCS || defined __SOFTFP__
+test_cpp
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <stddef.h>
+    2	#if !(defined __ARM_PCS || defined __SOFTFP__)
+    3	#error "unsatisfied condition: defined __ARM_PCS || defined __SOFTFP__"
+    4	#endif
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -E -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_insn vfpv3 vmov.f32 s0, #1.0
+check_inline_asm vfpv3_inline "vmov.f32 s0, #1.0"
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	void foo(void){ __asm__ volatile("vmov.f32 s0, #1.0"); }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ccoiugTj.s: Assembler messages:
+/tmp/ccoiugTj.s:25: Error: selected processor does not support `vmov.f32 s0,#1.0' in ARM mode
+check_as vfpv3_external vmov.f32 s0, #1.0
+test_as
+BEGIN /tmp/ffconf.iiRrWAGJ/test.S
+    1	vmov.f32 s0, #1.0
+END /tmp/ffconf.iiRrWAGJ/test.S
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -fPIC -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.S
+/tmp/ffconf.iiRrWAGJ/test.S: Assembler messages:
+/tmp/ffconf.iiRrWAGJ/test.S:1: Error: selected processor does not support `vmov.f32 s0,#1.0' in ARM mode
+check_insn setend setend be
+check_inline_asm setend_inline "setend be"
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	void foo(void){ __asm__ volatile("setend be"); }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_as setend_external setend be
+test_as
+BEGIN /tmp/ffconf.iiRrWAGJ/test.S
+    1	setend be
+END /tmp/ffconf.iiRrWAGJ/test.S
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -fPIC -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.S
+check_inline_asm asm_mod_q "add r0, %Q0, %R0" :: "r"((long long)0)
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	void foo(void){ __asm__ volatile("add r0, %Q0, %R0" :: "r"((long long)0)); }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_as as_arch_directive .arch armv7-a
+test_as
+BEGIN /tmp/ffconf.iiRrWAGJ/test.S
+    1	.arch armv7-a
+END /tmp/ffconf.iiRrWAGJ/test.S
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -fPIC -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.S
+check_as as_dn_directive ra .dn d0.i16
+test_as
+BEGIN /tmp/ffconf.iiRrWAGJ/test.S
+    1	ra .dn d0.i16
+END /tmp/ffconf.iiRrWAGJ/test.S
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -fPIC -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.S
+check_as as_fpu_directive .fpu neon
+test_as
+BEGIN /tmp/ffconf.iiRrWAGJ/test.S
+    1	.fpu neon
+END /tmp/ffconf.iiRrWAGJ/test.S
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -fPIC -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.S
+check_as as_object_arch .object_arch armv4
+test_as
+BEGIN /tmp/ffconf.iiRrWAGJ/test.S
+    1	.object_arch armv4
+END /tmp/ffconf.iiRrWAGJ/test.S
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -fPIC -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.S
+check_cc intrinsics_neon arm_neon.h int16x8_t test = vdupq_n_s16(0)
+test_code cc arm_neon.h int16x8_t test = vdupq_n_s16(0)
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <arm_neon.h>
+    2	int main(void) { int16x8_t test = vdupq_n_s16(0); return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+In file included from /tmp/ffconf.iiRrWAGJ/test.c:1:0:
+/opt/hisi-linux/x86-arm/arm-himix100-linux/lib/gcc/arm-linux-uclibceabi/6.3.0/include/arm_neon.h:31:2: error: #error "NEON intrinsics not available with the soft-float ABI.  Please use -mfloat-abi=softp or -mfloat-abi=hard"
+ #error "NEON intrinsics not available with the soft-float ABI.  Please use -mfloat-abi=softp or -mfloat-abi=hard"
+  ^~~~~
+/tmp/ffconf.iiRrWAGJ/test.c: In function 'main':
+/tmp/ffconf.iiRrWAGJ/test.c:2:18: error: unknown type name 'int16x8_t'
+ int main(void) { int16x8_t test = vdupq_n_s16(0); return 0; }
+                  ^~~~~~~~~
+/tmp/ffconf.iiRrWAGJ/test.c:2:35: warning: implicit declaration of function 'vdupq_n_s16' [-Wimplicit-function-declaration]
+ int main(void) { int16x8_t test = vdupq_n_s16(0); return 0; }
+                                   ^~~~~~~~~~~
+check_ldflags -Wl,--as-needed
+test_ldflags -Wl,--as-needed
+test_ld cc -Wl,--as-needed
+test_cc -Wl,--as-needed
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	int main(void){ return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -Wl,--as-needed -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o
+check_ldflags -Wl,-z,noexecstack
+test_ldflags -Wl,-z,noexecstack
+test_ld cc -Wl,-z,noexecstack
+test_cc -Wl,-z,noexecstack
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	int main(void){ return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -Wl,-z,noexecstack -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o
+check_builtin atomic_cas_ptr atomic.h void **ptr; void *oldval, *newval; atomic_cas_ptr(ptr, oldval, newval)
+test_code ld atomic.h void **ptr; void *oldval, *newval; atomic_cas_ptr(ptr, oldval, newval) cc
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <atomic.h>
+    2	int main(void) { void **ptr; void *oldval, *newval; atomic_cas_ptr(ptr, oldval, newval); return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+In file included from /opt/hisi-linux/x86-arm/arm-himix100-linux/target/usr/include/atomic.h:51:0,
+                 from /tmp/ffconf.iiRrWAGJ/test.c:1:
+/opt/hisi-linux/x86-arm/arm-himix100-linux/target/usr/include/bits/atomic.h:23:20: fatal error: sysdep.h: No such file or directory
+ #include <sysdep.h>
+                    ^
+compilation terminated.
+check_builtin machine_rw_barrier mbarrier.h __machine_rw_barrier()
+test_code ld mbarrier.h __machine_rw_barrier() cc
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <mbarrier.h>
+    2	int main(void) { __machine_rw_barrier(); return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:22: fatal error: mbarrier.h: No such file or directory
+ #include <mbarrier.h>
+                      ^
+compilation terminated.
+check_builtin MemoryBarrier windows.h MemoryBarrier()
+test_code ld windows.h MemoryBarrier() cc
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <windows.h>
+    2	int main(void) { MemoryBarrier(); return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:21: fatal error: windows.h: No such file or directory
+ #include <windows.h>
+                     ^
+compilation terminated.
+check_builtin sync_val_compare_and_swap  int *ptr; int oldval, newval; __sync_val_compare_and_swap(ptr, oldval, newval)
+test_code ld  int *ptr; int oldval, newval; __sync_val_compare_and_swap(ptr, oldval, newval) cc
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	int main(void) { int *ptr; int oldval, newval; __sync_val_compare_and_swap(ptr, oldval, newval); return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o
+check_builtin gmtime_r time.h time_t *time; struct tm *tm; gmtime_r(time, tm)
+test_code ld time.h time_t *time; struct tm *tm; gmtime_r(time, tm) cc
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <time.h>
+    2	int main(void) { time_t *time; struct tm *tm; gmtime_r(time, tm); return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o
+check_builtin localtime_r time.h time_t *time; struct tm *tm; localtime_r(time, tm)
+test_code ld time.h time_t *time; struct tm *tm; localtime_r(time, tm) cc
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <time.h>
+    2	int main(void) { time_t *time; struct tm *tm; localtime_r(time, tm); return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o
+check_builtin x264_csp_bgr stdint.h x264.h X264_CSP_BGR
+test_code ld stdint.h x264.h X264_CSP_BGR cc
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <stdint.h>
+    2	#include <x264.h>
+    3	int main(void) { X264_CSP_BGR; return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:2:18: fatal error: x264.h: No such file or directory
+ #include <x264.h>
+                  ^
+compilation terminated.
+check_func_headers malloc.h _aligned_malloc
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <malloc.h>
+    2	#include <stdint.h>
+    3	long check__aligned_malloc(void) { return (long) _aligned_malloc; }
+    4	int main(void) { int ret = 0;
+    5	 ret |= ((intptr_t)check__aligned_malloc) & 0xFFFF;
+    6	return ret; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c: In function 'check__aligned_malloc':
+/tmp/ffconf.iiRrWAGJ/test.c:3:50: error: '_aligned_malloc' undeclared (first use in this function)
+ long check__aligned_malloc(void) { return (long) _aligned_malloc; }
+                                                  ^~~~~~~~~~~~~~~
+/tmp/ffconf.iiRrWAGJ/test.c:3:50: note: each undeclared identifier is reported only once for each function it appears in
+check_func memalign
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	extern int memalign();
+    2	int main(void){ memalign(); }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o
+check_func posix_memalign
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	extern int posix_memalign();
+    2	int main(void){ posix_memalign(); }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o
+check_func access
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	extern int access();
+    2	int main(void){ access(); }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o
+check_func_headers stdlib.h arc4random
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <stdlib.h>
+    2	#include <stdint.h>
+    3	long check_arc4random(void) { return (long) arc4random; }
+    4	int main(void) { int ret = 0;
+    5	 ret |= ((intptr_t)check_arc4random) & 0xFFFF;
+    6	return ret; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c: In function 'check_arc4random':
+/tmp/ffconf.iiRrWAGJ/test.c:3:45: error: 'arc4random' undeclared (first use in this function)
+ long check_arc4random(void) { return (long) arc4random; }
+                                             ^~~~~~~~~~
+/tmp/ffconf.iiRrWAGJ/test.c:3:45: note: each undeclared identifier is reported only once for each function it appears in
+check_lib clock_gettime time.h clock_gettime
+check_func_headers time.h clock_gettime
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <time.h>
+    2	#include <stdint.h>
+    3	long check_clock_gettime(void) { return (long) clock_gettime; }
+    4	int main(void) { int ret = 0;
+    5	 ret |= ((intptr_t)check_clock_gettime) & 0xFFFF;
+    6	return ret; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o
+check_func fcntl
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	extern int fcntl();
+    2	int main(void){ fcntl(); }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o
+check_func fork
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	extern int fork();
+    2	int main(void){ fork(); }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o
+check_func gethrtime
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	extern int gethrtime();
+    2	int main(void){ gethrtime(); }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o
+/tmp/ffconf.iiRrWAGJ/test.o: In function `main':
+test.c:(.text+0x4): undefined reference to `gethrtime'
+collect2: error: ld returned 1 exit status
+check_func getopt
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	extern int getopt();
+    2	int main(void){ getopt(); }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o
+check_func getrusage
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	extern int getrusage();
+    2	int main(void){ getrusage(); }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o
+check_func gettimeofday
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	extern int gettimeofday();
+    2	int main(void){ gettimeofday(); }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o
+check_func isatty
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	extern int isatty();
+    2	int main(void){ isatty(); }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o
+check_func mkstemp
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	extern int mkstemp();
+    2	int main(void){ mkstemp(); }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o
+check_func mmap
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	extern int mmap();
+    2	int main(void){ mmap(); }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o
+check_func mprotect
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	extern int mprotect();
+    2	int main(void){ mprotect(); }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o
+check_func_headers time.h nanosleep
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <time.h>
+    2	#include <stdint.h>
+    3	long check_nanosleep(void) { return (long) nanosleep; }
+    4	int main(void) { int ret = 0;
+    5	 ret |= ((intptr_t)check_nanosleep) & 0xFFFF;
+    6	return ret; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o
+check_func sched_getaffinity
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	extern int sched_getaffinity();
+    2	int main(void){ sched_getaffinity(); }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o
+check_func setrlimit
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	extern int setrlimit();
+    2	int main(void){ setrlimit(); }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o
+check_struct sys/stat.h struct stat st_mtim.tv_nsec -D_BSD_SOURCE
+test_code cc sys/stat.h const void *p = &((struct stat *)0)->st_mtim.tv_nsec -D_BSD_SOURCE
+test_cc -D_BSD_SOURCE
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <sys/stat.h>
+    2	int main(void) { const void *p = &((struct stat *)0)->st_mtim.tv_nsec; return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -D_BSD_SOURCE -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_func strerror_r
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	extern int strerror_r();
+    2	int main(void){ strerror_r(); }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o
+check_func sysconf
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	extern int sysconf();
+    2	int main(void){ sysconf(); }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o
+check_func sysctl
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	extern int sysctl();
+    2	int main(void){ sysctl(); }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o
+check_func usleep
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	extern int usleep();
+    2	int main(void){ usleep(); }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o
+check_func_headers conio.h kbhit
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <conio.h>
+    2	#include <stdint.h>
+    3	long check_kbhit(void) { return (long) kbhit; }
+    4	int main(void) { int ret = 0;
+    5	 ret |= ((intptr_t)check_kbhit) & 0xFFFF;
+    6	return ret; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:19: fatal error: conio.h: No such file or directory
+ #include <conio.h>
+                   ^
+compilation terminated.
+check_func_headers io.h setmode
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <io.h>
+    2	#include <stdint.h>
+    3	long check_setmode(void) { return (long) setmode; }
+    4	int main(void) { int ret = 0;
+    5	 ret |= ((intptr_t)check_setmode) & 0xFFFF;
+    6	return ret; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:16: fatal error: io.h: No such file or directory
+ #include <io.h>
+                ^
+compilation terminated.
+check_func_headers lzo/lzo1x.h lzo1x_999_compress
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <lzo/lzo1x.h>
+    2	#include <stdint.h>
+    3	long check_lzo1x_999_compress(void) { return (long) lzo1x_999_compress; }
+    4	int main(void) { int ret = 0;
+    5	 ret |= ((intptr_t)check_lzo1x_999_compress) & 0xFFFF;
+    6	return ret; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:23: fatal error: lzo/lzo1x.h: No such file or directory
+ #include <lzo/lzo1x.h>
+                       ^
+compilation terminated.
+check_func_headers mach/mach_time.h mach_absolute_time
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <mach/mach_time.h>
+    2	#include <stdint.h>
+    3	long check_mach_absolute_time(void) { return (long) mach_absolute_time; }
+    4	int main(void) { int ret = 0;
+    5	 ret |= ((intptr_t)check_mach_absolute_time) & 0xFFFF;
+    6	return ret; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:28: fatal error: mach/mach_time.h: No such file or directory
+ #include <mach/mach_time.h>
+                            ^
+compilation terminated.
+check_func_headers stdlib.h getenv
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <stdlib.h>
+    2	#include <stdint.h>
+    3	long check_getenv(void) { return (long) getenv; }
+    4	int main(void) { int ret = 0;
+    5	 ret |= ((intptr_t)check_getenv) & 0xFFFF;
+    6	return ret; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o
+check_func_headers sys/stat.h lstat
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <sys/stat.h>
+    2	#include <stdint.h>
+    3	long check_lstat(void) { return (long) lstat; }
+    4	int main(void) { int ret = 0;
+    5	 ret |= ((intptr_t)check_lstat) & 0xFFFF;
+    6	return ret; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o
+check_func_headers windows.h GetProcessAffinityMask
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <windows.h>
+    2	#include <stdint.h>
+    3	long check_GetProcessAffinityMask(void) { return (long) GetProcessAffinityMask; }
+    4	int main(void) { int ret = 0;
+    5	 ret |= ((intptr_t)check_GetProcessAffinityMask) & 0xFFFF;
+    6	return ret; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:21: fatal error: windows.h: No such file or directory
+ #include <windows.h>
+                     ^
+compilation terminated.
+check_func_headers windows.h GetProcessTimes
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <windows.h>
+    2	#include <stdint.h>
+    3	long check_GetProcessTimes(void) { return (long) GetProcessTimes; }
+    4	int main(void) { int ret = 0;
+    5	 ret |= ((intptr_t)check_GetProcessTimes) & 0xFFFF;
+    6	return ret; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:21: fatal error: windows.h: No such file or directory
+ #include <windows.h>
+                     ^
+compilation terminated.
+check_func_headers windows.h GetSystemTimeAsFileTime
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <windows.h>
+    2	#include <stdint.h>
+    3	long check_GetSystemTimeAsFileTime(void) { return (long) GetSystemTimeAsFileTime; }
+    4	int main(void) { int ret = 0;
+    5	 ret |= ((intptr_t)check_GetSystemTimeAsFileTime) & 0xFFFF;
+    6	return ret; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:21: fatal error: windows.h: No such file or directory
+ #include <windows.h>
+                     ^
+compilation terminated.
+check_func_headers windows.h LoadLibrary
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <windows.h>
+    2	#include <stdint.h>
+    3	long check_LoadLibrary(void) { return (long) LoadLibrary; }
+    4	int main(void) { int ret = 0;
+    5	 ret |= ((intptr_t)check_LoadLibrary) & 0xFFFF;
+    6	return ret; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:21: fatal error: windows.h: No such file or directory
+ #include <windows.h>
+                     ^
+compilation terminated.
+check_func_headers windows.h MapViewOfFile
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <windows.h>
+    2	#include <stdint.h>
+    3	long check_MapViewOfFile(void) { return (long) MapViewOfFile; }
+    4	int main(void) { int ret = 0;
+    5	 ret |= ((intptr_t)check_MapViewOfFile) & 0xFFFF;
+    6	return ret; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:21: fatal error: windows.h: No such file or directory
+ #include <windows.h>
+                     ^
+compilation terminated.
+check_func_headers windows.h PeekNamedPipe
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <windows.h>
+    2	#include <stdint.h>
+    3	long check_PeekNamedPipe(void) { return (long) PeekNamedPipe; }
+    4	int main(void) { int ret = 0;
+    5	 ret |= ((intptr_t)check_PeekNamedPipe) & 0xFFFF;
+    6	return ret; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:21: fatal error: windows.h: No such file or directory
+ #include <windows.h>
+                     ^
+compilation terminated.
+check_func_headers windows.h SetConsoleTextAttribute
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <windows.h>
+    2	#include <stdint.h>
+    3	long check_SetConsoleTextAttribute(void) { return (long) SetConsoleTextAttribute; }
+    4	int main(void) { int ret = 0;
+    5	 ret |= ((intptr_t)check_SetConsoleTextAttribute) & 0xFFFF;
+    6	return ret; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:21: fatal error: windows.h: No such file or directory
+ #include <windows.h>
+                     ^
+compilation terminated.
+check_func_headers windows.h SetConsoleCtrlHandler
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <windows.h>
+    2	#include <stdint.h>
+    3	long check_SetConsoleCtrlHandler(void) { return (long) SetConsoleCtrlHandler; }
+    4	int main(void) { int ret = 0;
+    5	 ret |= ((intptr_t)check_SetConsoleCtrlHandler) & 0xFFFF;
+    6	return ret; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:21: fatal error: windows.h: No such file or directory
+ #include <windows.h>
+                     ^
+compilation terminated.
+check_func_headers windows.h Sleep
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <windows.h>
+    2	#include <stdint.h>
+    3	long check_Sleep(void) { return (long) Sleep; }
+    4	int main(void) { int ret = 0;
+    5	 ret |= ((intptr_t)check_Sleep) & 0xFFFF;
+    6	return ret; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:21: fatal error: windows.h: No such file or directory
+ #include <windows.h>
+                     ^
+compilation terminated.
+check_func_headers windows.h VirtualAlloc
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <windows.h>
+    2	#include <stdint.h>
+    3	long check_VirtualAlloc(void) { return (long) VirtualAlloc; }
+    4	int main(void) { int ret = 0;
+    5	 ret |= ((intptr_t)check_VirtualAlloc) & 0xFFFF;
+    6	return ret; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:21: fatal error: windows.h: No such file or directory
+ #include <windows.h>
+                     ^
+compilation terminated.
+check_func_headers glob.h glob
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <glob.h>
+    2	#include <stdint.h>
+    3	long check_glob(void) { return (long) glob; }
+    4	int main(void) { int ret = 0;
+    5	 ret |= ((intptr_t)check_glob) & 0xFFFF;
+    6	return ret; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o
+check_lib xlib X11/Xlib.h X11/extensions/Xvlib.h XvGetPortAttribute -lXv -lX11 -lXext
+check_func_headers X11/Xlib.h X11/extensions/Xvlib.h XvGetPortAttribute -lXv -lX11 -lXext
+test_ld cc -lXv -lX11 -lXext
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <X11/Xlib.h>
+    2	#include <X11/extensions/Xvlib.h>
+    3	#include <stdint.h>
+    4	long check_XvGetPortAttribute(void) { return (long) XvGetPortAttribute; }
+    5	int main(void) { int ret = 0;
+    6	 ret |= ((intptr_t)check_XvGetPortAttribute) & 0xFFFF;
+    7	return ret; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:22: fatal error: X11/Xlib.h: No such file or directory
+ #include <X11/Xlib.h>
+                      ^
+compilation terminated.
+check_headers direct.h
+test_cpp
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <direct.h>
+    2	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -E -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:20: fatal error: direct.h: No such file or directory
+ #include <direct.h>
+                    ^
+compilation terminated.
+check_headers dirent.h
+test_cpp
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <dirent.h>
+    2	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -E -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_headers dxgidebug.h
+test_cpp
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <dxgidebug.h>
+    2	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -E -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:23: fatal error: dxgidebug.h: No such file or directory
+ #include <dxgidebug.h>
+                       ^
+compilation terminated.
+check_headers dxva.h
+test_cpp
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <dxva.h>
+    2	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -E -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:18: fatal error: dxva.h: No such file or directory
+ #include <dxva.h>
+                  ^
+compilation terminated.
+check_headers dxva2api.h -D_WIN32_WINNT=0x0600
+test_cpp -D_WIN32_WINNT=0x0600
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <dxva2api.h>
+    2	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -D_WIN32_WINNT=0x0600 -E -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:22: fatal error: dxva2api.h: No such file or directory
+ #include <dxva2api.h>
+                      ^
+compilation terminated.
+check_headers io.h
+test_cpp
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <io.h>
+    2	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -E -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:16: fatal error: io.h: No such file or directory
+ #include <io.h>
+                ^
+compilation terminated.
+check_headers linux/perf_event.h
+test_cpp
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <linux/perf_event.h>
+    2	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -E -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_headers libcrystalhd/libcrystalhd_if.h
+test_cpp
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <libcrystalhd/libcrystalhd_if.h>
+    2	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -E -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:42: fatal error: libcrystalhd/libcrystalhd_if.h: No such file or directory
+ #include <libcrystalhd/libcrystalhd_if.h>
+                                          ^
+compilation terminated.
+check_headers malloc.h
+test_cpp
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <malloc.h>
+    2	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -E -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_headers net/udplite.h
+test_cpp
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <net/udplite.h>
+    2	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -E -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:25: fatal error: net/udplite.h: No such file or directory
+ #include <net/udplite.h>
+                         ^
+compilation terminated.
+check_headers poll.h
+test_cpp
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <poll.h>
+    2	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -E -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_headers sys/param.h
+test_cpp
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <sys/param.h>
+    2	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -E -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_headers sys/resource.h
+test_cpp
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <sys/resource.h>
+    2	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -E -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_headers sys/select.h
+test_cpp
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <sys/select.h>
+    2	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -E -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_headers sys/time.h
+test_cpp
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <sys/time.h>
+    2	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -E -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_headers sys/un.h
+test_cpp
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <sys/un.h>
+    2	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -E -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_headers termios.h
+test_cpp
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <termios.h>
+    2	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -E -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_headers unistd.h
+test_cpp
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <unistd.h>
+    2	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -E -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_headers valgrind/valgrind.h
+test_cpp
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <valgrind/valgrind.h>
+    2	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -E -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:31: fatal error: valgrind/valgrind.h: No such file or directory
+ #include <valgrind/valgrind.h>
+                               ^
+compilation terminated.
+check_func_headers VideoToolbox/VTCompressionSession.h VTCompressionSessionPrepareToEncodeFrames -framework VideoToolbox
+test_ld cc -framework VideoToolbox
+test_cc -framework VideoToolbox
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <VideoToolbox/VTCompressionSession.h>
+    2	#include <stdint.h>
+    3	long check_VTCompressionSessionPrepareToEncodeFrames(void) { return (long) VTCompressionSessionPrepareToEncodeFrames; }
+    4	int main(void) { int ret = 0;
+    5	 ret |= ((intptr_t)check_VTCompressionSessionPrepareToEncodeFrames) & 0xFFFF;
+    6	return ret; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -framework VideoToolbox -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc: error: VideoToolbox: No such file or directory
+arm-himix100-linux-gcc: error: unrecognized command line option '-framework'
+check_headers windows.h
+test_cpp
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <windows.h>
+    2	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -E -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:21: fatal error: windows.h: No such file or directory
+ #include <windows.h>
+                     ^
+compilation terminated.
+check_headers X11/extensions/XvMClib.h
+test_cpp
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <X11/extensions/XvMClib.h>
+    2	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -E -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:36: fatal error: X11/extensions/XvMClib.h: No such file or directory
+ #include <X11/extensions/XvMClib.h>
+                                    ^
+compilation terminated.
+check_headers asm/types.h
+test_cpp
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <asm/types.h>
+    2	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -E -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_builtin stdatomic stdatomic.h atomic_int foo, bar = ATOMIC_VAR_INIT(-1); atomic_store(&foo, 0); foo += bar
+test_code ld stdatomic.h atomic_int foo, bar = ATOMIC_VAR_INIT(-1); atomic_store(&foo, 0); foo += bar cc
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <stdatomic.h>
+    2	int main(void) { atomic_int foo, bar = ATOMIC_VAR_INIT(-1); atomic_store(&foo, 0); foo += bar; return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o
+check_lib advapi32 windows.h RegCloseKey -ladvapi32
+check_func_headers windows.h RegCloseKey -ladvapi32
+test_ld cc -ladvapi32
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <windows.h>
+    2	#include <stdint.h>
+    3	long check_RegCloseKey(void) { return (long) RegCloseKey; }
+    4	int main(void) { int ret = 0;
+    5	 ret |= ((intptr_t)check_RegCloseKey) & 0xFFFF;
+    6	return ret; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:21: fatal error: windows.h: No such file or directory
+ #include <windows.h>
+                     ^
+compilation terminated.
+check_lib bcrypt windows.h bcrypt.h BCryptGenRandom -lbcrypt
+check_func_headers windows.h bcrypt.h BCryptGenRandom -lbcrypt
+test_ld cc -lbcrypt
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <windows.h>
+    2	#include <bcrypt.h>
+    3	#include <stdint.h>
+    4	long check_BCryptGenRandom(void) { return (long) BCryptGenRandom; }
+    5	int main(void) { int ret = 0;
+    6	 ret |= ((intptr_t)check_BCryptGenRandom) & 0xFFFF;
+    7	return ret; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:21: fatal error: windows.h: No such file or directory
+ #include <windows.h>
+                     ^
+compilation terminated.
+check_lib ole32 windows.h CoTaskMemFree -lole32
+check_func_headers windows.h CoTaskMemFree -lole32
+test_ld cc -lole32
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <windows.h>
+    2	#include <stdint.h>
+    3	long check_CoTaskMemFree(void) { return (long) CoTaskMemFree; }
+    4	int main(void) { int ret = 0;
+    5	 ret |= ((intptr_t)check_CoTaskMemFree) & 0xFFFF;
+    6	return ret; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:21: fatal error: windows.h: No such file or directory
+ #include <windows.h>
+                     ^
+compilation terminated.
+check_lib shell32 windows.h shellapi.h CommandLineToArgvW -lshell32
+check_func_headers windows.h shellapi.h CommandLineToArgvW -lshell32
+test_ld cc -lshell32
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <windows.h>
+    2	#include <shellapi.h>
+    3	#include <stdint.h>
+    4	long check_CommandLineToArgvW(void) { return (long) CommandLineToArgvW; }
+    5	int main(void) { int ret = 0;
+    6	 ret |= ((intptr_t)check_CommandLineToArgvW) & 0xFFFF;
+    7	return ret; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:21: fatal error: windows.h: No such file or directory
+ #include <windows.h>
+                     ^
+compilation terminated.
+check_lib psapi windows.h psapi.h GetProcessMemoryInfo -lpsapi
+check_func_headers windows.h psapi.h GetProcessMemoryInfo -lpsapi
+test_ld cc -lpsapi
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <windows.h>
+    2	#include <psapi.h>
+    3	#include <stdint.h>
+    4	long check_GetProcessMemoryInfo(void) { return (long) GetProcessMemoryInfo; }
+    5	int main(void) { int ret = 0;
+    6	 ret |= ((intptr_t)check_GetProcessMemoryInfo) & 0xFFFF;
+    7	return ret; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:21: fatal error: windows.h: No such file or directory
+ #include <windows.h>
+                     ^
+compilation terminated.
+check_lib android android/native_window.h ANativeWindow_acquire -landroid
+check_func_headers android/native_window.h ANativeWindow_acquire -landroid
+test_ld cc -landroid
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <android/native_window.h>
+    2	#include <stdint.h>
+    3	long check_ANativeWindow_acquire(void) { return (long) ANativeWindow_acquire; }
+    4	int main(void) { int ret = 0;
+    5	 ret |= ((intptr_t)check_ANativeWindow_acquire) & 0xFFFF;
+    6	return ret; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:35: fatal error: android/native_window.h: No such file or directory
+ #include <android/native_window.h>
+                                   ^
+compilation terminated.
+check_lib mediandk stdint.h media/NdkImage.h AImage_delete -lmediandk
+check_func_headers stdint.h media/NdkImage.h AImage_delete -lmediandk
+test_ld cc -lmediandk
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <stdint.h>
+    2	#include <media/NdkImage.h>
+    3	#include <stdint.h>
+    4	long check_AImage_delete(void) { return (long) AImage_delete; }
+    5	int main(void) { int ret = 0;
+    6	 ret |= ((intptr_t)check_AImage_delete) & 0xFFFF;
+    7	return ret; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:2:28: fatal error: media/NdkImage.h: No such file or directory
+ #include <media/NdkImage.h>
+                            ^
+compilation terminated.
+check_lib camera2ndk stdbool.h stdint.h camera/NdkCameraManager.h ACameraManager_create -lcamera2ndk
+check_func_headers stdbool.h stdint.h camera/NdkCameraManager.h ACameraManager_create -lcamera2ndk
+test_ld cc -lcamera2ndk
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <stdbool.h>
+    2	#include <stdint.h>
+    3	#include <camera/NdkCameraManager.h>
+    4	#include <stdint.h>
+    5	long check_ACameraManager_create(void) { return (long) ACameraManager_create; }
+    6	int main(void) { int ret = 0;
+    7	 ret |= ((intptr_t)check_ACameraManager_create) & 0xFFFF;
+    8	return ret; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:3:37: fatal error: camera/NdkCameraManager.h: No such file or directory
+ #include <camera/NdkCameraManager.h>
+                                     ^
+compilation terminated.
+check_apple_framework AppKit
+check_header_objcc AppKit/AppKit.h
+test_objcc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.m
+    1	#include <AppKit/AppKit.h>
+    2	int main(void) { return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.m
+arm-himix100-linux-gcc -Werror=missing-prototypes -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.m
+arm-himix100-linux-gcc: error: /tmp/ffconf.iiRrWAGJ/test.m: Objective-C compiler not installed on this system
+check_apple_framework AudioToolbox
+check_header_objcc AudioToolbox/AudioToolbox.h
+test_objcc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.m
+    1	#include <AudioToolbox/AudioToolbox.h>
+    2	int main(void) { return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.m
+arm-himix100-linux-gcc -Werror=missing-prototypes -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.m
+arm-himix100-linux-gcc: error: /tmp/ffconf.iiRrWAGJ/test.m: Objective-C compiler not installed on this system
+check_apple_framework AVFoundation
+check_header_objcc AVFoundation/AVFoundation.h
+test_objcc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.m
+    1	#include <AVFoundation/AVFoundation.h>
+    2	int main(void) { return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.m
+arm-himix100-linux-gcc -Werror=missing-prototypes -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.m
+arm-himix100-linux-gcc: error: /tmp/ffconf.iiRrWAGJ/test.m: Objective-C compiler not installed on this system
+check_apple_framework CoreImage
+check_header_objcc CoreImage/CoreImage.h
+test_objcc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.m
+    1	#include <CoreImage/CoreImage.h>
+    2	int main(void) { return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.m
+arm-himix100-linux-gcc -Werror=missing-prototypes -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.m
+arm-himix100-linux-gcc: error: /tmp/ffconf.iiRrWAGJ/test.m: Objective-C compiler not installed on this system
+check_apple_framework VideoToolbox
+check_header_objcc VideoToolbox/VideoToolbox.h
+test_objcc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.m
+    1	#include <VideoToolbox/VideoToolbox.h>
+    2	int main(void) { return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.m
+arm-himix100-linux-gcc -Werror=missing-prototypes -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.m
+arm-himix100-linux-gcc: error: /tmp/ffconf.iiRrWAGJ/test.m: Objective-C compiler not installed on this system
+check_apple_framework CoreFoundation
+check_header_objcc CoreFoundation/CoreFoundation.h
+test_objcc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.m
+    1	#include <CoreFoundation/CoreFoundation.h>
+    2	int main(void) { return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.m
+arm-himix100-linux-gcc -Werror=missing-prototypes -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.m
+arm-himix100-linux-gcc: error: /tmp/ffconf.iiRrWAGJ/test.m: Objective-C compiler not installed on this system
+check_apple_framework CoreMedia
+check_header_objcc CoreMedia/CoreMedia.h
+test_objcc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.m
+    1	#include <CoreMedia/CoreMedia.h>
+    2	int main(void) { return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.m
+arm-himix100-linux-gcc -Werror=missing-prototypes -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.m
+arm-himix100-linux-gcc: error: /tmp/ffconf.iiRrWAGJ/test.m: Objective-C compiler not installed on this system
+check_apple_framework CoreVideo
+check_header_objcc CoreVideo/CoreVideo.h
+test_objcc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.m
+    1	#include <CoreVideo/CoreVideo.h>
+    2	int main(void) { return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.m
+arm-himix100-linux-gcc -Werror=missing-prototypes -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.m
+arm-himix100-linux-gcc: error: /tmp/ffconf.iiRrWAGJ/test.m: Objective-C compiler not installed on this system
+check_struct sys/time.h sys/resource.h struct rusage ru_maxrss
+test_code cc sys/time.h sys/resource.h const void *p = &((struct rusage *)0)->ru_maxrss
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <sys/time.h>
+    2	#include <sys/resource.h>
+    3	int main(void) { const void *p = &((struct rusage *)0)->ru_maxrss; return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_type windows.h dxva.h DXVA_PicParams_HEVC -DWINAPI_FAMILY=WINAPI_FAMILY_DESKTOP_APP -D_CRT_BUILD_DESKTOP_APP=0
+test_code cc windows.h dxva.h DXVA_PicParams_HEVC v -DWINAPI_FAMILY=WINAPI_FAMILY_DESKTOP_APP -D_CRT_BUILD_DESKTOP_APP=0
+test_cc -DWINAPI_FAMILY=WINAPI_FAMILY_DESKTOP_APP -D_CRT_BUILD_DESKTOP_APP=0
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <windows.h>
+    2	#include <dxva.h>
+    3	int main(void) { DXVA_PicParams_HEVC v; return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -DWINAPI_FAMILY=WINAPI_FAMILY_DESKTOP_APP -D_CRT_BUILD_DESKTOP_APP=0 -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:21: fatal error: windows.h: No such file or directory
+ #include <windows.h>
+                     ^
+compilation terminated.
+check_type windows.h dxva.h DXVA_PicParams_VP9 -DWINAPI_FAMILY=WINAPI_FAMILY_DESKTOP_APP -D_CRT_BUILD_DESKTOP_APP=0
+test_code cc windows.h dxva.h DXVA_PicParams_VP9 v -DWINAPI_FAMILY=WINAPI_FAMILY_DESKTOP_APP -D_CRT_BUILD_DESKTOP_APP=0
+test_cc -DWINAPI_FAMILY=WINAPI_FAMILY_DESKTOP_APP -D_CRT_BUILD_DESKTOP_APP=0
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <windows.h>
+    2	#include <dxva.h>
+    3	int main(void) { DXVA_PicParams_VP9 v; return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -DWINAPI_FAMILY=WINAPI_FAMILY_DESKTOP_APP -D_CRT_BUILD_DESKTOP_APP=0 -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:21: fatal error: windows.h: No such file or directory
+ #include <windows.h>
+                     ^
+compilation terminated.
+check_type windows.h d3d11.h ID3D11VideoDecoder
+test_code cc windows.h d3d11.h ID3D11VideoDecoder v
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <windows.h>
+    2	#include <d3d11.h>
+    3	int main(void) { ID3D11VideoDecoder v; return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:21: fatal error: windows.h: No such file or directory
+ #include <windows.h>
+                     ^
+compilation terminated.
+check_type windows.h d3d11.h ID3D11VideoContext
+test_code cc windows.h d3d11.h ID3D11VideoContext v
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <windows.h>
+    2	#include <d3d11.h>
+    3	int main(void) { ID3D11VideoContext v; return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:21: fatal error: windows.h: No such file or directory
+ #include <windows.h>
+                     ^
+compilation terminated.
+check_type d3d9.h dxva2api.h DXVA2_ConfigPictureDecode -D_WIN32_WINNT=0x0602
+test_code cc d3d9.h dxva2api.h DXVA2_ConfigPictureDecode v -D_WIN32_WINNT=0x0602
+test_cc -D_WIN32_WINNT=0x0602
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <d3d9.h>
+    2	#include <dxva2api.h>
+    3	int main(void) { DXVA2_ConfigPictureDecode v; return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -D_WIN32_WINNT=0x0602 -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:18: fatal error: d3d9.h: No such file or directory
+ #include <d3d9.h>
+                  ^
+compilation terminated.
+check_type va/va.h va/va_dec_hevc.h VAPictureParameterBufferHEVC
+test_code cc va/va.h va/va_dec_hevc.h VAPictureParameterBufferHEVC v
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <va/va.h>
+    2	#include <va/va_dec_hevc.h>
+    3	int main(void) { VAPictureParameterBufferHEVC v; return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:19: fatal error: va/va.h: No such file or directory
+ #include <va/va.h>
+                   ^
+compilation terminated.
+check_struct va/va.h VADecPictureParameterBufferVP9 bit_depth
+test_code cc va/va.h const void *p = &((VADecPictureParameterBufferVP9 *)0)->bit_depth
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <va/va.h>
+    2	int main(void) { const void *p = &((VADecPictureParameterBufferVP9 *)0)->bit_depth; return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:19: fatal error: va/va.h: No such file or directory
+ #include <va/va.h>
+                   ^
+compilation terminated.
+check_type va/va.h va/va_enc_hevc.h VAEncPictureParameterBufferHEVC
+test_code cc va/va.h va/va_enc_hevc.h VAEncPictureParameterBufferHEVC v
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <va/va.h>
+    2	#include <va/va_enc_hevc.h>
+    3	int main(void) { VAEncPictureParameterBufferHEVC v; return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:19: fatal error: va/va.h: No such file or directory
+ #include <va/va.h>
+                   ^
+compilation terminated.
+check_type va/va.h va/va_enc_jpeg.h VAEncPictureParameterBufferJPEG
+test_code cc va/va.h va/va_enc_jpeg.h VAEncPictureParameterBufferJPEG v
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <va/va.h>
+    2	#include <va/va_enc_jpeg.h>
+    3	int main(void) { VAEncPictureParameterBufferJPEG v; return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:19: fatal error: va/va.h: No such file or directory
+ #include <va/va.h>
+                   ^
+compilation terminated.
+check_type va/va.h va/va_enc_vp8.h VAEncPictureParameterBufferVP8
+test_code cc va/va.h va/va_enc_vp8.h VAEncPictureParameterBufferVP8 v
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <va/va.h>
+    2	#include <va/va_enc_vp8.h>
+    3	int main(void) { VAEncPictureParameterBufferVP8 v; return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:19: fatal error: va/va.h: No such file or directory
+ #include <va/va.h>
+                   ^
+compilation terminated.
+check_type va/va.h va/va_enc_vp9.h VAEncPictureParameterBufferVP9
+test_code cc va/va.h va/va_enc_vp9.h VAEncPictureParameterBufferVP9 v
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <va/va.h>
+    2	#include <va/va_enc_vp9.h>
+    3	int main(void) { VAEncPictureParameterBufferVP9 v; return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:19: fatal error: va/va.h: No such file or directory
+ #include <va/va.h>
+                   ^
+compilation terminated.
+check_type vdpau/vdpau.h VdpPictureInfoHEVC
+test_code cc vdpau/vdpau.h VdpPictureInfoHEVC v
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <vdpau/vdpau.h>
+    2	int main(void) { VdpPictureInfoHEVC v; return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:25: fatal error: vdpau/vdpau.h: No such file or directory
+ #include <vdpau/vdpau.h>
+                         ^
+compilation terminated.
+check_pkg_config ffnvcodec ffnvcodec >= 8.1.24.2 ffnvcodec/nvEncodeAPI.h ffnvcodec/dynlink_cuda.h ffnvcodec/dynlink_cuviddec.h ffnvcodec/dynlink_nvcuvid.h 
+test_pkg_config ffnvcodec ffnvcodec >= 8.1.24.2 ffnvcodec/nvEncodeAPI.h ffnvcodec/dynlink_cuda.h ffnvcodec/dynlink_cuviddec.h ffnvcodec/dynlink_nvcuvid.h 
+false --exists --print-errors ffnvcodec >= 8.1.24.2
+check_pkg_config ffnvcodec ffnvcodec >= 8.0.14.2 ffnvcodec < 8.1 ffnvcodec/nvEncodeAPI.h ffnvcodec/dynlink_cuda.h ffnvcodec/dynlink_cuviddec.h ffnvcodec/dynlink_nvcuvid.h 
+test_pkg_config ffnvcodec ffnvcodec >= 8.0.14.2 ffnvcodec < 8.1 ffnvcodec/nvEncodeAPI.h ffnvcodec/dynlink_cuda.h ffnvcodec/dynlink_cuviddec.h ffnvcodec/dynlink_nvcuvid.h 
+false --exists --print-errors ffnvcodec >= 8.0.14.2 ffnvcodec < 8.1
+check_cpp_condition winrt windows.h !WINAPI_FAMILY_PARTITION(WINAPI_PARTITION_DESKTOP)
+test_cpp_condition windows.h !WINAPI_FAMILY_PARTITION(WINAPI_PARTITION_DESKTOP)
+test_cpp
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <windows.h>
+    2	#if !(!WINAPI_FAMILY_PARTITION(WINAPI_PARTITION_DESKTOP))
+    3	#error "unsatisfied condition: !WINAPI_FAMILY_PARTITION(WINAPI_PARTITION_DESKTOP)"
+    4	#endif
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -E -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:21: fatal error: windows.h: No such file or directory
+ #include <windows.h>
+                     ^
+compilation terminated.
+check_func_headers windows.h process.h _beginthreadex
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <windows.h>
+    2	#include <process.h>
+    3	#include <stdint.h>
+    4	long check__beginthreadex(void) { return (long) _beginthreadex; }
+    5	int main(void) { int ret = 0;
+    6	 ret |= ((intptr_t)check__beginthreadex) & 0xFFFF;
+    7	return ret; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:21: fatal error: windows.h: No such file or directory
+ #include <windows.h>
+                     ^
+compilation terminated.
+check_pkg_config zlib zlib zlib.h zlibVersion
+test_pkg_config zlib zlib zlib.h zlibVersion
+false --exists --print-errors zlib
+check_lib zlib zlib.h zlibVersion -lz
+check_func_headers zlib.h zlibVersion -lz
+test_ld cc -lz
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <zlib.h>
+    2	#include <stdint.h>
+    3	long check_zlibVersion(void) { return (long) zlibVersion; }
+    4	int main(void) { int ret = 0;
+    5	 ret |= ((intptr_t)check_zlibVersion) & 0xFFFF;
+    6	return ret; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:18: fatal error: zlib.h: No such file or directory
+ #include <zlib.h>
+                  ^
+compilation terminated.
+check_lib bzlib bzlib.h BZ2_bzlibVersion -lbz2
+check_func_headers bzlib.h BZ2_bzlibVersion -lbz2
+test_ld cc -lbz2
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <bzlib.h>
+    2	#include <stdint.h>
+    3	long check_BZ2_bzlibVersion(void) { return (long) BZ2_bzlibVersion; }
+    4	int main(void) { int ret = 0;
+    5	 ret |= ((intptr_t)check_BZ2_bzlibVersion) & 0xFFFF;
+    6	return ret; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:19: fatal error: bzlib.h: No such file or directory
+ #include <bzlib.h>
+                   ^
+compilation terminated.
+check_lib lzma lzma.h lzma_version_number -llzma
+check_func_headers lzma.h lzma_version_number -llzma
+test_ld cc -llzma
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <lzma.h>
+    2	#include <stdint.h>
+    3	long check_lzma_version_number(void) { return (long) lzma_version_number; }
+    4	int main(void) { int ret = 0;
+    5	 ret |= ((intptr_t)check_lzma_version_number) & 0xFFFF;
+    6	return ret; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:18: fatal error: lzma.h: No such file or directory
+ #include <lzma.h>
+                  ^
+compilation terminated.
+check_lib libdl dlfcn.h dlopen dlsym
+check_func_headers dlfcn.h dlopen dlsym
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <dlfcn.h>
+    2	#include <stdint.h>
+    3	long check_dlopen(void) { return (long) dlopen; }
+    4	long check_dlsym(void) { return (long) dlsym; }
+    5	int main(void) { int ret = 0;
+    6	 ret |= ((intptr_t)check_dlopen) & 0xFFFF;
+    7	 ret |= ((intptr_t)check_dlsym) & 0xFFFF;
+    8	return ret; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o
+/tmp/ffconf.iiRrWAGJ/test.o: In function `check_dlopen':
+test.c:(.text+0x1c): undefined reference to `dlopen'
+/tmp/ffconf.iiRrWAGJ/test.o: In function `check_dlsym':
+test.c:(.text+0x3c): undefined reference to `dlsym'
+collect2: error: ld returned 1 exit status
+check_lib libdl dlfcn.h dlopen dlsym -ldl
+check_func_headers dlfcn.h dlopen dlsym -ldl
+test_ld cc -ldl
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <dlfcn.h>
+    2	#include <stdint.h>
+    3	long check_dlopen(void) { return (long) dlopen; }
+    4	long check_dlsym(void) { return (long) dlsym; }
+    5	int main(void) { int ret = 0;
+    6	 ret |= ((intptr_t)check_dlopen) & 0xFFFF;
+    7	 ret |= ((intptr_t)check_dlsym) & 0xFFFF;
+    8	return ret; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o -ldl
+check_lib libm math.h sin -lm
+check_func_headers math.h sin -lm
+test_ld cc -lm
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <math.h>
+    2	#include <stdint.h>
+    3	long check_sin(void) { return (long) sin; }
+    4	int main(void) { int ret = 0;
+    5	 ret |= ((intptr_t)check_sin) & 0xFFFF;
+    6	return ret; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o -lm
+check_mathfunc atanf 1 -lm
+test_ld cc -lm
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <math.h>
+    2	float foo(float f, float g) { return atanf(f); }
+    3	int main(void){ return (int) foo; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o -lm
+check_mathfunc atan2f 2 -lm
+test_ld cc -lm
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <math.h>
+    2	float foo(float f, float g) { return atan2f(f, g); }
+    3	int main(void){ return (int) foo; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o -lm
+check_mathfunc cbrt 1 -lm
+test_ld cc -lm
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <math.h>
+    2	float foo(float f, float g) { return cbrt(f); }
+    3	int main(void){ return (int) foo; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o -lm
+check_mathfunc cbrtf 1 -lm
+test_ld cc -lm
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <math.h>
+    2	float foo(float f, float g) { return cbrtf(f); }
+    3	int main(void){ return (int) foo; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o -lm
+check_mathfunc copysign 2 -lm
+test_ld cc -lm
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <math.h>
+    2	float foo(float f, float g) { return copysign(f, g); }
+    3	int main(void){ return (int) foo; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o -lm
+check_mathfunc cosf 1 -lm
+test_ld cc -lm
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <math.h>
+    2	float foo(float f, float g) { return cosf(f); }
+    3	int main(void){ return (int) foo; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o -lm
+check_mathfunc erf 1 -lm
+test_ld cc -lm
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <math.h>
+    2	float foo(float f, float g) { return erf(f); }
+    3	int main(void){ return (int) foo; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o -lm
+check_mathfunc exp2 1 -lm
+test_ld cc -lm
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <math.h>
+    2	float foo(float f, float g) { return exp2(f); }
+    3	int main(void){ return (int) foo; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o -lm
+check_mathfunc exp2f 1 -lm
+test_ld cc -lm
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <math.h>
+    2	float foo(float f, float g) { return exp2f(f); }
+    3	int main(void){ return (int) foo; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o -lm
+check_mathfunc expf 1 -lm
+test_ld cc -lm
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <math.h>
+    2	float foo(float f, float g) { return expf(f); }
+    3	int main(void){ return (int) foo; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o -lm
+check_mathfunc hypot 2 -lm
+test_ld cc -lm
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <math.h>
+    2	float foo(float f, float g) { return hypot(f, g); }
+    3	int main(void){ return (int) foo; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o -lm
+check_mathfunc isfinite 1 -lm
+test_ld cc -lm
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <math.h>
+    2	float foo(float f, float g) { return isfinite(f); }
+    3	int main(void){ return (int) foo; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o -lm
+check_mathfunc isinf 1 -lm
+test_ld cc -lm
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <math.h>
+    2	float foo(float f, float g) { return isinf(f); }
+    3	int main(void){ return (int) foo; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o -lm
+check_mathfunc isnan 1 -lm
+test_ld cc -lm
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <math.h>
+    2	float foo(float f, float g) { return isnan(f); }
+    3	int main(void){ return (int) foo; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o -lm
+check_mathfunc ldexpf 2 -lm
+test_ld cc -lm
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <math.h>
+    2	float foo(float f, float g) { return ldexpf(f, g); }
+    3	int main(void){ return (int) foo; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o -lm
+check_mathfunc llrint 1 -lm
+test_ld cc -lm
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <math.h>
+    2	float foo(float f, float g) { return llrint(f); }
+    3	int main(void){ return (int) foo; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o -lm
+check_mathfunc llrintf 1 -lm
+test_ld cc -lm
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <math.h>
+    2	float foo(float f, float g) { return llrintf(f); }
+    3	int main(void){ return (int) foo; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o -lm
+check_mathfunc log2 1 -lm
+test_ld cc -lm
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <math.h>
+    2	float foo(float f, float g) { return log2(f); }
+    3	int main(void){ return (int) foo; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o -lm
+check_mathfunc log2f 1 -lm
+test_ld cc -lm
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <math.h>
+    2	float foo(float f, float g) { return log2f(f); }
+    3	int main(void){ return (int) foo; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o -lm
+check_mathfunc log10f 1 -lm
+test_ld cc -lm
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <math.h>
+    2	float foo(float f, float g) { return log10f(f); }
+    3	int main(void){ return (int) foo; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o -lm
+check_mathfunc lrint 1 -lm
+test_ld cc -lm
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <math.h>
+    2	float foo(float f, float g) { return lrint(f); }
+    3	int main(void){ return (int) foo; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o -lm
+check_mathfunc lrintf 1 -lm
+test_ld cc -lm
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <math.h>
+    2	float foo(float f, float g) { return lrintf(f); }
+    3	int main(void){ return (int) foo; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o -lm
+check_mathfunc powf 2 -lm
+test_ld cc -lm
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <math.h>
+    2	float foo(float f, float g) { return powf(f, g); }
+    3	int main(void){ return (int) foo; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o -lm
+check_mathfunc rint 1 -lm
+test_ld cc -lm
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <math.h>
+    2	float foo(float f, float g) { return rint(f); }
+    3	int main(void){ return (int) foo; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o -lm
+check_mathfunc round 1 -lm
+test_ld cc -lm
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <math.h>
+    2	float foo(float f, float g) { return round(f); }
+    3	int main(void){ return (int) foo; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o -lm
+check_mathfunc roundf 1 -lm
+test_ld cc -lm
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <math.h>
+    2	float foo(float f, float g) { return roundf(f); }
+    3	int main(void){ return (int) foo; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o -lm
+check_mathfunc sinf 1 -lm
+test_ld cc -lm
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <math.h>
+    2	float foo(float f, float g) { return sinf(f); }
+    3	int main(void){ return (int) foo; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o -lm
+check_mathfunc trunc 1 -lm
+test_ld cc -lm
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <math.h>
+    2	float foo(float f, float g) { return trunc(f); }
+    3	int main(void){ return (int) foo; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o -lm
+check_mathfunc truncf 1 -lm
+test_ld cc -lm
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <math.h>
+    2	float foo(float f, float g) { return truncf(f); }
+    3	int main(void){ return (int) foo; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o -lm
+check_complexfunc cabs 1
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <complex.h>
+    2	#include <math.h>
+    3	float foo(complex float f, complex float g) { return cabs(f * I); }
+    4	int main(void){ return (int) foo; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o
+/tmp/ffconf.iiRrWAGJ/test.o: In function `foo':
+test.c:(.text+0x90): undefined reference to `cabs'
+collect2: error: ld returned 1 exit status
+check_complexfunc cexp 1
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <complex.h>
+    2	#include <math.h>
+    3	float foo(complex float f, complex float g) { return cexp(f * I); }
+    4	int main(void){ return (int) foo; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o
+/tmp/ffconf.iiRrWAGJ/test.o: In function `foo':
+test.c:(.text+0x98): undefined reference to `cexp'
+collect2: error: ld returned 1 exit status
+test_pkg_config sdl2 sdl2 >= 2.0.1 sdl2 < 2.1.0 SDL_events.h SDL_PollEvent
+false --exists --print-errors sdl2 >= 2.0.1 sdl2 < 2.1.0
+check_func SecIdentityCreate -Wl,-framework,CoreFoundation -Wl,-framework,Security
+test_ld cc -Wl,-framework,CoreFoundation -Wl,-framework,Security
+test_cc -Wl,-framework,CoreFoundation -Wl,-framework,Security
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	extern int SecIdentityCreate();
+    2	int main(void){ SecIdentityCreate(); }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -Wl,-framework,CoreFoundation -Wl,-framework,Security -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -Wl,-framework,CoreFoundation -Wl,-framework,Security -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o
+/opt/hisi-linux/x86-arm/arm-himix100-linux/host_bin/../lib/gcc/arm-linux-uclibceabi/6.3.0/../../../../arm-linux-uclibceabi/bin/ld: -f may not be used without -shared
+collect2: error: ld returned 1 exit status
+check_func_headers windows.h security.h InitializeSecurityContext -DSECURITY_WIN32 -lsecur32
+test_ld cc -DSECURITY_WIN32 -lsecur32
+test_cc -DSECURITY_WIN32
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <windows.h>
+    2	#include <security.h>
+    3	#include <stdint.h>
+    4	long check_InitializeSecurityContext(void) { return (long) InitializeSecurityContext; }
+    5	int main(void) { int ret = 0;
+    6	 ret |= ((intptr_t)check_InitializeSecurityContext) & 0xFFFF;
+    7	return ret; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -DSECURITY_WIN32 -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:21: fatal error: windows.h: No such file or directory
+ #include <windows.h>
+                     ^
+compilation terminated.
+check_headers linux/fb.h
+test_cpp
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <linux/fb.h>
+    2	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -E -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_headers linux/videodev2.h
+test_cpp
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <linux/videodev2.h>
+    2	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -E -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+test_code cc linux/videodev2.h struct v4l2_frmsizeenum vfse; vfse.discrete.width = 0;
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <linux/videodev2.h>
+    2	int main(void) { struct v4l2_frmsizeenum vfse; vfse.discrete.width = 0;; return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_cc v4l2_m2m linux/videodev2.h int i = V4L2_CAP_VIDEO_M2M_MPLANE | V4L2_CAP_VIDEO_M2M | V4L2_BUF_FLAG_LAST;
+test_code cc linux/videodev2.h int i = V4L2_CAP_VIDEO_M2M_MPLANE | V4L2_CAP_VIDEO_M2M | V4L2_BUF_FLAG_LAST;
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <linux/videodev2.h>
+    2	int main(void) { int i = V4L2_CAP_VIDEO_M2M_MPLANE | V4L2_CAP_VIDEO_M2M | V4L2_BUF_FLAG_LAST;; return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c: In function 'main':
+/tmp/ffconf.iiRrWAGJ/test.c:2:26: error: 'V4L2_CAP_VIDEO_M2M_MPLANE' undeclared (first use in this function)
+ int main(void) { int i = V4L2_CAP_VIDEO_M2M_MPLANE | V4L2_CAP_VIDEO_M2M | V4L2_BUF_FLAG_LAST;; return 0; }
+                          ^~~~~~~~~~~~~~~~~~~~~~~~~
+/tmp/ffconf.iiRrWAGJ/test.c:2:26: note: each undeclared identifier is reported only once for each function it appears in
+/tmp/ffconf.iiRrWAGJ/test.c:2:54: error: 'V4L2_CAP_VIDEO_M2M' undeclared (first use in this function)
+ int main(void) { int i = V4L2_CAP_VIDEO_M2M_MPLANE | V4L2_CAP_VIDEO_M2M | V4L2_BUF_FLAG_LAST;; return 0; }
+                                                      ^~~~~~~~~~~~~~~~~~
+/tmp/ffconf.iiRrWAGJ/test.c:2:75: error: 'V4L2_BUF_FLAG_LAST' undeclared (first use in this function)
+ int main(void) { int i = V4L2_CAP_VIDEO_M2M_MPLANE | V4L2_CAP_VIDEO_M2M | V4L2_BUF_FLAG_LAST;; return 0; }
+                                                                           ^~~~~~~~~~~~~~~~~~
+check_cc vc1_v4l2_m2m linux/videodev2.h int i = V4L2_PIX_FMT_VC1_ANNEX_G;
+test_code cc linux/videodev2.h int i = V4L2_PIX_FMT_VC1_ANNEX_G;
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <linux/videodev2.h>
+    2	int main(void) { int i = V4L2_PIX_FMT_VC1_ANNEX_G;; return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_cc mpeg1_v4l2_m2m linux/videodev2.h int i = V4L2_PIX_FMT_MPEG1;
+test_code cc linux/videodev2.h int i = V4L2_PIX_FMT_MPEG1;
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <linux/videodev2.h>
+    2	int main(void) { int i = V4L2_PIX_FMT_MPEG1;; return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_cc mpeg2_v4l2_m2m linux/videodev2.h int i = V4L2_PIX_FMT_MPEG2;
+test_code cc linux/videodev2.h int i = V4L2_PIX_FMT_MPEG2;
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <linux/videodev2.h>
+    2	int main(void) { int i = V4L2_PIX_FMT_MPEG2;; return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_cc mpeg4_v4l2_m2m linux/videodev2.h int i = V4L2_PIX_FMT_MPEG4;
+test_code cc linux/videodev2.h int i = V4L2_PIX_FMT_MPEG4;
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <linux/videodev2.h>
+    2	int main(void) { int i = V4L2_PIX_FMT_MPEG4;; return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_cc hevc_v4l2_m2m linux/videodev2.h int i = V4L2_PIX_FMT_HEVC;
+test_code cc linux/videodev2.h int i = V4L2_PIX_FMT_HEVC;
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <linux/videodev2.h>
+    2	int main(void) { int i = V4L2_PIX_FMT_HEVC;; return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c: In function 'main':
+/tmp/ffconf.iiRrWAGJ/test.c:2:26: error: 'V4L2_PIX_FMT_HEVC' undeclared (first use in this function)
+ int main(void) { int i = V4L2_PIX_FMT_HEVC;; return 0; }
+                          ^~~~~~~~~~~~~~~~~
+/tmp/ffconf.iiRrWAGJ/test.c:2:26: note: each undeclared identifier is reported only once for each function it appears in
+check_cc h263_v4l2_m2m linux/videodev2.h int i = V4L2_PIX_FMT_H263;
+test_code cc linux/videodev2.h int i = V4L2_PIX_FMT_H263;
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <linux/videodev2.h>
+    2	int main(void) { int i = V4L2_PIX_FMT_H263;; return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_cc h264_v4l2_m2m linux/videodev2.h int i = V4L2_PIX_FMT_H264;
+test_code cc linux/videodev2.h int i = V4L2_PIX_FMT_H264;
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <linux/videodev2.h>
+    2	int main(void) { int i = V4L2_PIX_FMT_H264;; return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_cc vp8_v4l2_m2m linux/videodev2.h int i = V4L2_PIX_FMT_VP8;
+test_code cc linux/videodev2.h int i = V4L2_PIX_FMT_VP8;
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <linux/videodev2.h>
+    2	int main(void) { int i = V4L2_PIX_FMT_VP8;; return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c: In function 'main':
+/tmp/ffconf.iiRrWAGJ/test.c:2:26: error: 'V4L2_PIX_FMT_VP8' undeclared (first use in this function)
+ int main(void) { int i = V4L2_PIX_FMT_VP8;; return 0; }
+                          ^~~~~~~~~~~~~~~~
+/tmp/ffconf.iiRrWAGJ/test.c:2:26: note: each undeclared identifier is reported only once for each function it appears in
+check_cc vp9_v4l2_m2m linux/videodev2.h int i = V4L2_PIX_FMT_VP9;
+test_code cc linux/videodev2.h int i = V4L2_PIX_FMT_VP9;
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <linux/videodev2.h>
+    2	int main(void) { int i = V4L2_PIX_FMT_VP9;; return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c: In function 'main':
+/tmp/ffconf.iiRrWAGJ/test.c:2:26: error: 'V4L2_PIX_FMT_VP9' undeclared (first use in this function)
+ int main(void) { int i = V4L2_PIX_FMT_VP9;; return 0; }
+                          ^~~~~~~~~~~~~~~~
+/tmp/ffconf.iiRrWAGJ/test.c:2:26: note: each undeclared identifier is reported only once for each function it appears in
+check_headers sys/videoio.h
+test_cpp
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <sys/videoio.h>
+    2	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -E -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:25: fatal error: sys/videoio.h: No such file or directory
+ #include <sys/videoio.h>
+                         ^
+compilation terminated.
+test_code cc sys/videoio.h struct v4l2_frmsizeenum vfse; vfse.discrete.width = 0;
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <sys/videoio.h>
+    2	int main(void) { struct v4l2_frmsizeenum vfse; vfse.discrete.width = 0;; return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:25: fatal error: sys/videoio.h: No such file or directory
+ #include <sys/videoio.h>
+                         ^
+compilation terminated.
+check_lib user32 windows.h winuser.h GetShellWindow -luser32
+check_func_headers windows.h winuser.h GetShellWindow -luser32
+test_ld cc -luser32
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <windows.h>
+    2	#include <winuser.h>
+    3	#include <stdint.h>
+    4	long check_GetShellWindow(void) { return (long) GetShellWindow; }
+    5	int main(void) { int ret = 0;
+    6	 ret |= ((intptr_t)check_GetShellWindow) & 0xFFFF;
+    7	return ret; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:21: fatal error: windows.h: No such file or directory
+ #include <windows.h>
+                     ^
+compilation terminated.
+check_lib vfw32 windows.h vfw.h capCreateCaptureWindow -lvfw32
+check_func_headers windows.h vfw.h capCreateCaptureWindow -lvfw32
+test_ld cc -lvfw32
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <windows.h>
+    2	#include <vfw.h>
+    3	#include <stdint.h>
+    4	long check_capCreateCaptureWindow(void) { return (long) capCreateCaptureWindow; }
+    5	int main(void) { int ret = 0;
+    6	 ret |= ((intptr_t)check_capCreateCaptureWindow) & 0xFFFF;
+    7	return ret; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:21: fatal error: windows.h: No such file or directory
+ #include <windows.h>
+                     ^
+compilation terminated.
+check_cpp_condition vfwcap_defines vfw.h WM_CAP_DRIVER_CONNECT > WM_USER
+test_cpp_condition vfw.h WM_CAP_DRIVER_CONNECT > WM_USER
+test_cpp
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <vfw.h>
+    2	#if !(WM_CAP_DRIVER_CONNECT > WM_USER)
+    3	#error "unsatisfied condition: WM_CAP_DRIVER_CONNECT > WM_USER"
+    4	#endif
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -E -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:17: fatal error: vfw.h: No such file or directory
+ #include <vfw.h>
+                 ^
+compilation terminated.
+check_type dshow.h IBaseFilter
+test_code cc dshow.h IBaseFilter v
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <dshow.h>
+    2	int main(void) { IBaseFilter v; return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:19: fatal error: dshow.h: No such file or directory
+ #include <dshow.h>
+                   ^
+compilation terminated.
+check_headers dev/bktr/ioctl_meteor.h dev/bktr/ioctl_bt848.h
+test_cpp
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <dev/bktr/ioctl_meteor.h>
+    2	#include <dev/bktr/ioctl_bt848.h>
+    3	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -E -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:35: fatal error: dev/bktr/ioctl_meteor.h: No such file or directory
+ #include <dev/bktr/ioctl_meteor.h>
+                                   ^
+compilation terminated.
+check_headers machine/ioctl_meteor.h machine/ioctl_bt848.h
+test_cpp
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <machine/ioctl_meteor.h>
+    2	#include <machine/ioctl_bt848.h>
+    3	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -E -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:34: fatal error: machine/ioctl_meteor.h: No such file or directory
+ #include <machine/ioctl_meteor.h>
+                                  ^
+compilation terminated.
+check_headers dev/video/meteor/ioctl_meteor.h dev/video/bktr/ioctl_bt848.h
+test_cpp
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <dev/video/meteor/ioctl_meteor.h>
+    2	#include <dev/video/bktr/ioctl_bt848.h>
+    3	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -E -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:43: fatal error: dev/video/meteor/ioctl_meteor.h: No such file or directory
+ #include <dev/video/meteor/ioctl_meteor.h>
+                                           ^
+compilation terminated.
+check_headers dev/ic/bt8xx.h
+test_cpp
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <dev/ic/bt8xx.h>
+    2	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -E -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:26: fatal error: dev/ic/bt8xx.h: No such file or directory
+ #include <dev/ic/bt8xx.h>
+                          ^
+compilation terminated.
+check_struct sys/soundcard.h audio_buf_info bytes
+test_code cc sys/soundcard.h const void *p = &((audio_buf_info *)0)->bytes
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <sys/soundcard.h>
+    2	int main(void) { const void *p = &((audio_buf_info *)0)->bytes; return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_pkg_config alsa alsa alsa/asoundlib.h snd_pcm_htimestamp
+test_pkg_config alsa alsa alsa/asoundlib.h snd_pcm_htimestamp
+false --exists --print-errors alsa
+check_lib alsa alsa/asoundlib.h snd_pcm_htimestamp -lasound
+check_func_headers alsa/asoundlib.h snd_pcm_htimestamp -lasound
+test_ld cc -lasound
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <alsa/asoundlib.h>
+    2	#include <stdint.h>
+    3	long check_snd_pcm_htimestamp(void) { return (long) snd_pcm_htimestamp; }
+    4	int main(void) { int ret = 0;
+    5	 ret |= ((intptr_t)check_snd_pcm_htimestamp) & 0xFFFF;
+    6	return ret; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:28: fatal error: alsa/asoundlib.h: No such file or directory
+ #include <alsa/asoundlib.h>
+                            ^
+compilation terminated.
+check_lib sndio sndio.h sio_open -lsndio
+check_func_headers sndio.h sio_open -lsndio
+test_ld cc -lsndio
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <sndio.h>
+    2	#include <stdint.h>
+    3	long check_sio_open(void) { return (long) sio_open; }
+    4	int main(void) { int ret = 0;
+    5	 ret |= ((intptr_t)check_sio_open) & 0xFFFF;
+    6	return ret; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:19: fatal error: sndio.h: No such file or directory
+ #include <sndio.h>
+                   ^
+compilation terminated.
+check_pkg_config libxcb xcb >= 1.4 xcb/xcb.h xcb_connect
+test_pkg_config libxcb xcb >= 1.4 xcb/xcb.h xcb_connect
+false --exists --print-errors xcb >= 1.4
+check_func_headers windows.h CreateDIBSection -lgdi32
+test_ld cc -lgdi32
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <windows.h>
+    2	#include <stdint.h>
+    3	long check_CreateDIBSection(void) { return (long) CreateDIBSection; }
+    4	int main(void) { int ret = 0;
+    5	 ret |= ((intptr_t)check_CreateDIBSection) & 0xFFFF;
+    6	return ret; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:21: fatal error: windows.h: No such file or directory
+ #include <windows.h>
+                     ^
+compilation terminated.
+test_cpp
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#ifdef WINAPI_FAMILY
+    2	#include <winapifamily.h>
+    3	#if WINAPI_FAMILY_PARTITION(WINAPI_PARTITION_DESKTOP)
+    4	#error desktop, not uwp
+    5	#else
+    6	// WINAPI_FAMILY_APP, WINAPI_FAMILY_PHONE_APP => UWP
+    7	#endif
+    8	#else
+    9	#error no family set
+   10	#endif
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -E -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:9:2: error: #error no family set
+ #error no family set
+  ^~~~~
+check_pkg_config vaapi libva >= 0.35.0 va/va.h vaInitialize
+test_pkg_config vaapi libva >= 0.35.0 va/va.h vaInitialize
+false --exists --print-errors libva >= 0.35.0
+check_cpp_condition vdpau vdpau/vdpau.h defined VDP_DECODER_PROFILE_MPEG4_PART2_ASP
+test_cpp_condition vdpau/vdpau.h defined VDP_DECODER_PROFILE_MPEG4_PART2_ASP
+test_cpp
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <vdpau/vdpau.h>
+    2	#if !(defined VDP_DECODER_PROFILE_MPEG4_PART2_ASP)
+    3	#error "unsatisfied condition: defined VDP_DECODER_PROFILE_MPEG4_PART2_ASP"
+    4	#endif
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -E -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:25: fatal error: vdpau/vdpau.h: No such file or directory
+ #include <vdpau/vdpau.h>
+                         ^
+compilation terminated.
+check_lib crystalhd stdint.h libcrystalhd/libcrystalhd_if.h DtsCrystalHDVersion -lcrystalhd
+check_func_headers stdint.h libcrystalhd/libcrystalhd_if.h DtsCrystalHDVersion -lcrystalhd
+test_ld cc -lcrystalhd
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <stdint.h>
+    2	#include <libcrystalhd/libcrystalhd_if.h>
+    3	#include <stdint.h>
+    4	long check_DtsCrystalHDVersion(void) { return (long) DtsCrystalHDVersion; }
+    5	int main(void) { int ret = 0;
+    6	 ret |= ((intptr_t)check_DtsCrystalHDVersion) & 0xFFFF;
+    7	return ret; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:2:42: fatal error: libcrystalhd/libcrystalhd_if.h: No such file or directory
+ #include <libcrystalhd/libcrystalhd_if.h>
+                                          ^
+compilation terminated.
+check_cpp_condition amf AMF/core/Version.h (AMF_VERSION_MAJOR << 48 | AMF_VERSION_MINOR << 32 | AMF_VERSION_RELEASE << 16 | AMF_VERSION_BUILD_NUM) >= 0x0001000400040001
+test_cpp_condition AMF/core/Version.h (AMF_VERSION_MAJOR << 48 | AMF_VERSION_MINOR << 32 | AMF_VERSION_RELEASE << 16 | AMF_VERSION_BUILD_NUM) >= 0x0001000400040001
+test_cpp
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	#include <AMF/core/Version.h>
+    2	#if !((AMF_VERSION_MAJOR << 48 | AMF_VERSION_MINOR << 32 | AMF_VERSION_RELEASE << 16 | AMF_VERSION_BUILD_NUM) >= 0x0001000400040001)
+    3	#error "unsatisfied condition: (AMF_VERSION_MAJOR << 48 | AMF_VERSION_MINOR << 32 | AMF_VERSION_RELEASE << 16 | AMF_VERSION_BUILD_NUM) >= 0x0001000400040001"
+    4	#endif
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -E -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:30: fatal error: AMF/core/Version.h: No such file or directory
+ #include <AMF/core/Version.h>
+                              ^
+compilation terminated.
+check_cflags -Wdeclaration-after-statement
+test_cflags -Wdeclaration-after-statement
+test_cc -Wdeclaration-after-statement
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -Wdeclaration-after-statement -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_cflags -Wall
+test_cflags -Wall
+test_cc -Wall
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -Wdeclaration-after-statement -Wall -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_cflags -Wdisabled-optimization
+test_cflags -Wdisabled-optimization
+test_cc -Wdisabled-optimization
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -Wdeclaration-after-statement -Wall -Wdisabled-optimization -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_cflags -Wpointer-arith
+test_cflags -Wpointer-arith
+test_cc -Wpointer-arith
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -Wdeclaration-after-statement -Wall -Wdisabled-optimization -Wpointer-arith -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_cflags -Wredundant-decls
+test_cflags -Wredundant-decls
+test_cc -Wredundant-decls
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -Wdeclaration-after-statement -Wall -Wdisabled-optimization -Wpointer-arith -Wredundant-decls -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_cflags -Wwrite-strings
+test_cflags -Wwrite-strings
+test_cc -Wwrite-strings
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -Wdeclaration-after-statement -Wall -Wdisabled-optimization -Wpointer-arith -Wredundant-decls -Wwrite-strings -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_cflags -Wtype-limits
+test_cflags -Wtype-limits
+test_cc -Wtype-limits
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -Wdeclaration-after-statement -Wall -Wdisabled-optimization -Wpointer-arith -Wredundant-decls -Wwrite-strings -Wtype-limits -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_cflags -Wundef
+test_cflags -Wundef
+test_cc -Wundef
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -Wdeclaration-after-statement -Wall -Wdisabled-optimization -Wpointer-arith -Wredundant-decls -Wwrite-strings -Wtype-limits -Wundef -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_cflags -Wmissing-prototypes
+test_cflags -Wmissing-prototypes
+test_cc -Wmissing-prototypes
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -Wdeclaration-after-statement -Wall -Wdisabled-optimization -Wpointer-arith -Wredundant-decls -Wwrite-strings -Wtype-limits -Wundef -Wmissing-prototypes -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_cflags -Wno-pointer-to-int-cast
+test_cflags -Wno-pointer-to-int-cast
+test_cc -Wno-pointer-to-int-cast
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -Wdeclaration-after-statement -Wall -Wdisabled-optimization -Wpointer-arith -Wredundant-decls -Wwrite-strings -Wtype-limits -Wundef -Wmissing-prototypes -Wno-pointer-to-int-cast -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_cflags -Wstrict-prototypes
+test_cflags -Wstrict-prototypes
+test_cc -Wstrict-prototypes
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -Wdeclaration-after-statement -Wall -Wdisabled-optimization -Wpointer-arith -Wredundant-decls -Wwrite-strings -Wtype-limits -Wundef -Wmissing-prototypes -Wno-pointer-to-int-cast -Wstrict-prototypes -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_cflags -Wempty-body
+test_cflags -Wempty-body
+test_cc -Wempty-body
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -Wdeclaration-after-statement -Wall -Wdisabled-optimization -Wpointer-arith -Wredundant-decls -Wwrite-strings -Wtype-limits -Wundef -Wmissing-prototypes -Wno-pointer-to-int-cast -Wstrict-prototypes -Wempty-body -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+test_cflags -Werror=unused-command-line-argument
+test_cc -Werror=unused-command-line-argument
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -Wdeclaration-after-statement -Wall -Wdisabled-optimization -Wpointer-arith -Wredundant-decls -Wwrite-strings -Wtype-limits -Wundef -Wmissing-prototypes -Wno-pointer-to-int-cast -Wstrict-prototypes -Wempty-body -Werror=unused-command-line-argument -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+cc1: error: -Werror=unused-command-line-argument: no option -Wunused-command-line-argument
+test_cflags -Werror=unknown-warning-option
+test_cc -Werror=unknown-warning-option
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -Wdeclaration-after-statement -Wall -Wdisabled-optimization -Wpointer-arith -Wredundant-decls -Wwrite-strings -Wtype-limits -Wundef -Wmissing-prototypes -Wno-pointer-to-int-cast -Wstrict-prototypes -Wempty-body -Werror=unknown-warning-option -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+cc1: error: -Werror=unknown-warning-option: no option -Wunknown-warning-option
+test_cflags -Wparentheses
+test_cc -Wparentheses
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -Wdeclaration-after-statement -Wall -Wdisabled-optimization -Wpointer-arith -Wredundant-decls -Wwrite-strings -Wtype-limits -Wundef -Wmissing-prototypes -Wno-pointer-to-int-cast -Wstrict-prototypes -Wempty-body -Wparentheses -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+test_cflags -Wswitch
+test_cc -Wswitch
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -Wdeclaration-after-statement -Wall -Wdisabled-optimization -Wpointer-arith -Wredundant-decls -Wwrite-strings -Wtype-limits -Wundef -Wmissing-prototypes -Wno-pointer-to-int-cast -Wstrict-prototypes -Wempty-body -Wno-parentheses -Wswitch -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+test_cflags -Wformat-zero-length
+test_cc -Wformat-zero-length
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -Wdeclaration-after-statement -Wall -Wdisabled-optimization -Wpointer-arith -Wredundant-decls -Wwrite-strings -Wtype-limits -Wundef -Wmissing-prototypes -Wno-pointer-to-int-cast -Wstrict-prototypes -Wempty-body -Wno-parentheses -Wno-switch -Wformat-zero-length -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+test_cflags -Wpointer-sign
+test_cc -Wpointer-sign
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -Wdeclaration-after-statement -Wall -Wdisabled-optimization -Wpointer-arith -Wredundant-decls -Wwrite-strings -Wtype-limits -Wundef -Wmissing-prototypes -Wno-pointer-to-int-cast -Wstrict-prototypes -Wempty-body -Wno-parentheses -Wno-switch -Wno-format-zero-length -Wpointer-sign -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+test_cflags -Wunused-const-variable
+test_cc -Wunused-const-variable
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -Wdeclaration-after-statement -Wall -Wdisabled-optimization -Wpointer-arith -Wredundant-decls -Wwrite-strings -Wtype-limits -Wundef -Wmissing-prototypes -Wno-pointer-to-int-cast -Wstrict-prototypes -Wempty-body -Wno-parentheses -Wno-switch -Wno-format-zero-length -Wno-pointer-sign -Wunused-const-variable -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+test_cflags -Wbool-operation
+test_cc -Wbool-operation
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -Wdeclaration-after-statement -Wall -Wdisabled-optimization -Wpointer-arith -Wredundant-decls -Wwrite-strings -Wtype-limits -Wundef -Wmissing-prototypes -Wno-pointer-to-int-cast -Wstrict-prototypes -Wempty-body -Wno-parentheses -Wno-switch -Wno-format-zero-length -Wno-pointer-sign -Wno-unused-const-variable -Wbool-operation -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc: error: unrecognized command line option '-Wbool-operation'; did you mean '-Wall-deprecation'?
+test_cflags -Wdeprecated-declarations
+test_cc -Wdeprecated-declarations
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -Wdeclaration-after-statement -Wall -Wdisabled-optimization -Wpointer-arith -Wredundant-decls -Wwrite-strings -Wtype-limits -Wundef -Wmissing-prototypes -Wno-pointer-to-int-cast -Wstrict-prototypes -Wempty-body -Wno-parentheses -Wno-switch -Wno-format-zero-length -Wno-pointer-sign -Wno-unused-const-variable -Wdeprecated-declarations -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+test_cflags -Wunused-variable
+test_cc -Wunused-variable
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -Wdeclaration-after-statement -Wall -Wdisabled-optimization -Wpointer-arith -Wredundant-decls -Wwrite-strings -Wtype-limits -Wundef -Wmissing-prototypes -Wno-pointer-to-int-cast -Wstrict-prototypes -Wempty-body -Wno-parentheses -Wno-switch -Wno-format-zero-length -Wno-pointer-sign -Wno-unused-const-variable -Wunused-variable -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	void (^block)(void);
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -Wdeclaration-after-statement -Wall -Wdisabled-optimization -Wpointer-arith -Wredundant-decls -Wwrite-strings -Wtype-limits -Wundef -Wmissing-prototypes -Wno-pointer-to-int-cast -Wstrict-prototypes -Wempty-body -Wno-parentheses -Wno-switch -Wno-format-zero-length -Wno-pointer-sign -Wno-unused-const-variable -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:1:7: error: expected identifier or '(' before '^' token
+ void (^block)(void);
+       ^
+check_ldflags -Wl,--warn-common
+test_ldflags -Wl,--warn-common
+test_ld cc -Wl,--warn-common
+test_cc -Wl,--warn-common
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	int main(void){ return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -Wdeclaration-after-statement -Wall -Wdisabled-optimization -Wpointer-arith -Wredundant-decls -Wwrite-strings -Wtype-limits -Wundef -Wmissing-prototypes -Wno-pointer-to-int-cast -Wstrict-prototypes -Wempty-body -Wno-parentheses -Wno-switch -Wno-format-zero-length -Wno-pointer-sign -Wno-unused-const-variable -Wl,--warn-common -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -Wl,--warn-common -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o
+check_ldflags -Wl,-rpath-link=libpostproc:libswresample:libswscale:libavfilter:libavdevice:libavformat:libavcodec:libavutil:libavresample
+test_ldflags -Wl,-rpath-link=libpostproc:libswresample:libswscale:libavfilter:libavdevice:libavformat:libavcodec:libavutil:libavresample
+test_ld cc -Wl,-rpath-link=libpostproc:libswresample:libswscale:libavfilter:libavdevice:libavformat:libavcodec:libavutil:libavresample
+test_cc -Wl,-rpath-link=libpostproc:libswresample:libswscale:libavfilter:libavdevice:libavformat:libavcodec:libavutil:libavresample
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	int main(void){ return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -Wdeclaration-after-statement -Wall -Wdisabled-optimization -Wpointer-arith -Wredundant-decls -Wwrite-strings -Wtype-limits -Wundef -Wmissing-prototypes -Wno-pointer-to-int-cast -Wstrict-prototypes -Wempty-body -Wno-parentheses -Wno-switch -Wno-format-zero-length -Wno-pointer-sign -Wno-unused-const-variable -Wl,-rpath-link=libpostproc:libswresample:libswscale:libavfilter:libavdevice:libavformat:libavcodec:libavutil:libavresample -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -Wl,--warn-common -Wl,-rpath-link=libpostproc:libswresample:libswscale:libavfilter:libavdevice:libavformat:libavcodec:libavutil:libavresample -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o
+test_ldflags -Wl,-Bsymbolic
+test_ld cc -Wl,-Bsymbolic
+test_cc -Wl,-Bsymbolic
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	int main(void){ return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -Wdeclaration-after-statement -Wall -Wdisabled-optimization -Wpointer-arith -Wredundant-decls -Wwrite-strings -Wtype-limits -Wundef -Wmissing-prototypes -Wno-pointer-to-int-cast -Wstrict-prototypes -Wempty-body -Wno-parentheses -Wno-switch -Wno-format-zero-length -Wno-pointer-sign -Wno-unused-const-variable -Wl,-Bsymbolic -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -Wl,--warn-common -Wl,-rpath-link=libpostproc:libswresample:libswscale:libavfilter:libavdevice:libavformat:libavcodec:libavutil:libavresample -Wl,-Bsymbolic -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o
+check_stripflags -x
+test_stripflags -x
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	int main(void) { return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -Wdeclaration-after-statement -Wall -Wdisabled-optimization -Wpointer-arith -Wredundant-decls -Wwrite-strings -Wtype-limits -Wundef -Wmissing-prototypes -Wno-pointer-to-int-cast -Wstrict-prototypes -Wempty-body -Wno-parentheses -Wno-switch -Wno-format-zero-length -Wno-pointer-sign -Wno-unused-const-variable -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-strip -x /tmp/ffconf.iiRrWAGJ/test.o
+check_ld cc proper_dce
+test_ld cc
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	extern const int array[512];
+    2	static inline int func(void) { return array[0]; }
+    3	int main(void) { return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -Wdeclaration-after-statement -Wall -Wdisabled-optimization -Wpointer-arith -Wredundant-decls -Wwrite-strings -Wtype-limits -Wundef -Wmissing-prototypes -Wno-pointer-to-int-cast -Wstrict-prototypes -Wempty-body -Wno-parentheses -Wno-switch -Wno-format-zero-length -Wno-pointer-sign -Wno-unused-const-variable -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -Wl,--warn-common -Wl,-rpath-link=libpostproc:libswresample:libswscale:libavfilter:libavdevice:libavformat:libavcodec:libavutil:libavresample -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o
+test_ldflags -Wl,--version-script,/tmp/ffconf.iiRrWAGJ/test.ver
+test_ld cc -Wl,--version-script,/tmp/ffconf.iiRrWAGJ/test.ver
+test_cc -Wl,--version-script,/tmp/ffconf.iiRrWAGJ/test.ver
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	int main(void){ return 0; }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -Wdeclaration-after-statement -Wall -Wdisabled-optimization -Wpointer-arith -Wredundant-decls -Wwrite-strings -Wtype-limits -Wundef -Wmissing-prototypes -Wno-pointer-to-int-cast -Wstrict-prototypes -Wempty-body -Wno-parentheses -Wno-switch -Wno-format-zero-length -Wno-pointer-sign -Wno-unused-const-variable -Wl,--version-script,/tmp/ffconf.iiRrWAGJ/test.ver -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -Wl,--warn-common -Wl,-rpath-link=libpostproc:libswresample:libswscale:libavfilter:libavdevice:libavformat:libavcodec:libavutil:libavresample -Wl,--version-script,/tmp/ffconf.iiRrWAGJ/test.ver -o /tmp/ffconf.iiRrWAGJ/test /tmp/ffconf.iiRrWAGJ/test.o
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	void ff_foo(void) __asm__ ("av_foo@VERSION");
+    2	void ff_foo(void) { __asm__(""); }
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -Wdeclaration-after-statement -Wall -Wdisabled-optimization -Wpointer-arith -Wredundant-decls -Wwrite-strings -Wtype-limits -Wundef -Wmissing-prototypes -Wno-pointer-to-int-cast -Wstrict-prototypes -Wempty-body -Wno-parentheses -Wno-switch -Wno-format-zero-length -Wno-pointer-sign -Wno-unused-const-variable -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ccKVD4dZ.s: Assembler messages:
+/tmp/ccKVD4dZ.s:18: Error: unrecognized symbol type ""
+/tmp/ccKVD4dZ.s:19: Error: bad instruction `av_foo'
+/tmp/ccKVD4dZ.s:25: Error: expected comma after name `av_foo' in .size directive
+test_cc
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	__asm__(".symver ff_foo,av_foo@VERSION");
+    2	void ff_foo(void) {}
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -Wdeclaration-after-statement -Wall -Wdisabled-optimization -Wpointer-arith -Wredundant-decls -Wwrite-strings -Wtype-limits -Wundef -Wmissing-prototypes -Wno-pointer-to-int-cast -Wstrict-prototypes -Wempty-body -Wno-parentheses -Wno-switch -Wno-format-zero-length -Wno-pointer-sign -Wno-unused-const-variable -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+/tmp/ffconf.iiRrWAGJ/test.c:2:6: warning: no previous prototype for 'ff_foo' [-Wmissing-prototypes]
+ void ff_foo(void) {}
+      ^~~~~~
+check_cflags -Os
+test_cflags -Os
+test_cc -Os
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -Wdeclaration-after-statement -Wall -Wdisabled-optimization -Wpointer-arith -Wredundant-decls -Wwrite-strings -Wtype-limits -Wundef -Wmissing-prototypes -Wno-pointer-to-int-cast -Wstrict-prototypes -Wempty-body -Wno-parentheses -Wno-switch -Wno-format-zero-length -Wno-pointer-sign -Wno-unused-const-variable -Os -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_cflags -fno-math-errno
+test_cflags -fno-math-errno
+test_cc -fno-math-errno
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -Wdeclaration-after-statement -Wall -Wdisabled-optimization -Wpointer-arith -Wredundant-decls -Wwrite-strings -Wtype-limits -Wundef -Wmissing-prototypes -Wno-pointer-to-int-cast -Wstrict-prototypes -Wempty-body -Wno-parentheses -Wno-switch -Wno-format-zero-length -Wno-pointer-sign -Wno-unused-const-variable -Os -fno-math-errno -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_cflags -fno-signed-zeros
+test_cflags -fno-signed-zeros
+test_cc -fno-signed-zeros
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -Wdeclaration-after-statement -Wall -Wdisabled-optimization -Wpointer-arith -Wredundant-decls -Wwrite-strings -Wtype-limits -Wundef -Wmissing-prototypes -Wno-pointer-to-int-cast -Wstrict-prototypes -Wempty-body -Wno-parentheses -Wno-switch -Wno-format-zero-length -Wno-pointer-sign -Wno-unused-const-variable -Os -fno-math-errno -fno-signed-zeros -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+test_cc -mno-red-zone
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -Wdeclaration-after-statement -Wall -Wdisabled-optimization -Wpointer-arith -Wredundant-decls -Wwrite-strings -Wtype-limits -Wundef -Wmissing-prototypes -Wno-pointer-to-int-cast -Wstrict-prototypes -Wempty-body -Wno-parentheses -Wno-switch -Wno-format-zero-length -Wno-pointer-sign -Wno-unused-const-variable -Os -fno-math-errno -fno-signed-zeros -mno-red-zone -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc: error: unrecognized command line option '-mno-red-zone'; did you mean '-fno-regmove'?
+check_cflags -fno-tree-vectorize
+test_cflags -fno-tree-vectorize
+test_cc -fno-tree-vectorize
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -Wdeclaration-after-statement -Wall -Wdisabled-optimization -Wpointer-arith -Wredundant-decls -Wwrite-strings -Wtype-limits -Wundef -Wmissing-prototypes -Wno-pointer-to-int-cast -Wstrict-prototypes -Wempty-body -Wno-parentheses -Wno-switch -Wno-format-zero-length -Wno-pointer-sign -Wno-unused-const-variable -Os -fno-math-errno -fno-signed-zeros -fno-tree-vectorize -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_cflags -Werror=format-security
+test_cflags -Werror=format-security
+test_cc -Werror=format-security
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -Wdeclaration-after-statement -Wall -Wdisabled-optimization -Wpointer-arith -Wredundant-decls -Wwrite-strings -Wtype-limits -Wundef -Wmissing-prototypes -Wno-pointer-to-int-cast -Wstrict-prototypes -Wempty-body -Wno-parentheses -Wno-switch -Wno-format-zero-length -Wno-pointer-sign -Wno-unused-const-variable -Os -fno-math-errno -fno-signed-zeros -fno-tree-vectorize -Werror=format-security -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_cflags -Werror=implicit-function-declaration
+test_cflags -Werror=implicit-function-declaration
+test_cc -Werror=implicit-function-declaration
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -Wdeclaration-after-statement -Wall -Wdisabled-optimization -Wpointer-arith -Wredundant-decls -Wwrite-strings -Wtype-limits -Wundef -Wmissing-prototypes -Wno-pointer-to-int-cast -Wstrict-prototypes -Wempty-body -Wno-parentheses -Wno-switch -Wno-format-zero-length -Wno-pointer-sign -Wno-unused-const-variable -Os -fno-math-errno -fno-signed-zeros -fno-tree-vectorize -Werror=format-security -Werror=implicit-function-declaration -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_cflags -Werror=missing-prototypes
+test_cflags -Werror=missing-prototypes
+test_cc -Werror=missing-prototypes
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -Wdeclaration-after-statement -Wall -Wdisabled-optimization -Wpointer-arith -Wredundant-decls -Wwrite-strings -Wtype-limits -Wundef -Wmissing-prototypes -Wno-pointer-to-int-cast -Wstrict-prototypes -Wempty-body -Wno-parentheses -Wno-switch -Wno-format-zero-length -Wno-pointer-sign -Wno-unused-const-variable -Os -fno-math-errno -fno-signed-zeros -fno-tree-vectorize -Werror=format-security -Werror=implicit-function-declaration -Werror=missing-prototypes -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_cflags -Werror=return-type
+test_cflags -Werror=return-type
+test_cc -Werror=return-type
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -Wdeclaration-after-statement -Wall -Wdisabled-optimization -Wpointer-arith -Wredundant-decls -Wwrite-strings -Wtype-limits -Wundef -Wmissing-prototypes -Wno-pointer-to-int-cast -Wstrict-prototypes -Wempty-body -Wno-parentheses -Wno-switch -Wno-format-zero-length -Wno-pointer-sign -Wno-unused-const-variable -Os -fno-math-errno -fno-signed-zeros -fno-tree-vectorize -Werror=format-security -Werror=implicit-function-declaration -Werror=missing-prototypes -Werror=return-type -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_cflags -Werror=vla
+test_cflags -Werror=vla
+test_cc -Werror=vla
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -Wdeclaration-after-statement -Wall -Wdisabled-optimization -Wpointer-arith -Wredundant-decls -Wwrite-strings -Wtype-limits -Wundef -Wmissing-prototypes -Wno-pointer-to-int-cast -Wstrict-prototypes -Wempty-body -Wno-parentheses -Wno-switch -Wno-format-zero-length -Wno-pointer-sign -Wno-unused-const-variable -Os -fno-math-errno -fno-signed-zeros -fno-tree-vectorize -Werror=format-security -Werror=implicit-function-declaration -Werror=missing-prototypes -Werror=return-type -Werror=vla -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_cflags -Wformat
+test_cflags -Wformat
+test_cc -Wformat
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -Wdeclaration-after-statement -Wall -Wdisabled-optimization -Wpointer-arith -Wredundant-decls -Wwrite-strings -Wtype-limits -Wundef -Wmissing-prototypes -Wno-pointer-to-int-cast -Wstrict-prototypes -Wempty-body -Wno-parentheses -Wno-switch -Wno-format-zero-length -Wno-pointer-sign -Wno-unused-const-variable -Os -fno-math-errno -fno-signed-zeros -fno-tree-vectorize -Werror=format-security -Werror=implicit-function-declaration -Werror=missing-prototypes -Werror=return-type -Werror=vla -Wformat -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+check_cflags -fdiagnostics-color=auto
+test_cflags -fdiagnostics-color=auto
+test_cc -fdiagnostics-color=auto
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -Wdeclaration-after-statement -Wall -Wdisabled-optimization -Wpointer-arith -Wredundant-decls -Wwrite-strings -Wtype-limits -Wundef -Wmissing-prototypes -Wno-pointer-to-int-cast -Wstrict-prototypes -Wempty-body -Wno-parentheses -Wno-switch -Wno-format-zero-length -Wno-pointer-sign -Wno-unused-const-variable -Os -fno-math-errno -fno-signed-zeros -fno-tree-vectorize -Werror=format-security -Werror=implicit-function-declaration -Werror=missing-prototypes -Werror=return-type -Werror=vla -Wformat -fdiagnostics-color=auto -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
+test_cflags -Wmaybe-uninitialized
+test_cc -Wmaybe-uninitialized
+BEGIN /tmp/ffconf.iiRrWAGJ/test.c
+    1	int x;
+END /tmp/ffconf.iiRrWAGJ/test.c
+arm-himix100-linux-gcc -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -Wdeclaration-after-statement -Wall -Wdisabled-optimization -Wpointer-arith -Wredundant-decls -Wwrite-strings -Wtype-limits -Wundef -Wmissing-prototypes -Wno-pointer-to-int-cast -Wstrict-prototypes -Wempty-body -Wno-parentheses -Wno-switch -Wno-format-zero-length -Wno-pointer-sign -Wno-unused-const-variable -Os -fno-math-errno -fno-signed-zeros -fno-tree-vectorize -Werror=format-security -Werror=implicit-function-declaration -Werror=missing-prototypes -Werror=return-type -Werror=vla -Wformat -fdiagnostics-color=auto -Wmaybe-uninitialized -c -o /tmp/ffconf.iiRrWAGJ/test.o /tmp/ffconf.iiRrWAGJ/test.c
diff -uparN ffmpeg-4.1/ffbuild/config.mak ffmpeg-y/ffbuild/config.mak
--- ffmpeg-4.1/ffbuild/config.mak	1970-01-01 08:00:00.000000000 +0800
+++ ffmpeg-y/ffbuild/config.mak	2019-06-29 11:49:36.833017667 +0800
@@ -0,0 +1,2611 @@
+# Automatically generated by configure - do not modify!
+ifndef FFMPEG_CONFIG_MAK
+FFMPEG_CONFIG_MAK=1
+FFMPEG_CONFIGURATION=--prefix=./install --enable-cross-compile --disable-doc --disable-htmlpages --target-os=linux --enable-static --disable-shared --disable-debug --disable-iconv --enable-small --disable-network --disable-filters --disable-devices --disable-programs --disable-swresample --disable-swscale --disable-avdevice --disable-postproc --disable-avfilter --disable-protocols --disable-pthreads --disable-runtime-cpudetect --disable-everything --enable-pic --enable-protocol=file --disable-muxers --enable-demuxer=mov --disable-neon --disable-inline-asm --disable-asm --disable-armv6 --disable-armv6t2 --disable-armv5te --disable-vfp --disable-hardcoded-tables --disable-mediacodec --enable-bsf=h264_mp4toannexb --enable-bsf=hevc_mp4toannexb --disable-pixelutils --enable-demuxer=wav --disable-gpl --cpu=cortex-a7 --arch=armv7-a --cross-prefix=arm-himix100-linux-
+prefix=./install
+LIBDIR=$(DESTDIR)${prefix}/lib
+SHLIBDIR=$(DESTDIR)${prefix}/lib
+INCDIR=$(DESTDIR)${prefix}/include
+BINDIR=$(DESTDIR)${prefix}/bin
+DATADIR=$(DESTDIR)${prefix}/share/ffmpeg
+DOCDIR=$(DESTDIR)${prefix}/share/doc/ffmpeg
+MANDIR=$(DESTDIR)${prefix}/share/man
+PKGCONFIGDIR=$(DESTDIR)${prefix}/lib/pkgconfig
+INSTALL_NAME_DIR=
+SRC_PATH=.
+SRC_LINK=.
+ifndef MAIN_MAKEFILE
+SRC_PATH:=$(SRC_PATH:.%=..%)
+endif
+CC_IDENT=gcc 6.3.0 (HC&C V100R002C00B032_20190114)
+ARCH=c
+INTRINSICS=none
+EXTERN_PREFIX=
+CC=arm-himix100-linux-gcc
+CXX=arm-himix100-linux-g++
+AS=arm-himix100-linux-gcc
+OBJCC=arm-himix100-linux-gcc
+LD=arm-himix100-linux-gcc
+DEPCC=arm-himix100-linux-gcc
+DEPCCFLAGS= $(CPPFLAGS)
+DEPAS=arm-himix100-linux-gcc
+DEPASFLAGS= $(CPPFLAGS)
+X86ASM=nasm
+DEPX86ASM=nasm
+DEPX86ASMFLAGS=$(X86ASMFLAGS)
+AR=arm-himix100-linux-ar
+ARFLAGS=rcD
+AR_O=$@
+AR_CMD=arm-himix100-linux-ar
+NM_CMD=arm-himix100-linux-nm -g
+RANLIB=arm-himix100-linux-ranlib -D
+STRIP=arm-himix100-linux-strip
+STRIPTYPE=direct
+NVCC=nvcc
+CP=cp -p
+LN_S=ln -s -f
+CPPFLAGS= -D_ISOC99_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -DPIC
+CFLAGS=   -mcpu=cortex-a7 -std=c11 -fomit-frame-pointer -fPIC -marm -Wdeclaration-after-statement -Wall -Wdisabled-optimization -Wpointer-arith -Wredundant-decls -Wwrite-strings -Wtype-limits -Wundef -Wmissing-prototypes -Wno-pointer-to-int-cast -Wstrict-prototypes -Wempty-body -Wno-parentheses -Wno-switch -Wno-format-zero-length -Wno-pointer-sign -Wno-unused-const-variable -Os -fno-math-errno -fno-signed-zeros -fno-tree-vectorize -Werror=format-security -Werror=implicit-function-declaration -Werror=missing-prototypes -Werror=return-type -Werror=vla -Wformat -fdiagnostics-color=auto -Wno-maybe-uninitialized
+CXXFLAGS=  -D__STDC_CONSTANT_MACROS -std=c++11
+OBJCFLAGS=  
+ASFLAGS=   -mcpu=cortex-a7 -fPIC
+NVCCFLAGS=-gencode arch=compute_30,code=sm_30 -O2 -m32
+AS_C=-c
+AS_O=-o $@
+OBJCC_C=-c
+OBJCC_E=-E -o $@
+OBJCC_O=-o $@
+CC_C=-c
+CC_E=-E -o $@
+CC_O=-o $@
+CXX_C=-c
+CXX_O=-o $@
+NVCC_C=-c
+NVCC_O=-o $@
+LD_O=-o $@
+X86ASM_O=-o $@
+LD_LIB=-l%
+LD_PATH=-L
+DLLTOOL=
+WINDRES=arm-himix100-linux-windres
+DEPWINDRES=arm-himix100-linux-gcc
+DOXYGEN=doxygen
+LDFLAGS=  -mcpu=cortex-a7 -Wl,--as-needed -Wl,-z,noexecstack -Wl,--warn-common -Wl,-rpath-link=libpostproc:libswresample:libswscale:libavfilter:libavdevice:libavformat:libavcodec:libavutil:libavresample
+LDEXEFLAGS=
+LDSOFLAGS=
+SHFLAGS=-shared -Wl,-soname,$$(@F) -Wl,-Bsymbolic -Wl,--version-script,$(SUBDIR)lib$(NAME).ver
+ASMSTRIPFLAGS= -x
+X86ASMFLAGS=
+BUILDSUF=
+PROGSSUF=
+FULLNAME=$(NAME)$(BUILDSUF)
+LIBPREF=lib
+LIBSUF=.a
+LIBNAME=$(LIBPREF)$(FULLNAME)$(LIBSUF)
+SLIBPREF=lib
+SLIBSUF=.so
+EXESUF=
+EXTRA_VERSION=
+CCDEP=
+CXXDEP=
+CCDEP_FLAGS=
+ASDEP=
+ASDEP_FLAGS=
+X86ASMDEP=
+X86ASMDEP_FLAGS=
+CC_DEPFLAGS=-MMD -MF $(@:.o=.d) -MT $@
+AS_DEPFLAGS=-MMD -MF $(@:.o=.d) -MT $@
+X86ASM_DEPFLAGS=
+HOSTCC=gcc
+HOSTLD=gcc
+HOSTCFLAGS=  -std=c99 -Wall -O3
+HOSTCPPFLAGS= -D_ISOC99_SOURCE -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600 -D_POSIX_C_SOURCE=200112 -D_XOPEN_SOURCE=600
+HOSTEXESUF=
+HOSTLDFLAGS= 
+HOSTEXTRALIBS=-lm
+DEPHOSTCC=gcc
+DEPHOSTCCFLAGS= $(HOSTCCFLAGS)
+HOSTCCDEP=
+HOSTCCDEP_FLAGS=
+HOSTCC_DEPFLAGS=-MMD -MF $(@:.o=.d) -MT $@
+HOSTCC_C=-c
+HOSTCC_O=-o $@
+HOSTLD_O=-o $@
+TARGET_EXEC= 
+TARGET_PATH=$(CURDIR)
+TARGET_SAMPLES=$(SAMPLES)
+CFLAGS-ffplay=
+CFLAGS_HEADERS= -Wno-deprecated-declarations -Wno-unused-variable
+LIB_INSTALL_EXTRA_CMD=$$(RANLIB) "$(LIBDIR)/$(LIBNAME)"
+EXTRALIBS=
+COMPAT_OBJS=
+INSTALL=install
+LIBTARGET=
+SLIBNAME=$(SLIBPREF)$(FULLNAME)$(SLIBSUF)
+SLIBNAME_WITH_VERSION=$(SLIBNAME).$(LIBVERSION)
+SLIBNAME_WITH_MAJOR=$(SLIBNAME).$(LIBMAJOR)
+SLIB_CREATE_DEF_CMD=
+SLIB_EXTRA_CMD=
+SLIB_INSTALL_NAME=$(SLIBNAME_WITH_VERSION)
+SLIB_INSTALL_LINKS=$(SLIBNAME_WITH_MAJOR) $(SLIBNAME)
+SLIB_INSTALL_EXTRA_LIB=
+SLIB_INSTALL_EXTRA_SHLIB=
+VERSION_SCRIPT_POSTPROCESS_CMD=cat
+SAMPLES:=$(FATE_SAMPLES)
+NOREDZONE_FLAGS=
+LIBFUZZER_PATH=
+IGNORE_TESTS=
+avdevice_FFLIBS=avformat avcodec avutil
+avfilter_FFLIBS=avutil
+swscale_FFLIBS=avutil
+postproc_FFLIBS=avutil
+avformat_FFLIBS=avcodec avutil
+avcodec_FFLIBS=avutil
+swresample_FFLIBS=avutil
+avresample_FFLIBS=avutil
+avutil_FFLIBS=
+EXTRALIBS-avdevice=-lm
+EXTRALIBS-avfilter=-lm
+EXTRALIBS-swscale=-lm
+EXTRALIBS-postproc=-lm
+EXTRALIBS-avformat=-lm
+EXTRALIBS-avcodec=-lm
+EXTRALIBS-swresample=-lm
+EXTRALIBS-avresample=-lm
+EXTRALIBS-avutil=-lm
+EXTRALIBS-ffplay=
+EXTRALIBS-ffprobe=
+EXTRALIBS-ffmpeg=
+EXTRALIBS-cpu_init=
+EXTRALIBS-cws2fws=
+!ARCH_AARCH64=yes
+!ARCH_ALPHA=yes
+!ARCH_ARM=yes
+!ARCH_AVR32=yes
+!ARCH_AVR32_AP=yes
+!ARCH_AVR32_UC=yes
+!ARCH_BFIN=yes
+!ARCH_IA64=yes
+!ARCH_M68K=yes
+!ARCH_MIPS=yes
+!ARCH_MIPS64=yes
+!ARCH_PARISC=yes
+!ARCH_PPC=yes
+!ARCH_PPC64=yes
+!ARCH_S390=yes
+!ARCH_SH4=yes
+!ARCH_SPARC=yes
+!ARCH_SPARC64=yes
+!ARCH_TILEGX=yes
+!ARCH_TILEPRO=yes
+!ARCH_TOMI=yes
+!ARCH_X86=yes
+!ARCH_X86_32=yes
+!ARCH_X86_64=yes
+!HAVE_ARMV5TE=yes
+!HAVE_ARMV6=yes
+!HAVE_ARMV6T2=yes
+!HAVE_ARMV8=yes
+!HAVE_NEON=yes
+!HAVE_VFP=yes
+!HAVE_VFPV3=yes
+!HAVE_SETEND=yes
+!HAVE_ALTIVEC=yes
+!HAVE_DCBZL=yes
+!HAVE_LDBRX=yes
+!HAVE_POWER8=yes
+!HAVE_PPC4XX=yes
+!HAVE_VSX=yes
+!HAVE_AESNI=yes
+!HAVE_AMD3DNOW=yes
+!HAVE_AMD3DNOWEXT=yes
+!HAVE_AVX=yes
+!HAVE_AVX2=yes
+!HAVE_AVX512=yes
+!HAVE_FMA3=yes
+!HAVE_FMA4=yes
+!HAVE_MMX=yes
+!HAVE_MMXEXT=yes
+!HAVE_SSE=yes
+!HAVE_SSE2=yes
+!HAVE_SSE3=yes
+!HAVE_SSE4=yes
+!HAVE_SSE42=yes
+!HAVE_SSSE3=yes
+!HAVE_XOP=yes
+!HAVE_CPUNOP=yes
+!HAVE_I686=yes
+!HAVE_MIPSFPU=yes
+!HAVE_MIPS32R2=yes
+!HAVE_MIPS32R5=yes
+!HAVE_MIPS64R2=yes
+!HAVE_MIPS32R6=yes
+!HAVE_MIPS64R6=yes
+!HAVE_MIPSDSP=yes
+!HAVE_MIPSDSPR2=yes
+!HAVE_MSA=yes
+!HAVE_LOONGSON2=yes
+!HAVE_LOONGSON3=yes
+!HAVE_MMI=yes
+!HAVE_ARMV5TE_EXTERNAL=yes
+!HAVE_ARMV6_EXTERNAL=yes
+!HAVE_ARMV6T2_EXTERNAL=yes
+!HAVE_ARMV8_EXTERNAL=yes
+!HAVE_NEON_EXTERNAL=yes
+!HAVE_VFP_EXTERNAL=yes
+!HAVE_VFPV3_EXTERNAL=yes
+HAVE_SETEND_EXTERNAL=yes
+!HAVE_ALTIVEC_EXTERNAL=yes
+!HAVE_DCBZL_EXTERNAL=yes
+!HAVE_LDBRX_EXTERNAL=yes
+!HAVE_POWER8_EXTERNAL=yes
+!HAVE_PPC4XX_EXTERNAL=yes
+!HAVE_VSX_EXTERNAL=yes
+!HAVE_AESNI_EXTERNAL=yes
+!HAVE_AMD3DNOW_EXTERNAL=yes
+!HAVE_AMD3DNOWEXT_EXTERNAL=yes
+!HAVE_AVX_EXTERNAL=yes
+!HAVE_AVX2_EXTERNAL=yes
+!HAVE_AVX512_EXTERNAL=yes
+!HAVE_FMA3_EXTERNAL=yes
+!HAVE_FMA4_EXTERNAL=yes
+!HAVE_MMX_EXTERNAL=yes
+!HAVE_MMXEXT_EXTERNAL=yes
+!HAVE_SSE_EXTERNAL=yes
+!HAVE_SSE2_EXTERNAL=yes
+!HAVE_SSE3_EXTERNAL=yes
+!HAVE_SSE4_EXTERNAL=yes
+!HAVE_SSE42_EXTERNAL=yes
+!HAVE_SSSE3_EXTERNAL=yes
+!HAVE_XOP_EXTERNAL=yes
+!HAVE_CPUNOP_EXTERNAL=yes
+!HAVE_I686_EXTERNAL=yes
+!HAVE_MIPSFPU_EXTERNAL=yes
+!HAVE_MIPS32R2_EXTERNAL=yes
+!HAVE_MIPS32R5_EXTERNAL=yes
+!HAVE_MIPS64R2_EXTERNAL=yes
+!HAVE_MIPS32R6_EXTERNAL=yes
+!HAVE_MIPS64R6_EXTERNAL=yes
+!HAVE_MIPSDSP_EXTERNAL=yes
+!HAVE_MIPSDSPR2_EXTERNAL=yes
+!HAVE_MSA_EXTERNAL=yes
+!HAVE_LOONGSON2_EXTERNAL=yes
+!HAVE_LOONGSON3_EXTERNAL=yes
+!HAVE_MMI_EXTERNAL=yes
+!HAVE_ARMV5TE_INLINE=yes
+!HAVE_ARMV6_INLINE=yes
+!HAVE_ARMV6T2_INLINE=yes
+!HAVE_ARMV8_INLINE=yes
+!HAVE_NEON_INLINE=yes
+!HAVE_VFP_INLINE=yes
+!HAVE_VFPV3_INLINE=yes
+!HAVE_SETEND_INLINE=yes
+!HAVE_ALTIVEC_INLINE=yes
+!HAVE_DCBZL_INLINE=yes
+!HAVE_LDBRX_INLINE=yes
+!HAVE_POWER8_INLINE=yes
+!HAVE_PPC4XX_INLINE=yes
+!HAVE_VSX_INLINE=yes
+!HAVE_AESNI_INLINE=yes
+!HAVE_AMD3DNOW_INLINE=yes
+!HAVE_AMD3DNOWEXT_INLINE=yes
+!HAVE_AVX_INLINE=yes
+!HAVE_AVX2_INLINE=yes
+!HAVE_AVX512_INLINE=yes
+!HAVE_FMA3_INLINE=yes
+!HAVE_FMA4_INLINE=yes
+!HAVE_MMX_INLINE=yes
+!HAVE_MMXEXT_INLINE=yes
+!HAVE_SSE_INLINE=yes
+!HAVE_SSE2_INLINE=yes
+!HAVE_SSE3_INLINE=yes
+!HAVE_SSE4_INLINE=yes
+!HAVE_SSE42_INLINE=yes
+!HAVE_SSSE3_INLINE=yes
+!HAVE_XOP_INLINE=yes
+!HAVE_CPUNOP_INLINE=yes
+!HAVE_I686_INLINE=yes
+!HAVE_MIPSFPU_INLINE=yes
+!HAVE_MIPS32R2_INLINE=yes
+!HAVE_MIPS32R5_INLINE=yes
+!HAVE_MIPS64R2_INLINE=yes
+!HAVE_MIPS32R6_INLINE=yes
+!HAVE_MIPS64R6_INLINE=yes
+!HAVE_MIPSDSP_INLINE=yes
+!HAVE_MIPSDSPR2_INLINE=yes
+!HAVE_MSA_INLINE=yes
+!HAVE_LOONGSON2_INLINE=yes
+!HAVE_LOONGSON3_INLINE=yes
+!HAVE_MMI_INLINE=yes
+!HAVE_ALIGNED_STACK=yes
+!HAVE_FAST_64BIT=yes
+HAVE_FAST_CLZ=yes
+!HAVE_FAST_CMOV=yes
+!HAVE_LOCAL_ALIGNED=yes
+!HAVE_SIMD_ALIGN_16=yes
+!HAVE_SIMD_ALIGN_32=yes
+!HAVE_SIMD_ALIGN_64=yes
+!HAVE_ATOMIC_CAS_PTR=yes
+!HAVE_MACHINE_RW_BARRIER=yes
+!HAVE_MEMORYBARRIER=yes
+!HAVE_MM_EMPTY=yes
+!HAVE_RDTSC=yes
+!HAVE_SEM_TIMEDWAIT=yes
+HAVE_SYNC_VAL_COMPARE_AND_SWAP=yes
+!HAVE_CABS=yes
+!HAVE_CEXP=yes
+!HAVE_INLINE_ASM=yes
+HAVE_SYMVER=yes
+!HAVE_X86ASM=yes
+!HAVE_BIGENDIAN=yes
+HAVE_FAST_UNALIGNED=yes
+!HAVE_ARPA_INET_H=yes
+HAVE_ASM_TYPES_H=yes
+!HAVE_CDIO_PARANOIA_H=yes
+!HAVE_CDIO_PARANOIA_PARANOIA_H=yes
+!HAVE_CUDA_H=yes
+!HAVE_DISPATCH_DISPATCH_H=yes
+!HAVE_DEV_BKTR_IOCTL_BT848_H=yes
+!HAVE_DEV_BKTR_IOCTL_METEOR_H=yes
+!HAVE_DEV_IC_BT8XX_H=yes
+!HAVE_DEV_VIDEO_BKTR_IOCTL_BT848_H=yes
+!HAVE_DEV_VIDEO_METEOR_IOCTL_METEOR_H=yes
+!HAVE_DIRECT_H=yes
+HAVE_DIRENT_H=yes
+!HAVE_DXGIDEBUG_H=yes
+!HAVE_DXVA_H=yes
+!HAVE_ES2_GL_H=yes
+!HAVE_GSM_H=yes
+!HAVE_IO_H=yes
+HAVE_LINUX_PERF_EVENT_H=yes
+!HAVE_MACHINE_IOCTL_BT848_H=yes
+!HAVE_MACHINE_IOCTL_METEOR_H=yes
+HAVE_MALLOC_H=yes
+!HAVE_OPENCV2_CORE_CORE_C_H=yes
+!HAVE_OPENGL_GL3_H=yes
+HAVE_POLL_H=yes
+HAVE_SYS_PARAM_H=yes
+HAVE_SYS_RESOURCE_H=yes
+HAVE_SYS_SELECT_H=yes
+HAVE_SYS_SOUNDCARD_H=yes
+HAVE_SYS_TIME_H=yes
+HAVE_SYS_UN_H=yes
+!HAVE_SYS_VIDEOIO_H=yes
+HAVE_TERMIOS_H=yes
+!HAVE_UDPLITE_H=yes
+HAVE_UNISTD_H=yes
+!HAVE_VALGRIND_VALGRIND_H=yes
+!HAVE_WINDOWS_H=yes
+!HAVE_WINSOCK2_H=yes
+!HAVE_INTRINSICS_NEON=yes
+HAVE_ATANF=yes
+HAVE_ATAN2F=yes
+HAVE_CBRT=yes
+HAVE_CBRTF=yes
+HAVE_COPYSIGN=yes
+HAVE_COSF=yes
+HAVE_ERF=yes
+HAVE_EXP2=yes
+HAVE_EXP2F=yes
+HAVE_EXPF=yes
+HAVE_HYPOT=yes
+HAVE_ISFINITE=yes
+HAVE_ISINF=yes
+HAVE_ISNAN=yes
+HAVE_LDEXPF=yes
+HAVE_LLRINT=yes
+HAVE_LLRINTF=yes
+HAVE_LOG2=yes
+HAVE_LOG2F=yes
+HAVE_LOG10F=yes
+HAVE_LRINT=yes
+HAVE_LRINTF=yes
+HAVE_POWF=yes
+HAVE_RINT=yes
+HAVE_ROUND=yes
+HAVE_ROUNDF=yes
+HAVE_SINF=yes
+HAVE_TRUNC=yes
+HAVE_TRUNCF=yes
+!HAVE_DOS_PATHS=yes
+!HAVE_LIBC_MSVCRT=yes
+!HAVE_MMAL_PARAMETER_VIDEO_MAX_NUM_CALLBACKS=yes
+HAVE_SECTION_DATA_REL_RO=yes
+!HAVE_THREADS=yes
+!HAVE_UWP=yes
+!HAVE_WINRT=yes
+HAVE_ACCESS=yes
+!HAVE_ALIGNED_MALLOC=yes
+!HAVE_ARC4RANDOM=yes
+HAVE_CLOCK_GETTIME=yes
+!HAVE_CLOSESOCKET=yes
+!HAVE_COMMANDLINETOARGVW=yes
+HAVE_FCNTL=yes
+!HAVE_GETADDRINFO=yes
+!HAVE_GETHRTIME=yes
+HAVE_GETOPT=yes
+!HAVE_GETPROCESSAFFINITYMASK=yes
+!HAVE_GETPROCESSMEMORYINFO=yes
+!HAVE_GETPROCESSTIMES=yes
+HAVE_GETRUSAGE=yes
+!HAVE_GETSYSTEMTIMEASFILETIME=yes
+HAVE_GETTIMEOFDAY=yes
+HAVE_GLOB=yes
+!HAVE_GLXGETPROCADDRESS=yes
+HAVE_GMTIME_R=yes
+!HAVE_INET_ATON=yes
+HAVE_ISATTY=yes
+!HAVE_KBHIT=yes
+HAVE_LOCALTIME_R=yes
+HAVE_LSTAT=yes
+!HAVE_LZO1X_999_COMPRESS=yes
+!HAVE_MACH_ABSOLUTE_TIME=yes
+!HAVE_MAPVIEWOFFILE=yes
+HAVE_MEMALIGN=yes
+HAVE_MKSTEMP=yes
+HAVE_MMAP=yes
+HAVE_MPROTECT=yes
+HAVE_NANOSLEEP=yes
+!HAVE_PEEKNAMEDPIPE=yes
+HAVE_POSIX_MEMALIGN=yes
+!HAVE_PTHREAD_CANCEL=yes
+HAVE_SCHED_GETAFFINITY=yes
+!HAVE_SECITEMIMPORT=yes
+!HAVE_SETCONSOLETEXTATTRIBUTE=yes
+!HAVE_SETCONSOLECTRLHANDLER=yes
+!HAVE_SETMODE=yes
+HAVE_SETRLIMIT=yes
+!HAVE_SLEEP=yes
+HAVE_STRERROR_R=yes
+HAVE_SYSCONF=yes
+HAVE_SYSCTL=yes
+HAVE_USLEEP=yes
+!HAVE_UTGETOSTYPEFROMSTRING=yes
+!HAVE_VIRTUALALLOC=yes
+!HAVE_WGLGETPROCADDRESS=yes
+!HAVE_BCRYPT=yes
+!HAVE_VAAPI_DRM=yes
+!HAVE_VAAPI_X11=yes
+!HAVE_VDPAU_X11=yes
+!HAVE_PTHREADS=yes
+!HAVE_OS2THREADS=yes
+!HAVE_W32THREADS=yes
+HAVE_AS_ARCH_DIRECTIVE=yes
+HAVE_AS_DN_DIRECTIVE=yes
+HAVE_AS_FPU_DIRECTIVE=yes
+!HAVE_AS_FUNC=yes
+HAVE_AS_OBJECT_ARCH=yes
+HAVE_ASM_MOD_Q=yes
+!HAVE_BLOCKS_EXTENSION=yes
+!HAVE_EBP_AVAILABLE=yes
+!HAVE_EBX_AVAILABLE=yes
+!HAVE_GNU_AS=yes
+!HAVE_GNU_WINDRES=yes
+!HAVE_IBM_ASM=yes
+!HAVE_INLINE_ASM_DIRECT_SYMBOL_REFS=yes
+HAVE_INLINE_ASM_LABELS=yes
+HAVE_INLINE_ASM_NONLOCAL_LABELS=yes
+HAVE_PRAGMA_DEPRECATED=yes
+HAVE_RSYNC_CONTIMEOUT=yes
+!HAVE_SYMVER_ASM_LABEL=yes
+HAVE_SYMVER_GNU_ASM=yes
+!HAVE_VFP_ARGS=yes
+!HAVE_XFORM_ASM=yes
+!HAVE_XMM_CLOBBERS=yes
+!HAVE_KCMVIDEOCODECTYPE_HEVC=yes
+!HAVE_SOCKLEN_T=yes
+!HAVE_STRUCT_ADDRINFO=yes
+!HAVE_STRUCT_GROUP_SOURCE_REQ=yes
+!HAVE_STRUCT_IP_MREQ_SOURCE=yes
+!HAVE_STRUCT_IPV6_MREQ=yes
+!HAVE_STRUCT_MSGHDR_MSG_FLAGS=yes
+!HAVE_STRUCT_POLLFD=yes
+HAVE_STRUCT_RUSAGE_RU_MAXRSS=yes
+!HAVE_STRUCT_SCTP_EVENT_SUBSCRIBE=yes
+!HAVE_STRUCT_SOCKADDR_IN6=yes
+!HAVE_STRUCT_SOCKADDR_SA_LEN=yes
+!HAVE_STRUCT_SOCKADDR_STORAGE=yes
+HAVE_STRUCT_STAT_ST_MTIM_TV_NSEC=yes
+HAVE_STRUCT_V4L2_FRMIVALENUM_DISCRETE=yes
+HAVE_MAKEINFO=yes
+HAVE_MAKEINFO_HTML=yes
+!HAVE_OPENCL_D3D11=yes
+!HAVE_OPENCL_DRM_ARM=yes
+!HAVE_OPENCL_DRM_BEIGNET=yes
+!HAVE_OPENCL_DXVA2=yes
+!HAVE_OPENCL_VAAPI_BEIGNET=yes
+!HAVE_OPENCL_VAAPI_INTEL_MEDIA=yes
+HAVE_PERL=yes
+HAVE_POD2MAN=yes
+!HAVE_TEXI2HTML=yes
+!CONFIG_DOC=yes
+!CONFIG_HTMLPAGES=yes
+CONFIG_MANPAGES=yes
+CONFIG_PODPAGES=yes
+CONFIG_TXTPAGES=yes
+CONFIG_AVIO_DIR_CMD_EXAMPLE=yes
+CONFIG_AVIO_READING_EXAMPLE=yes
+CONFIG_DECODE_AUDIO_EXAMPLE=yes
+CONFIG_DECODE_VIDEO_EXAMPLE=yes
+CONFIG_DEMUXING_DECODING_EXAMPLE=yes
+CONFIG_ENCODE_AUDIO_EXAMPLE=yes
+CONFIG_ENCODE_VIDEO_EXAMPLE=yes
+CONFIG_EXTRACT_MVS_EXAMPLE=yes
+!CONFIG_FILTER_AUDIO_EXAMPLE=yes
+!CONFIG_FILTERING_AUDIO_EXAMPLE=yes
+!CONFIG_FILTERING_VIDEO_EXAMPLE=yes
+CONFIG_HTTP_MULTICLIENT_EXAMPLE=yes
+CONFIG_HW_DECODE_EXAMPLE=yes
+CONFIG_METADATA_EXAMPLE=yes
+!CONFIG_MUXING_EXAMPLE=yes
+!CONFIG_QSVDEC_EXAMPLE=yes
+CONFIG_REMUXING_EXAMPLE=yes
+!CONFIG_RESAMPLING_AUDIO_EXAMPLE=yes
+!CONFIG_SCALING_VIDEO_EXAMPLE=yes
+!CONFIG_TRANSCODE_AAC_EXAMPLE=yes
+!CONFIG_TRANSCODING_EXAMPLE=yes
+!CONFIG_VAAPI_ENCODE_EXAMPLE=yes
+!CONFIG_VAAPI_TRANSCODE_EXAMPLE=yes
+!CONFIG_AVISYNTH=yes
+!CONFIG_FREI0R=yes
+!CONFIG_LIBCDIO=yes
+!CONFIG_LIBDAVS2=yes
+!CONFIG_LIBRUBBERBAND=yes
+!CONFIG_LIBVIDSTAB=yes
+!CONFIG_LIBX264=yes
+!CONFIG_LIBX265=yes
+!CONFIG_LIBXAVS=yes
+!CONFIG_LIBXAVS2=yes
+!CONFIG_LIBXVID=yes
+!CONFIG_DECKLINK=yes
+!CONFIG_LIBNDI_NEWTEK=yes
+!CONFIG_LIBFDK_AAC=yes
+!CONFIG_OPENSSL=yes
+!CONFIG_LIBTLS=yes
+!CONFIG_GMP=yes
+!CONFIG_LIBLENSFUN=yes
+!CONFIG_LIBOPENCORE_AMRNB=yes
+!CONFIG_LIBOPENCORE_AMRWB=yes
+!CONFIG_LIBVMAF=yes
+!CONFIG_LIBVO_AMRWBENC=yes
+!CONFIG_MBEDTLS=yes
+!CONFIG_RKMPP=yes
+!CONFIG_LIBSMBCLIENT=yes
+!CONFIG_CHROMAPRINT=yes
+!CONFIG_GCRYPT=yes
+!CONFIG_GNUTLS=yes
+!CONFIG_JNI=yes
+!CONFIG_LADSPA=yes
+!CONFIG_LIBAOM=yes
+!CONFIG_LIBASS=yes
+!CONFIG_LIBBLURAY=yes
+!CONFIG_LIBBS2B=yes
+!CONFIG_LIBCACA=yes
+!CONFIG_LIBCELT=yes
+!CONFIG_LIBCODEC2=yes
+!CONFIG_LIBDC1394=yes
+!CONFIG_LIBDRM=yes
+!CONFIG_LIBFLITE=yes
+!CONFIG_LIBFONTCONFIG=yes
+!CONFIG_LIBFREETYPE=yes
+!CONFIG_LIBFRIBIDI=yes
+!CONFIG_LIBGME=yes
+!CONFIG_LIBGSM=yes
+!CONFIG_LIBIEC61883=yes
+!CONFIG_LIBILBC=yes
+!CONFIG_LIBJACK=yes
+!CONFIG_LIBKLVANC=yes
+!CONFIG_LIBKVAZAAR=yes
+!CONFIG_LIBMODPLUG=yes
+!CONFIG_LIBMP3LAME=yes
+!CONFIG_LIBMYSOFA=yes
+!CONFIG_LIBOPENCV=yes
+!CONFIG_LIBOPENH264=yes
+!CONFIG_LIBOPENJPEG=yes
+!CONFIG_LIBOPENMPT=yes
+!CONFIG_LIBOPUS=yes
+!CONFIG_LIBPULSE=yes
+!CONFIG_LIBRSVG=yes
+!CONFIG_LIBRTMP=yes
+!CONFIG_LIBSHINE=yes
+!CONFIG_LIBSMBCLIENT=yes
+!CONFIG_LIBSNAPPY=yes
+!CONFIG_LIBSOXR=yes
+!CONFIG_LIBSPEEX=yes
+!CONFIG_LIBSRT=yes
+!CONFIG_LIBSSH=yes
+!CONFIG_LIBTENSORFLOW=yes
+!CONFIG_LIBTESSERACT=yes
+!CONFIG_LIBTHEORA=yes
+!CONFIG_LIBTWOLAME=yes
+!CONFIG_LIBV4L2=yes
+!CONFIG_LIBVORBIS=yes
+!CONFIG_LIBVPX=yes
+!CONFIG_LIBWAVPACK=yes
+!CONFIG_LIBWEBP=yes
+!CONFIG_LIBXML2=yes
+!CONFIG_LIBZIMG=yes
+!CONFIG_LIBZMQ=yes
+!CONFIG_LIBZVBI=yes
+!CONFIG_LV2=yes
+!CONFIG_MEDIACODEC=yes
+!CONFIG_OPENAL=yes
+!CONFIG_OPENGL=yes
+!CONFIG_VAPOURSYNTH=yes
+!CONFIG_ALSA=yes
+!CONFIG_APPKIT=yes
+!CONFIG_AVFOUNDATION=yes
+!CONFIG_BZLIB=yes
+!CONFIG_COREIMAGE=yes
+!CONFIG_ICONV=yes
+!CONFIG_LIBXCB=yes
+!CONFIG_LIBXCB_SHM=yes
+!CONFIG_LIBXCB_SHAPE=yes
+!CONFIG_LIBXCB_XFIXES=yes
+!CONFIG_LZMA=yes
+!CONFIG_SCHANNEL=yes
+!CONFIG_SDL2=yes
+!CONFIG_SECURETRANSPORT=yes
+!CONFIG_SNDIO=yes
+!CONFIG_XLIB=yes
+!CONFIG_ZLIB=yes
+!CONFIG_CUDA_SDK=yes
+!CONFIG_LIBNPP=yes
+!CONFIG_LIBMFX=yes
+!CONFIG_MMAL=yes
+!CONFIG_OMX=yes
+!CONFIG_OPENCL=yes
+!CONFIG_AMF=yes
+!CONFIG_AUDIOTOOLBOX=yes
+!CONFIG_CRYSTALHD=yes
+!CONFIG_CUDA=yes
+!CONFIG_CUVID=yes
+!CONFIG_D3D11VA=yes
+!CONFIG_DXVA2=yes
+!CONFIG_FFNVCODEC=yes
+!CONFIG_NVDEC=yes
+!CONFIG_NVENC=yes
+!CONFIG_VAAPI=yes
+!CONFIG_VDPAU=yes
+!CONFIG_VIDEOTOOLBOX=yes
+!CONFIG_V4L2_M2M=yes
+!CONFIG_XVMC=yes
+!CONFIG_FTRAPV=yes
+!CONFIG_GRAY=yes
+!CONFIG_HARDCODED_TABLES=yes
+!CONFIG_OMX_RPI=yes
+!CONFIG_RUNTIME_CPUDETECT=yes
+CONFIG_SAFE_BITSTREAM_READER=yes
+!CONFIG_SHARED=yes
+CONFIG_SMALL=yes
+CONFIG_STATIC=yes
+CONFIG_SWSCALE_ALPHA=yes
+!CONFIG_GPL=yes
+!CONFIG_NONFREE=yes
+!CONFIG_VERSION3=yes
+!CONFIG_AVDEVICE=yes
+!CONFIG_AVFILTER=yes
+!CONFIG_SWSCALE=yes
+!CONFIG_POSTPROC=yes
+CONFIG_AVFORMAT=yes
+CONFIG_AVCODEC=yes
+!CONFIG_SWRESAMPLE=yes
+!CONFIG_AVRESAMPLE=yes
+CONFIG_AVUTIL=yes
+!CONFIG_FFPLAY=yes
+!CONFIG_FFPROBE=yes
+!CONFIG_FFMPEG=yes
+!CONFIG_DCT=yes
+!CONFIG_DWT=yes
+!CONFIG_ERROR_RESILIENCE=yes
+CONFIG_FAAN=yes
+CONFIG_FAST_UNALIGNED=yes
+!CONFIG_FFT=yes
+!CONFIG_LSP=yes
+!CONFIG_LZO=yes
+!CONFIG_MDCT=yes
+!CONFIG_PIXELUTILS=yes
+!CONFIG_NETWORK=yes
+!CONFIG_RDFT=yes
+!CONFIG_AUTODETECT=yes
+!CONFIG_FONTCONFIG=yes
+CONFIG_LINUX_PERF=yes
+!CONFIG_MEMORY_POISONING=yes
+!CONFIG_NEON_CLOBBER_TEST=yes
+!CONFIG_OSSFUZZ=yes
+CONFIG_PIC=yes
+!CONFIG_THUMB=yes
+!CONFIG_VALGRIND_BACKTRACE=yes
+!CONFIG_XMM_CLOBBER_TEST=yes
+CONFIG_BSFS=yes
+!CONFIG_DECODERS=yes
+!CONFIG_ENCODERS=yes
+!CONFIG_HWACCELS=yes
+!CONFIG_PARSERS=yes
+!CONFIG_INDEVS=yes
+!CONFIG_OUTDEVS=yes
+!CONFIG_FILTERS=yes
+CONFIG_DEMUXERS=yes
+!CONFIG_MUXERS=yes
+CONFIG_PROTOCOLS=yes
+!CONFIG_AANDCTTABLES=yes
+!CONFIG_AC3DSP=yes
+!CONFIG_ADTS_HEADER=yes
+!CONFIG_AUDIO_FRAME_QUEUE=yes
+!CONFIG_AUDIODSP=yes
+!CONFIG_BLOCKDSP=yes
+!CONFIG_BSWAPDSP=yes
+!CONFIG_CABAC=yes
+!CONFIG_CBS=yes
+!CONFIG_CBS_AV1=yes
+!CONFIG_CBS_H264=yes
+!CONFIG_CBS_H265=yes
+!CONFIG_CBS_JPEG=yes
+!CONFIG_CBS_MPEG2=yes
+!CONFIG_CBS_VP9=yes
+!CONFIG_DIRAC_PARSE=yes
+!CONFIG_DNN=yes
+!CONFIG_DVPROFILE=yes
+!CONFIG_EXIF=yes
+CONFIG_FAANDCT=yes
+CONFIG_FAANIDCT=yes
+CONFIG_FDCTDSP=yes
+!CONFIG_FLACDSP=yes
+!CONFIG_FMTCONVERT=yes
+!CONFIG_FRAME_THREAD_ENCODER=yes
+!CONFIG_G722DSP=yes
+!CONFIG_GOLOMB=yes
+!CONFIG_GPLV3=yes
+!CONFIG_H263DSP=yes
+!CONFIG_H264CHROMA=yes
+!CONFIG_H264DSP=yes
+!CONFIG_H264PARSE=yes
+!CONFIG_H264PRED=yes
+!CONFIG_H264QPEL=yes
+!CONFIG_HEVCPARSE=yes
+!CONFIG_HPELDSP=yes
+!CONFIG_HUFFMAN=yes
+!CONFIG_HUFFYUVDSP=yes
+!CONFIG_HUFFYUVENCDSP=yes
+CONFIG_IDCTDSP=yes
+!CONFIG_IIRFILTER=yes
+!CONFIG_MDCT15=yes
+!CONFIG_INTRAX8=yes
+CONFIG_ISO_MEDIA=yes
+!CONFIG_IVIDSP=yes
+!CONFIG_JPEGTABLES=yes
+!CONFIG_LGPLV3=yes
+!CONFIG_LIBX262=yes
+!CONFIG_LLAUDDSP=yes
+!CONFIG_LLVIDDSP=yes
+!CONFIG_LLVIDENCDSP=yes
+!CONFIG_LPC=yes
+!CONFIG_LZF=yes
+!CONFIG_ME_CMP=yes
+!CONFIG_MPEG_ER=yes
+!CONFIG_MPEGAUDIO=yes
+!CONFIG_MPEGAUDIODSP=yes
+!CONFIG_MPEGAUDIOHEADER=yes
+!CONFIG_MPEGVIDEO=yes
+!CONFIG_MPEGVIDEOENC=yes
+!CONFIG_MSS34DSP=yes
+!CONFIG_PIXBLOCKDSP=yes
+!CONFIG_QPELDSP=yes
+!CONFIG_QSV=yes
+!CONFIG_QSVDEC=yes
+!CONFIG_QSVENC=yes
+!CONFIG_QSVVPP=yes
+!CONFIG_RANGECODER=yes
+CONFIG_RIFFDEC=yes
+!CONFIG_RIFFENC=yes
+!CONFIG_RTPDEC=yes
+!CONFIG_RTPENC_CHAIN=yes
+!CONFIG_RV34DSP=yes
+!CONFIG_SINEWIN=yes
+!CONFIG_SNAPPY=yes
+!CONFIG_SRTP=yes
+!CONFIG_STARTCODE=yes
+!CONFIG_TEXTUREDSP=yes
+!CONFIG_TEXTUREDSPENC=yes
+!CONFIG_TPELDSP=yes
+!CONFIG_VAAPI_1=yes
+!CONFIG_VAAPI_ENCODE=yes
+!CONFIG_VC1DSP=yes
+!CONFIG_VIDEODSP=yes
+!CONFIG_VP3DSP=yes
+!CONFIG_VP56DSP=yes
+!CONFIG_VP8DSP=yes
+!CONFIG_WMA_FREQS=yes
+!CONFIG_WMV2DSP=yes
+!CONFIG_AAC_ADTSTOASC_BSF=yes
+!CONFIG_AV1_METADATA_BSF=yes
+!CONFIG_CHOMP_BSF=yes
+!CONFIG_DUMP_EXTRADATA_BSF=yes
+!CONFIG_DCA_CORE_BSF=yes
+!CONFIG_EAC3_CORE_BSF=yes
+!CONFIG_EXTRACT_EXTRADATA_BSF=yes
+!CONFIG_FILTER_UNITS_BSF=yes
+!CONFIG_H264_METADATA_BSF=yes
+CONFIG_H264_MP4TOANNEXB_BSF=yes
+!CONFIG_H264_REDUNDANT_PPS_BSF=yes
+!CONFIG_HAPQA_EXTRACT_BSF=yes
+!CONFIG_HEVC_METADATA_BSF=yes
+CONFIG_HEVC_MP4TOANNEXB_BSF=yes
+!CONFIG_IMX_DUMP_HEADER_BSF=yes
+!CONFIG_MJPEG2JPEG_BSF=yes
+!CONFIG_MJPEGA_DUMP_HEADER_BSF=yes
+!CONFIG_MP3_HEADER_DECOMPRESS_BSF=yes
+!CONFIG_MPEG2_METADATA_BSF=yes
+!CONFIG_MPEG4_UNPACK_BFRAMES_BSF=yes
+!CONFIG_MOV2TEXTSUB_BSF=yes
+!CONFIG_NOISE_BSF=yes
+CONFIG_NULL_BSF=yes
+!CONFIG_REMOVE_EXTRADATA_BSF=yes
+!CONFIG_TEXT2MOVSUB_BSF=yes
+!CONFIG_TRACE_HEADERS_BSF=yes
+!CONFIG_VP9_METADATA_BSF=yes
+!CONFIG_VP9_RAW_REORDER_BSF=yes
+!CONFIG_VP9_SUPERFRAME_BSF=yes
+!CONFIG_VP9_SUPERFRAME_SPLIT_BSF=yes
+!CONFIG_AASC_DECODER=yes
+!CONFIG_AIC_DECODER=yes
+!CONFIG_ALIAS_PIX_DECODER=yes
+!CONFIG_AMV_DECODER=yes
+!CONFIG_ANM_DECODER=yes
+!CONFIG_ANSI_DECODER=yes
+!CONFIG_APNG_DECODER=yes
+!CONFIG_ASV1_DECODER=yes
+!CONFIG_ASV2_DECODER=yes
+!CONFIG_AURA_DECODER=yes
+!CONFIG_AURA2_DECODER=yes
+!CONFIG_AVRP_DECODER=yes
+!CONFIG_AVRN_DECODER=yes
+!CONFIG_AVS_DECODER=yes
+!CONFIG_AVUI_DECODER=yes
+!CONFIG_AYUV_DECODER=yes
+!CONFIG_BETHSOFTVID_DECODER=yes
+!CONFIG_BFI_DECODER=yes
+!CONFIG_BINK_DECODER=yes
+!CONFIG_BITPACKED_DECODER=yes
+!CONFIG_BMP_DECODER=yes
+!CONFIG_BMV_VIDEO_DECODER=yes
+!CONFIG_BRENDER_PIX_DECODER=yes
+!CONFIG_C93_DECODER=yes
+!CONFIG_CAVS_DECODER=yes
+!CONFIG_CDGRAPHICS_DECODER=yes
+!CONFIG_CDXL_DECODER=yes
+!CONFIG_CFHD_DECODER=yes
+!CONFIG_CINEPAK_DECODER=yes
+!CONFIG_CLEARVIDEO_DECODER=yes
+!CONFIG_CLJR_DECODER=yes
+!CONFIG_CLLC_DECODER=yes
+!CONFIG_COMFORTNOISE_DECODER=yes
+!CONFIG_CPIA_DECODER=yes
+!CONFIG_CSCD_DECODER=yes
+!CONFIG_CYUV_DECODER=yes
+!CONFIG_DDS_DECODER=yes
+!CONFIG_DFA_DECODER=yes
+!CONFIG_DIRAC_DECODER=yes
+!CONFIG_DNXHD_DECODER=yes
+!CONFIG_DPX_DECODER=yes
+!CONFIG_DSICINVIDEO_DECODER=yes
+!CONFIG_DVAUDIO_DECODER=yes
+!CONFIG_DVVIDEO_DECODER=yes
+!CONFIG_DXA_DECODER=yes
+!CONFIG_DXTORY_DECODER=yes
+!CONFIG_DXV_DECODER=yes
+!CONFIG_EACMV_DECODER=yes
+!CONFIG_EAMAD_DECODER=yes
+!CONFIG_EATGQ_DECODER=yes
+!CONFIG_EATGV_DECODER=yes
+!CONFIG_EATQI_DECODER=yes
+!CONFIG_EIGHTBPS_DECODER=yes
+!CONFIG_EIGHTSVX_EXP_DECODER=yes
+!CONFIG_EIGHTSVX_FIB_DECODER=yes
+!CONFIG_ESCAPE124_DECODER=yes
+!CONFIG_ESCAPE130_DECODER=yes
+!CONFIG_EXR_DECODER=yes
+!CONFIG_FFV1_DECODER=yes
+!CONFIG_FFVHUFF_DECODER=yes
+!CONFIG_FIC_DECODER=yes
+!CONFIG_FITS_DECODER=yes
+!CONFIG_FLASHSV_DECODER=yes
+!CONFIG_FLASHSV2_DECODER=yes
+!CONFIG_FLIC_DECODER=yes
+!CONFIG_FLV_DECODER=yes
+!CONFIG_FMVC_DECODER=yes
+!CONFIG_FOURXM_DECODER=yes
+!CONFIG_FRAPS_DECODER=yes
+!CONFIG_FRWU_DECODER=yes
+!CONFIG_G2M_DECODER=yes
+!CONFIG_GDV_DECODER=yes
+!CONFIG_GIF_DECODER=yes
+!CONFIG_H261_DECODER=yes
+!CONFIG_H263_DECODER=yes
+!CONFIG_H263I_DECODER=yes
+!CONFIG_H263P_DECODER=yes
+!CONFIG_H263_V4L2M2M_DECODER=yes
+!CONFIG_H264_DECODER=yes
+!CONFIG_H264_CRYSTALHD_DECODER=yes
+!CONFIG_H264_V4L2M2M_DECODER=yes
+!CONFIG_H264_MEDIACODEC_DECODER=yes
+!CONFIG_H264_MMAL_DECODER=yes
+!CONFIG_H264_QSV_DECODER=yes
+!CONFIG_H264_RKMPP_DECODER=yes
+!CONFIG_HAP_DECODER=yes
+!CONFIG_HEVC_DECODER=yes
+!CONFIG_HEVC_QSV_DECODER=yes
+!CONFIG_HEVC_RKMPP_DECODER=yes
+!CONFIG_HEVC_V4L2M2M_DECODER=yes
+!CONFIG_HNM4_VIDEO_DECODER=yes
+!CONFIG_HQ_HQA_DECODER=yes
+!CONFIG_HQX_DECODER=yes
+!CONFIG_HUFFYUV_DECODER=yes
+!CONFIG_IDCIN_DECODER=yes
+!CONFIG_IFF_ILBM_DECODER=yes
+!CONFIG_IMM4_DECODER=yes
+!CONFIG_INDEO2_DECODER=yes
+!CONFIG_INDEO3_DECODER=yes
+!CONFIG_INDEO4_DECODER=yes
+!CONFIG_INDEO5_DECODER=yes
+!CONFIG_INTERPLAY_VIDEO_DECODER=yes
+!CONFIG_JPEG2000_DECODER=yes
+!CONFIG_JPEGLS_DECODER=yes
+!CONFIG_JV_DECODER=yes
+!CONFIG_KGV1_DECODER=yes
+!CONFIG_KMVC_DECODER=yes
+!CONFIG_LAGARITH_DECODER=yes
+!CONFIG_LOCO_DECODER=yes
+!CONFIG_M101_DECODER=yes
+!CONFIG_MAGICYUV_DECODER=yes
+!CONFIG_MDEC_DECODER=yes
+!CONFIG_MIMIC_DECODER=yes
+!CONFIG_MJPEG_DECODER=yes
+!CONFIG_MJPEGB_DECODER=yes
+!CONFIG_MMVIDEO_DECODER=yes
+!CONFIG_MOTIONPIXELS_DECODER=yes
+!CONFIG_MPEG1VIDEO_DECODER=yes
+!CONFIG_MPEG2VIDEO_DECODER=yes
+!CONFIG_MPEG4_DECODER=yes
+!CONFIG_MPEG4_CRYSTALHD_DECODER=yes
+!CONFIG_MPEG4_V4L2M2M_DECODER=yes
+!CONFIG_MPEG4_MMAL_DECODER=yes
+!CONFIG_MPEGVIDEO_DECODER=yes
+!CONFIG_MPEG1_V4L2M2M_DECODER=yes
+!CONFIG_MPEG2_MMAL_DECODER=yes
+!CONFIG_MPEG2_CRYSTALHD_DECODER=yes
+!CONFIG_MPEG2_V4L2M2M_DECODER=yes
+!CONFIG_MPEG2_QSV_DECODER=yes
+!CONFIG_MPEG2_MEDIACODEC_DECODER=yes
+!CONFIG_MSA1_DECODER=yes
+!CONFIG_MSCC_DECODER=yes
+!CONFIG_MSMPEG4V1_DECODER=yes
+!CONFIG_MSMPEG4V2_DECODER=yes
+!CONFIG_MSMPEG4V3_DECODER=yes
+!CONFIG_MSMPEG4_CRYSTALHD_DECODER=yes
+!CONFIG_MSRLE_DECODER=yes
+!CONFIG_MSS1_DECODER=yes
+!CONFIG_MSS2_DECODER=yes
+!CONFIG_MSVIDEO1_DECODER=yes
+!CONFIG_MSZH_DECODER=yes
+!CONFIG_MTS2_DECODER=yes
+!CONFIG_MVC1_DECODER=yes
+!CONFIG_MVC2_DECODER=yes
+!CONFIG_MWSC_DECODER=yes
+!CONFIG_MXPEG_DECODER=yes
+!CONFIG_NUV_DECODER=yes
+!CONFIG_PAF_VIDEO_DECODER=yes
+!CONFIG_PAM_DECODER=yes
+!CONFIG_PBM_DECODER=yes
+!CONFIG_PCX_DECODER=yes
+!CONFIG_PGM_DECODER=yes
+!CONFIG_PGMYUV_DECODER=yes
+!CONFIG_PICTOR_DECODER=yes
+!CONFIG_PIXLET_DECODER=yes
+!CONFIG_PNG_DECODER=yes
+!CONFIG_PPM_DECODER=yes
+!CONFIG_PRORES_DECODER=yes
+!CONFIG_PROSUMER_DECODER=yes
+!CONFIG_PSD_DECODER=yes
+!CONFIG_PTX_DECODER=yes
+!CONFIG_QDRAW_DECODER=yes
+!CONFIG_QPEG_DECODER=yes
+!CONFIG_QTRLE_DECODER=yes
+!CONFIG_R10K_DECODER=yes
+!CONFIG_R210_DECODER=yes
+!CONFIG_RASC_DECODER=yes
+!CONFIG_RAWVIDEO_DECODER=yes
+!CONFIG_RL2_DECODER=yes
+!CONFIG_ROQ_DECODER=yes
+!CONFIG_RPZA_DECODER=yes
+!CONFIG_RSCC_DECODER=yes
+!CONFIG_RV10_DECODER=yes
+!CONFIG_RV20_DECODER=yes
+!CONFIG_RV30_DECODER=yes
+!CONFIG_RV40_DECODER=yes
+!CONFIG_S302M_DECODER=yes
+!CONFIG_SANM_DECODER=yes
+!CONFIG_SCPR_DECODER=yes
+!CONFIG_SCREENPRESSO_DECODER=yes
+!CONFIG_SDX2_DPCM_DECODER=yes
+!CONFIG_SGI_DECODER=yes
+!CONFIG_SGIRLE_DECODER=yes
+!CONFIG_SHEERVIDEO_DECODER=yes
+!CONFIG_SMACKER_DECODER=yes
+!CONFIG_SMC_DECODER=yes
+!CONFIG_SMVJPEG_DECODER=yes
+!CONFIG_SNOW_DECODER=yes
+!CONFIG_SP5X_DECODER=yes
+!CONFIG_SPEEDHQ_DECODER=yes
+!CONFIG_SRGC_DECODER=yes
+!CONFIG_SUNRAST_DECODER=yes
+!CONFIG_SVQ1_DECODER=yes
+!CONFIG_SVQ3_DECODER=yes
+!CONFIG_TARGA_DECODER=yes
+!CONFIG_TARGA_Y216_DECODER=yes
+!CONFIG_TDSC_DECODER=yes
+!CONFIG_THEORA_DECODER=yes
+!CONFIG_THP_DECODER=yes
+!CONFIG_TIERTEXSEQVIDEO_DECODER=yes
+!CONFIG_TIFF_DECODER=yes
+!CONFIG_TMV_DECODER=yes
+!CONFIG_TRUEMOTION1_DECODER=yes
+!CONFIG_TRUEMOTION2_DECODER=yes
+!CONFIG_TRUEMOTION2RT_DECODER=yes
+!CONFIG_TSCC_DECODER=yes
+!CONFIG_TSCC2_DECODER=yes
+!CONFIG_TXD_DECODER=yes
+!CONFIG_ULTI_DECODER=yes
+!CONFIG_UTVIDEO_DECODER=yes
+!CONFIG_V210_DECODER=yes
+!CONFIG_V210X_DECODER=yes
+!CONFIG_V308_DECODER=yes
+!CONFIG_V408_DECODER=yes
+!CONFIG_V410_DECODER=yes
+!CONFIG_VB_DECODER=yes
+!CONFIG_VBLE_DECODER=yes
+!CONFIG_VC1_DECODER=yes
+!CONFIG_VC1_CRYSTALHD_DECODER=yes
+!CONFIG_VC1IMAGE_DECODER=yes
+!CONFIG_VC1_MMAL_DECODER=yes
+!CONFIG_VC1_QSV_DECODER=yes
+!CONFIG_VC1_V4L2M2M_DECODER=yes
+!CONFIG_VCR1_DECODER=yes
+!CONFIG_VMDVIDEO_DECODER=yes
+!CONFIG_VMNC_DECODER=yes
+!CONFIG_VP3_DECODER=yes
+!CONFIG_VP5_DECODER=yes
+!CONFIG_VP6_DECODER=yes
+!CONFIG_VP6A_DECODER=yes
+!CONFIG_VP6F_DECODER=yes
+!CONFIG_VP7_DECODER=yes
+!CONFIG_VP8_DECODER=yes
+!CONFIG_VP8_RKMPP_DECODER=yes
+!CONFIG_VP8_V4L2M2M_DECODER=yes
+!CONFIG_VP9_DECODER=yes
+!CONFIG_VP9_RKMPP_DECODER=yes
+!CONFIG_VP9_V4L2M2M_DECODER=yes
+!CONFIG_VQA_DECODER=yes
+!CONFIG_WEBP_DECODER=yes
+!CONFIG_WCMV_DECODER=yes
+!CONFIG_WRAPPED_AVFRAME_DECODER=yes
+!CONFIG_WMV1_DECODER=yes
+!CONFIG_WMV2_DECODER=yes
+!CONFIG_WMV3_DECODER=yes
+!CONFIG_WMV3_CRYSTALHD_DECODER=yes
+!CONFIG_WMV3IMAGE_DECODER=yes
+!CONFIG_WNV1_DECODER=yes
+!CONFIG_XAN_WC3_DECODER=yes
+!CONFIG_XAN_WC4_DECODER=yes
+!CONFIG_XBM_DECODER=yes
+!CONFIG_XFACE_DECODER=yes
+!CONFIG_XL_DECODER=yes
+!CONFIG_XPM_DECODER=yes
+!CONFIG_XWD_DECODER=yes
+!CONFIG_Y41P_DECODER=yes
+!CONFIG_YLC_DECODER=yes
+!CONFIG_YOP_DECODER=yes
+!CONFIG_YUV4_DECODER=yes
+!CONFIG_ZERO12V_DECODER=yes
+!CONFIG_ZEROCODEC_DECODER=yes
+!CONFIG_ZLIB_DECODER=yes
+!CONFIG_ZMBV_DECODER=yes
+!CONFIG_AAC_DECODER=yes
+!CONFIG_AAC_FIXED_DECODER=yes
+!CONFIG_AAC_LATM_DECODER=yes
+!CONFIG_AC3_DECODER=yes
+!CONFIG_AC3_FIXED_DECODER=yes
+!CONFIG_ALAC_DECODER=yes
+!CONFIG_ALS_DECODER=yes
+!CONFIG_AMRNB_DECODER=yes
+!CONFIG_AMRWB_DECODER=yes
+!CONFIG_APE_DECODER=yes
+!CONFIG_APTX_DECODER=yes
+!CONFIG_APTX_HD_DECODER=yes
+!CONFIG_ATRAC1_DECODER=yes
+!CONFIG_ATRAC3_DECODER=yes
+!CONFIG_ATRAC3AL_DECODER=yes
+!CONFIG_ATRAC3P_DECODER=yes
+!CONFIG_ATRAC3PAL_DECODER=yes
+!CONFIG_ATRAC9_DECODER=yes
+!CONFIG_BINKAUDIO_DCT_DECODER=yes
+!CONFIG_BINKAUDIO_RDFT_DECODER=yes
+!CONFIG_BMV_AUDIO_DECODER=yes
+!CONFIG_COOK_DECODER=yes
+!CONFIG_DCA_DECODER=yes
+!CONFIG_DOLBY_E_DECODER=yes
+!CONFIG_DSD_LSBF_DECODER=yes
+!CONFIG_DSD_MSBF_DECODER=yes
+!CONFIG_DSD_LSBF_PLANAR_DECODER=yes
+!CONFIG_DSD_MSBF_PLANAR_DECODER=yes
+!CONFIG_DSICINAUDIO_DECODER=yes
+!CONFIG_DSS_SP_DECODER=yes
+!CONFIG_DST_DECODER=yes
+!CONFIG_EAC3_DECODER=yes
+!CONFIG_EVRC_DECODER=yes
+!CONFIG_FFWAVESYNTH_DECODER=yes
+!CONFIG_FLAC_DECODER=yes
+!CONFIG_G723_1_DECODER=yes
+!CONFIG_G729_DECODER=yes
+!CONFIG_GSM_DECODER=yes
+!CONFIG_GSM_MS_DECODER=yes
+!CONFIG_IAC_DECODER=yes
+!CONFIG_ILBC_DECODER=yes
+!CONFIG_IMC_DECODER=yes
+!CONFIG_INTERPLAY_ACM_DECODER=yes
+!CONFIG_MACE3_DECODER=yes
+!CONFIG_MACE6_DECODER=yes
+!CONFIG_METASOUND_DECODER=yes
+!CONFIG_MLP_DECODER=yes
+!CONFIG_MP1_DECODER=yes
+!CONFIG_MP1FLOAT_DECODER=yes
+!CONFIG_MP2_DECODER=yes
+!CONFIG_MP2FLOAT_DECODER=yes
+!CONFIG_MP3FLOAT_DECODER=yes
+!CONFIG_MP3_DECODER=yes
+!CONFIG_MP3ADUFLOAT_DECODER=yes
+!CONFIG_MP3ADU_DECODER=yes
+!CONFIG_MP3ON4FLOAT_DECODER=yes
+!CONFIG_MP3ON4_DECODER=yes
+!CONFIG_MPC7_DECODER=yes
+!CONFIG_MPC8_DECODER=yes
+!CONFIG_NELLYMOSER_DECODER=yes
+!CONFIG_ON2AVC_DECODER=yes
+!CONFIG_OPUS_DECODER=yes
+!CONFIG_PAF_AUDIO_DECODER=yes
+!CONFIG_QCELP_DECODER=yes
+!CONFIG_QDM2_DECODER=yes
+!CONFIG_QDMC_DECODER=yes
+!CONFIG_RA_144_DECODER=yes
+!CONFIG_RA_288_DECODER=yes
+!CONFIG_RALF_DECODER=yes
+!CONFIG_SBC_DECODER=yes
+!CONFIG_SHORTEN_DECODER=yes
+!CONFIG_SIPR_DECODER=yes
+!CONFIG_SMACKAUD_DECODER=yes
+!CONFIG_SONIC_DECODER=yes
+!CONFIG_TAK_DECODER=yes
+!CONFIG_TRUEHD_DECODER=yes
+!CONFIG_TRUESPEECH_DECODER=yes
+!CONFIG_TTA_DECODER=yes
+!CONFIG_TWINVQ_DECODER=yes
+!CONFIG_VMDAUDIO_DECODER=yes
+!CONFIG_VORBIS_DECODER=yes
+!CONFIG_WAVPACK_DECODER=yes
+!CONFIG_WMALOSSLESS_DECODER=yes
+!CONFIG_WMAPRO_DECODER=yes
+!CONFIG_WMAV1_DECODER=yes
+!CONFIG_WMAV2_DECODER=yes
+!CONFIG_WMAVOICE_DECODER=yes
+!CONFIG_WS_SND1_DECODER=yes
+!CONFIG_XMA1_DECODER=yes
+!CONFIG_XMA2_DECODER=yes
+!CONFIG_PCM_ALAW_DECODER=yes
+!CONFIG_PCM_BLURAY_DECODER=yes
+!CONFIG_PCM_DVD_DECODER=yes
+!CONFIG_PCM_F16LE_DECODER=yes
+!CONFIG_PCM_F24LE_DECODER=yes
+!CONFIG_PCM_F32BE_DECODER=yes
+!CONFIG_PCM_F32LE_DECODER=yes
+!CONFIG_PCM_F64BE_DECODER=yes
+!CONFIG_PCM_F64LE_DECODER=yes
+!CONFIG_PCM_LXF_DECODER=yes
+!CONFIG_PCM_MULAW_DECODER=yes
+!CONFIG_PCM_S8_DECODER=yes
+!CONFIG_PCM_S8_PLANAR_DECODER=yes
+!CONFIG_PCM_S16BE_DECODER=yes
+!CONFIG_PCM_S16BE_PLANAR_DECODER=yes
+!CONFIG_PCM_S16LE_DECODER=yes
+!CONFIG_PCM_S16LE_PLANAR_DECODER=yes
+!CONFIG_PCM_S24BE_DECODER=yes
+!CONFIG_PCM_S24DAUD_DECODER=yes
+!CONFIG_PCM_S24LE_DECODER=yes
+!CONFIG_PCM_S24LE_PLANAR_DECODER=yes
+!CONFIG_PCM_S32BE_DECODER=yes
+!CONFIG_PCM_S32LE_DECODER=yes
+!CONFIG_PCM_S32LE_PLANAR_DECODER=yes
+!CONFIG_PCM_S64BE_DECODER=yes
+!CONFIG_PCM_S64LE_DECODER=yes
+!CONFIG_PCM_U8_DECODER=yes
+!CONFIG_PCM_U16BE_DECODER=yes
+!CONFIG_PCM_U16LE_DECODER=yes
+!CONFIG_PCM_U24BE_DECODER=yes
+!CONFIG_PCM_U24LE_DECODER=yes
+!CONFIG_PCM_U32BE_DECODER=yes
+!CONFIG_PCM_U32LE_DECODER=yes
+!CONFIG_PCM_VIDC_DECODER=yes
+!CONFIG_PCM_ZORK_DECODER=yes
+!CONFIG_GREMLIN_DPCM_DECODER=yes
+!CONFIG_INTERPLAY_DPCM_DECODER=yes
+!CONFIG_ROQ_DPCM_DECODER=yes
+!CONFIG_SOL_DPCM_DECODER=yes
+!CONFIG_XAN_DPCM_DECODER=yes
+!CONFIG_ADPCM_4XM_DECODER=yes
+!CONFIG_ADPCM_ADX_DECODER=yes
+!CONFIG_ADPCM_AFC_DECODER=yes
+!CONFIG_ADPCM_AICA_DECODER=yes
+!CONFIG_ADPCM_CT_DECODER=yes
+!CONFIG_ADPCM_DTK_DECODER=yes
+!CONFIG_ADPCM_EA_DECODER=yes
+!CONFIG_ADPCM_EA_MAXIS_XA_DECODER=yes
+!CONFIG_ADPCM_EA_R1_DECODER=yes
+!CONFIG_ADPCM_EA_R2_DECODER=yes
+!CONFIG_ADPCM_EA_R3_DECODER=yes
+!CONFIG_ADPCM_EA_XAS_DECODER=yes
+!CONFIG_ADPCM_G722_DECODER=yes
+!CONFIG_ADPCM_G726_DECODER=yes
+!CONFIG_ADPCM_G726LE_DECODER=yes
+!CONFIG_ADPCM_IMA_AMV_DECODER=yes
+!CONFIG_ADPCM_IMA_APC_DECODER=yes
+!CONFIG_ADPCM_IMA_DAT4_DECODER=yes
+!CONFIG_ADPCM_IMA_DK3_DECODER=yes
+!CONFIG_ADPCM_IMA_DK4_DECODER=yes
+!CONFIG_ADPCM_IMA_EA_EACS_DECODER=yes
+!CONFIG_ADPCM_IMA_EA_SEAD_DECODER=yes
+!CONFIG_ADPCM_IMA_ISS_DECODER=yes
+!CONFIG_ADPCM_IMA_OKI_DECODER=yes
+!CONFIG_ADPCM_IMA_QT_DECODER=yes
+!CONFIG_ADPCM_IMA_RAD_DECODER=yes
+!CONFIG_ADPCM_IMA_SMJPEG_DECODER=yes
+!CONFIG_ADPCM_IMA_WAV_DECODER=yes
+!CONFIG_ADPCM_IMA_WS_DECODER=yes
+!CONFIG_ADPCM_MS_DECODER=yes
+!CONFIG_ADPCM_MTAF_DECODER=yes
+!CONFIG_ADPCM_PSX_DECODER=yes
+!CONFIG_ADPCM_SBPRO_2_DECODER=yes
+!CONFIG_ADPCM_SBPRO_3_DECODER=yes
+!CONFIG_ADPCM_SBPRO_4_DECODER=yes
+!CONFIG_ADPCM_SWF_DECODER=yes
+!CONFIG_ADPCM_THP_DECODER=yes
+!CONFIG_ADPCM_THP_LE_DECODER=yes
+!CONFIG_ADPCM_VIMA_DECODER=yes
+!CONFIG_ADPCM_XA_DECODER=yes
+!CONFIG_ADPCM_YAMAHA_DECODER=yes
+!CONFIG_SSA_DECODER=yes
+!CONFIG_ASS_DECODER=yes
+!CONFIG_CCAPTION_DECODER=yes
+!CONFIG_DVBSUB_DECODER=yes
+!CONFIG_DVDSUB_DECODER=yes
+!CONFIG_JACOSUB_DECODER=yes
+!CONFIG_MICRODVD_DECODER=yes
+!CONFIG_MOVTEXT_DECODER=yes
+!CONFIG_MPL2_DECODER=yes
+!CONFIG_PGSSUB_DECODER=yes
+!CONFIG_PJS_DECODER=yes
+!CONFIG_REALTEXT_DECODER=yes
+!CONFIG_SAMI_DECODER=yes
+!CONFIG_SRT_DECODER=yes
+!CONFIG_STL_DECODER=yes
+!CONFIG_SUBRIP_DECODER=yes
+!CONFIG_SUBVIEWER_DECODER=yes
+!CONFIG_SUBVIEWER1_DECODER=yes
+!CONFIG_TEXT_DECODER=yes
+!CONFIG_VPLAYER_DECODER=yes
+!CONFIG_WEBVTT_DECODER=yes
+!CONFIG_XSUB_DECODER=yes
+!CONFIG_AAC_AT_DECODER=yes
+!CONFIG_AC3_AT_DECODER=yes
+!CONFIG_ADPCM_IMA_QT_AT_DECODER=yes
+!CONFIG_ALAC_AT_DECODER=yes
+!CONFIG_AMR_NB_AT_DECODER=yes
+!CONFIG_EAC3_AT_DECODER=yes
+!CONFIG_GSM_MS_AT_DECODER=yes
+!CONFIG_ILBC_AT_DECODER=yes
+!CONFIG_MP1_AT_DECODER=yes
+!CONFIG_MP2_AT_DECODER=yes
+!CONFIG_MP3_AT_DECODER=yes
+!CONFIG_PCM_ALAW_AT_DECODER=yes
+!CONFIG_PCM_MULAW_AT_DECODER=yes
+!CONFIG_QDMC_AT_DECODER=yes
+!CONFIG_QDM2_AT_DECODER=yes
+!CONFIG_LIBAOM_AV1_DECODER=yes
+!CONFIG_LIBCELT_DECODER=yes
+!CONFIG_LIBCODEC2_DECODER=yes
+!CONFIG_LIBDAVS2_DECODER=yes
+!CONFIG_LIBFDK_AAC_DECODER=yes
+!CONFIG_LIBGSM_DECODER=yes
+!CONFIG_LIBGSM_MS_DECODER=yes
+!CONFIG_LIBILBC_DECODER=yes
+!CONFIG_LIBOPENCORE_AMRNB_DECODER=yes
+!CONFIG_LIBOPENCORE_AMRWB_DECODER=yes
+!CONFIG_LIBOPENJPEG_DECODER=yes
+!CONFIG_LIBOPUS_DECODER=yes
+!CONFIG_LIBRSVG_DECODER=yes
+!CONFIG_LIBSPEEX_DECODER=yes
+!CONFIG_LIBVORBIS_DECODER=yes
+!CONFIG_LIBVPX_VP8_DECODER=yes
+!CONFIG_LIBVPX_VP9_DECODER=yes
+!CONFIG_LIBZVBI_TELETEXT_DECODER=yes
+!CONFIG_BINTEXT_DECODER=yes
+!CONFIG_XBIN_DECODER=yes
+!CONFIG_IDF_DECODER=yes
+!CONFIG_LIBOPENH264_DECODER=yes
+!CONFIG_H264_CUVID_DECODER=yes
+!CONFIG_HEVC_CUVID_DECODER=yes
+!CONFIG_HEVC_MEDIACODEC_DECODER=yes
+!CONFIG_MJPEG_CUVID_DECODER=yes
+!CONFIG_MPEG1_CUVID_DECODER=yes
+!CONFIG_MPEG2_CUVID_DECODER=yes
+!CONFIG_MPEG4_CUVID_DECODER=yes
+!CONFIG_MPEG4_MEDIACODEC_DECODER=yes
+!CONFIG_VC1_CUVID_DECODER=yes
+!CONFIG_VP8_CUVID_DECODER=yes
+!CONFIG_VP8_MEDIACODEC_DECODER=yes
+!CONFIG_VP8_QSV_DECODER=yes
+!CONFIG_VP9_CUVID_DECODER=yes
+!CONFIG_VP9_MEDIACODEC_DECODER=yes
+!CONFIG_A64MULTI_ENCODER=yes
+!CONFIG_A64MULTI5_ENCODER=yes
+!CONFIG_ALIAS_PIX_ENCODER=yes
+!CONFIG_AMV_ENCODER=yes
+!CONFIG_APNG_ENCODER=yes
+!CONFIG_ASV1_ENCODER=yes
+!CONFIG_ASV2_ENCODER=yes
+!CONFIG_AVRP_ENCODER=yes
+!CONFIG_AVUI_ENCODER=yes
+!CONFIG_AYUV_ENCODER=yes
+!CONFIG_BMP_ENCODER=yes
+!CONFIG_CINEPAK_ENCODER=yes
+!CONFIG_CLJR_ENCODER=yes
+!CONFIG_COMFORTNOISE_ENCODER=yes
+!CONFIG_DNXHD_ENCODER=yes
+!CONFIG_DPX_ENCODER=yes
+!CONFIG_DVVIDEO_ENCODER=yes
+!CONFIG_FFV1_ENCODER=yes
+!CONFIG_FFVHUFF_ENCODER=yes
+!CONFIG_FITS_ENCODER=yes
+!CONFIG_FLASHSV_ENCODER=yes
+!CONFIG_FLASHSV2_ENCODER=yes
+!CONFIG_FLV_ENCODER=yes
+!CONFIG_GIF_ENCODER=yes
+!CONFIG_H261_ENCODER=yes
+!CONFIG_H263_ENCODER=yes
+!CONFIG_H263P_ENCODER=yes
+!CONFIG_HAP_ENCODER=yes
+!CONFIG_HUFFYUV_ENCODER=yes
+!CONFIG_JPEG2000_ENCODER=yes
+!CONFIG_JPEGLS_ENCODER=yes
+!CONFIG_LJPEG_ENCODER=yes
+!CONFIG_MAGICYUV_ENCODER=yes
+!CONFIG_MJPEG_ENCODER=yes
+!CONFIG_MPEG1VIDEO_ENCODER=yes
+!CONFIG_MPEG2VIDEO_ENCODER=yes
+!CONFIG_MPEG4_ENCODER=yes
+!CONFIG_MSMPEG4V2_ENCODER=yes
+!CONFIG_MSMPEG4V3_ENCODER=yes
+!CONFIG_MSVIDEO1_ENCODER=yes
+!CONFIG_PAM_ENCODER=yes
+!CONFIG_PBM_ENCODER=yes
+!CONFIG_PCX_ENCODER=yes
+!CONFIG_PGM_ENCODER=yes
+!CONFIG_PGMYUV_ENCODER=yes
+!CONFIG_PNG_ENCODER=yes
+!CONFIG_PPM_ENCODER=yes
+!CONFIG_PRORES_ENCODER=yes
+!CONFIG_PRORES_AW_ENCODER=yes
+!CONFIG_PRORES_KS_ENCODER=yes
+!CONFIG_QTRLE_ENCODER=yes
+!CONFIG_R10K_ENCODER=yes
+!CONFIG_R210_ENCODER=yes
+!CONFIG_RAWVIDEO_ENCODER=yes
+!CONFIG_ROQ_ENCODER=yes
+!CONFIG_RV10_ENCODER=yes
+!CONFIG_RV20_ENCODER=yes
+!CONFIG_S302M_ENCODER=yes
+!CONFIG_SGI_ENCODER=yes
+!CONFIG_SNOW_ENCODER=yes
+!CONFIG_SUNRAST_ENCODER=yes
+!CONFIG_SVQ1_ENCODER=yes
+!CONFIG_TARGA_ENCODER=yes
+!CONFIG_TIFF_ENCODER=yes
+!CONFIG_UTVIDEO_ENCODER=yes
+!CONFIG_V210_ENCODER=yes
+!CONFIG_V308_ENCODER=yes
+!CONFIG_V408_ENCODER=yes
+!CONFIG_V410_ENCODER=yes
+!CONFIG_VC2_ENCODER=yes
+!CONFIG_WRAPPED_AVFRAME_ENCODER=yes
+!CONFIG_WMV1_ENCODER=yes
+!CONFIG_WMV2_ENCODER=yes
+!CONFIG_XBM_ENCODER=yes
+!CONFIG_XFACE_ENCODER=yes
+!CONFIG_XWD_ENCODER=yes
+!CONFIG_Y41P_ENCODER=yes
+!CONFIG_YUV4_ENCODER=yes
+!CONFIG_ZLIB_ENCODER=yes
+!CONFIG_ZMBV_ENCODER=yes
+!CONFIG_AAC_ENCODER=yes
+!CONFIG_AC3_ENCODER=yes
+!CONFIG_AC3_FIXED_ENCODER=yes
+!CONFIG_ALAC_ENCODER=yes
+!CONFIG_APTX_ENCODER=yes
+!CONFIG_APTX_HD_ENCODER=yes
+!CONFIG_DCA_ENCODER=yes
+!CONFIG_EAC3_ENCODER=yes
+!CONFIG_FLAC_ENCODER=yes
+!CONFIG_G723_1_ENCODER=yes
+!CONFIG_MLP_ENCODER=yes
+!CONFIG_MP2_ENCODER=yes
+!CONFIG_MP2FIXED_ENCODER=yes
+!CONFIG_NELLYMOSER_ENCODER=yes
+!CONFIG_OPUS_ENCODER=yes
+!CONFIG_RA_144_ENCODER=yes
+!CONFIG_SBC_ENCODER=yes
+!CONFIG_SONIC_ENCODER=yes
+!CONFIG_SONIC_LS_ENCODER=yes
+!CONFIG_TRUEHD_ENCODER=yes
+!CONFIG_TTA_ENCODER=yes
+!CONFIG_VORBIS_ENCODER=yes
+!CONFIG_WAVPACK_ENCODER=yes
+!CONFIG_WMAV1_ENCODER=yes
+!CONFIG_WMAV2_ENCODER=yes
+!CONFIG_PCM_ALAW_ENCODER=yes
+!CONFIG_PCM_F32BE_ENCODER=yes
+!CONFIG_PCM_F32LE_ENCODER=yes
+!CONFIG_PCM_F64BE_ENCODER=yes
+!CONFIG_PCM_F64LE_ENCODER=yes
+!CONFIG_PCM_MULAW_ENCODER=yes
+!CONFIG_PCM_S8_ENCODER=yes
+!CONFIG_PCM_S8_PLANAR_ENCODER=yes
+!CONFIG_PCM_S16BE_ENCODER=yes
+!CONFIG_PCM_S16BE_PLANAR_ENCODER=yes
+!CONFIG_PCM_S16LE_ENCODER=yes
+!CONFIG_PCM_S16LE_PLANAR_ENCODER=yes
+!CONFIG_PCM_S24BE_ENCODER=yes
+!CONFIG_PCM_S24DAUD_ENCODER=yes
+!CONFIG_PCM_S24LE_ENCODER=yes
+!CONFIG_PCM_S24LE_PLANAR_ENCODER=yes
+!CONFIG_PCM_S32BE_ENCODER=yes
+!CONFIG_PCM_S32LE_ENCODER=yes
+!CONFIG_PCM_S32LE_PLANAR_ENCODER=yes
+!CONFIG_PCM_S64BE_ENCODER=yes
+!CONFIG_PCM_S64LE_ENCODER=yes
+!CONFIG_PCM_U8_ENCODER=yes
+!CONFIG_PCM_U16BE_ENCODER=yes
+!CONFIG_PCM_U16LE_ENCODER=yes
+!CONFIG_PCM_U24BE_ENCODER=yes
+!CONFIG_PCM_U24LE_ENCODER=yes
+!CONFIG_PCM_U32BE_ENCODER=yes
+!CONFIG_PCM_U32LE_ENCODER=yes
+!CONFIG_PCM_VIDC_ENCODER=yes
+!CONFIG_ROQ_DPCM_ENCODER=yes
+!CONFIG_ADPCM_ADX_ENCODER=yes
+!CONFIG_ADPCM_G722_ENCODER=yes
+!CONFIG_ADPCM_G726_ENCODER=yes
+!CONFIG_ADPCM_G726LE_ENCODER=yes
+!CONFIG_ADPCM_IMA_QT_ENCODER=yes
+!CONFIG_ADPCM_IMA_WAV_ENCODER=yes
+!CONFIG_ADPCM_MS_ENCODER=yes
+!CONFIG_ADPCM_SWF_ENCODER=yes
+!CONFIG_ADPCM_YAMAHA_ENCODER=yes
+!CONFIG_SSA_ENCODER=yes
+!CONFIG_ASS_ENCODER=yes
+!CONFIG_DVBSUB_ENCODER=yes
+!CONFIG_DVDSUB_ENCODER=yes
+!CONFIG_MOVTEXT_ENCODER=yes
+!CONFIG_SRT_ENCODER=yes
+!CONFIG_SUBRIP_ENCODER=yes
+!CONFIG_TEXT_ENCODER=yes
+!CONFIG_WEBVTT_ENCODER=yes
+!CONFIG_XSUB_ENCODER=yes
+!CONFIG_AAC_AT_ENCODER=yes
+!CONFIG_ALAC_AT_ENCODER=yes
+!CONFIG_ILBC_AT_ENCODER=yes
+!CONFIG_PCM_ALAW_AT_ENCODER=yes
+!CONFIG_PCM_MULAW_AT_ENCODER=yes
+!CONFIG_LIBAOM_AV1_ENCODER=yes
+!CONFIG_LIBCODEC2_ENCODER=yes
+!CONFIG_LIBFDK_AAC_ENCODER=yes
+!CONFIG_LIBGSM_ENCODER=yes
+!CONFIG_LIBGSM_MS_ENCODER=yes
+!CONFIG_LIBILBC_ENCODER=yes
+!CONFIG_LIBMP3LAME_ENCODER=yes
+!CONFIG_LIBOPENCORE_AMRNB_ENCODER=yes
+!CONFIG_LIBOPENJPEG_ENCODER=yes
+!CONFIG_LIBOPUS_ENCODER=yes
+!CONFIG_LIBSHINE_ENCODER=yes
+!CONFIG_LIBSPEEX_ENCODER=yes
+!CONFIG_LIBTHEORA_ENCODER=yes
+!CONFIG_LIBTWOLAME_ENCODER=yes
+!CONFIG_LIBVO_AMRWBENC_ENCODER=yes
+!CONFIG_LIBVORBIS_ENCODER=yes
+!CONFIG_LIBVPX_VP8_ENCODER=yes
+!CONFIG_LIBVPX_VP9_ENCODER=yes
+!CONFIG_LIBWAVPACK_ENCODER=yes
+!CONFIG_LIBWEBP_ANIM_ENCODER=yes
+!CONFIG_LIBWEBP_ENCODER=yes
+!CONFIG_LIBX262_ENCODER=yes
+!CONFIG_LIBX264_ENCODER=yes
+!CONFIG_LIBX264RGB_ENCODER=yes
+!CONFIG_LIBX265_ENCODER=yes
+!CONFIG_LIBXAVS_ENCODER=yes
+!CONFIG_LIBXAVS2_ENCODER=yes
+!CONFIG_LIBXVID_ENCODER=yes
+!CONFIG_H263_V4L2M2M_ENCODER=yes
+!CONFIG_LIBOPENH264_ENCODER=yes
+!CONFIG_H264_AMF_ENCODER=yes
+!CONFIG_H264_NVENC_ENCODER=yes
+!CONFIG_H264_OMX_ENCODER=yes
+!CONFIG_H264_QSV_ENCODER=yes
+!CONFIG_H264_V4L2M2M_ENCODER=yes
+!CONFIG_H264_VAAPI_ENCODER=yes
+!CONFIG_H264_VIDEOTOOLBOX_ENCODER=yes
+!CONFIG_NVENC_ENCODER=yes
+!CONFIG_NVENC_H264_ENCODER=yes
+!CONFIG_NVENC_HEVC_ENCODER=yes
+!CONFIG_HEVC_AMF_ENCODER=yes
+!CONFIG_HEVC_NVENC_ENCODER=yes
+!CONFIG_HEVC_QSV_ENCODER=yes
+!CONFIG_HEVC_V4L2M2M_ENCODER=yes
+!CONFIG_HEVC_VAAPI_ENCODER=yes
+!CONFIG_HEVC_VIDEOTOOLBOX_ENCODER=yes
+!CONFIG_LIBKVAZAAR_ENCODER=yes
+!CONFIG_MJPEG_QSV_ENCODER=yes
+!CONFIG_MJPEG_VAAPI_ENCODER=yes
+!CONFIG_MPEG2_QSV_ENCODER=yes
+!CONFIG_MPEG2_VAAPI_ENCODER=yes
+!CONFIG_MPEG4_V4L2M2M_ENCODER=yes
+!CONFIG_VP8_V4L2M2M_ENCODER=yes
+!CONFIG_VP8_VAAPI_ENCODER=yes
+!CONFIG_VP9_VAAPI_ENCODER=yes
+!CONFIG_H263_VAAPI_HWACCEL=yes
+!CONFIG_H263_VIDEOTOOLBOX_HWACCEL=yes
+!CONFIG_H264_D3D11VA_HWACCEL=yes
+!CONFIG_H264_D3D11VA2_HWACCEL=yes
+!CONFIG_H264_DXVA2_HWACCEL=yes
+!CONFIG_H264_NVDEC_HWACCEL=yes
+!CONFIG_H264_VAAPI_HWACCEL=yes
+!CONFIG_H264_VDPAU_HWACCEL=yes
+!CONFIG_H264_VIDEOTOOLBOX_HWACCEL=yes
+!CONFIG_HEVC_D3D11VA_HWACCEL=yes
+!CONFIG_HEVC_D3D11VA2_HWACCEL=yes
+!CONFIG_HEVC_DXVA2_HWACCEL=yes
+!CONFIG_HEVC_NVDEC_HWACCEL=yes
+!CONFIG_HEVC_VAAPI_HWACCEL=yes
+!CONFIG_HEVC_VDPAU_HWACCEL=yes
+!CONFIG_HEVC_VIDEOTOOLBOX_HWACCEL=yes
+!CONFIG_MJPEG_NVDEC_HWACCEL=yes
+!CONFIG_MJPEG_VAAPI_HWACCEL=yes
+!CONFIG_MPEG1_NVDEC_HWACCEL=yes
+!CONFIG_MPEG1_VDPAU_HWACCEL=yes
+!CONFIG_MPEG1_VIDEOTOOLBOX_HWACCEL=yes
+!CONFIG_MPEG1_XVMC_HWACCEL=yes
+!CONFIG_MPEG2_D3D11VA_HWACCEL=yes
+!CONFIG_MPEG2_D3D11VA2_HWACCEL=yes
+!CONFIG_MPEG2_NVDEC_HWACCEL=yes
+!CONFIG_MPEG2_DXVA2_HWACCEL=yes
+!CONFIG_MPEG2_VAAPI_HWACCEL=yes
+!CONFIG_MPEG2_VDPAU_HWACCEL=yes
+!CONFIG_MPEG2_VIDEOTOOLBOX_HWACCEL=yes
+!CONFIG_MPEG2_XVMC_HWACCEL=yes
+!CONFIG_MPEG4_NVDEC_HWACCEL=yes
+!CONFIG_MPEG4_VAAPI_HWACCEL=yes
+!CONFIG_MPEG4_VDPAU_HWACCEL=yes
+!CONFIG_MPEG4_VIDEOTOOLBOX_HWACCEL=yes
+!CONFIG_VC1_D3D11VA_HWACCEL=yes
+!CONFIG_VC1_D3D11VA2_HWACCEL=yes
+!CONFIG_VC1_DXVA2_HWACCEL=yes
+!CONFIG_VC1_NVDEC_HWACCEL=yes
+!CONFIG_VC1_VAAPI_HWACCEL=yes
+!CONFIG_VC1_VDPAU_HWACCEL=yes
+!CONFIG_VP8_NVDEC_HWACCEL=yes
+!CONFIG_VP8_VAAPI_HWACCEL=yes
+!CONFIG_VP9_D3D11VA_HWACCEL=yes
+!CONFIG_VP9_D3D11VA2_HWACCEL=yes
+!CONFIG_VP9_DXVA2_HWACCEL=yes
+!CONFIG_VP9_NVDEC_HWACCEL=yes
+!CONFIG_VP9_VAAPI_HWACCEL=yes
+!CONFIG_WMV3_D3D11VA_HWACCEL=yes
+!CONFIG_WMV3_D3D11VA2_HWACCEL=yes
+!CONFIG_WMV3_DXVA2_HWACCEL=yes
+!CONFIG_WMV3_NVDEC_HWACCEL=yes
+!CONFIG_WMV3_VAAPI_HWACCEL=yes
+!CONFIG_WMV3_VDPAU_HWACCEL=yes
+!CONFIG_AAC_PARSER=yes
+!CONFIG_AAC_LATM_PARSER=yes
+!CONFIG_AC3_PARSER=yes
+!CONFIG_ADX_PARSER=yes
+!CONFIG_AV1_PARSER=yes
+!CONFIG_AVS2_PARSER=yes
+!CONFIG_BMP_PARSER=yes
+!CONFIG_CAVSVIDEO_PARSER=yes
+!CONFIG_COOK_PARSER=yes
+!CONFIG_DCA_PARSER=yes
+!CONFIG_DIRAC_PARSER=yes
+!CONFIG_DNXHD_PARSER=yes
+!CONFIG_DPX_PARSER=yes
+!CONFIG_DVAUDIO_PARSER=yes
+!CONFIG_DVBSUB_PARSER=yes
+!CONFIG_DVDSUB_PARSER=yes
+!CONFIG_DVD_NAV_PARSER=yes
+!CONFIG_FLAC_PARSER=yes
+!CONFIG_G729_PARSER=yes
+!CONFIG_GSM_PARSER=yes
+!CONFIG_H261_PARSER=yes
+!CONFIG_H263_PARSER=yes
+!CONFIG_H264_PARSER=yes
+!CONFIG_HEVC_PARSER=yes
+!CONFIG_MJPEG_PARSER=yes
+!CONFIG_MLP_PARSER=yes
+!CONFIG_MPEG4VIDEO_PARSER=yes
+!CONFIG_MPEGAUDIO_PARSER=yes
+!CONFIG_MPEGVIDEO_PARSER=yes
+!CONFIG_OPUS_PARSER=yes
+!CONFIG_PNG_PARSER=yes
+!CONFIG_PNM_PARSER=yes
+!CONFIG_RV30_PARSER=yes
+!CONFIG_RV40_PARSER=yes
+!CONFIG_SBC_PARSER=yes
+!CONFIG_SIPR_PARSER=yes
+!CONFIG_TAK_PARSER=yes
+!CONFIG_VC1_PARSER=yes
+!CONFIG_VORBIS_PARSER=yes
+!CONFIG_VP3_PARSER=yes
+!CONFIG_VP8_PARSER=yes
+!CONFIG_VP9_PARSER=yes
+!CONFIG_XMA_PARSER=yes
+!CONFIG_ALSA_INDEV=yes
+!CONFIG_ANDROID_CAMERA_INDEV=yes
+!CONFIG_AVFOUNDATION_INDEV=yes
+!CONFIG_BKTR_INDEV=yes
+!CONFIG_DECKLINK_INDEV=yes
+!CONFIG_LIBNDI_NEWTEK_INDEV=yes
+!CONFIG_DSHOW_INDEV=yes
+!CONFIG_FBDEV_INDEV=yes
+!CONFIG_GDIGRAB_INDEV=yes
+!CONFIG_IEC61883_INDEV=yes
+!CONFIG_JACK_INDEV=yes
+!CONFIG_KMSGRAB_INDEV=yes
+!CONFIG_LAVFI_INDEV=yes
+!CONFIG_OPENAL_INDEV=yes
+!CONFIG_OSS_INDEV=yes
+!CONFIG_PULSE_INDEV=yes
+!CONFIG_SNDIO_INDEV=yes
+!CONFIG_V4L2_INDEV=yes
+!CONFIG_VFWCAP_INDEV=yes
+!CONFIG_XCBGRAB_INDEV=yes
+!CONFIG_LIBCDIO_INDEV=yes
+!CONFIG_LIBDC1394_INDEV=yes
+!CONFIG_ALSA_OUTDEV=yes
+!CONFIG_CACA_OUTDEV=yes
+!CONFIG_DECKLINK_OUTDEV=yes
+!CONFIG_LIBNDI_NEWTEK_OUTDEV=yes
+!CONFIG_FBDEV_OUTDEV=yes
+!CONFIG_OPENGL_OUTDEV=yes
+!CONFIG_OSS_OUTDEV=yes
+!CONFIG_PULSE_OUTDEV=yes
+!CONFIG_SDL2_OUTDEV=yes
+!CONFIG_SNDIO_OUTDEV=yes
+!CONFIG_V4L2_OUTDEV=yes
+!CONFIG_XV_OUTDEV=yes
+!CONFIG_ABENCH_FILTER=yes
+!CONFIG_ACOMPRESSOR_FILTER=yes
+!CONFIG_ACONTRAST_FILTER=yes
+!CONFIG_ACOPY_FILTER=yes
+!CONFIG_ACUE_FILTER=yes
+!CONFIG_ACROSSFADE_FILTER=yes
+!CONFIG_ACROSSOVER_FILTER=yes
+!CONFIG_ACRUSHER_FILTER=yes
+!CONFIG_ADECLICK_FILTER=yes
+!CONFIG_ADECLIP_FILTER=yes
+!CONFIG_ADELAY_FILTER=yes
+!CONFIG_ADERIVATIVE_FILTER=yes
+!CONFIG_AECHO_FILTER=yes
+!CONFIG_AEMPHASIS_FILTER=yes
+!CONFIG_AEVAL_FILTER=yes
+!CONFIG_AFADE_FILTER=yes
+!CONFIG_AFFTDN_FILTER=yes
+!CONFIG_AFFTFILT_FILTER=yes
+!CONFIG_AFIR_FILTER=yes
+!CONFIG_AFORMAT_FILTER=yes
+!CONFIG_AGATE_FILTER=yes
+!CONFIG_AIIR_FILTER=yes
+!CONFIG_AINTEGRAL_FILTER=yes
+!CONFIG_AINTERLEAVE_FILTER=yes
+!CONFIG_ALIMITER_FILTER=yes
+!CONFIG_ALLPASS_FILTER=yes
+!CONFIG_ALOOP_FILTER=yes
+!CONFIG_AMERGE_FILTER=yes
+!CONFIG_AMETADATA_FILTER=yes
+!CONFIG_AMIX_FILTER=yes
+!CONFIG_AMULTIPLY_FILTER=yes
+!CONFIG_ANEQUALIZER_FILTER=yes
+!CONFIG_ANULL_FILTER=yes
+!CONFIG_APAD_FILTER=yes
+!CONFIG_APERMS_FILTER=yes
+!CONFIG_APHASER_FILTER=yes
+!CONFIG_APULSATOR_FILTER=yes
+!CONFIG_AREALTIME_FILTER=yes
+!CONFIG_ARESAMPLE_FILTER=yes
+!CONFIG_AREVERSE_FILTER=yes
+!CONFIG_ASELECT_FILTER=yes
+!CONFIG_ASENDCMD_FILTER=yes
+!CONFIG_ASETNSAMPLES_FILTER=yes
+!CONFIG_ASETPTS_FILTER=yes
+!CONFIG_ASETRATE_FILTER=yes
+!CONFIG_ASETTB_FILTER=yes
+!CONFIG_ASHOWINFO_FILTER=yes
+!CONFIG_ASIDEDATA_FILTER=yes
+!CONFIG_ASPLIT_FILTER=yes
+!CONFIG_ASTATS_FILTER=yes
+!CONFIG_ASTREAMSELECT_FILTER=yes
+!CONFIG_ATEMPO_FILTER=yes
+!CONFIG_ATRIM_FILTER=yes
+!CONFIG_AZMQ_FILTER=yes
+!CONFIG_BANDPASS_FILTER=yes
+!CONFIG_BANDREJECT_FILTER=yes
+!CONFIG_BASS_FILTER=yes
+!CONFIG_BIQUAD_FILTER=yes
+!CONFIG_BS2B_FILTER=yes
+!CONFIG_CHANNELMAP_FILTER=yes
+!CONFIG_CHANNELSPLIT_FILTER=yes
+!CONFIG_CHORUS_FILTER=yes
+!CONFIG_COMPAND_FILTER=yes
+!CONFIG_COMPENSATIONDELAY_FILTER=yes
+!CONFIG_CROSSFEED_FILTER=yes
+!CONFIG_CRYSTALIZER_FILTER=yes
+!CONFIG_DCSHIFT_FILTER=yes
+!CONFIG_DRMETER_FILTER=yes
+!CONFIG_DYNAUDNORM_FILTER=yes
+!CONFIG_EARWAX_FILTER=yes
+!CONFIG_EBUR128_FILTER=yes
+!CONFIG_EQUALIZER_FILTER=yes
+!CONFIG_EXTRASTEREO_FILTER=yes
+!CONFIG_FIREQUALIZER_FILTER=yes
+!CONFIG_FLANGER_FILTER=yes
+!CONFIG_HAAS_FILTER=yes
+!CONFIG_HDCD_FILTER=yes
+!CONFIG_HEADPHONE_FILTER=yes
+!CONFIG_HIGHPASS_FILTER=yes
+!CONFIG_HIGHSHELF_FILTER=yes
+!CONFIG_JOIN_FILTER=yes
+!CONFIG_LADSPA_FILTER=yes
+!CONFIG_LOUDNORM_FILTER=yes
+!CONFIG_LOWPASS_FILTER=yes
+!CONFIG_LOWSHELF_FILTER=yes
+!CONFIG_LV2_FILTER=yes
+!CONFIG_MCOMPAND_FILTER=yes
+!CONFIG_PAN_FILTER=yes
+!CONFIG_REPLAYGAIN_FILTER=yes
+!CONFIG_RESAMPLE_FILTER=yes
+!CONFIG_RUBBERBAND_FILTER=yes
+!CONFIG_SIDECHAINCOMPRESS_FILTER=yes
+!CONFIG_SIDECHAINGATE_FILTER=yes
+!CONFIG_SILENCEDETECT_FILTER=yes
+!CONFIG_SILENCEREMOVE_FILTER=yes
+!CONFIG_SOFALIZER_FILTER=yes
+!CONFIG_STEREOTOOLS_FILTER=yes
+!CONFIG_STEREOWIDEN_FILTER=yes
+!CONFIG_SUPEREQUALIZER_FILTER=yes
+!CONFIG_SURROUND_FILTER=yes
+!CONFIG_TREBLE_FILTER=yes
+!CONFIG_TREMOLO_FILTER=yes
+!CONFIG_VIBRATO_FILTER=yes
+!CONFIG_VOLUME_FILTER=yes
+!CONFIG_VOLUMEDETECT_FILTER=yes
+!CONFIG_AEVALSRC_FILTER=yes
+!CONFIG_ANOISESRC_FILTER=yes
+!CONFIG_ANULLSRC_FILTER=yes
+!CONFIG_FLITE_FILTER=yes
+!CONFIG_HILBERT_FILTER=yes
+!CONFIG_SINC_FILTER=yes
+!CONFIG_SINE_FILTER=yes
+!CONFIG_ANULLSINK_FILTER=yes
+!CONFIG_ALPHAEXTRACT_FILTER=yes
+!CONFIG_ALPHAMERGE_FILTER=yes
+!CONFIG_AMPLIFY_FILTER=yes
+!CONFIG_ASS_FILTER=yes
+!CONFIG_ATADENOISE_FILTER=yes
+!CONFIG_AVGBLUR_FILTER=yes
+!CONFIG_AVGBLUR_OPENCL_FILTER=yes
+!CONFIG_BBOX_FILTER=yes
+!CONFIG_BENCH_FILTER=yes
+!CONFIG_BITPLANENOISE_FILTER=yes
+!CONFIG_BLACKDETECT_FILTER=yes
+!CONFIG_BLACKFRAME_FILTER=yes
+!CONFIG_BLEND_FILTER=yes
+!CONFIG_BM3D_FILTER=yes
+!CONFIG_BOXBLUR_FILTER=yes
+!CONFIG_BOXBLUR_OPENCL_FILTER=yes
+!CONFIG_BWDIF_FILTER=yes
+!CONFIG_CHROMAHOLD_FILTER=yes
+!CONFIG_CHROMAKEY_FILTER=yes
+!CONFIG_CIESCOPE_FILTER=yes
+!CONFIG_CODECVIEW_FILTER=yes
+!CONFIG_COLORBALANCE_FILTER=yes
+!CONFIG_COLORCHANNELMIXER_FILTER=yes
+!CONFIG_COLORKEY_FILTER=yes
+!CONFIG_COLORLEVELS_FILTER=yes
+!CONFIG_COLORMATRIX_FILTER=yes
+!CONFIG_COLORSPACE_FILTER=yes
+!CONFIG_CONVOLUTION_FILTER=yes
+!CONFIG_CONVOLUTION_OPENCL_FILTER=yes
+!CONFIG_CONVOLVE_FILTER=yes
+!CONFIG_COPY_FILTER=yes
+!CONFIG_COREIMAGE_FILTER=yes
+!CONFIG_COVER_RECT_FILTER=yes
+!CONFIG_CROP_FILTER=yes
+!CONFIG_CROPDETECT_FILTER=yes
+!CONFIG_CUE_FILTER=yes
+!CONFIG_CURVES_FILTER=yes
+!CONFIG_DATASCOPE_FILTER=yes
+!CONFIG_DCTDNOIZ_FILTER=yes
+!CONFIG_DEBAND_FILTER=yes
+!CONFIG_DEBLOCK_FILTER=yes
+!CONFIG_DECIMATE_FILTER=yes
+!CONFIG_DECONVOLVE_FILTER=yes
+!CONFIG_DEFLATE_FILTER=yes
+!CONFIG_DEFLICKER_FILTER=yes
+!CONFIG_DEINTERLACE_QSV_FILTER=yes
+!CONFIG_DEINTERLACE_VAAPI_FILTER=yes
+!CONFIG_DEJUDDER_FILTER=yes
+!CONFIG_DELOGO_FILTER=yes
+!CONFIG_DENOISE_VAAPI_FILTER=yes
+!CONFIG_DESHAKE_FILTER=yes
+!CONFIG_DESPILL_FILTER=yes
+!CONFIG_DETELECINE_FILTER=yes
+!CONFIG_DILATION_FILTER=yes
+!CONFIG_DILATION_OPENCL_FILTER=yes
+!CONFIG_DISPLACE_FILTER=yes
+!CONFIG_DOUBLEWEAVE_FILTER=yes
+!CONFIG_DRAWBOX_FILTER=yes
+!CONFIG_DRAWGRAPH_FILTER=yes
+!CONFIG_DRAWGRID_FILTER=yes
+!CONFIG_DRAWTEXT_FILTER=yes
+!CONFIG_EDGEDETECT_FILTER=yes
+!CONFIG_ELBG_FILTER=yes
+!CONFIG_ENTROPY_FILTER=yes
+!CONFIG_EQ_FILTER=yes
+!CONFIG_EROSION_FILTER=yes
+!CONFIG_EROSION_OPENCL_FILTER=yes
+!CONFIG_EXTRACTPLANES_FILTER=yes
+!CONFIG_FADE_FILTER=yes
+!CONFIG_FFTDNOIZ_FILTER=yes
+!CONFIG_FFTFILT_FILTER=yes
+!CONFIG_FIELD_FILTER=yes
+!CONFIG_FIELDHINT_FILTER=yes
+!CONFIG_FIELDMATCH_FILTER=yes
+!CONFIG_FIELDORDER_FILTER=yes
+!CONFIG_FILLBORDERS_FILTER=yes
+!CONFIG_FIND_RECT_FILTER=yes
+!CONFIG_FLOODFILL_FILTER=yes
+!CONFIG_FORMAT_FILTER=yes
+!CONFIG_FPS_FILTER=yes
+!CONFIG_FRAMEPACK_FILTER=yes
+!CONFIG_FRAMERATE_FILTER=yes
+!CONFIG_FRAMESTEP_FILTER=yes
+!CONFIG_FREI0R_FILTER=yes
+!CONFIG_FSPP_FILTER=yes
+!CONFIG_GBLUR_FILTER=yes
+!CONFIG_GEQ_FILTER=yes
+!CONFIG_GRADFUN_FILTER=yes
+!CONFIG_GRAPHMONITOR_FILTER=yes
+!CONFIG_GREYEDGE_FILTER=yes
+!CONFIG_HALDCLUT_FILTER=yes
+!CONFIG_HFLIP_FILTER=yes
+!CONFIG_HISTEQ_FILTER=yes
+!CONFIG_HISTOGRAM_FILTER=yes
+!CONFIG_HQDN3D_FILTER=yes
+!CONFIG_HQX_FILTER=yes
+!CONFIG_HSTACK_FILTER=yes
+!CONFIG_HUE_FILTER=yes
+!CONFIG_HWDOWNLOAD_FILTER=yes
+!CONFIG_HWMAP_FILTER=yes
+!CONFIG_HWUPLOAD_FILTER=yes
+!CONFIG_HWUPLOAD_CUDA_FILTER=yes
+!CONFIG_HYSTERESIS_FILTER=yes
+!CONFIG_IDET_FILTER=yes
+!CONFIG_IL_FILTER=yes
+!CONFIG_INFLATE_FILTER=yes
+!CONFIG_INTERLACE_FILTER=yes
+!CONFIG_INTERLEAVE_FILTER=yes
+!CONFIG_KERNDEINT_FILTER=yes
+!CONFIG_LENSCORRECTION_FILTER=yes
+!CONFIG_LENSFUN_FILTER=yes
+!CONFIG_LIBVMAF_FILTER=yes
+!CONFIG_LIMITER_FILTER=yes
+!CONFIG_LOOP_FILTER=yes
+!CONFIG_LUMAKEY_FILTER=yes
+!CONFIG_LUT_FILTER=yes
+!CONFIG_LUT1D_FILTER=yes
+!CONFIG_LUT2_FILTER=yes
+!CONFIG_LUT3D_FILTER=yes
+!CONFIG_LUTRGB_FILTER=yes
+!CONFIG_LUTYUV_FILTER=yes
+!CONFIG_MASKEDCLAMP_FILTER=yes
+!CONFIG_MASKEDMERGE_FILTER=yes
+!CONFIG_MCDEINT_FILTER=yes
+!CONFIG_MERGEPLANES_FILTER=yes
+!CONFIG_MESTIMATE_FILTER=yes
+!CONFIG_METADATA_FILTER=yes
+!CONFIG_MIDEQUALIZER_FILTER=yes
+!CONFIG_MINTERPOLATE_FILTER=yes
+!CONFIG_MIX_FILTER=yes
+!CONFIG_MPDECIMATE_FILTER=yes
+!CONFIG_NEGATE_FILTER=yes
+!CONFIG_NLMEANS_FILTER=yes
+!CONFIG_NNEDI_FILTER=yes
+!CONFIG_NOFORMAT_FILTER=yes
+!CONFIG_NOISE_FILTER=yes
+!CONFIG_NORMALIZE_FILTER=yes
+!CONFIG_NULL_FILTER=yes
+!CONFIG_OCR_FILTER=yes
+!CONFIG_OCV_FILTER=yes
+!CONFIG_OSCILLOSCOPE_FILTER=yes
+!CONFIG_OVERLAY_FILTER=yes
+!CONFIG_OVERLAY_OPENCL_FILTER=yes
+!CONFIG_OVERLAY_QSV_FILTER=yes
+!CONFIG_OWDENOISE_FILTER=yes
+!CONFIG_PAD_FILTER=yes
+!CONFIG_PALETTEGEN_FILTER=yes
+!CONFIG_PALETTEUSE_FILTER=yes
+!CONFIG_PERMS_FILTER=yes
+!CONFIG_PERSPECTIVE_FILTER=yes
+!CONFIG_PHASE_FILTER=yes
+!CONFIG_PIXDESCTEST_FILTER=yes
+!CONFIG_PIXSCOPE_FILTER=yes
+!CONFIG_PP_FILTER=yes
+!CONFIG_PP7_FILTER=yes
+!CONFIG_PREMULTIPLY_FILTER=yes
+!CONFIG_PREWITT_FILTER=yes
+!CONFIG_PREWITT_OPENCL_FILTER=yes
+!CONFIG_PROCAMP_VAAPI_FILTER=yes
+!CONFIG_PROGRAM_OPENCL_FILTER=yes
+!CONFIG_PSEUDOCOLOR_FILTER=yes
+!CONFIG_PSNR_FILTER=yes
+!CONFIG_PULLUP_FILTER=yes
+!CONFIG_QP_FILTER=yes
+!CONFIG_RANDOM_FILTER=yes
+!CONFIG_READEIA608_FILTER=yes
+!CONFIG_READVITC_FILTER=yes
+!CONFIG_REALTIME_FILTER=yes
+!CONFIG_REMAP_FILTER=yes
+!CONFIG_REMOVEGRAIN_FILTER=yes
+!CONFIG_REMOVELOGO_FILTER=yes
+!CONFIG_REPEATFIELDS_FILTER=yes
+!CONFIG_REVERSE_FILTER=yes
+!CONFIG_ROBERTS_FILTER=yes
+!CONFIG_ROBERTS_OPENCL_FILTER=yes
+!CONFIG_ROTATE_FILTER=yes
+!CONFIG_SAB_FILTER=yes
+!CONFIG_SCALE_FILTER=yes
+!CONFIG_SCALE_CUDA_FILTER=yes
+!CONFIG_SCALE_NPP_FILTER=yes
+!CONFIG_SCALE_QSV_FILTER=yes
+!CONFIG_SCALE_VAAPI_FILTER=yes
+!CONFIG_SCALE2REF_FILTER=yes
+!CONFIG_SELECT_FILTER=yes
+!CONFIG_SELECTIVECOLOR_FILTER=yes
+!CONFIG_SENDCMD_FILTER=yes
+!CONFIG_SEPARATEFIELDS_FILTER=yes
+!CONFIG_SETDAR_FILTER=yes
+!CONFIG_SETFIELD_FILTER=yes
+!CONFIG_SETPARAMS_FILTER=yes
+!CONFIG_SETPTS_FILTER=yes
+!CONFIG_SETRANGE_FILTER=yes
+!CONFIG_SETSAR_FILTER=yes
+!CONFIG_SETTB_FILTER=yes
+!CONFIG_SHARPNESS_VAAPI_FILTER=yes
+!CONFIG_SHOWINFO_FILTER=yes
+!CONFIG_SHOWPALETTE_FILTER=yes
+!CONFIG_SHUFFLEFRAMES_FILTER=yes
+!CONFIG_SHUFFLEPLANES_FILTER=yes
+!CONFIG_SIDEDATA_FILTER=yes
+!CONFIG_SIGNALSTATS_FILTER=yes
+!CONFIG_SIGNATURE_FILTER=yes
+!CONFIG_SMARTBLUR_FILTER=yes
+!CONFIG_SOBEL_FILTER=yes
+!CONFIG_SOBEL_OPENCL_FILTER=yes
+!CONFIG_SPLIT_FILTER=yes
+!CONFIG_SPP_FILTER=yes
+!CONFIG_SR_FILTER=yes
+!CONFIG_SSIM_FILTER=yes
+!CONFIG_STEREO3D_FILTER=yes
+!CONFIG_STREAMSELECT_FILTER=yes
+!CONFIG_SUBTITLES_FILTER=yes
+!CONFIG_SUPER2XSAI_FILTER=yes
+!CONFIG_SWAPRECT_FILTER=yes
+!CONFIG_SWAPUV_FILTER=yes
+!CONFIG_TBLEND_FILTER=yes
+!CONFIG_TELECINE_FILTER=yes
+!CONFIG_THRESHOLD_FILTER=yes
+!CONFIG_THUMBNAIL_FILTER=yes
+!CONFIG_THUMBNAIL_CUDA_FILTER=yes
+!CONFIG_TILE_FILTER=yes
+!CONFIG_TINTERLACE_FILTER=yes
+!CONFIG_TLUT2_FILTER=yes
+!CONFIG_TMIX_FILTER=yes
+!CONFIG_TONEMAP_FILTER=yes
+!CONFIG_TONEMAP_OPENCL_FILTER=yes
+!CONFIG_TRANSPOSE_FILTER=yes
+!CONFIG_TRANSPOSE_NPP_FILTER=yes
+!CONFIG_TRIM_FILTER=yes
+!CONFIG_UNPREMULTIPLY_FILTER=yes
+!CONFIG_UNSHARP_FILTER=yes
+!CONFIG_UNSHARP_OPENCL_FILTER=yes
+!CONFIG_USPP_FILTER=yes
+!CONFIG_VAGUEDENOISER_FILTER=yes
+!CONFIG_VECTORSCOPE_FILTER=yes
+!CONFIG_VFLIP_FILTER=yes
+!CONFIG_VFRDET_FILTER=yes
+!CONFIG_VIBRANCE_FILTER=yes
+!CONFIG_VIDSTABDETECT_FILTER=yes
+!CONFIG_VIDSTABTRANSFORM_FILTER=yes
+!CONFIG_VIGNETTE_FILTER=yes
+!CONFIG_VMAFMOTION_FILTER=yes
+!CONFIG_VPP_QSV_FILTER=yes
+!CONFIG_VSTACK_FILTER=yes
+!CONFIG_W3FDIF_FILTER=yes
+!CONFIG_WAVEFORM_FILTER=yes
+!CONFIG_WEAVE_FILTER=yes
+!CONFIG_XBR_FILTER=yes
+!CONFIG_XSTACK_FILTER=yes
+!CONFIG_YADIF_FILTER=yes
+!CONFIG_YADIF_CUDA_FILTER=yes
+!CONFIG_ZMQ_FILTER=yes
+!CONFIG_ZOOMPAN_FILTER=yes
+!CONFIG_ZSCALE_FILTER=yes
+!CONFIG_ALLRGB_FILTER=yes
+!CONFIG_ALLYUV_FILTER=yes
+!CONFIG_CELLAUTO_FILTER=yes
+!CONFIG_COLOR_FILTER=yes
+!CONFIG_COREIMAGESRC_FILTER=yes
+!CONFIG_FREI0R_SRC_FILTER=yes
+!CONFIG_HALDCLUTSRC_FILTER=yes
+!CONFIG_LIFE_FILTER=yes
+!CONFIG_MANDELBROT_FILTER=yes
+!CONFIG_MPTESTSRC_FILTER=yes
+!CONFIG_NULLSRC_FILTER=yes
+!CONFIG_OPENCLSRC_FILTER=yes
+!CONFIG_PAL75BARS_FILTER=yes
+!CONFIG_PAL100BARS_FILTER=yes
+!CONFIG_RGBTESTSRC_FILTER=yes
+!CONFIG_SMPTEBARS_FILTER=yes
+!CONFIG_SMPTEHDBARS_FILTER=yes
+!CONFIG_TESTSRC_FILTER=yes
+!CONFIG_TESTSRC2_FILTER=yes
+!CONFIG_YUVTESTSRC_FILTER=yes
+!CONFIG_NULLSINK_FILTER=yes
+!CONFIG_ABITSCOPE_FILTER=yes
+!CONFIG_ADRAWGRAPH_FILTER=yes
+!CONFIG_AGRAPHMONITOR_FILTER=yes
+!CONFIG_AHISTOGRAM_FILTER=yes
+!CONFIG_APHASEMETER_FILTER=yes
+!CONFIG_AVECTORSCOPE_FILTER=yes
+!CONFIG_CONCAT_FILTER=yes
+!CONFIG_SHOWCQT_FILTER=yes
+!CONFIG_SHOWFREQS_FILTER=yes
+!CONFIG_SHOWSPECTRUM_FILTER=yes
+!CONFIG_SHOWSPECTRUMPIC_FILTER=yes
+!CONFIG_SHOWVOLUME_FILTER=yes
+!CONFIG_SHOWWAVES_FILTER=yes
+!CONFIG_SHOWWAVESPIC_FILTER=yes
+!CONFIG_SPECTRUMSYNTH_FILTER=yes
+!CONFIG_AMOVIE_FILTER=yes
+!CONFIG_MOVIE_FILTER=yes
+!CONFIG_AFIFO_FILTER=yes
+!CONFIG_FIFO_FILTER=yes
+!CONFIG_AA_DEMUXER=yes
+!CONFIG_AAC_DEMUXER=yes
+!CONFIG_AC3_DEMUXER=yes
+!CONFIG_ACM_DEMUXER=yes
+!CONFIG_ACT_DEMUXER=yes
+!CONFIG_ADF_DEMUXER=yes
+!CONFIG_ADP_DEMUXER=yes
+!CONFIG_ADS_DEMUXER=yes
+!CONFIG_ADX_DEMUXER=yes
+!CONFIG_AEA_DEMUXER=yes
+!CONFIG_AFC_DEMUXER=yes
+!CONFIG_AIFF_DEMUXER=yes
+!CONFIG_AIX_DEMUXER=yes
+!CONFIG_AMR_DEMUXER=yes
+!CONFIG_AMRNB_DEMUXER=yes
+!CONFIG_AMRWB_DEMUXER=yes
+!CONFIG_ANM_DEMUXER=yes
+!CONFIG_APC_DEMUXER=yes
+!CONFIG_APE_DEMUXER=yes
+!CONFIG_APNG_DEMUXER=yes
+!CONFIG_APTX_DEMUXER=yes
+!CONFIG_APTX_HD_DEMUXER=yes
+!CONFIG_AQTITLE_DEMUXER=yes
+!CONFIG_ASF_DEMUXER=yes
+!CONFIG_ASF_O_DEMUXER=yes
+!CONFIG_ASS_DEMUXER=yes
+!CONFIG_AST_DEMUXER=yes
+!CONFIG_AU_DEMUXER=yes
+!CONFIG_AVI_DEMUXER=yes
+!CONFIG_AVISYNTH_DEMUXER=yes
+!CONFIG_AVR_DEMUXER=yes
+!CONFIG_AVS_DEMUXER=yes
+!CONFIG_AVS2_DEMUXER=yes
+!CONFIG_BETHSOFTVID_DEMUXER=yes
+!CONFIG_BFI_DEMUXER=yes
+!CONFIG_BINTEXT_DEMUXER=yes
+!CONFIG_BINK_DEMUXER=yes
+!CONFIG_BIT_DEMUXER=yes
+!CONFIG_BMV_DEMUXER=yes
+!CONFIG_BFSTM_DEMUXER=yes
+!CONFIG_BRSTM_DEMUXER=yes
+!CONFIG_BOA_DEMUXER=yes
+!CONFIG_C93_DEMUXER=yes
+!CONFIG_CAF_DEMUXER=yes
+!CONFIG_CAVSVIDEO_DEMUXER=yes
+!CONFIG_CDG_DEMUXER=yes
+!CONFIG_CDXL_DEMUXER=yes
+!CONFIG_CINE_DEMUXER=yes
+!CONFIG_CODEC2_DEMUXER=yes
+!CONFIG_CODEC2RAW_DEMUXER=yes
+!CONFIG_CONCAT_DEMUXER=yes
+!CONFIG_DASH_DEMUXER=yes
+!CONFIG_DATA_DEMUXER=yes
+!CONFIG_DAUD_DEMUXER=yes
+!CONFIG_DCSTR_DEMUXER=yes
+!CONFIG_DFA_DEMUXER=yes
+!CONFIG_DIRAC_DEMUXER=yes
+!CONFIG_DNXHD_DEMUXER=yes
+!CONFIG_DSF_DEMUXER=yes
+!CONFIG_DSICIN_DEMUXER=yes
+!CONFIG_DSS_DEMUXER=yes
+!CONFIG_DTS_DEMUXER=yes
+!CONFIG_DTSHD_DEMUXER=yes
+!CONFIG_DV_DEMUXER=yes
+!CONFIG_DVBSUB_DEMUXER=yes
+!CONFIG_DVBTXT_DEMUXER=yes
+!CONFIG_DXA_DEMUXER=yes
+!CONFIG_EA_DEMUXER=yes
+!CONFIG_EA_CDATA_DEMUXER=yes
+!CONFIG_EAC3_DEMUXER=yes
+!CONFIG_EPAF_DEMUXER=yes
+!CONFIG_FFMETADATA_DEMUXER=yes
+!CONFIG_FILMSTRIP_DEMUXER=yes
+!CONFIG_FITS_DEMUXER=yes
+!CONFIG_FLAC_DEMUXER=yes
+!CONFIG_FLIC_DEMUXER=yes
+!CONFIG_FLV_DEMUXER=yes
+!CONFIG_LIVE_FLV_DEMUXER=yes
+!CONFIG_FOURXM_DEMUXER=yes
+!CONFIG_FRM_DEMUXER=yes
+!CONFIG_FSB_DEMUXER=yes
+!CONFIG_G722_DEMUXER=yes
+!CONFIG_G723_1_DEMUXER=yes
+!CONFIG_G726_DEMUXER=yes
+!CONFIG_G726LE_DEMUXER=yes
+!CONFIG_G729_DEMUXER=yes
+!CONFIG_GDV_DEMUXER=yes
+!CONFIG_GENH_DEMUXER=yes
+!CONFIG_GIF_DEMUXER=yes
+!CONFIG_GSM_DEMUXER=yes
+!CONFIG_GXF_DEMUXER=yes
+!CONFIG_H261_DEMUXER=yes
+!CONFIG_H263_DEMUXER=yes
+!CONFIG_H264_DEMUXER=yes
+!CONFIG_HEVC_DEMUXER=yes
+!CONFIG_HLS_DEMUXER=yes
+!CONFIG_HNM_DEMUXER=yes
+!CONFIG_ICO_DEMUXER=yes
+!CONFIG_IDCIN_DEMUXER=yes
+!CONFIG_IDF_DEMUXER=yes
+!CONFIG_IFF_DEMUXER=yes
+!CONFIG_ILBC_DEMUXER=yes
+!CONFIG_IMAGE2_DEMUXER=yes
+!CONFIG_IMAGE2PIPE_DEMUXER=yes
+!CONFIG_IMAGE2_ALIAS_PIX_DEMUXER=yes
+!CONFIG_IMAGE2_BRENDER_PIX_DEMUXER=yes
+!CONFIG_INGENIENT_DEMUXER=yes
+!CONFIG_IPMOVIE_DEMUXER=yes
+!CONFIG_IRCAM_DEMUXER=yes
+!CONFIG_ISS_DEMUXER=yes
+!CONFIG_IV8_DEMUXER=yes
+!CONFIG_IVF_DEMUXER=yes
+!CONFIG_IVR_DEMUXER=yes
+!CONFIG_JACOSUB_DEMUXER=yes
+!CONFIG_JV_DEMUXER=yes
+!CONFIG_LMLM4_DEMUXER=yes
+!CONFIG_LOAS_DEMUXER=yes
+!CONFIG_LRC_DEMUXER=yes
+!CONFIG_LVF_DEMUXER=yes
+!CONFIG_LXF_DEMUXER=yes
+!CONFIG_M4V_DEMUXER=yes
+!CONFIG_MATROSKA_DEMUXER=yes
+!CONFIG_MGSTS_DEMUXER=yes
+!CONFIG_MICRODVD_DEMUXER=yes
+!CONFIG_MJPEG_DEMUXER=yes
+!CONFIG_MJPEG_2000_DEMUXER=yes
+!CONFIG_MLP_DEMUXER=yes
+!CONFIG_MLV_DEMUXER=yes
+!CONFIG_MM_DEMUXER=yes
+!CONFIG_MMF_DEMUXER=yes
+CONFIG_MOV_DEMUXER=yes
+!CONFIG_MP3_DEMUXER=yes
+!CONFIG_MPC_DEMUXER=yes
+!CONFIG_MPC8_DEMUXER=yes
+!CONFIG_MPEGPS_DEMUXER=yes
+!CONFIG_MPEGTS_DEMUXER=yes
+!CONFIG_MPEGTSRAW_DEMUXER=yes
+!CONFIG_MPEGVIDEO_DEMUXER=yes
+!CONFIG_MPJPEG_DEMUXER=yes
+!CONFIG_MPL2_DEMUXER=yes
+!CONFIG_MPSUB_DEMUXER=yes
+!CONFIG_MSF_DEMUXER=yes
+!CONFIG_MSNWC_TCP_DEMUXER=yes
+!CONFIG_MTAF_DEMUXER=yes
+!CONFIG_MTV_DEMUXER=yes
+!CONFIG_MUSX_DEMUXER=yes
+!CONFIG_MV_DEMUXER=yes
+!CONFIG_MVI_DEMUXER=yes
+!CONFIG_MXF_DEMUXER=yes
+!CONFIG_MXG_DEMUXER=yes
+!CONFIG_NC_DEMUXER=yes
+!CONFIG_NISTSPHERE_DEMUXER=yes
+!CONFIG_NSP_DEMUXER=yes
+!CONFIG_NSV_DEMUXER=yes
+!CONFIG_NUT_DEMUXER=yes
+!CONFIG_NUV_DEMUXER=yes
+!CONFIG_OGG_DEMUXER=yes
+!CONFIG_OMA_DEMUXER=yes
+!CONFIG_PAF_DEMUXER=yes
+!CONFIG_PCM_ALAW_DEMUXER=yes
+!CONFIG_PCM_MULAW_DEMUXER=yes
+!CONFIG_PCM_VIDC_DEMUXER=yes
+!CONFIG_PCM_F64BE_DEMUXER=yes
+!CONFIG_PCM_F64LE_DEMUXER=yes
+!CONFIG_PCM_F32BE_DEMUXER=yes
+!CONFIG_PCM_F32LE_DEMUXER=yes
+!CONFIG_PCM_S32BE_DEMUXER=yes
+!CONFIG_PCM_S32LE_DEMUXER=yes
+!CONFIG_PCM_S24BE_DEMUXER=yes
+!CONFIG_PCM_S24LE_DEMUXER=yes
+!CONFIG_PCM_S16BE_DEMUXER=yes
+!CONFIG_PCM_S16LE_DEMUXER=yes
+!CONFIG_PCM_S8_DEMUXER=yes
+!CONFIG_PCM_U32BE_DEMUXER=yes
+!CONFIG_PCM_U32LE_DEMUXER=yes
+!CONFIG_PCM_U24BE_DEMUXER=yes
+!CONFIG_PCM_U24LE_DEMUXER=yes
+!CONFIG_PCM_U16BE_DEMUXER=yes
+!CONFIG_PCM_U16LE_DEMUXER=yes
+!CONFIG_PCM_U8_DEMUXER=yes
+!CONFIG_PJS_DEMUXER=yes
+!CONFIG_PMP_DEMUXER=yes
+!CONFIG_PVA_DEMUXER=yes
+!CONFIG_PVF_DEMUXER=yes
+!CONFIG_QCP_DEMUXER=yes
+!CONFIG_R3D_DEMUXER=yes
+!CONFIG_RAWVIDEO_DEMUXER=yes
+!CONFIG_REALTEXT_DEMUXER=yes
+!CONFIG_REDSPARK_DEMUXER=yes
+!CONFIG_RL2_DEMUXER=yes
+!CONFIG_RM_DEMUXER=yes
+!CONFIG_ROQ_DEMUXER=yes
+!CONFIG_RPL_DEMUXER=yes
+!CONFIG_RSD_DEMUXER=yes
+!CONFIG_RSO_DEMUXER=yes
+!CONFIG_RTP_DEMUXER=yes
+!CONFIG_RTSP_DEMUXER=yes
+!CONFIG_S337M_DEMUXER=yes
+!CONFIG_SAMI_DEMUXER=yes
+!CONFIG_SAP_DEMUXER=yes
+!CONFIG_SBC_DEMUXER=yes
+!CONFIG_SBG_DEMUXER=yes
+!CONFIG_SCC_DEMUXER=yes
+!CONFIG_SDP_DEMUXER=yes
+!CONFIG_SDR2_DEMUXER=yes
+!CONFIG_SDS_DEMUXER=yes
+!CONFIG_SDX_DEMUXER=yes
+!CONFIG_SEGAFILM_DEMUXER=yes
+!CONFIG_SER_DEMUXER=yes
+!CONFIG_SHORTEN_DEMUXER=yes
+!CONFIG_SIFF_DEMUXER=yes
+!CONFIG_SLN_DEMUXER=yes
+!CONFIG_SMACKER_DEMUXER=yes
+!CONFIG_SMJPEG_DEMUXER=yes
+!CONFIG_SMUSH_DEMUXER=yes
+!CONFIG_SOL_DEMUXER=yes
+!CONFIG_SOX_DEMUXER=yes
+!CONFIG_SPDIF_DEMUXER=yes
+!CONFIG_SRT_DEMUXER=yes
+!CONFIG_STR_DEMUXER=yes
+!CONFIG_STL_DEMUXER=yes
+!CONFIG_SUBVIEWER1_DEMUXER=yes
+!CONFIG_SUBVIEWER_DEMUXER=yes
+!CONFIG_SUP_DEMUXER=yes
+!CONFIG_SVAG_DEMUXER=yes
+!CONFIG_SWF_DEMUXER=yes
+!CONFIG_TAK_DEMUXER=yes
+!CONFIG_TEDCAPTIONS_DEMUXER=yes
+!CONFIG_THP_DEMUXER=yes
+!CONFIG_THREEDOSTR_DEMUXER=yes
+!CONFIG_TIERTEXSEQ_DEMUXER=yes
+!CONFIG_TMV_DEMUXER=yes
+!CONFIG_TRUEHD_DEMUXER=yes
+!CONFIG_TTA_DEMUXER=yes
+!CONFIG_TXD_DEMUXER=yes
+!CONFIG_TTY_DEMUXER=yes
+!CONFIG_TY_DEMUXER=yes
+!CONFIG_V210_DEMUXER=yes
+!CONFIG_V210X_DEMUXER=yes
+!CONFIG_VAG_DEMUXER=yes
+!CONFIG_VC1_DEMUXER=yes
+!CONFIG_VC1T_DEMUXER=yes
+!CONFIG_VIVO_DEMUXER=yes
+!CONFIG_VMD_DEMUXER=yes
+!CONFIG_VOBSUB_DEMUXER=yes
+!CONFIG_VOC_DEMUXER=yes
+!CONFIG_VPK_DEMUXER=yes
+!CONFIG_VPLAYER_DEMUXER=yes
+!CONFIG_VQF_DEMUXER=yes
+!CONFIG_W64_DEMUXER=yes
+CONFIG_WAV_DEMUXER=yes
+!CONFIG_WC3_DEMUXER=yes
+!CONFIG_WEBM_DASH_MANIFEST_DEMUXER=yes
+!CONFIG_WEBVTT_DEMUXER=yes
+!CONFIG_WSAUD_DEMUXER=yes
+!CONFIG_WSD_DEMUXER=yes
+!CONFIG_WSVQA_DEMUXER=yes
+!CONFIG_WTV_DEMUXER=yes
+!CONFIG_WVE_DEMUXER=yes
+!CONFIG_WV_DEMUXER=yes
+!CONFIG_XA_DEMUXER=yes
+!CONFIG_XBIN_DEMUXER=yes
+!CONFIG_XMV_DEMUXER=yes
+!CONFIG_XVAG_DEMUXER=yes
+!CONFIG_XWMA_DEMUXER=yes
+!CONFIG_YOP_DEMUXER=yes
+!CONFIG_YUV4MPEGPIPE_DEMUXER=yes
+!CONFIG_IMAGE_BMP_PIPE_DEMUXER=yes
+!CONFIG_IMAGE_DDS_PIPE_DEMUXER=yes
+!CONFIG_IMAGE_DPX_PIPE_DEMUXER=yes
+!CONFIG_IMAGE_EXR_PIPE_DEMUXER=yes
+!CONFIG_IMAGE_J2K_PIPE_DEMUXER=yes
+!CONFIG_IMAGE_JPEG_PIPE_DEMUXER=yes
+!CONFIG_IMAGE_JPEGLS_PIPE_DEMUXER=yes
+!CONFIG_IMAGE_PAM_PIPE_DEMUXER=yes
+!CONFIG_IMAGE_PBM_PIPE_DEMUXER=yes
+!CONFIG_IMAGE_PCX_PIPE_DEMUXER=yes
+!CONFIG_IMAGE_PGMYUV_PIPE_DEMUXER=yes
+!CONFIG_IMAGE_PGM_PIPE_DEMUXER=yes
+!CONFIG_IMAGE_PICTOR_PIPE_DEMUXER=yes
+!CONFIG_IMAGE_PNG_PIPE_DEMUXER=yes
+!CONFIG_IMAGE_PPM_PIPE_DEMUXER=yes
+!CONFIG_IMAGE_PSD_PIPE_DEMUXER=yes
+!CONFIG_IMAGE_QDRAW_PIPE_DEMUXER=yes
+!CONFIG_IMAGE_SGI_PIPE_DEMUXER=yes
+!CONFIG_IMAGE_SVG_PIPE_DEMUXER=yes
+!CONFIG_IMAGE_SUNRAST_PIPE_DEMUXER=yes
+!CONFIG_IMAGE_TIFF_PIPE_DEMUXER=yes
+!CONFIG_IMAGE_WEBP_PIPE_DEMUXER=yes
+!CONFIG_IMAGE_XPM_PIPE_DEMUXER=yes
+!CONFIG_IMAGE_XWD_PIPE_DEMUXER=yes
+!CONFIG_LIBGME_DEMUXER=yes
+!CONFIG_LIBMODPLUG_DEMUXER=yes
+!CONFIG_LIBOPENMPT_DEMUXER=yes
+!CONFIG_VAPOURSYNTH_DEMUXER=yes
+!CONFIG_A64_MUXER=yes
+!CONFIG_AC3_MUXER=yes
+!CONFIG_ADTS_MUXER=yes
+!CONFIG_ADX_MUXER=yes
+!CONFIG_AIFF_MUXER=yes
+!CONFIG_AMR_MUXER=yes
+!CONFIG_APNG_MUXER=yes
+!CONFIG_APTX_MUXER=yes
+!CONFIG_APTX_HD_MUXER=yes
+!CONFIG_ASF_MUXER=yes
+!CONFIG_ASS_MUXER=yes
+!CONFIG_AST_MUXER=yes
+!CONFIG_ASF_STREAM_MUXER=yes
+!CONFIG_AU_MUXER=yes
+!CONFIG_AVI_MUXER=yes
+!CONFIG_AVM2_MUXER=yes
+!CONFIG_AVS2_MUXER=yes
+!CONFIG_BIT_MUXER=yes
+!CONFIG_CAF_MUXER=yes
+!CONFIG_CAVSVIDEO_MUXER=yes
+!CONFIG_CODEC2_MUXER=yes
+!CONFIG_CODEC2RAW_MUXER=yes
+!CONFIG_CRC_MUXER=yes
+!CONFIG_DASH_MUXER=yes
+!CONFIG_DATA_MUXER=yes
+!CONFIG_DAUD_MUXER=yes
+!CONFIG_DIRAC_MUXER=yes
+!CONFIG_DNXHD_MUXER=yes
+!CONFIG_DTS_MUXER=yes
+!CONFIG_DV_MUXER=yes
+!CONFIG_EAC3_MUXER=yes
+!CONFIG_F4V_MUXER=yes
+!CONFIG_FFMETADATA_MUXER=yes
+!CONFIG_FIFO_MUXER=yes
+!CONFIG_FIFO_TEST_MUXER=yes
+!CONFIG_FILMSTRIP_MUXER=yes
+!CONFIG_FITS_MUXER=yes
+!CONFIG_FLAC_MUXER=yes
+!CONFIG_FLV_MUXER=yes
+!CONFIG_FRAMECRC_MUXER=yes
+!CONFIG_FRAMEHASH_MUXER=yes
+!CONFIG_FRAMEMD5_MUXER=yes
+!CONFIG_G722_MUXER=yes
+!CONFIG_G723_1_MUXER=yes
+!CONFIG_G726_MUXER=yes
+!CONFIG_G726LE_MUXER=yes
+!CONFIG_GIF_MUXER=yes
+!CONFIG_GSM_MUXER=yes
+!CONFIG_GXF_MUXER=yes
+!CONFIG_H261_MUXER=yes
+!CONFIG_H263_MUXER=yes
+!CONFIG_H264_MUXER=yes
+!CONFIG_HASH_MUXER=yes
+!CONFIG_HDS_MUXER=yes
+!CONFIG_HEVC_MUXER=yes
+!CONFIG_HLS_MUXER=yes
+!CONFIG_ICO_MUXER=yes
+!CONFIG_ILBC_MUXER=yes
+!CONFIG_IMAGE2_MUXER=yes
+!CONFIG_IMAGE2PIPE_MUXER=yes
+!CONFIG_IPOD_MUXER=yes
+!CONFIG_IRCAM_MUXER=yes
+!CONFIG_ISMV_MUXER=yes
+!CONFIG_IVF_MUXER=yes
+!CONFIG_JACOSUB_MUXER=yes
+!CONFIG_LATM_MUXER=yes
+!CONFIG_LRC_MUXER=yes
+!CONFIG_M4V_MUXER=yes
+!CONFIG_MD5_MUXER=yes
+!CONFIG_MATROSKA_MUXER=yes
+!CONFIG_MATROSKA_AUDIO_MUXER=yes
+!CONFIG_MICRODVD_MUXER=yes
+!CONFIG_MJPEG_MUXER=yes
+!CONFIG_MLP_MUXER=yes
+!CONFIG_MMF_MUXER=yes
+!CONFIG_MOV_MUXER=yes
+!CONFIG_MP2_MUXER=yes
+!CONFIG_MP3_MUXER=yes
+!CONFIG_MP4_MUXER=yes
+!CONFIG_MPEG1SYSTEM_MUXER=yes
+!CONFIG_MPEG1VCD_MUXER=yes
+!CONFIG_MPEG1VIDEO_MUXER=yes
+!CONFIG_MPEG2DVD_MUXER=yes
+!CONFIG_MPEG2SVCD_MUXER=yes
+!CONFIG_MPEG2VIDEO_MUXER=yes
+!CONFIG_MPEG2VOB_MUXER=yes
+!CONFIG_MPEGTS_MUXER=yes
+!CONFIG_MPJPEG_MUXER=yes
+!CONFIG_MXF_MUXER=yes
+!CONFIG_MXF_D10_MUXER=yes
+!CONFIG_MXF_OPATOM_MUXER=yes
+!CONFIG_NULL_MUXER=yes
+!CONFIG_NUT_MUXER=yes
+!CONFIG_OGA_MUXER=yes
+!CONFIG_OGG_MUXER=yes
+!CONFIG_OGV_MUXER=yes
+!CONFIG_OMA_MUXER=yes
+!CONFIG_OPUS_MUXER=yes
+!CONFIG_PCM_ALAW_MUXER=yes
+!CONFIG_PCM_MULAW_MUXER=yes
+!CONFIG_PCM_VIDC_MUXER=yes
+!CONFIG_PCM_F64BE_MUXER=yes
+!CONFIG_PCM_F64LE_MUXER=yes
+!CONFIG_PCM_F32BE_MUXER=yes
+!CONFIG_PCM_F32LE_MUXER=yes
+!CONFIG_PCM_S32BE_MUXER=yes
+!CONFIG_PCM_S32LE_MUXER=yes
+!CONFIG_PCM_S24BE_MUXER=yes
+!CONFIG_PCM_S24LE_MUXER=yes
+!CONFIG_PCM_S16BE_MUXER=yes
+!CONFIG_PCM_S16LE_MUXER=yes
+!CONFIG_PCM_S8_MUXER=yes
+!CONFIG_PCM_U32BE_MUXER=yes
+!CONFIG_PCM_U32LE_MUXER=yes
+!CONFIG_PCM_U24BE_MUXER=yes
+!CONFIG_PCM_U24LE_MUXER=yes
+!CONFIG_PCM_U16BE_MUXER=yes
+!CONFIG_PCM_U16LE_MUXER=yes
+!CONFIG_PCM_U8_MUXER=yes
+!CONFIG_PSP_MUXER=yes
+!CONFIG_RAWVIDEO_MUXER=yes
+!CONFIG_RM_MUXER=yes
+!CONFIG_ROQ_MUXER=yes
+!CONFIG_RSO_MUXER=yes
+!CONFIG_RTP_MUXER=yes
+!CONFIG_RTP_MPEGTS_MUXER=yes
+!CONFIG_RTSP_MUXER=yes
+!CONFIG_SAP_MUXER=yes
+!CONFIG_SBC_MUXER=yes
+!CONFIG_SCC_MUXER=yes
+!CONFIG_SEGAFILM_MUXER=yes
+!CONFIG_SEGMENT_MUXER=yes
+!CONFIG_STREAM_SEGMENT_MUXER=yes
+!CONFIG_SINGLEJPEG_MUXER=yes
+!CONFIG_SMJPEG_MUXER=yes
+!CONFIG_SMOOTHSTREAMING_MUXER=yes
+!CONFIG_SOX_MUXER=yes
+!CONFIG_SPX_MUXER=yes
+!CONFIG_SPDIF_MUXER=yes
+!CONFIG_SRT_MUXER=yes
+!CONFIG_SUP_MUXER=yes
+!CONFIG_SWF_MUXER=yes
+!CONFIG_TEE_MUXER=yes
+!CONFIG_TG2_MUXER=yes
+!CONFIG_TGP_MUXER=yes
+!CONFIG_MKVTIMESTAMP_V2_MUXER=yes
+!CONFIG_TRUEHD_MUXER=yes
+!CONFIG_TTA_MUXER=yes
+!CONFIG_UNCODEDFRAMECRC_MUXER=yes
+!CONFIG_VC1_MUXER=yes
+!CONFIG_VC1T_MUXER=yes
+!CONFIG_VOC_MUXER=yes
+!CONFIG_W64_MUXER=yes
+!CONFIG_WAV_MUXER=yes
+!CONFIG_WEBM_MUXER=yes
+!CONFIG_WEBM_DASH_MANIFEST_MUXER=yes
+!CONFIG_WEBM_CHUNK_MUXER=yes
+!CONFIG_WEBP_MUXER=yes
+!CONFIG_WEBVTT_MUXER=yes
+!CONFIG_WTV_MUXER=yes
+!CONFIG_WV_MUXER=yes
+!CONFIG_YUV4MPEGPIPE_MUXER=yes
+!CONFIG_CHROMAPRINT_MUXER=yes
+!CONFIG_ASYNC_PROTOCOL=yes
+!CONFIG_BLURAY_PROTOCOL=yes
+!CONFIG_CACHE_PROTOCOL=yes
+!CONFIG_CONCAT_PROTOCOL=yes
+!CONFIG_CRYPTO_PROTOCOL=yes
+!CONFIG_DATA_PROTOCOL=yes
+!CONFIG_FFRTMPCRYPT_PROTOCOL=yes
+!CONFIG_FFRTMPHTTP_PROTOCOL=yes
+CONFIG_FILE_PROTOCOL=yes
+!CONFIG_FTP_PROTOCOL=yes
+!CONFIG_GOPHER_PROTOCOL=yes
+!CONFIG_HLS_PROTOCOL=yes
+!CONFIG_HTTP_PROTOCOL=yes
+!CONFIG_HTTPPROXY_PROTOCOL=yes
+!CONFIG_HTTPS_PROTOCOL=yes
+!CONFIG_ICECAST_PROTOCOL=yes
+!CONFIG_MMSH_PROTOCOL=yes
+!CONFIG_MMST_PROTOCOL=yes
+!CONFIG_MD5_PROTOCOL=yes
+!CONFIG_PIPE_PROTOCOL=yes
+!CONFIG_PROMPEG_PROTOCOL=yes
+!CONFIG_RTMP_PROTOCOL=yes
+!CONFIG_RTMPE_PROTOCOL=yes
+!CONFIG_RTMPS_PROTOCOL=yes
+!CONFIG_RTMPT_PROTOCOL=yes
+!CONFIG_RTMPTE_PROTOCOL=yes
+!CONFIG_RTMPTS_PROTOCOL=yes
+!CONFIG_RTP_PROTOCOL=yes
+!CONFIG_SCTP_PROTOCOL=yes
+!CONFIG_SRTP_PROTOCOL=yes
+!CONFIG_SUBFILE_PROTOCOL=yes
+!CONFIG_TEE_PROTOCOL=yes
+!CONFIG_TCP_PROTOCOL=yes
+!CONFIG_TLS_PROTOCOL=yes
+!CONFIG_UDP_PROTOCOL=yes
+!CONFIG_UDPLITE_PROTOCOL=yes
+!CONFIG_UNIX_PROTOCOL=yes
+!CONFIG_LIBRTMP_PROTOCOL=yes
+!CONFIG_LIBRTMPE_PROTOCOL=yes
+!CONFIG_LIBRTMPS_PROTOCOL=yes
+!CONFIG_LIBRTMPT_PROTOCOL=yes
+!CONFIG_LIBRTMPTE_PROTOCOL=yes
+!CONFIG_LIBSRT_PROTOCOL=yes
+!CONFIG_LIBSSH_PROTOCOL=yes
+!CONFIG_LIBSMBCLIENT_PROTOCOL=yes
+endif # FFMPEG_CONFIG_MAK
diff -uparN ffmpeg-4.1/ffbuild/config.sh ffmpeg-y/ffbuild/config.sh
--- ffmpeg-4.1/ffbuild/config.sh	1970-01-01 08:00:00.000000000 +0800
+++ ffmpeg-y/ffbuild/config.sh	2019-06-29 11:49:36.833017667 +0800
@@ -0,0 +1,28 @@
+# Automatically generated by configure - do not modify!
+shared=no
+build_suffix=
+prefix=./install
+libdir=${prefix}/lib
+incdir=${prefix}/include
+rpath=
+source_path=.
+LIBPREF=lib
+LIBSUF=.a
+extralibs_avutil="-lm"
+extralibs_avcodec="-lm"
+extralibs_avformat="-lm"
+extralibs_avdevice="-lm"
+extralibs_avfilter="-lm"
+extralibs_avresample="-lm"
+extralibs_postproc="-lm"
+extralibs_swscale="-lm"
+extralibs_swresample="-lm"
+avdevice_deps="avformat avcodec avutil"
+avfilter_deps="avutil"
+swscale_deps="avutil"
+postproc_deps="avutil"
+avformat_deps="avcodec avutil"
+avcodec_deps="avutil"
+swresample_deps="avutil"
+avresample_deps="avutil"
+avutil_deps=""
diff -uparN ffmpeg-4.1/libavcodec/bsf_list.c ffmpeg-y/libavcodec/bsf_list.c
--- ffmpeg-4.1/libavcodec/bsf_list.c	1970-01-01 08:00:00.000000000 +0800
+++ ffmpeg-y/libavcodec/bsf_list.c	2019-06-29 11:49:36.781017666 +0800
@@ -0,0 +1,5 @@
+static const AVBitStreamFilter * const bitstream_filters[] = {
+    &ff_h264_mp4toannexb_bsf,
+    &ff_hevc_mp4toannexb_bsf,
+    &ff_null_bsf,
+    NULL };
diff -uparN ffmpeg-4.1/libavcodec/cbs_av1.c ffmpeg-y/libavcodec/cbs_av1.c
--- ffmpeg-4.1/libavcodec/cbs_av1.c	2018-11-06 07:22:33.000000000 +0800
+++ ffmpeg-y/libavcodec/cbs_av1.c	2019-06-29 11:49:36.785017666 +0800
@@ -29,45 +29,67 @@ static int cbs_av1_read_uvlc(CodedBitstr
                              const char *name, uint32_t *write_to,
                              uint32_t range_min, uint32_t range_max)
 {
-    uint32_t value;
-    int position, zeroes, i, j;
-    char bits[65];
+    uint32_t zeroes, bits_value, value;
+    int position;
 
     if (ctx->trace_enable)
         position = get_bits_count(gbc);
 
-    zeroes = i = 0;
+    zeroes = 0;
     while (1) {
-        if (get_bits_left(gbc) < zeroes + 1) {
+        if (get_bits_left(gbc) < 1) {
             av_log(ctx->log_ctx, AV_LOG_ERROR, "Invalid uvlc code at "
                    "%s: bitstream ended.\n", name);
             return AVERROR_INVALIDDATA;
         }
 
-        if (get_bits1(gbc)) {
-            bits[i++] = '1';
+        if (get_bits1(gbc))
             break;
-        } else {
-            bits[i++] = '0';
-            ++zeroes;
-        }
+        ++zeroes;
     }
 
     if (zeroes >= 32) {
         value = MAX_UINT_BITS(32);
     } else {
-        value = get_bits_long(gbc, zeroes);
-
-        for (j = 0; j < zeroes; j++)
-            bits[i++] = (value >> (zeroes - j - 1) & 1) ? '1' : '0';
+        if (get_bits_left(gbc) < zeroes) {
+            av_log(ctx->log_ctx, AV_LOG_ERROR, "Invalid uvlc code at "
+                   "%s: bitstream ended.\n", name);
+            return AVERROR_INVALIDDATA;
+        }
 
-        value += (1 << zeroes) - 1;
+        bits_value = get_bits_long(gbc, zeroes);
+        value = bits_value + (UINT32_C(1) << zeroes) - 1;
     }
 
     if (ctx->trace_enable) {
+        char bits[65];
+        int i, j, k;
+
+        if (zeroes >= 32) {
+            while (zeroes > 32) {
+                k = FFMIN(zeroes - 32, 32);
+                for (i = 0; i < k; i++)
+                    bits[i] = '0';
+                bits[i] = 0;
+                ff_cbs_trace_syntax_element(ctx, position, name,
+                                            NULL, bits, 0);
+                zeroes -= k;
+                position += k;
+            }
+        }
+
+        for (i = 0; i < zeroes; i++)
+            bits[i] = '0';
+        bits[i++] = '1';
+
+        if (zeroes < 32) {
+            for (j = 0; j < zeroes; j++)
+                bits[i++] = (bits_value >> (zeroes - j - 1) & 1) ? '1' : '0';
+        }
+
         bits[i] = 0;
-        ff_cbs_trace_syntax_element(ctx, position, name, NULL,
-                                    bits, value);
+        ff_cbs_trace_syntax_element(ctx, position, name,
+                                    NULL, bits, value);
     }
 
     if (value < range_min || value > range_max) {
diff -uparN ffmpeg-4.1/libavcodec/codec_list.c ffmpeg-y/libavcodec/codec_list.c
--- ffmpeg-4.1/libavcodec/codec_list.c	1970-01-01 08:00:00.000000000 +0800
+++ ffmpeg-y/libavcodec/codec_list.c	2019-06-29 11:49:36.825017667 +0800
@@ -0,0 +1,2 @@
+static const AVCodec * const codec_list[] = {
+    NULL };
diff -uparN ffmpeg-4.1/libavcodec/htmlsubtitles.c ffmpeg-y/libavcodec/htmlsubtitles.c
--- ffmpeg-4.1/libavcodec/htmlsubtitles.c	2018-11-02 02:34:24.000000000 +0800
+++ ffmpeg-y/libavcodec/htmlsubtitles.c	2019-06-29 11:49:36.781017666 +0800
@@ -24,6 +24,7 @@
 #include "libavutil/common.h"
 #include "libavutil/parseutils.h"
 #include "htmlsubtitles.h"
+#include <ctype.h>
 
 static int html_color_parse(void *log_ctx, const char *str)
 {
@@ -44,14 +45,32 @@ static void rstrip_spaces_buf(AVBPrint *
             buf->str[--buf->len] = 0;
 }
 
+/*
+ * Fast code for scanning text enclosed in braces. Functionally
+ * equivalent to this sscanf call:
+ *
+ * sscanf(in, "{\\an%*1u}%n", &len) >= 0 && len > 0
+ */
+static int scanbraces(const char* in) {
+    if (strncmp(in, "{\\an", 4) != 0) {
+        return 0;
+    }
+    if (!isdigit(in[4])) {
+        return 0;
+    }
+    if (in[5] != '}') {
+        return 0;
+    }
+    return 1;
+}
+
 /* skip all {\xxx} substrings except for {\an%d}
    and all microdvd like styles such as {Y:xxx} */
 static void handle_open_brace(AVBPrint *dst, const char **inp, int *an, int *closing_brace_missing)
 {
-    int len = 0;
     const char *in = *inp;
 
-    *an += sscanf(in, "{\\an%*1u}%n", &len) >= 0 && len > 0;
+    *an += scanbraces(in);
 
     if (!*closing_brace_missing) {
         if (   (*an != 1 && in[1] == '\\')
@@ -75,6 +94,34 @@ struct font_tag {
 };
 
 /*
+ * Fast code for scanning the rest of a tag. Functionally equivalent to
+ * this sscanf call:
+ *
+ * sscanf(in, "%127[^<>]>%n", buffer, lenp) == 2
+ */
+static int scantag(const char* in, char* buffer, int* lenp) {
+    int len;
+
+    for (len = 0; len < 128; len++) {
+        const char c = *in++;
+        switch (c) {
+        case '\0':
+            return 0;
+        case '<':
+            return 0;
+        case '>':
+            buffer[len] = '\0';
+            *lenp = len+1;
+            return 1;
+        default:
+            break;
+        }
+        buffer[len] = c;
+    }
+    return 0;
+}
+
+/*
  * The general politic of the convert is to mask unsupported tags or formatting
  * errors (but still alert the user/subtitles writer with an error/warning)
  * without dropping any actual text content for the final user.
@@ -155,7 +202,7 @@ int ff_htmlmarkup_to_ass(void *log_ctx,
 
             len = 0;
 
-            if (sscanf(in+tag_close+1, "%127[^<>]>%n", buffer, &len) >= 1 && len > 0) {
+            if (scantag(in+tag_close+1, buffer, &len) && len > 0) {
                 const int skip = len + tag_close;
                 const char *tagname = buffer;
                 while (*tagname == ' ') {
diff -uparN ffmpeg-4.1/libavcodec/libavcodec.version ffmpeg-y/libavcodec/libavcodec.version
--- ffmpeg-4.1/libavcodec/libavcodec.version	1970-01-01 08:00:00.000000000 +0800
+++ ffmpeg-y/libavcodec/libavcodec.version	2019-06-29 11:50:15.237018730 +0800
@@ -0,0 +1,3 @@
+libavcodec_VERSION=58.35.100
+libavcodec_VERSION_MAJOR=58
+libavcodec_VERSION_MINOR=35
diff -uparN ffmpeg-4.1/libavcodec/Makefile ffmpeg-y/libavcodec/Makefile
--- ffmpeg-4.1/libavcodec/Makefile	2018-11-06 07:22:25.000000000 +0800
+++ ffmpeg-y/libavcodec/Makefile	2019-06-29 11:49:36.777017665 +0800
@@ -6,13 +6,10 @@ HEADERS = ac3_parser.h
           avcodec.h                                                     \
           avdct.h                                                       \
           avfft.h                                                       \
-          d3d11va.h                                                     \
           dirac.h                                                       \
           dv_profile.h                                                  \
           dxva2.h                                                       \
-          jni.h                                                         \
           mediacodec.h                                                  \
-          qsv.h                                                         \
           vaapi.h                                                       \
           vdpau.h                                                       \
           version.h                                                     \
@@ -20,10 +17,7 @@ HEADERS = ac3_parser.h
           vorbis_parser.h                                               \
           xvmc.h                                                        \
 
-OBJS = ac3_parser.o                                                     \
-       adts_parser.o                                                    \
-       allcodecs.o                                                      \
-       avdct.o                                                          \
+OBJS = allcodecs.o                                                      \
        avpacket.o                                                       \
        avpicture.o                                                      \
        bitstream.o                                                      \
@@ -31,25 +25,19 @@ OBJS = ac3_parser.o
        bitstream_filters.o                                              \
        bsf.o                                                            \
        codec_desc.o                                                     \
-       d3d11va.o                                                        \
        decode.o                                                         \
        dirac.o                                                          \
        dv_profile.o                                                     \
-       encode.o                                                         \
        imgconvert.o                                                     \
-       jni.o                                                            \
        mathtables.o                                                     \
        mediacodec.o                                                     \
        mpeg12framerate.o                                                \
        options.o                                                        \
-       mjpegenc_huffman.o                                               \
        parser.o                                                         \
        parsers.o                                                        \
        profiles.o                                                       \
-       qsv_api.o                                                        \
        raw.o                                                            \
        utils.o                                                          \
-       vorbis_parser.o                                                  \
        xiph.o                                                           \
 
 # subsystems
diff -uparN ffmpeg-4.1/libavcodec/mpeg4videodec.c ffmpeg-y/libavcodec/mpeg4videodec.c
--- ffmpeg-4.1/libavcodec/mpeg4videodec.c	2018-11-06 07:22:33.000000000 +0800
+++ ffmpeg-y/libavcodec/mpeg4videodec.c	2019-06-29 11:49:36.757017665 +0800
@@ -1899,14 +1899,20 @@ static int mpeg4_decode_studio_block(Mpe
             code >>= 1;
             run = (1 << (additional_code_len - 1)) + code;
             idx += run;
+            if (idx > 63)
+                return AVERROR_INVALIDDATA;
             j = scantable[idx++];
             block[j] = sign ? 1 : -1;
         } else if (group >= 13 && group <= 20) {
             /* Level value (Table B.49) */
+            if (idx > 63)
+                return AVERROR_INVALIDDATA;
             j = scantable[idx++];
             block[j] = get_xbits(&s->gb, additional_code_len);
         } else if (group == 21) {
             /* Escape */
+            if (idx > 63)
+                return AVERROR_INVALIDDATA;
             j = scantable[idx++];
             additional_code_len = s->avctx->bits_per_raw_sample + s->dct_precision + 4;
             flc = get_bits(&s->gb, additional_code_len);
@@ -3057,6 +3063,7 @@ static int decode_studio_vop_header(Mpeg
         return 0;
 
     s->partitioned_frame = 0;
+    s->interlaced_dct = 0;
     s->decode_mb = mpeg4_decode_studio_mb;
 
     decode_smpte_tc(ctx, gb);
diff -uparN ffmpeg-4.1/libavcodec/parser_list.c ffmpeg-y/libavcodec/parser_list.c
--- ffmpeg-4.1/libavcodec/parser_list.c	1970-01-01 08:00:00.000000000 +0800
+++ ffmpeg-y/libavcodec/parser_list.c	2019-06-29 11:49:36.785017666 +0800
@@ -0,0 +1,2 @@
+static const AVCodecParser * const parser_list[] = {
+    NULL };
diff -uparN ffmpeg-4.1/libavcodec/x86/aacencdsp.asm ffmpeg-y/libavcodec/x86/aacencdsp.asm
--- ffmpeg-4.1/libavcodec/x86/aacencdsp.asm	2018-11-02 02:34:25.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/aacencdsp.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,86 +0,0 @@
-;******************************************************************************
-;* SIMD optimized AAC encoder DSP functions
-;*
-;* Copyright (C) 2016 Rostislav Pehlivanov <atomnuker@gmail.com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-
-float_abs_mask: times 4 dd 0x7fffffff
-
-SECTION .text
-
-;*******************************************************************
-;void ff_abs_pow34(float *out, const float *in, const int size);
-;*******************************************************************
-INIT_XMM sse
-cglobal abs_pow34, 3, 3, 3, out, in, size
-    mova   m2, [float_abs_mask]
-    shl    sizeq, 2
-    add    inq, sizeq
-    add    outq, sizeq
-    neg    sizeq
-.loop:
-    andps  m0, m2, [inq+sizeq]
-    sqrtps m1, m0
-    mulps  m0, m1
-    sqrtps m0, m0
-    mova   [outq+sizeq], m0
-    add    sizeq, mmsize
-    jl    .loop
-    RET
-
-;*******************************************************************
-;void ff_aac_quantize_bands(int *out, const float *in, const float *scaled,
-;                           int size, int is_signed, int maxval, const float Q34,
-;                           const float rounding)
-;*******************************************************************
-INIT_XMM sse2
-cglobal aac_quantize_bands, 5, 5, 6, out, in, scaled, size, is_signed, maxval, Q34, rounding
-%if UNIX64 == 0
-    movss     m0, Q34m
-    movss     m1, roundingm
-    cvtsi2ss  m3, dword maxvalm
-%else
-    cvtsi2ss  m3, maxvald
-%endif
-    shufps    m0, m0, 0
-    shufps    m1, m1, 0
-    shufps    m3, m3, 0
-    shl       is_signedd, 31
-    movd      m4, is_signedd
-    shufps    m4, m4, 0
-    shl       sized,   2
-    add       inq, sizeq
-    add       outq, sizeq
-    add       scaledq, sizeq
-    neg       sizeq
-.loop:
-    mulps     m2, m0, [scaledq+sizeq]
-    addps     m2, m1
-    minps     m2, m3
-    andps     m5, m4, [inq+sizeq]
-    orps      m2, m5
-    cvttps2dq m2, m2
-    mova      [outq+sizeq], m2
-    add       sizeq, mmsize
-    jl       .loop
-    RET
diff -uparN ffmpeg-4.1/libavcodec/x86/aacpsdsp.asm ffmpeg-y/libavcodec/x86/aacpsdsp.asm
--- ffmpeg-4.1/libavcodec/x86/aacpsdsp.asm	2018-11-02 02:34:25.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/aacpsdsp.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,487 +0,0 @@
-;******************************************************************************
-;* SIMD optimized MPEG-4 Parametric Stereo decoding functions
-;*
-;* Copyright (C) 2015 James Almer
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-
-ps_p1m1p1m1: dd 0, 0x80000000, 0, 0x80000000
-
-SECTION .text
-
-;*************************************************************************
-;void ff_ps_add_squares_<opt>(float *dst, const float (*src)[2], int n);
-;*************************************************************************
-%macro PS_ADD_SQUARES 1
-cglobal ps_add_squares, 3, 3, %1, dst, src, n
-    shl    nd, 3
-    add  srcq, nq
-    neg    nq
-
-align 16
-.loop:
-    movaps m0, [srcq+nq]
-    movaps m1, [srcq+nq+mmsize]
-    mulps  m0, m0
-    mulps  m1, m1
-    HADDPS m0, m1, m2
-    addps  m0, [dstq]
-    movaps [dstq], m0
-    add  dstq, mmsize
-    add    nq, mmsize*2
-    jl .loop
-    REP_RET
-%endmacro
-
-INIT_XMM sse
-PS_ADD_SQUARES 2
-INIT_XMM sse3
-PS_ADD_SQUARES 3
-
-;*******************************************************************
-;void ff_ps_mul_pair_single_sse(float (*dst)[2], float (*src0)[2],
-;                                   float *src1, int n);
-;*******************************************************************
-INIT_XMM sse
-cglobal ps_mul_pair_single, 4, 4, 4, dst, src1, src2, n
-    shl      nd, 3
-    add   src1q, nq
-    add    dstq, nq
-    neg      nq
-
-align 16
-.loop:
-    movu     m0, [src1q+nq]
-    movu     m1, [src1q+nq+mmsize]
-    mova     m2, [src2q]
-    mova     m3, m2
-    unpcklps m2, m2
-    unpckhps m3, m3
-    mulps    m0, m2
-    mulps    m1, m3
-    mova [dstq+nq], m0
-    mova [dstq+nq+mmsize], m1
-    add   src2q, mmsize
-    add      nq, mmsize*2
-    jl .loop
-    REP_RET
-
-;***********************************************************************
-;void ff_ps_stereo_interpolate_sse3(float (*l)[2], float (*r)[2],
-;                                   float h[2][4], float h_step[2][4],
-;                                   int len);
-;***********************************************************************
-INIT_XMM sse3
-cglobal ps_stereo_interpolate, 5, 5, 6, l, r, h, h_step, n
-    movaps   m0, [hq]
-    movaps   m1, [h_stepq]
-    unpcklps m4, m0, m0
-    unpckhps m0, m0
-    unpcklps m5, m1, m1
-    unpckhps m1, m1
-    shl      nd, 3
-    add      lq, nq
-    add      rq, nq
-    neg      nq
-
-align 16
-.loop:
-    addps    m4, m5
-    addps    m0, m1
-    movddup  m2, [lq+nq]
-    movddup  m3, [rq+nq]
-    mulps    m2, m4
-    mulps    m3, m0
-    addps    m2, m3
-    movsd  [lq+nq], m2
-    movhps [rq+nq], m2
-    add      nq, 8
-    jl .loop
-    REP_RET
-
-;***************************************************************************
-;void ps_stereo_interpolate_ipdopd_sse3(float (*l)[2], float (*r)[2],
-;                                       float h[2][4], float h_step[2][4],
-;                                       int len);
-;***************************************************************************
-INIT_XMM sse3
-cglobal ps_stereo_interpolate_ipdopd, 5, 5, 10, l, r, h, h_step, n
-    movaps   m0, [hq]
-    movaps   m1, [hq+mmsize]
-%if ARCH_X86_64
-    movaps   m8, [h_stepq]
-    movaps   m9, [h_stepq+mmsize]
-    %define  H_STEP0 m8
-    %define  H_STEP1 m9
-%else
-    %define  H_STEP0 [h_stepq]
-    %define  H_STEP1 [h_stepq+mmsize]
-%endif
-    shl      nd, 3
-    add      lq, nq
-    add      rq, nq
-    neg      nq
-
-align 16
-.loop:
-    addps    m0, H_STEP0
-    addps    m1, H_STEP1
-    movddup  m2, [lq+nq]
-    movddup  m3, [rq+nq]
-    shufps   m4, m2, m2, q2301
-    shufps   m5, m3, m3, q2301
-    unpcklps m6, m0, m0
-    unpckhps m7, m0, m0
-    mulps    m2, m6
-    mulps    m3, m7
-    unpcklps m6, m1, m1
-    unpckhps m7, m1, m1
-    mulps    m4, m6
-    mulps    m5, m7
-    addps    m2, m3
-    addsubps m2, m4
-    addsubps m2, m5
-    movsd  [lq+nq], m2
-    movhps [rq+nq], m2
-    add      nq, 8
-    jl .loop
-    REP_RET
-
-;**********************************************************
-;void ps_hybrid_analysis_ileave_sse(float out[2][38][64],
-;                                   float (*in)[32][2],
-;                                   int i, int len)
-;**********************************************************
-INIT_XMM sse
-cglobal ps_hybrid_analysis_ileave, 3, 7, 5, out, in, i, len, in0, in1, tmp
-    movsxdifnidn        iq, id
-    mov               lend, 32 << 3
-    lea                inq, [inq+iq*4]
-    mov               tmpd, id
-    shl               tmpd, 8
-    add               outq, tmpq
-    mov               tmpd, 64
-    sub               tmpd, id
-    mov                 id, tmpd
-
-    test                id, 1
-    jne .loop4
-    test                id, 2
-    jne .loop8
-
-align 16
-.loop16:
-    mov               in0q, inq
-    mov               in1q, 38*64*4
-    add               in1q, in0q
-    mov               tmpd, lend
-
-.inner_loop16:
-    movaps              m0, [in0q]
-    movaps              m1, [in1q]
-    movaps              m2, [in0q+lenq]
-    movaps              m3, [in1q+lenq]
-    TRANSPOSE4x4PS 0, 1, 2, 3, 4
-    movaps          [outq], m0
-    movaps     [outq+lenq], m1
-    movaps   [outq+lenq*2], m2
-    movaps [outq+3*32*2*4], m3
-    lea               in0q, [in0q+lenq*2]
-    lea               in1q, [in1q+lenq*2]
-    add               outq, mmsize
-    sub               tmpd, mmsize
-    jg .inner_loop16
-    add                inq, 16
-    add               outq, 3*32*2*4
-    sub                 id, 4
-    jg .loop16
-    RET
-
-align 16
-.loop8:
-    mov               in0q, inq
-    mov               in1q, 38*64*4
-    add               in1q, in0q
-    mov               tmpd, lend
-
-.inner_loop8:
-    movlps              m0, [in0q]
-    movlps              m1, [in1q]
-    movhps              m0, [in0q+lenq]
-    movhps              m1, [in1q+lenq]
-    SBUTTERFLYPS 0, 1, 2
-    SBUTTERFLYPD 0, 1, 2
-    movaps          [outq], m0
-    movaps     [outq+lenq], m1
-    lea               in0q, [in0q+lenq*2]
-    lea               in1q, [in1q+lenq*2]
-    add               outq, mmsize
-    sub               tmpd, mmsize
-    jg .inner_loop8
-    add                inq, 8
-    add               outq, lenq
-    sub                 id, 2
-    jg .loop16
-    RET
-
-align 16
-.loop4:
-    mov               in0q, inq
-    mov               in1q, 38*64*4
-    add               in1q, in0q
-    mov               tmpd, lend
-
-.inner_loop4:
-    movss               m0, [in0q]
-    movss               m1, [in1q]
-    movss               m2, [in0q+lenq]
-    movss               m3, [in1q+lenq]
-    movlhps             m0, m1
-    movlhps             m2, m3
-    shufps              m0, m2, q2020
-    movaps          [outq], m0
-    lea               in0q, [in0q+lenq*2]
-    lea               in1q, [in1q+lenq*2]
-    add               outq, mmsize
-    sub               tmpd, mmsize
-    jg .inner_loop4
-    add                inq, 4
-    sub                 id, 1
-    test                id, 2
-    jne .loop8
-    cmp                 id, 4
-    jge .loop16
-    RET
-
-;***********************************************************
-;void ps_hybrid_synthesis_deint_sse4(float out[2][38][64],
-;                                    float (*in)[32][2],
-;                                    int i, int len)
-;***********************************************************
-%macro HYBRID_SYNTHESIS_DEINT 0
-cglobal ps_hybrid_synthesis_deint, 3, 7, 5, out, in, i, len, out0, out1, tmp
-%if cpuflag(sse4)
-%define MOVH movsd
-%else
-%define MOVH movlps
-%endif
-    movsxdifnidn        iq, id
-    mov               lend, 32 << 3
-    lea               outq, [outq+iq*4]
-    mov               tmpd, id
-    shl               tmpd, 8
-    add                inq, tmpq
-    mov               tmpd, 64
-    sub               tmpd, id
-    mov                 id, tmpd
-
-    test                id, 1
-    jne .loop4
-    test                id, 2
-    jne .loop8
-
-align 16
-.loop16:
-    mov              out0q, outq
-    mov              out1q, 38*64*4
-    add              out1q, out0q
-    mov               tmpd, lend
-
-.inner_loop16:
-    movaps              m0, [inq]
-    movaps              m1, [inq+lenq]
-    movaps              m2, [inq+lenq*2]
-    movaps              m3, [inq+3*32*2*4]
-    TRANSPOSE4x4PS 0, 1, 2, 3, 4
-    movaps         [out0q], m0
-    movaps         [out1q], m1
-    movaps    [out0q+lenq], m2
-    movaps    [out1q+lenq], m3
-    lea              out0q, [out0q+lenq*2]
-    lea              out1q, [out1q+lenq*2]
-    add                inq, mmsize
-    sub               tmpd, mmsize
-    jg .inner_loop16
-    add               outq, 16
-    add                inq, 3*32*2*4
-    sub                 id, 4
-    jg .loop16
-    RET
-
-align 16
-.loop8:
-    mov              out0q, outq
-    mov              out1q, 38*64*4
-    add              out1q, out0q
-    mov               tmpd, lend
-
-.inner_loop8:
-    movaps              m0, [inq]
-    movaps              m1, [inq+lenq]
-    SBUTTERFLYPS 0, 1, 2
-    SBUTTERFLYPD 0, 1, 2
-    MOVH           [out0q], m0
-    MOVH           [out1q], m1
-    movhps    [out0q+lenq], m0
-    movhps    [out1q+lenq], m1
-    lea              out0q, [out0q+lenq*2]
-    lea              out1q, [out1q+lenq*2]
-    add                inq, mmsize
-    sub               tmpd, mmsize
-    jg .inner_loop8
-    add               outq, 8
-    add                inq, lenq
-    sub                 id, 2
-    jg .loop16
-    RET
-
-align 16
-.loop4:
-    mov              out0q, outq
-    mov              out1q, 38*64*4
-    add              out1q, out0q
-    mov               tmpd, lend
-
-.inner_loop4:
-    movaps              m0, [inq]
-    movss          [out0q], m0
-%if cpuflag(sse4)
-    extractps      [out1q], m0, 1
-    extractps [out0q+lenq], m0, 2
-    extractps [out1q+lenq], m0, 3
-%else
-    movhlps             m1, m0
-    movss     [out0q+lenq], m1
-    shufps              m0, m0, 0xb1
-    movss          [out1q], m0
-    movhlps             m1, m0
-    movss     [out1q+lenq], m1
-%endif
-    lea              out0q, [out0q+lenq*2]
-    lea              out1q, [out1q+lenq*2]
-    add                inq, mmsize
-    sub               tmpd, mmsize
-    jg .inner_loop4
-    add               outq, 4
-    sub                 id, 1
-    test                id, 2
-    jne .loop8
-    cmp                 id, 4
-    jge .loop16
-    RET
-%endmacro
-
-INIT_XMM sse
-HYBRID_SYNTHESIS_DEINT
-INIT_XMM sse4
-HYBRID_SYNTHESIS_DEINT
-
-;*******************************************************************
-;void ff_ps_hybrid_analysis_<opt>(float (*out)[2], float (*in)[2],
-;                                 const float (*filter)[8][2],
-;                                 ptrdiff_t stride, int n);
-;*******************************************************************
-%macro PS_HYBRID_ANALYSIS_LOOP 3
-    movu     %1, [inq+mmsize*%3]
-    movu     m1, [inq+mmsize*(5-%3)+8]
-%if cpuflag(sse3)
-    pshufd   %2, %1, q2301
-    pshufd   m4, m1, q0123
-    pshufd   m1, m1, q1032
-    pshufd   m2, [filterq+nq+mmsize*%3], q2301
-    addsubps %2, m4
-    addsubps %1, m1
-%else
-    mova     m2, [filterq+nq+mmsize*%3]
-    mova     %2, %1
-    mova     m4, m1
-    shufps   %2, %2, q2301
-    shufps   m4, m4, q0123
-    shufps   m1, m1, q1032
-    shufps   m2, m2, q2301
-    xorps    m4, m7
-    xorps    m1, m7
-    subps    %2, m4
-    subps    %1, m1
-%endif
-    mulps    %2, m2
-    mulps    %1, m2
-%if %3
-    addps    m3, %2
-    addps    m0, %1
-%endif
-%endmacro
-
-%macro PS_HYBRID_ANALYSIS 0
-cglobal ps_hybrid_analysis, 5, 5, 8, out, in, filter, stride, n
-%if cpuflag(sse3)
-%define MOVH movsd
-%else
-%define MOVH movlps
-%endif
-    shl strideq, 3
-    shl nd, 6
-    add filterq, nq
-    neg nq
-    mova m7, [ps_p1m1p1m1]
-
-align 16
-.loop:
-    PS_HYBRID_ANALYSIS_LOOP m0, m3, 0
-    PS_HYBRID_ANALYSIS_LOOP m5, m6, 1
-    PS_HYBRID_ANALYSIS_LOOP m5, m6, 2
-
-%if cpuflag(sse3)
-    pshufd   m3, m3, q2301
-    xorps    m0, m7
-    hsubps   m3, m0
-    pshufd   m1, m3, q0020
-    pshufd   m3, m3, q0031
-    addps    m1, m3
-    movsd    m2, [inq+6*8]
-%else
-    mova     m1, m3
-    mova     m2, m0
-    shufps   m1, m1, q2301
-    shufps   m2, m2, q2301
-    subps    m1, m3
-    addps    m2, m0
-    unpcklps m3, m1, m2
-    unpckhps m1, m2
-    addps    m1, m3
-    movu     m2, [inq+6*8] ; faster than movlps and no risk of overread
-%endif
-    movss    m3, [filterq+nq+8*6]
-    SPLATD   m3
-    mulps    m2, m3
-    addps    m1, m2
-    MOVH [outq], m1
-    add    outq, strideq
-    add      nq, 64
-    jl .loop
-    REP_RET
-%endmacro
-
-INIT_XMM sse
-PS_HYBRID_ANALYSIS
-INIT_XMM sse3
-PS_HYBRID_ANALYSIS
diff -uparN ffmpeg-4.1/libavcodec/x86/ac3dsp.asm ffmpeg-y/libavcodec/x86/ac3dsp.asm
--- ffmpeg-4.1/libavcodec/x86/ac3dsp.asm	2018-01-01 06:35:48.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/ac3dsp.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,552 +0,0 @@
-;*****************************************************************************
-;* x86-optimized AC-3 DSP functions
-;* Copyright (c) 2011 Justin Ruggles
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-
-; 16777216.0f - used in ff_float_to_fixed24()
-pf_1_24: times 4 dd 0x4B800000
-
-; used in ff_ac3_compute_mantissa_size()
-cextern ac3_bap_bits
-pw_bap_mul1: dw 21846, 21846, 0, 32768, 21846, 21846, 0, 32768
-pw_bap_mul2: dw 5, 7, 0, 7, 5, 7, 0, 7
-
-; used in ff_ac3_extract_exponents()
-cextern pd_1
-pd_151: times 4 dd 151
-
-; used in ff_apply_window_int16()
-pb_revwords: SHUFFLE_MASK_W 7, 6, 5, 4, 3, 2, 1, 0
-pd_16384: times 4 dd 16384
-
-SECTION .text
-
-;-----------------------------------------------------------------------------
-; void ff_ac3_exponent_min(uint8_t *exp, int num_reuse_blocks, int nb_coefs)
-;-----------------------------------------------------------------------------
-
-%macro AC3_EXPONENT_MIN 0
-cglobal ac3_exponent_min, 3, 4, 2, exp, reuse_blks, expn, offset
-    shl  reuse_blksq, 8
-    jz .end
-    LOOP_ALIGN
-.nextexp:
-    mov      offsetq, reuse_blksq
-    mova          m0, [expq+offsetq]
-    sub      offsetq, 256
-    LOOP_ALIGN
-.nextblk:
-    PMINUB        m0, [expq+offsetq], m1
-    sub      offsetq, 256
-    jae .nextblk
-    mova      [expq], m0
-    add         expq, mmsize
-    sub        expnq, mmsize
-    jg .nextexp
-.end:
-    REP_RET
-%endmacro
-
-%define LOOP_ALIGN
-INIT_MMX mmx
-AC3_EXPONENT_MIN
-%if HAVE_MMXEXT_EXTERNAL
-%define LOOP_ALIGN ALIGN 16
-INIT_MMX mmxext
-AC3_EXPONENT_MIN
-%endif
-%if HAVE_SSE2_EXTERNAL
-INIT_XMM sse2
-AC3_EXPONENT_MIN
-%endif
-%undef LOOP_ALIGN
-
-;-----------------------------------------------------------------------------
-; int ff_ac3_max_msb_abs_int16(const int16_t *src, int len)
-;
-; This function uses 2 different methods to calculate a valid result.
-; 1) logical 'or' of abs of each element
-;        This is used for ssse3 because of the pabsw instruction.
-;        It is also used for mmx because of the lack of min/max instructions.
-; 2) calculate min/max for the array, then or(abs(min),abs(max))
-;        This is used for mmxext and sse2 because they have pminsw/pmaxsw.
-;-----------------------------------------------------------------------------
-
-; logical 'or' of 4 or 8 words in an mmx or xmm register into the low word
-%macro OR_WORDS_HORIZ 2 ; src, tmp
-%if cpuflag(sse2)
-    movhlps     %2, %1
-    por         %1, %2
-    pshuflw     %2, %1, q0032
-    por         %1, %2
-    pshuflw     %2, %1, q0001
-    por         %1, %2
-%elif cpuflag(mmxext)
-    pshufw      %2, %1, q0032
-    por         %1, %2
-    pshufw      %2, %1, q0001
-    por         %1, %2
-%else ; mmx
-    movq        %2, %1
-    psrlq       %2, 32
-    por         %1, %2
-    movq        %2, %1
-    psrlq       %2, 16
-    por         %1, %2
-%endif
-%endmacro
-
-%macro AC3_MAX_MSB_ABS_INT16 1
-cglobal ac3_max_msb_abs_int16, 2,2,5, src, len
-    pxor        m2, m2
-    pxor        m3, m3
-.loop:
-%ifidn %1, min_max
-    mova        m0, [srcq]
-    mova        m1, [srcq+mmsize]
-    pminsw      m2, m0
-    pminsw      m2, m1
-    pmaxsw      m3, m0
-    pmaxsw      m3, m1
-%else ; or_abs
-%if notcpuflag(ssse3)
-    mova        m0, [srcq]
-    mova        m1, [srcq+mmsize]
-    ABS2        m0, m1, m3, m4
-%else ; ssse3
-    ; using memory args is faster for ssse3
-    pabsw       m0, [srcq]
-    pabsw       m1, [srcq+mmsize]
-%endif
-    por         m2, m0
-    por         m2, m1
-%endif
-    add       srcq, mmsize*2
-    sub       lend, mmsize
-    ja .loop
-%ifidn %1, min_max
-    ABS2        m2, m3, m0, m1
-    por         m2, m3
-%endif
-    OR_WORDS_HORIZ m2, m0
-    movd       eax, m2
-    and        eax, 0xFFFF
-    RET
-%endmacro
-
-INIT_MMX mmx
-AC3_MAX_MSB_ABS_INT16 or_abs
-INIT_MMX mmxext
-AC3_MAX_MSB_ABS_INT16 min_max
-INIT_XMM sse2
-AC3_MAX_MSB_ABS_INT16 min_max
-INIT_XMM ssse3
-AC3_MAX_MSB_ABS_INT16 or_abs
-
-;-----------------------------------------------------------------------------
-; macro used for ff_ac3_lshift_int16() and ff_ac3_rshift_int32()
-;-----------------------------------------------------------------------------
-
-%macro AC3_SHIFT 3 ; l/r, 16/32, shift instruction, instruction set
-cglobal ac3_%1shift_int%2, 3, 3, 5, src, len, shift
-    movd      m0, shiftd
-.loop:
-    mova      m1, [srcq         ]
-    mova      m2, [srcq+mmsize  ]
-    mova      m3, [srcq+mmsize*2]
-    mova      m4, [srcq+mmsize*3]
-    %3        m1, m0
-    %3        m2, m0
-    %3        m3, m0
-    %3        m4, m0
-    mova  [srcq         ], m1
-    mova  [srcq+mmsize  ], m2
-    mova  [srcq+mmsize*2], m3
-    mova  [srcq+mmsize*3], m4
-    add     srcq, mmsize*4
-    sub     lend, mmsize*32/%2
-    ja .loop
-.end:
-    REP_RET
-%endmacro
-
-;-----------------------------------------------------------------------------
-; void ff_ac3_lshift_int16(int16_t *src, unsigned int len, unsigned int shift)
-;-----------------------------------------------------------------------------
-
-INIT_MMX mmx
-AC3_SHIFT l, 16, psllw
-INIT_XMM sse2
-AC3_SHIFT l, 16, psllw
-
-;-----------------------------------------------------------------------------
-; void ff_ac3_rshift_int32(int32_t *src, unsigned int len, unsigned int shift)
-;-----------------------------------------------------------------------------
-
-INIT_MMX mmx
-AC3_SHIFT r, 32, psrad
-INIT_XMM sse2
-AC3_SHIFT r, 32, psrad
-
-;-----------------------------------------------------------------------------
-; void ff_float_to_fixed24(int32_t *dst, const float *src, unsigned int len)
-;-----------------------------------------------------------------------------
-
-; The 3DNow! version is not bit-identical because pf2id uses truncation rather
-; than round-to-nearest.
-INIT_MMX 3dnow
-cglobal float_to_fixed24, 3, 3, 0, dst, src, len
-    movq   m0, [pf_1_24]
-.loop:
-    movq   m1, [srcq   ]
-    movq   m2, [srcq+8 ]
-    movq   m3, [srcq+16]
-    movq   m4, [srcq+24]
-    pfmul  m1, m0
-    pfmul  m2, m0
-    pfmul  m3, m0
-    pfmul  m4, m0
-    pf2id  m1, m1
-    pf2id  m2, m2
-    pf2id  m3, m3
-    pf2id  m4, m4
-    movq  [dstq   ], m1
-    movq  [dstq+8 ], m2
-    movq  [dstq+16], m3
-    movq  [dstq+24], m4
-    add  srcq, 32
-    add  dstq, 32
-    sub  lend, 8
-    ja .loop
-    femms
-    RET
-
-INIT_XMM sse
-cglobal float_to_fixed24, 3, 3, 3, dst, src, len
-    movaps     m0, [pf_1_24]
-.loop:
-    movaps     m1, [srcq   ]
-    movaps     m2, [srcq+16]
-    mulps      m1, m0
-    mulps      m2, m0
-    cvtps2pi  mm0, m1
-    movhlps    m1, m1
-    cvtps2pi  mm1, m1
-    cvtps2pi  mm2, m2
-    movhlps    m2, m2
-    cvtps2pi  mm3, m2
-    movq  [dstq   ], mm0
-    movq  [dstq+ 8], mm1
-    movq  [dstq+16], mm2
-    movq  [dstq+24], mm3
-    add      srcq, 32
-    add      dstq, 32
-    sub      lend, 8
-    ja .loop
-    emms
-    RET
-
-INIT_XMM sse2
-cglobal float_to_fixed24, 3, 3, 9, dst, src, len
-    movaps     m0, [pf_1_24]
-.loop:
-    movaps     m1, [srcq    ]
-    movaps     m2, [srcq+16 ]
-    movaps     m3, [srcq+32 ]
-    movaps     m4, [srcq+48 ]
-%ifdef m8
-    movaps     m5, [srcq+64 ]
-    movaps     m6, [srcq+80 ]
-    movaps     m7, [srcq+96 ]
-    movaps     m8, [srcq+112]
-%endif
-    mulps      m1, m0
-    mulps      m2, m0
-    mulps      m3, m0
-    mulps      m4, m0
-%ifdef m8
-    mulps      m5, m0
-    mulps      m6, m0
-    mulps      m7, m0
-    mulps      m8, m0
-%endif
-    cvtps2dq   m1, m1
-    cvtps2dq   m2, m2
-    cvtps2dq   m3, m3
-    cvtps2dq   m4, m4
-%ifdef m8
-    cvtps2dq   m5, m5
-    cvtps2dq   m6, m6
-    cvtps2dq   m7, m7
-    cvtps2dq   m8, m8
-%endif
-    movdqa  [dstq    ], m1
-    movdqa  [dstq+16 ], m2
-    movdqa  [dstq+32 ], m3
-    movdqa  [dstq+48 ], m4
-%ifdef m8
-    movdqa  [dstq+64 ], m5
-    movdqa  [dstq+80 ], m6
-    movdqa  [dstq+96 ], m7
-    movdqa  [dstq+112], m8
-    add      srcq, 128
-    add      dstq, 128
-    sub      lenq, 32
-%else
-    add      srcq, 64
-    add      dstq, 64
-    sub      lenq, 16
-%endif
-    ja .loop
-    REP_RET
-
-;------------------------------------------------------------------------------
-; int ff_ac3_compute_mantissa_size(uint16_t mant_cnt[6][16])
-;------------------------------------------------------------------------------
-
-%macro PHADDD4 2 ; xmm src, xmm tmp
-    movhlps  %2, %1
-    paddd    %1, %2
-    pshufd   %2, %1, 0x1
-    paddd    %1, %2
-%endmacro
-
-INIT_XMM sse2
-cglobal ac3_compute_mantissa_size, 1, 2, 4, mant_cnt, sum
-    movdqa      m0, [mant_cntq      ]
-    movdqa      m1, [mant_cntq+ 1*16]
-    paddw       m0, [mant_cntq+ 2*16]
-    paddw       m1, [mant_cntq+ 3*16]
-    paddw       m0, [mant_cntq+ 4*16]
-    paddw       m1, [mant_cntq+ 5*16]
-    paddw       m0, [mant_cntq+ 6*16]
-    paddw       m1, [mant_cntq+ 7*16]
-    paddw       m0, [mant_cntq+ 8*16]
-    paddw       m1, [mant_cntq+ 9*16]
-    paddw       m0, [mant_cntq+10*16]
-    paddw       m1, [mant_cntq+11*16]
-    pmaddwd     m0, [ac3_bap_bits   ]
-    pmaddwd     m1, [ac3_bap_bits+16]
-    paddd       m0, m1
-    PHADDD4     m0, m1
-    movd      sumd, m0
-    movdqa      m3, [pw_bap_mul1]
-    movhpd      m0, [mant_cntq     +2]
-    movlpd      m0, [mant_cntq+1*32+2]
-    movhpd      m1, [mant_cntq+2*32+2]
-    movlpd      m1, [mant_cntq+3*32+2]
-    movhpd      m2, [mant_cntq+4*32+2]
-    movlpd      m2, [mant_cntq+5*32+2]
-    pmulhuw     m0, m3
-    pmulhuw     m1, m3
-    pmulhuw     m2, m3
-    paddusw     m0, m1
-    paddusw     m0, m2
-    pmaddwd     m0, [pw_bap_mul2]
-    PHADDD4     m0, m1
-    movd       eax, m0
-    add        eax, sumd
-    RET
-
-;------------------------------------------------------------------------------
-; void ff_ac3_extract_exponents(uint8_t *exp, int32_t *coef, int nb_coefs)
-;------------------------------------------------------------------------------
-
-%macro PABSD 1-2 ; src/dst, unused
-%if cpuflag(ssse3)
-    pabsd    %1, %1
-%else ; src/dst, tmp
-    pxor     %2, %2
-    pcmpgtd  %2, %1
-    pxor     %1, %2
-    psubd    %1, %2
-%endif
-%endmacro
-
-%macro AC3_EXTRACT_EXPONENTS 0
-cglobal ac3_extract_exponents, 3, 3, 4, exp, coef, len
-    add     expq, lenq
-    lea    coefq, [coefq+4*lenq]
-    neg     lenq
-    mova      m2, [pd_1]
-    mova      m3, [pd_151]
-.loop:
-    ; move 4 32-bit coefs to xmm0
-    mova      m0, [coefq+4*lenq]
-    ; absolute value
-    PABSD     m0, m1
-    ; convert to float and extract exponents
-    pslld     m0, 1
-    por       m0, m2
-    cvtdq2ps  m1, m0
-    psrld     m1, 23
-    mova      m0, m3
-    psubd     m0, m1
-    ; move the lowest byte in each of 4 dwords to the low dword
-    ; NOTE: We cannot just extract the low bytes with pshufb because the dword
-    ;       result for 16777215 is -1 due to float inaccuracy. Using packuswb
-    ;       clips this to 0, which is the correct exponent.
-    packssdw  m0, m0
-    packuswb  m0, m0
-    movd  [expq+lenq], m0
-
-    add     lenq, 4
-    jl .loop
-    REP_RET
-%endmacro
-
-%if HAVE_SSE2_EXTERNAL
-INIT_XMM sse2
-AC3_EXTRACT_EXPONENTS
-%endif
-%if HAVE_SSSE3_EXTERNAL
-INIT_XMM ssse3
-AC3_EXTRACT_EXPONENTS
-%endif
-
-;-----------------------------------------------------------------------------
-; void ff_apply_window_int16(int16_t *output, const int16_t *input,
-;                            const int16_t *window, unsigned int len)
-;-----------------------------------------------------------------------------
-
-%macro REVERSE_WORDS 1-2
-%if cpuflag(ssse3) && notcpuflag(atom)
-    pshufb  %1, %2
-%elif cpuflag(sse2)
-    pshuflw  %1, %1, 0x1B
-    pshufhw  %1, %1, 0x1B
-    pshufd   %1, %1, 0x4E
-%elif cpuflag(mmxext)
-    pshufw   %1, %1, 0x1B
-%endif
-%endmacro
-
-%macro MUL16FIXED 3
-%if cpuflag(ssse3) ; dst, src, unused
-; dst = ((dst * src) + (1<<14)) >> 15
-    pmulhrsw   %1, %2
-%elif cpuflag(mmxext) ; dst, src, temp
-; dst = (dst * src) >> 15
-; pmulhw cuts off the bottom bit, so we have to lshift by 1 and add it back
-; in from the pmullw result.
-    mova    %3, %1
-    pmulhw  %1, %2
-    pmullw  %3, %2
-    psrlw   %3, 15
-    psllw   %1, 1
-    por     %1, %3
-%endif
-%endmacro
-
-%macro APPLY_WINDOW_INT16 1 ; %1 bitexact version
-%if %1
-cglobal apply_window_int16, 4,5,6, output, input, window, offset, offset2
-%else
-cglobal apply_window_int16_round, 4,5,6, output, input, window, offset, offset2
-%endif
-    lea     offset2q, [offsetq-mmsize]
-%if cpuflag(ssse3) && notcpuflag(atom)
-    mova          m5, [pb_revwords]
-    ALIGN 16
-%elif %1
-    mova          m5, [pd_16384]
-%endif
-.loop:
-%if cpuflag(ssse3)
-    ; This version does the 16x16->16 multiplication in-place without expanding
-    ; to 32-bit. The ssse3 version is bit-identical.
-    mova          m0, [windowq+offset2q]
-    mova          m1, [ inputq+offset2q]
-    pmulhrsw      m1, m0
-    REVERSE_WORDS m0, m5
-    pmulhrsw      m0, [ inputq+offsetq ]
-    mova  [outputq+offset2q], m1
-    mova  [outputq+offsetq ], m0
-%elif %1
-    ; This version expands 16-bit to 32-bit, multiplies by the window,
-    ; adds 16384 for rounding, right shifts 15, then repacks back to words to
-    ; save to the output. The window is reversed for the second half.
-    mova          m3, [windowq+offset2q]
-    mova          m4, [ inputq+offset2q]
-    pxor          m0, m0
-    punpcklwd     m0, m3
-    punpcklwd     m1, m4
-    pmaddwd       m0, m1
-    paddd         m0, m5
-    psrad         m0, 15
-    pxor          m2, m2
-    punpckhwd     m2, m3
-    punpckhwd     m1, m4
-    pmaddwd       m2, m1
-    paddd         m2, m5
-    psrad         m2, 15
-    packssdw      m0, m2
-    mova  [outputq+offset2q], m0
-    REVERSE_WORDS m3
-    mova          m4, [ inputq+offsetq]
-    pxor          m0, m0
-    punpcklwd     m0, m3
-    punpcklwd     m1, m4
-    pmaddwd       m0, m1
-    paddd         m0, m5
-    psrad         m0, 15
-    pxor          m2, m2
-    punpckhwd     m2, m3
-    punpckhwd     m1, m4
-    pmaddwd       m2, m1
-    paddd         m2, m5
-    psrad         m2, 15
-    packssdw      m0, m2
-    mova  [outputq+offsetq], m0
-%else
-    ; This version does the 16x16->16 multiplication in-place without expanding
-    ; to 32-bit. The mmxext and sse2 versions do not use rounding, and
-    ; therefore are not bit-identical to the C version.
-    mova          m0, [windowq+offset2q]
-    mova          m1, [ inputq+offset2q]
-    mova          m2, [ inputq+offsetq ]
-    MUL16FIXED    m1, m0, m3
-    REVERSE_WORDS m0
-    MUL16FIXED    m2, m0, m3
-    mova  [outputq+offset2q], m1
-    mova  [outputq+offsetq ], m2
-%endif
-    add      offsetd, mmsize
-    sub     offset2d, mmsize
-    jae .loop
-    REP_RET
-%endmacro
-
-INIT_MMX mmxext
-APPLY_WINDOW_INT16 0
-INIT_XMM sse2
-APPLY_WINDOW_INT16 0
-
-INIT_MMX mmxext
-APPLY_WINDOW_INT16 1
-INIT_XMM sse2
-APPLY_WINDOW_INT16 1
-INIT_XMM ssse3
-APPLY_WINDOW_INT16 1
-INIT_XMM ssse3, atom
-APPLY_WINDOW_INT16 1
diff -uparN ffmpeg-4.1/libavcodec/x86/ac3dsp_downmix.asm ffmpeg-y/libavcodec/x86/ac3dsp_downmix.asm
--- ffmpeg-4.1/libavcodec/x86/ac3dsp_downmix.asm	2018-11-02 02:34:25.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/ac3dsp_downmix.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,187 +0,0 @@
-;*****************************************************************************
-;* x86-optimized AC-3 downmixing
-;* Copyright (c) 2012 Justin Ruggles
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-;******************************************************************************
-;* This is based on the channel mixing asm in libavresample, but it is
-;* simplified for only float coefficients and only 3 to 6 channels.
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION .text
-
-;-----------------------------------------------------------------------------
-; functions to downmix from 3 to 6 channels to mono or stereo
-; void ff_ac3_downmix_*(float **samples, float **matrix, int len);
-;-----------------------------------------------------------------------------
-
-%macro AC3_DOWNMIX 2 ; %1 = in channels, %2 = out channels
-; define some names to make the code clearer
-%assign  in_channels %1
-%assign out_channels %2
-%assign stereo out_channels - 1
-
-; determine how many matrix elements must go on the stack vs. mmregs
-%assign matrix_elements in_channels * out_channels
-%if stereo
-    %assign needed_mmregs 4
-%else
-    %assign needed_mmregs 3
-%endif
-%assign matrix_elements_mm num_mmregs - needed_mmregs
-%if matrix_elements < matrix_elements_mm
-    %assign matrix_elements_mm matrix_elements
-%endif
-%assign total_mmregs needed_mmregs+matrix_elements_mm
-%if matrix_elements_mm < matrix_elements
-    %assign matrix_elements_stack matrix_elements - matrix_elements_mm
-%else
-    %assign matrix_elements_stack 0
-%endif
-
-cglobal ac3_downmix_%1_to_%2, 3,in_channels+1,total_mmregs,0-matrix_elements_stack*mmsize, src0, src1, len, src2, src3, src4, src5
-
-; load matrix pointers
-%define matrix0q r1q
-%define matrix1q r3q
-%if stereo
-    mov      matrix1q, [matrix0q+gprsize]
-%endif
-    mov      matrix0q, [matrix0q]
-
-; define matrix coeff names
-%assign %%i 0
-%assign %%j needed_mmregs
-%rep in_channels
-    %if %%i >= matrix_elements_mm
-        CAT_XDEFINE mx_stack_0_, %%i, 1
-        CAT_XDEFINE mx_0_, %%i, [rsp+(%%i-matrix_elements_mm)*mmsize]
-    %else
-        CAT_XDEFINE mx_stack_0_, %%i, 0
-        CAT_XDEFINE mx_0_, %%i, m %+ %%j
-        %assign %%j %%j+1
-    %endif
-    %assign %%i %%i+1
-%endrep
-%if stereo
-%assign %%i 0
-%rep in_channels
-    %if in_channels + %%i >= matrix_elements_mm
-        CAT_XDEFINE mx_stack_1_, %%i, 1
-        CAT_XDEFINE mx_1_, %%i, [rsp+(in_channels+%%i-matrix_elements_mm)*mmsize]
-    %else
-        CAT_XDEFINE mx_stack_1_, %%i, 0
-        CAT_XDEFINE mx_1_, %%i, m %+ %%j
-        %assign %%j %%j+1
-    %endif
-    %assign %%i %%i+1
-%endrep
-%endif
-
-; load/splat matrix coeffs
-%assign %%i 0
-%rep in_channels
-    %if mx_stack_0_ %+ %%i
-        VBROADCASTSS m0, [matrix0q+4*%%i]
-        mova  mx_0_ %+ %%i, m0
-    %else
-        VBROADCASTSS mx_0_ %+ %%i, [matrix0q+4*%%i]
-    %endif
-    %if stereo
-    %if mx_stack_1_ %+ %%i
-        VBROADCASTSS m0, [matrix1q+4*%%i]
-        mova  mx_1_ %+ %%i, m0
-    %else
-        VBROADCASTSS mx_1_ %+ %%i, [matrix1q+4*%%i]
-    %endif
-    %endif
-    %assign %%i %%i+1
-%endrep
-
-    lea          lenq, [4*r2d]
-    ; load channel pointers to registers
-%assign %%i 1
-%rep (in_channels - 1)
-    mov         src %+ %%i %+ q, [src0q+%%i*gprsize]
-    add         src %+ %%i %+ q, lenq
-    %assign %%i %%i+1
-%endrep
-    mov         src0q, [src0q]
-    add         src0q, lenq
-    neg          lenq
-.loop:
-    %if stereo || mx_stack_0_0
-    mova           m0, [src0q+lenq]
-    %endif
-    %if stereo
-    mulps          m1, m0, mx_1_0
-    %endif
-    %if stereo || mx_stack_0_0
-    mulps          m0, m0, mx_0_0
-    %else
-    mulps          m0, mx_0_0, [src0q+lenq]
-    %endif
-%assign %%i 1
-%rep (in_channels - 1)
-    %define src_ptr src %+ %%i %+ q
-    ; avoid extra load for mono if matrix is in a mm register
-    %if stereo || mx_stack_0_ %+ %%i
-    mova           m2, [src_ptr+lenq]
-    %endif
-    %if stereo
-    FMULADD_PS     m1, m2, mx_1_ %+ %%i, m1, m3
-    %endif
-    %if stereo || mx_stack_0_ %+ %%i
-    FMULADD_PS     m0, m2, mx_0_ %+ %%i, m0, m2
-    %else
-    FMULADD_PS     m0, mx_0_ %+ %%i, [src_ptr+lenq], m0, m1
-    %endif
-    %assign %%i %%i+1
-%endrep
-    mova [src0q+lenq], m0
-    %if stereo
-    mova [src1q+lenq], m1
-    %endif
-
-    add          lenq, mmsize
-    jl .loop
-    RET
-%endmacro
-
-%macro AC3_DOWNMIX_FUNCS 0
-%assign %%i 3
-%rep 4
-    INIT_XMM sse
-    AC3_DOWNMIX %%i, 1
-    AC3_DOWNMIX %%i, 2
-    INIT_YMM avx
-    AC3_DOWNMIX %%i, 1
-    AC3_DOWNMIX %%i, 2
-    %if HAVE_FMA3_EXTERNAL
-    INIT_YMM fma3
-    AC3_DOWNMIX %%i, 1
-    AC3_DOWNMIX %%i, 2
-    %endif
-    %assign %%i %%i+1
-%endrep
-%endmacro
-
-AC3_DOWNMIX_FUNCS
diff -uparN ffmpeg-4.1/libavcodec/x86/alacdsp.asm ffmpeg-y/libavcodec/x86/alacdsp.asm
--- ffmpeg-4.1/libavcodec/x86/alacdsp.asm	2018-07-17 17:27:41.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/alacdsp.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,133 +0,0 @@
-;******************************************************************************
-;* ALAC DSP SIMD optimizations
-;*
-;* Copyright (C) 2015 James Almer
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION .text
-
-INIT_XMM sse4
-%if ARCH_X86_64
-cglobal alac_decorrelate_stereo, 2, 5, 8, buf0, len, shift, weight, buf1
-%else
-cglobal alac_decorrelate_stereo, 2, 3, 8, buf0, len, shift, weight
-%define  buf1q  r2q
-%endif
-    movd    m6, shiftm
-    movd    m7, weightm
-    SPLATD  m7
-    shl   lend, 2
-    mov  buf1q, [buf0q + gprsize]
-    mov  buf0q, [buf0q]
-    add  buf1q, lenq
-    add  buf0q, lenq
-    neg  lenq
-
-align 16
-.loop:
-    mova    m0, [buf0q + lenq]
-    mova    m1, [buf0q + lenq + mmsize]
-    mova    m2, [buf1q + lenq]
-    mova    m3, [buf1q + lenq + mmsize]
-    pmulld  m4, m2, m7
-    pmulld  m5, m3, m7
-    psrad   m4, m6
-    psrad   m5, m6
-    psubd   m0, m4
-    psubd   m1, m5
-    paddd   m2, m0
-    paddd   m3, m1
-    mova [buf1q + lenq], m0
-    mova [buf1q + lenq + mmsize], m1
-    mova [buf0q + lenq], m2
-    mova [buf0q + lenq + mmsize], m3
-
-    add   lenq, mmsize*2
-    jl .loop
-    RET
-
-INIT_XMM sse2
-cglobal alac_append_extra_bits_stereo, 2, 5, 5, buf0, exbuf0, buf1, exbuf1, len
-    movifnidn lend, lenm
-    movd      m4, r2m ; exbits
-    shl     lend, 2
-    mov    buf1q, [buf0q + gprsize]
-    mov    buf0q, [buf0q]
-    mov  exbuf1q, [exbuf0q + gprsize]
-    mov  exbuf0q, [exbuf0q]
-    add    buf1q, lenq
-    add    buf0q, lenq
-    add  exbuf1q, lenq
-    add  exbuf0q, lenq
-    neg lenq
-
-align 16
-.loop:
-    mova      m0, [buf0q + lenq]
-    mova      m1, [buf0q + lenq + mmsize]
-    pslld     m0, m4
-    pslld     m1, m4
-    mova      m2, [buf1q + lenq]
-    mova      m3, [buf1q + lenq + mmsize]
-    pslld     m2, m4
-    pslld     m3, m4
-    por       m0, [exbuf0q + lenq]
-    por       m1, [exbuf0q + lenq + mmsize]
-    por       m2, [exbuf1q + lenq]
-    por       m3, [exbuf1q + lenq + mmsize]
-    mova [buf0q + lenq         ], m0
-    mova [buf0q + lenq + mmsize], m1
-    mova [buf1q + lenq         ], m2
-    mova [buf1q + lenq + mmsize], m3
-
-    add     lenq, mmsize*2
-    jl .loop
-    REP_RET
-
-%if ARCH_X86_64
-cglobal alac_append_extra_bits_mono, 2, 5, 3, buf, exbuf, exbits, ch, len
-%else
-cglobal alac_append_extra_bits_mono, 2, 3, 3, buf, exbuf, len
-%define exbitsm r2m
-%endif
-    movifnidn lend, r4m
-    movd     m2, exbitsm
-    shl    lend, 2
-    mov    bufq, [bufq]
-    mov  exbufq, [exbufq]
-    add    bufq, lenq
-    add  exbufq, lenq
-    neg lenq
-
-align 16
-.loop:
-    mova      m0, [bufq + lenq]
-    mova      m1, [bufq + lenq + mmsize]
-    pslld     m0, m2
-    pslld     m1, m2
-    por       m0, [exbufq + lenq]
-    por       m1, [exbufq + lenq + mmsize]
-    mova [bufq + lenq], m0
-    mova [bufq + lenq + mmsize], m1
-
-    add     lenq, mmsize*2
-    jl .loop
-    REP_RET
diff -uparN ffmpeg-4.1/libavcodec/x86/audiodsp.asm ffmpeg-y/libavcodec/x86/audiodsp.asm
--- ffmpeg-4.1/libavcodec/x86/audiodsp.asm	2018-11-03 08:17:29.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/audiodsp.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,174 +0,0 @@
-;******************************************************************************
-;* optimized audio functions
-;* Copyright (c) 2008 Loren Merritt
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION .text
-
-%macro SCALARPRODUCT 0
-; int ff_scalarproduct_int16(int16_t *v1, int16_t *v2, int order)
-cglobal scalarproduct_int16, 3,3,3, v1, v2, order
-    add orderd, orderd
-    add v1q, orderq
-    add v2q, orderq
-    neg orderq
-    pxor    m2, m2
-.loop:
-    movu    m0, [v1q + orderq]
-    movu    m1, [v1q + orderq + mmsize]
-    pmaddwd m0, [v2q + orderq]
-    pmaddwd m1, [v2q + orderq + mmsize]
-    paddd   m2, m0
-    paddd   m2, m1
-    add     orderq, mmsize*2
-    jl .loop
-    HADDD   m2, m0
-    movd   eax, m2
-%if mmsize == 8
-    emms
-%endif
-    RET
-%endmacro
-
-INIT_MMX mmxext
-SCALARPRODUCT
-INIT_XMM sse2
-SCALARPRODUCT
-
-
-;-----------------------------------------------------------------------------
-; void ff_vector_clip_int32(int32_t *dst, const int32_t *src, int32_t min,
-;                           int32_t max, unsigned int len)
-;-----------------------------------------------------------------------------
-
-; %1 = number of xmm registers used
-; %2 = number of inline load/process/store loops per asm loop
-; %3 = process 4*mmsize (%3=0) or 8*mmsize (%3=1) bytes per loop
-; %4 = CLIPD function takes min/max as float instead of int (SSE2 version)
-; %5 = suffix
-%macro VECTOR_CLIP_INT32 4-5
-cglobal vector_clip_int32%5, 5,5,%1, dst, src, min, max, len
-%if %4
-    cvtsi2ss  m4, minm
-    cvtsi2ss  m5, maxm
-%else
-    movd      m4, minm
-    movd      m5, maxm
-%endif
-    SPLATD    m4
-    SPLATD    m5
-.loop:
-%assign %%i 0
-%rep %2
-    mova      m0,  [srcq + mmsize * (0 + %%i)]
-    mova      m1,  [srcq + mmsize * (1 + %%i)]
-    mova      m2,  [srcq + mmsize * (2 + %%i)]
-    mova      m3,  [srcq + mmsize * (3 + %%i)]
-%if %3
-    mova      m7,  [srcq + mmsize * (4 + %%i)]
-    mova      m8,  [srcq + mmsize * (5 + %%i)]
-    mova      m9,  [srcq + mmsize * (6 + %%i)]
-    mova      m10, [srcq + mmsize * (7 + %%i)]
-%endif
-    CLIPD  m0,  m4, m5, m6
-    CLIPD  m1,  m4, m5, m6
-    CLIPD  m2,  m4, m5, m6
-    CLIPD  m3,  m4, m5, m6
-%if %3
-    CLIPD  m7,  m4, m5, m6
-    CLIPD  m8,  m4, m5, m6
-    CLIPD  m9,  m4, m5, m6
-    CLIPD  m10, m4, m5, m6
-%endif
-    mova  [dstq + mmsize * (0 + %%i)], m0
-    mova  [dstq + mmsize * (1 + %%i)], m1
-    mova  [dstq + mmsize * (2 + %%i)], m2
-    mova  [dstq + mmsize * (3 + %%i)], m3
-%if %3
-    mova  [dstq + mmsize * (4 + %%i)], m7
-    mova  [dstq + mmsize * (5 + %%i)], m8
-    mova  [dstq + mmsize * (6 + %%i)], m9
-    mova  [dstq + mmsize * (7 + %%i)], m10
-%endif
-%assign %%i (%%i + 4 * (1 + %3))
-%endrep
-    add     srcq, mmsize*4*(%2+%3)
-    add     dstq, mmsize*4*(%2+%3)
-    sub     lend, mmsize*(%2+%3)
-    jg .loop
-    REP_RET
-%endmacro
-
-INIT_MMX mmx
-VECTOR_CLIP_INT32 0, 1, 0, 0
-INIT_XMM sse2
-VECTOR_CLIP_INT32 6, 1, 0, 0, _int
-VECTOR_CLIP_INT32 6, 2, 0, 1
-INIT_XMM sse4
-%ifdef m8
-VECTOR_CLIP_INT32 11, 1, 1, 0
-%else
-VECTOR_CLIP_INT32 6, 1, 0, 0
-%endif
-
-; void ff_vector_clipf_sse(float *dst, const float *src,
-;                          int len, float min, float max)
-INIT_XMM sse
-cglobal vector_clipf, 3, 3, 6, dst, src, len, min, max
-%if ARCH_X86_32
-    VBROADCASTSS m0, minm
-    VBROADCASTSS m1, maxm
-%elif WIN64
-    SWAP 0, 3
-    VBROADCASTSS m0, m0
-    VBROADCASTSS m1, maxm
-%else ; 64bit sysv
-    VBROADCASTSS m0, m0
-    VBROADCASTSS m1, m1
-%endif
-
-    movsxdifnidn lenq, lend
-
-.loop:
-    mova m2, [srcq + 4 * lenq - 4 * mmsize]
-    mova m3, [srcq + 4 * lenq - 3 * mmsize]
-    mova m4, [srcq + 4 * lenq - 2 * mmsize]
-    mova m5, [srcq + 4 * lenq - 1 * mmsize]
-
-    maxps m2, m0
-    maxps m3, m0
-    maxps m4, m0
-    maxps m5, m0
-
-    minps m2, m1
-    minps m3, m1
-    minps m4, m1
-    minps m5, m1
-
-    mova [dstq + 4 * lenq - 4 * mmsize], m2
-    mova [dstq + 4 * lenq - 3 * mmsize], m3
-    mova [dstq + 4 * lenq - 2 * mmsize], m4
-    mova [dstq + 4 * lenq - 1 * mmsize], m5
-
-    sub lenq, mmsize
-    jg .loop
-
-    RET
diff -uparN ffmpeg-4.1/libavcodec/x86/blockdsp.asm ffmpeg-y/libavcodec/x86/blockdsp.asm
--- ffmpeg-4.1/libavcodec/x86/blockdsp.asm	2018-11-02 02:34:25.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/blockdsp.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,88 +0,0 @@
-;******************************************************************************
-;* SIMD-optimized clear block functions
-;* Copyright (c) 2002 Michael Niedermayer
-;* Copyright (c) 2008 Loren Merritt
-;* Copyright (c) 2009 Fiona Glaser
-;*
-;* AVX version by Jokyo Images
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION .text
-
-;----------------------------------------
-; void ff_clear_block(int16_t *blocks);
-;----------------------------------------
-; %1 = number of xmm registers used
-; %2 = number of inline store loops
-%macro CLEAR_BLOCK 2
-cglobal clear_block, 1, 1, %1, blocks
-    ZERO  m0, m0, m0
-%assign %%i 0
-%rep %2
-    mova  [blocksq+mmsize*(0+%%i)], m0
-    mova  [blocksq+mmsize*(1+%%i)], m0
-    mova  [blocksq+mmsize*(2+%%i)], m0
-    mova  [blocksq+mmsize*(3+%%i)], m0
-%assign %%i %%i+4
-%endrep
-    RET
-%endmacro
-
-INIT_MMX mmx
-%define ZERO pxor
-CLEAR_BLOCK 0, 4
-INIT_XMM sse
-%define ZERO xorps
-CLEAR_BLOCK 1, 2
-INIT_YMM avx
-CLEAR_BLOCK 1, 1
-
-;-----------------------------------------
-; void ff_clear_blocks(int16_t *blocks);
-;-----------------------------------------
-; %1 = number of xmm registers used
-%macro CLEAR_BLOCKS 1
-cglobal clear_blocks, 1, 2, %1, blocks, len
-    add   blocksq, 768
-    mov      lenq, -768
-    ZERO       m0, m0, m0
-.loop:
-    mova  [blocksq+lenq+mmsize*0], m0
-    mova  [blocksq+lenq+mmsize*1], m0
-    mova  [blocksq+lenq+mmsize*2], m0
-    mova  [blocksq+lenq+mmsize*3], m0
-    mova  [blocksq+lenq+mmsize*4], m0
-    mova  [blocksq+lenq+mmsize*5], m0
-    mova  [blocksq+lenq+mmsize*6], m0
-    mova  [blocksq+lenq+mmsize*7], m0
-    add   lenq, mmsize*8
-    js .loop
-    RET
-%endmacro
-
-INIT_MMX mmx
-%define ZERO pxor
-CLEAR_BLOCKS 0
-INIT_XMM sse
-%define ZERO xorps
-CLEAR_BLOCKS 1
-INIT_YMM avx
-CLEAR_BLOCKS 1
diff -uparN ffmpeg-4.1/libavcodec/x86/bswapdsp.asm ffmpeg-y/libavcodec/x86/bswapdsp.asm
--- ffmpeg-4.1/libavcodec/x86/bswapdsp.asm	2018-11-03 08:17:29.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/bswapdsp.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,159 +0,0 @@
-;******************************************************************************
-;* optimized bswap buffer functions
-;* Copyright (c) 2008 Loren Merritt
-;* Copyright (c) 2003-2013 Michael Niedermayer
-;* Copyright (c) 2013 Daniel Kang
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-pb_bswap32: db 3, 2, 1, 0, 7, 6, 5, 4, 11, 10, 9, 8, 15, 14, 13, 12
-
-cextern pb_80
-
-SECTION .text
-
-; %1 = aligned/unaligned
-%macro BSWAP_LOOPS  1
-    mov      r3d, r2d
-    sar      r2d, 3
-    jz       .left4_%1
-%if cpuflag(avx2)
-    sar      r2d, 1
-    jz       .left8_%1
-%endif
-.loop8_%1:
-    mov%1    m0, [r1 +  0]
-    mov%1    m1, [r1 + mmsize]
-%if cpuflag(ssse3)||cpuflag(avx2)
-    pshufb   m0, m2
-    pshufb   m1, m2
-    mov%1    [r0 +  0], m0
-    mov%1    [r0 + mmsize], m1
-%else
-    pshuflw  m0, m0, 10110001b
-    pshuflw  m1, m1, 10110001b
-    pshufhw  m0, m0, 10110001b
-    pshufhw  m1, m1, 10110001b
-    mova     m2, m0
-    mova     m3, m1
-    psllw    m0, 8
-    psllw    m1, 8
-    psrlw    m2, 8
-    psrlw    m3, 8
-    por      m2, m0
-    por      m3, m1
-    mov%1    [r0 +  0], m2
-    mov%1    [r0 + 16], m3
-%endif
-    add      r0, mmsize*2
-    add      r1, mmsize*2
-    dec      r2d
-    jnz      .loop8_%1
-%if cpuflag(avx2)
-.left8_%1:
-    mov      r2d, r3d
-    test     r3d, 8
-    jz       .left4_%1
-    mov%1    m0, [r1]
-    pshufb   m0, m2
-    mov%1    [r0 +  0], m0
-    add r1, mmsize
-    add r0, mmsize
-%endif
-.left4_%1:
-    mov      r2d, r3d
-    test     r3d, 4
-    jz       .left
-    mov%1    xm0, [r1]
-%if cpuflag(ssse3)
-    pshufb   xm0, xm2
-    mov%1    [r0], xm0
-%else
-    pshuflw  m0, m0, 10110001b
-    pshufhw  m0, m0, 10110001b
-    mova     m2, m0
-    psllw    m0, 8
-    psrlw    m2, 8
-    por      m2, m0
-    mov%1    [r0], m2
-%endif
-    add      r1, 16
-    add      r0, 16
-%endmacro
-
-; void ff_bswap_buf(uint32_t *dst, const uint32_t *src, int w);
-%macro BSWAP32_BUF 0
-%if cpuflag(ssse3)||cpuflag(avx2)
-cglobal bswap32_buf, 3,4,3
-    mov      r3, r1
-    VBROADCASTI128  m2, [pb_bswap32]
-%else
-cglobal bswap32_buf, 3,4,5
-    mov      r3, r1
-%endif
-    or       r3, r0
-    test     r3, mmsize - 1
-    jz       .start_align
-    BSWAP_LOOPS  u
-    jmp      .left
-.start_align:
-    BSWAP_LOOPS  a
-.left:
-%if cpuflag(ssse3)
-    test     r2d, 2
-    jz       .left1
-    movq     xm0, [r1]
-    pshufb   xm0, xm2
-    movq     [r0], xm0
-    add      r1, 8
-    add      r0, 8
-.left1:
-    test     r2d, 1
-    jz       .end
-    mov      r2d, [r1]
-    bswap    r2d
-    mov      [r0], r2d
-%else
-    and      r2d, 3
-    jz       .end
-.loop2:
-    mov      r3d, [r1]
-    bswap    r3d
-    mov      [r0], r3d
-    add      r1, 4
-    add      r0, 4
-    dec      r2d
-    jnz      .loop2
-%endif
-.end:
-    RET
-%endmacro
-
-INIT_XMM sse2
-BSWAP32_BUF
-
-INIT_XMM ssse3
-BSWAP32_BUF
-
-%if HAVE_AVX2_EXTERNAL
-INIT_YMM avx2
-BSWAP32_BUF
-%endif
diff -uparN ffmpeg-4.1/libavcodec/x86/cavsidct.asm ffmpeg-y/libavcodec/x86/cavsidct.asm
--- ffmpeg-4.1/libavcodec/x86/cavsidct.asm	2018-11-02 02:34:25.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/cavsidct.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,211 +0,0 @@
-; Chinese AVS video (AVS1-P2, JiZhun profile) decoder
-; Copyright (c) 2006  Stefan Gehrer <stefan.gehrer@gmx.de>
-;
-; MMX-optimized DSP functions, based on H.264 optimizations by
-; Michael Niedermayer and Loren Merritt
-; Conversion from gcc syntax to x264asm syntax with modifications
-; by Ronald S. Bultje <rsbultje@gmail.com>
-;
-; This file is part of FFmpeg.
-;
-; FFmpeg is free software; you can redistribute it and/or
-; modify it under the terms of the GNU Lesser General Public
-; License as published by the Free Software Foundation; either
-; version 2.1 of the License, or (at your option) any later version.
-;
-; FFmpeg is distributed in the hope that it will be useful,
-; but WITHOUT ANY WARRANTY; without even the implied warranty of
-; MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-; Lesser General Public License for more details.
-;
-; You should have received a copy of the GNU Lesser General Public License
-; along with FFmpeg; if not, write to the Free Software Foundation,
-; Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-
-%include "libavutil/x86/x86util.asm"
-
-cextern pw_4
-cextern pw_64
-
-SECTION .text
-
-%macro CAVS_IDCT8_1D 2-3 1 ; source, round, init_load
-%if %3 == 1
-    mova            m4, [%1+7*16]       ; m4 = src7
-    mova            m5, [%1+1*16]       ; m5 = src1
-    mova            m2, [%1+5*16]       ; m2 = src5
-    mova            m7, [%1+3*16]       ; m7 = src3
-%else
-    SWAP             1, 7
-    SWAP             4, 6
-%endif
-    mova            m0, m4
-    mova            m3, m5
-    mova            m6, m2
-    mova            m1, m7
-
-    paddw           m4, m4              ; m4 = 2*src7
-    paddw           m3, m3              ; m3 = 2*src1
-    paddw           m6, m6              ; m6 = 2*src5
-    paddw           m1, m1              ; m1 = 2*src3
-    paddw           m0, m4              ; m0 = 3*src7
-    paddw           m5, m3              ; m5 = 3*src1
-    paddw           m2, m6              ; m2 = 3*src5
-    paddw           m7, m1              ; m7 = 3*src3
-    psubw           m5, m4              ; m5 = 3*src1 - 2*src7 = a0
-    paddw           m7, m6              ; m7 = 3*src3 - 2*src5 = a1
-    psubw           m1, m2              ; m1 = 2*src3 - 3*src5 = a2
-    paddw           m3, m0              ; m3 = 2*src1 - 3*src7 = a3
-
-    mova            m4, m5
-    mova            m6, m7
-    mova            m0, m3
-    mova            m2, m1
-    SUMSUB_BA     w, 7, 5               ; m7 = a0 + a1, m5 = a0 - a1
-    paddw           m7, m3              ; m7 = a0 + a1 + a3
-    paddw           m5, m1              ; m5 = a0 - a1 + a2
-    paddw           m7, m7
-    paddw           m5, m5
-    paddw           m7, m6              ; m7 = b4
-    paddw           m5, m4              ; m5 = b5
-
-    SUMSUB_BA     w, 1, 3               ; m1 = a3 + a2, m3 = a3 - a2
-    psubw           m4, m1              ; m4 = a0 - a2 - a3
-    mova            m1, m4              ; m1 = a0 - a2 - a3
-    psubw           m3, m6              ; m3 = a3 - a2 - a1
-    paddw           m1, m1
-    paddw           m3, m3
-    psubw           m1, m2              ; m1 = b7
-    paddw           m3, m0              ; m3 = b6
-
-    mova            m2, [%1+2*16]       ; m2 = src2
-    mova            m6, [%1+6*16]       ; m6 = src6
-    mova            m4, m2
-    mova            m0, m6
-    psllw           m4, 2               ; m4 = 4*src2
-    psllw           m6, 2               ; m6 = 4*src6
-    paddw           m2, m4              ; m2 = 5*src2
-    paddw           m0, m6              ; m0 = 5*src6
-    paddw           m2, m2
-    paddw           m0, m0
-    psubw           m4, m0              ; m4 = 4*src2 - 10*src6 = a7
-    paddw           m6, m2              ; m6 = 4*src6 + 10*src2 = a6
-
-    mova            m2, [%1+0*16]       ; m2 = src0
-    mova            m0, [%1+4*16]       ; m0 = src4
-    SUMSUB_BA     w, 0, 2               ; m0 = src0 + src4, m2 = src0 - src4
-    psllw           m0, 3
-    psllw           m2, 3
-    paddw           m0, %2              ; add rounding bias
-    paddw           m2, %2              ; add rounding bias
-
-    SUMSUB_BA     w, 6, 0               ; m6 = a4 + a6, m0 = a4 - a6
-    SUMSUB_BA     w, 4, 2               ; m4 = a5 + a7, m2 = a5 - a7
-    SUMSUB_BA     w, 7, 6               ; m7 = dst0, m6 = dst7
-    SUMSUB_BA     w, 5, 4               ; m5 = dst1, m4 = dst6
-    SUMSUB_BA     w, 3, 2               ; m3 = dst2, m2 = dst5
-    SUMSUB_BA     w, 1, 0               ; m1 = dst3, m0 = dst4
-%endmacro
-
-INIT_MMX mmx
-cglobal cavs_idct8, 2, 4, 8, 8 * 16, out, in, cnt, tmp
-    mov           cntd, 2
-    mov           tmpq, rsp
-
-.loop_1:
-    CAVS_IDCT8_1D  inq, [pw_4]
-    psraw           m7, 3
-    psraw           m6, 3
-    psraw           m5, 3
-    psraw           m4, 3
-    psraw           m3, 3
-    psraw           m2, 3
-    psraw           m1, 3
-    psraw           m0, 3
-    mova        [tmpq], m7
-    TRANSPOSE4x4W    0, 2, 4, 6, 7
-    mova    [tmpq+1*8], m0
-    mova    [tmpq+3*8], m2
-    mova    [tmpq+5*8], m4
-    mova    [tmpq+7*8], m6
-    mova            m7, [tmpq]
-    TRANSPOSE4x4W    7, 5, 3, 1, 0
-    mova    [tmpq+0*8], m7
-    mova    [tmpq+2*8], m5
-    mova    [tmpq+4*8], m3
-    mova    [tmpq+6*8], m1
-
-    add            inq, mmsize
-    add           tmpq, 64
-    dec           cntd
-    jg .loop_1
-
-    mov           cntd, 2
-    mov           tmpq, rsp
-.loop_2:
-    CAVS_IDCT8_1D tmpq, [pw_64]
-    psraw           m7, 7
-    psraw           m6, 7
-    psraw           m5, 7
-    psraw           m4, 7
-    psraw           m3, 7
-    psraw           m2, 7
-    psraw           m1, 7
-    psraw           m0, 7
-
-    mova   [outq+0*16], m7
-    mova   [outq+1*16], m5
-    mova   [outq+2*16], m3
-    mova   [outq+3*16], m1
-    mova   [outq+4*16], m0
-    mova   [outq+5*16], m2
-    mova   [outq+6*16], m4
-    mova   [outq+7*16], m6
-
-    add           outq, mmsize
-    add           tmpq, mmsize
-    dec           cntd
-    jg .loop_2
-
-    RET
-
-INIT_XMM sse2
-cglobal cavs_idct8, 2, 2, 8 + ARCH_X86_64, 0 - 8 * 16, out, in
-    CAVS_IDCT8_1D  inq, [pw_4]
-    psraw           m7, 3
-    psraw           m6, 3
-    psraw           m5, 3
-    psraw           m4, 3
-    psraw           m3, 3
-    psraw           m2, 3
-    psraw           m1, 3
-    psraw           m0, 3
-%if ARCH_X86_64
-    TRANSPOSE8x8W    7, 5, 3, 1, 0, 2, 4, 6, 8
-    mova    [rsp+4*16], m0
-%else
-    mova    [rsp+0*16], m4
-    TRANSPOSE8x8W    7, 5, 3, 1, 0, 2, 4, 6, [rsp+0*16], [rsp+4*16], 1
-%endif
-    mova    [rsp+0*16], m7
-    mova    [rsp+2*16], m3
-    mova    [rsp+6*16], m4
-    CAVS_IDCT8_1D  rsp, [pw_64], 0
-    psraw           m7, 7
-    psraw           m6, 7
-    psraw           m5, 7
-    psraw           m4, 7
-    psraw           m3, 7
-    psraw           m2, 7
-    psraw           m1, 7
-    psraw           m0, 7
-
-    mova   [outq+0*16], m7
-    mova   [outq+1*16], m5
-    mova   [outq+2*16], m3
-    mova   [outq+3*16], m1
-    mova   [outq+4*16], m0
-    mova   [outq+5*16], m2
-    mova   [outq+6*16], m4
-    mova   [outq+7*16], m6
-    RET
diff -uparN ffmpeg-4.1/libavcodec/x86/dcadsp.asm ffmpeg-y/libavcodec/x86/dcadsp.asm
--- ffmpeg-4.1/libavcodec/x86/dcadsp.asm	2018-11-02 02:34:25.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/dcadsp.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,301 +0,0 @@
-;******************************************************************************
-;* SIMD-optimized functions for the DCA decoder
-;* Copyright (C) 2016 James Almer
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION .text
-
-%define sizeof_float 4
-%define FMA3_OFFSET (8 * cpuflag(fma3))
-
-%macro LFE_FIR0_FLOAT 0
-cglobal lfe_fir0_float, 4, 6, 12 + cpuflag(fma3)*4, samples, lfe, coeff, nblocks, cnt1, cnt2
-    shr nblocksd, 1
-    sub     lfeq, 7*sizeof_float
-    mov    cnt1d, 32*sizeof_float
-    mov    cnt2d, 32*sizeof_float-8-FMA3_OFFSET
-    lea   coeffq, [coeffq+cnt1q*8]
-    add samplesq, cnt1q
-    neg    cnt1q
-
-.loop:
-%if cpuflag(avx)
-    cvtdq2ps  m4, [lfeq+16]
-    cvtdq2ps  m5, [lfeq   ]
-    shufps    m7, m4, m4, q0123
-    shufps    m6, m5, m5, q0123
-%elif cpuflag(sse2)
-    movu      m4, [lfeq+16]
-    movu      m5, [lfeq   ]
-    cvtdq2ps  m4, m4
-    cvtdq2ps  m5, m5
-    pshufd    m7, m4, q0123
-    pshufd    m6, m5, q0123
-%else
-    cvtpi2ps  m4, [lfeq+16]
-    cvtpi2ps  m0, [lfeq+24]
-    cvtpi2ps  m5, [lfeq   ]
-    cvtpi2ps  m1, [lfeq+8 ]
-    shufps    m4, m0, q1010
-    shufps    m5, m1, q1010
-    shufps    m7, m4, m4, q0123
-    shufps    m6, m5, m5, q0123
-%endif
-
-.inner_loop:
-%if ARCH_X86_64
-    movaps    m8, [coeffq+cnt1q*8   ]
-    movaps    m9, [coeffq+cnt1q*8+16]
-    movaps   m10, [coeffq+cnt1q*8+32]
-    movaps   m11, [coeffq+cnt1q*8+48]
-%if cpuflag(fma3)
-    movaps   m12, [coeffq+cnt1q*8+64]
-    movaps   m13, [coeffq+cnt1q*8+80]
-    movaps   m14, [coeffq+cnt1q*8+96]
-    movaps   m15, [coeffq+cnt1q*8+112]
-    mulps     m0, m7, m8
-    mulps     m1, m7, m10
-    mulps     m2, m7, m12
-    mulps     m3, m7, m14
-    fmaddps   m0, m6, m9, m0
-    fmaddps   m1, m6, m11, m1
-    fmaddps   m2, m6, m13, m2
-    fmaddps   m3, m6, m15, m3
-
-    haddps    m0, m1
-    haddps    m2, m3
-    haddps    m0, m2
-    movaps [samplesq+cnt1q], m0
-%else
-    mulps     m0, m7, m8
-    mulps     m1, m6, m9
-    mulps     m2, m7, m10
-    mulps     m3, m6, m11
-    addps     m0, m1
-    addps     m2, m3
-
-    unpckhps  m3, m0, m2
-    unpcklps  m0, m2
-    addps     m3, m0
-    movhlps   m2, m3
-    addps     m2, m3
-    movlps [samplesq+cnt1q], m2
-%endif
-%else ; ARCH_X86_32
-%if cpuflag(fma3)
-    mulps     m0, m7, [coeffq+cnt1q*8    ]
-    mulps     m1, m7, [coeffq+cnt1q*8+32 ]
-    mulps     m2, m7, [coeffq+cnt1q*8+64 ]
-    mulps     m3, m7, [coeffq+cnt1q*8+96 ]
-    fmaddps   m0, m6, [coeffq+cnt1q*8+16 ], m0
-    fmaddps   m1, m6, [coeffq+cnt1q*8+48 ], m1
-    fmaddps   m2, m6, [coeffq+cnt1q*8+80 ], m2
-    fmaddps   m3, m6, [coeffq+cnt1q*8+112], m3
-
-    haddps    m0, m1
-    haddps    m2, m3
-    haddps    m0, m2
-    movaps [samplesq+cnt1q], m0
-%else
-    mulps     m0, m7, [coeffq+cnt1q*8   ]
-    mulps     m1, m6, [coeffq+cnt1q*8+16]
-    mulps     m2, m7, [coeffq+cnt1q*8+32]
-    mulps     m3, m6, [coeffq+cnt1q*8+48]
-    addps     m0, m1
-    addps     m2, m3
-
-    unpckhps  m3, m0, m2
-    unpcklps  m0, m2
-    addps     m3, m0
-    movhlps   m2, m3
-    addps     m2, m3
-    movlps [samplesq+cnt1q], m2
-%endif
-%endif; ARCH
-
-%if ARCH_X86_64
-%if cpuflag(fma3)
-    mulps     m8, m5
-    mulps    m10, m5
-    mulps    m12, m5
-    mulps    m14, m5
-    fmaddps   m8, m4, m9, m8
-    fmaddps  m10, m4, m11, m10
-    fmaddps  m12, m4, m13, m12
-    fmaddps  m14, m4, m15, m14
-
-    haddps   m10, m8
-    haddps   m14, m12
-    haddps   m14, m10
-    movaps [samplesq+cnt2q], m14
-%else
-    mulps     m8, m5
-    mulps     m9, m4
-    mulps    m10, m5
-    mulps    m11, m4
-    addps     m8, m9
-    addps    m10, m11
-
-    unpckhps m11, m10, m8
-    unpcklps m10, m8
-    addps    m11, m10
-    movhlps   m8, m11
-    addps     m8, m11
-    movlps [samplesq+cnt2q], m8
-%endif
-%else ; ARCH_X86_32
-%if cpuflag(fma3)
-    mulps     m0, m5, [coeffq+cnt1q*8    ]
-    mulps     m1, m5, [coeffq+cnt1q*8+32 ]
-    mulps     m2, m5, [coeffq+cnt1q*8+64 ]
-    mulps     m3, m5, [coeffq+cnt1q*8+96 ]
-    fmaddps   m0, m4, [coeffq+cnt1q*8+16 ], m0
-    fmaddps   m1, m4, [coeffq+cnt1q*8+48 ], m1
-    fmaddps   m2, m4, [coeffq+cnt1q*8+80 ], m2
-    fmaddps   m3, m4, [coeffq+cnt1q*8+112], m3
-
-    haddps    m1, m0
-    haddps    m3, m2
-    haddps    m3, m1
-    movaps [samplesq+cnt2q], m3
-%else
-    mulps     m0, m5, [coeffq+cnt1q*8   ]
-    mulps     m1, m4, [coeffq+cnt1q*8+16]
-    mulps     m2, m5, [coeffq+cnt1q*8+32]
-    mulps     m3, m4, [coeffq+cnt1q*8+48]
-    addps     m0, m1
-    addps     m2, m3
-
-    unpckhps  m3, m2, m0
-    unpcklps  m2, m0
-    addps     m3, m2
-    movhlps   m0, m3
-    addps     m0, m3
-    movlps [samplesq+cnt2q], m0
-%endif
-%endif; ARCH
-
-    sub    cnt2d, 8 + FMA3_OFFSET
-    add    cnt1q, 8 + FMA3_OFFSET
-    jl .inner_loop
-
-    add     lfeq, 4
-    add samplesq,  64*sizeof_float
-    mov    cnt1q, -32*sizeof_float
-    mov    cnt2d,  32*sizeof_float-8-FMA3_OFFSET
-    sub nblocksd, 1
-    jg .loop
-    RET
-%endmacro
-
-%if ARCH_X86_32
-INIT_XMM sse
-LFE_FIR0_FLOAT
-%endif
-INIT_XMM sse2
-LFE_FIR0_FLOAT
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-LFE_FIR0_FLOAT
-%endif
-%if HAVE_FMA3_EXTERNAL
-INIT_XMM fma3
-LFE_FIR0_FLOAT
-%endif
-
-%macro LFE_FIR1_FLOAT 0
-cglobal lfe_fir1_float, 4, 6, 10, samples, lfe, coeff, nblocks, cnt1, cnt2
-    shr nblocksd, 2
-    sub     lfeq, 3*sizeof_float
-    mov    cnt1d, 64*sizeof_float
-    mov    cnt2d, 64*sizeof_float-16
-    lea   coeffq, [coeffq+cnt1q*4]
-    add samplesq, cnt1q
-    neg    cnt1q
-
-.loop:
-%if cpuflag(avx)
-    cvtdq2ps  m4, [lfeq]
-    shufps    m5, m4, m4, q0123
-%elif cpuflag(sse2)
-    movu      m4, [lfeq]
-    cvtdq2ps  m4, m4
-    pshufd    m5, m4, q0123
-%endif
-
-.inner_loop:
-    movaps    m6, [coeffq+cnt1q*4   ]
-    movaps    m7, [coeffq+cnt1q*4+16]
-    mulps     m0, m5, m6
-    mulps     m1, m5, m7
-%if ARCH_X86_64
-    movaps    m8, [coeffq+cnt1q*4+32]
-    movaps    m9, [coeffq+cnt1q*4+48]
-    mulps     m2, m5, m8
-    mulps     m3, m5, m9
-%else
-    mulps     m2, m5, [coeffq+cnt1q*4+32]
-    mulps     m3, m5, [coeffq+cnt1q*4+48]
-%endif
-
-    haddps    m0, m1
-    haddps    m2, m3
-    haddps    m0, m2
-    movaps [samplesq+cnt1q], m0
-
-    mulps     m6, m4
-    mulps     m7, m4
-%if ARCH_X86_64
-    mulps     m8, m4
-    mulps     m9, m4
-
-    haddps    m6, m7
-    haddps    m8, m9
-    haddps    m6, m8
-%else
-    mulps     m2, m4, [coeffq+cnt1q*4+32]
-    mulps     m3, m4, [coeffq+cnt1q*4+48]
-
-    haddps    m6, m7
-    haddps    m2, m3
-    haddps    m6, m2
-%endif
-    movaps [samplesq+cnt2q], m6
-
-    sub    cnt2d, 16
-    add    cnt1q, 16
-    jl .inner_loop
-
-    add     lfeq, sizeof_float
-    add samplesq, 128*sizeof_float
-    mov    cnt1q, -64*sizeof_float
-    mov    cnt2d,  64*sizeof_float-16
-    sub nblocksd, 1
-    jg .loop
-    RET
-%endmacro
-
-INIT_XMM sse3
-LFE_FIR1_FLOAT
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-LFE_FIR1_FLOAT
-%endif
diff -uparN ffmpeg-4.1/libavcodec/x86/dct32.asm ffmpeg-y/libavcodec/x86/dct32.asm
--- ffmpeg-4.1/libavcodec/x86/dct32.asm	2018-11-03 08:17:29.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/dct32.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,491 +0,0 @@
-;******************************************************************************
-;* 32 point SSE-optimized DCT transform
-;* Copyright (c) 2010 Vitor Sessak
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA 32
-
-ps_p1p1m1m1: dd 0, 0, 0x80000000, 0x80000000, 0, 0, 0x80000000, 0x80000000
-
-ps_cos_vec: dd   0.500603,  0.505471,  0.515447,  0.531043
-            dd   0.553104,  0.582935,  0.622504,  0.674808
-            dd -10.190008, -3.407609, -2.057781, -1.484165
-            dd  -1.169440, -0.972568, -0.839350, -0.744536
-            dd   0.502419,  0.522499,  0.566944,  0.646822
-            dd   0.788155,  1.060678,  1.722447,  5.101149
-            dd   0.509796,  0.601345,  0.899976,  2.562916
-            dd   0.509796,  0.601345,  0.899976,  2.562916
-            dd   1.000000,  1.000000,  1.306563,  0.541196
-            dd   1.000000,  1.000000,  1.306563,  0.541196
-            dd   1.000000,  0.707107,  1.000000, -0.707107
-            dd   1.000000,  0.707107,  1.000000, -0.707107
-            dd   0.707107,  0.707107,  0.707107,  0.707107
-
-%macro BUTTERFLY 4
-    subps  %4, %1, %2
-    addps  %2, %2, %1
-    mulps  %1, %4, %3
-%endmacro
-
-%macro BUTTERFLY0 5
-%if cpuflag(sse2) && notcpuflag(avx)
-    pshufd %4, %1, %5
-    xorps  %1, %2
-    addps  %1, %4
-    mulps  %1, %3
-%else
-    shufps %4, %1, %1, %5
-    xorps  %1, %1, %2
-    addps  %4, %4, %1
-    mulps  %1, %4, %3
-%endif
-%endmacro
-
-%macro BUTTERFLY2 4
-    BUTTERFLY0 %1, %2, %3, %4, 0x1b
-%endmacro
-
-%macro BUTTERFLY3 4
-    BUTTERFLY0 %1, %2, %3, %4, 0xb1
-%endmacro
-
-%macro BUTTERFLY3V 5
-    movaps m%5, m%1
-    addps  m%1, m%2
-    subps  m%5, m%2
-    SWAP %2, %5
-    mulps  m%2, [ps_cos_vec+192]
-    movaps m%5, m%3
-    addps  m%3, m%4
-    subps  m%4, m%5
-    mulps  m%4, [ps_cos_vec+192]
-%endmacro
-
-%macro PASS6_AND_PERMUTE 0
-    mov         tmpd, [outq+4]
-    movss         m7, [outq+72]
-    addss         m7, [outq+76]
-    movss         m3, [outq+56]
-    addss         m3, [outq+60]
-    addss         m4, m3
-    movss         m2, [outq+52]
-    addss         m2, m3
-    movss         m3, [outq+104]
-    addss         m3, [outq+108]
-    addss         m1, m3
-    addss         m5, m4
-    movss [outq+ 16], m1
-    movss         m1, [outq+100]
-    addss         m1, m3
-    movss         m3, [outq+40]
-    movss [outq+ 48], m1
-    addss         m3, [outq+44]
-    movss         m1, [outq+100]
-    addss         m4, m3
-    addss         m3, m2
-    addss         m1, [outq+108]
-    movss [outq+ 40], m3
-    addss         m2, [outq+36]
-    movss         m3, [outq+8]
-    movss [outq+ 56], m2
-    addss         m3, [outq+12]
-    movss [outq+ 32], m3
-    movss         m3, [outq+80]
-    movss [outq+  8], m5
-    movss [outq+ 80], m1
-    movss         m2, [outq+52]
-    movss         m5, [outq+120]
-    addss         m5, [outq+124]
-    movss         m1, [outq+64]
-    addss         m2, [outq+60]
-    addss         m0, m5
-    addss         m5, [outq+116]
-    mov    [outq+64], tmpd
-    addss         m6, m0
-    addss         m1, m6
-    mov         tmpd, [outq+12]
-    mov   [outq+ 96], tmpd
-    movss [outq+  4], m1
-    movss         m1, [outq+24]
-    movss [outq+ 24], m4
-    movss         m4, [outq+88]
-    addss         m4, [outq+92]
-    addss         m3, m4
-    addss         m4, [outq+84]
-    mov         tmpd, [outq+108]
-    addss         m1, [outq+28]
-    addss         m0, m1
-    addss         m1, m5
-    addss         m6, m3
-    addss         m3, m0
-    addss         m0, m7
-    addss         m5, [outq+20]
-    addss         m7, m1
-    movss [outq+ 12], m6
-    mov   [outq+112], tmpd
-    movss         m6, [outq+28]
-    movss [outq+ 28], m0
-    movss         m0, [outq+36]
-    movss [outq+ 36], m7
-    addss         m1, m4
-    movss         m7, [outq+116]
-    addss         m0, m2
-    addss         m7, [outq+124]
-    movss [outq+ 72], m0
-    movss         m0, [outq+44]
-    addss         m2, m0
-    movss [outq+ 44], m1
-    movss [outq+ 88], m2
-    addss         m0, [outq+60]
-    mov         tmpd, [outq+60]
-    mov   [outq+120], tmpd
-    movss [outq+104], m0
-    addss         m4, m5
-    addss         m5, [outq+68]
-    movss  [outq+52], m4
-    movss  [outq+60], m5
-    movss         m4, [outq+68]
-    movss         m5, [outq+20]
-    movss [outq+ 20], m3
-    addss         m5, m7
-    addss         m7, m6
-    addss         m4, m5
-    movss         m2, [outq+84]
-    addss         m2, [outq+92]
-    addss         m5, m2
-    movss [outq+ 68], m4
-    addss         m2, m7
-    movss         m4, [outq+76]
-    movss [outq+ 84], m2
-    movss [outq+ 76], m5
-    addss         m7, m4
-    addss         m6, [outq+124]
-    addss         m4, m6
-    addss         m6, [outq+92]
-    movss [outq+100], m4
-    movss [outq+108], m6
-    movss         m6, [outq+92]
-    movss  [outq+92], m7
-    addss         m6, [outq+124]
-    movss [outq+116], m6
-%endmacro
-
-INIT_YMM avx
-SECTION .text
-%if HAVE_AVX_EXTERNAL
-; void ff_dct32_float_avx(FFTSample *out, const FFTSample *in)
-cglobal dct32_float, 2,3,8, out, in, tmp
-    ; pass 1
-    vmovaps     m4, [inq+0]
-    vinsertf128 m5, m5, [inq+96], 1
-    vinsertf128 m5, m5, [inq+112], 0
-    vshufps     m5, m5, m5, 0x1b
-    BUTTERFLY   m4, m5, [ps_cos_vec], m6
-
-    vmovaps     m2, [inq+64]
-    vinsertf128 m6, m6, [inq+32], 1
-    vinsertf128 m6, m6, [inq+48], 0
-    vshufps     m6, m6, m6, 0x1b
-    BUTTERFLY   m2, m6, [ps_cos_vec+32], m0
-
-    ; pass 2
-
-    BUTTERFLY  m5, m6, [ps_cos_vec+64], m0
-    BUTTERFLY  m4, m2, [ps_cos_vec+64], m7
-
-
-    ; pass 3
-    vperm2f128  m3, m6, m4, 0x31
-    vperm2f128  m1, m6, m4, 0x20
-    vshufps     m3, m3, m3, 0x1b
-
-    BUTTERFLY   m1, m3, [ps_cos_vec+96], m6
-
-
-    vperm2f128  m4, m5, m2, 0x20
-    vperm2f128  m5, m5, m2, 0x31
-    vshufps     m5, m5, m5, 0x1b
-
-    BUTTERFLY   m4, m5, [ps_cos_vec+96], m6
-
-    ; pass 4
-    vmovaps m6, [ps_p1p1m1m1+0]
-    vmovaps m2, [ps_cos_vec+128]
-
-    BUTTERFLY2  m5, m6, m2, m7
-    BUTTERFLY2  m4, m6, m2, m7
-    BUTTERFLY2  m1, m6, m2, m7
-    BUTTERFLY2  m3, m6, m2, m7
-
-
-    ; pass 5
-    vshufps m6, m6, m6, 0xcc
-    vmovaps m2, [ps_cos_vec+160]
-
-    BUTTERFLY3  m5, m6, m2, m7
-    BUTTERFLY3  m4, m6, m2, m7
-    BUTTERFLY3  m1, m6, m2, m7
-    BUTTERFLY3  m3, m6, m2, m7
-
-    vperm2f128  m6, m3, m3, 0x31
-    vmovaps [outq], m3
-
-    vextractf128  [outq+64], m5, 1
-    vextractf128  [outq+32], m5, 0
-
-    vextractf128  [outq+80], m4, 1
-    vextractf128  [outq+48], m4, 0
-
-    vperm2f128  m0, m1, m1, 0x31
-    vmovaps [outq+96], m1
-
-    vzeroupper
-
-    ;    pass 6, no SIMD...
-INIT_XMM
-    PASS6_AND_PERMUTE
-    RET
-%endif
-
-%if ARCH_X86_64
-%define SPILL SWAP
-%define UNSPILL SWAP
-
-%macro PASS5 0
-    nop ; FIXME code alignment
-    SWAP 5, 8
-    SWAP 4, 12
-    SWAP 6, 14
-    SWAP 7, 13
-    SWAP 0, 15
-    PERMUTE 9,10, 10,12, 11,14, 12,9, 13,11, 14,13
-    TRANSPOSE4x4PS 8, 9, 10, 11, 0
-    BUTTERFLY3V    8, 9, 10, 11, 0
-    addps   m10, m11
-    TRANSPOSE4x4PS 12, 13, 14, 15, 0
-    BUTTERFLY3V    12, 13, 14, 15, 0
-    addps   m14, m15
-    addps   m12, m14
-    addps   m14, m13
-    addps   m13, m15
-%endmacro
-
-%macro PASS6 0
-    SWAP 9, 12
-    SWAP 11, 14
-    movss [outq+0x00], m8
-    pshuflw m0, m8, 0xe
-    movss [outq+0x10], m9
-    pshuflw m1, m9, 0xe
-    movss [outq+0x20], m10
-    pshuflw m2, m10, 0xe
-    movss [outq+0x30], m11
-    pshuflw m3, m11, 0xe
-    movss [outq+0x40], m12
-    pshuflw m4, m12, 0xe
-    movss [outq+0x50], m13
-    pshuflw m5, m13, 0xe
-    movss [outq+0x60], m14
-    pshuflw m6, m14, 0xe
-    movaps [outq+0x70], m15
-    pshuflw m7, m15, 0xe
-    addss   m0, m1
-    addss   m1, m2
-    movss [outq+0x08], m0
-    addss   m2, m3
-    movss [outq+0x18], m1
-    addss   m3, m4
-    movss [outq+0x28], m2
-    addss   m4, m5
-    movss [outq+0x38], m3
-    addss   m5, m6
-    movss [outq+0x48], m4
-    addss   m6, m7
-    movss [outq+0x58], m5
-    movss [outq+0x68], m6
-    movss [outq+0x78], m7
-
-    PERMUTE 1,8, 3,9, 5,10, 7,11, 9,12, 11,13, 13,14, 8,1, 10,3, 12,5, 14,7
-    movhlps m0, m1
-    pshufd  m1, m1, 3
-    SWAP 0, 2, 4, 6, 8, 10, 12, 14
-    SWAP 1, 3, 5, 7, 9, 11, 13, 15
-%rep 7
-    movhlps m0, m1
-    pshufd  m1, m1, 3
-    addss   m15, m1
-    SWAP 0, 2, 4, 6, 8, 10, 12, 14
-    SWAP 1, 3, 5, 7, 9, 11, 13, 15
-%endrep
-%assign i 4
-%rep 15
-    addss m0, m1
-    movss [outq+i], m0
-    SWAP 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15
-    %assign i i+8
-%endrep
-%endmacro
-
-%else ; ARCH_X86_32
-%macro SPILL 2 ; xmm#, mempos
-    movaps [outq+(%2-8)*16], m%1
-%endmacro
-%macro UNSPILL 2
-    movaps m%1, [outq+(%2-8)*16]
-%endmacro
-
-%define PASS6 PASS6_AND_PERMUTE
-%macro PASS5 0
-    movaps      m2, [ps_cos_vec+160]
-    shufps      m3, m3, 0xcc
-
-    BUTTERFLY3  m5, m3, m2, m1
-    SPILL 5, 8
-
-    UNSPILL 1, 9
-    BUTTERFLY3  m1, m3, m2, m5
-    SPILL 1, 14
-
-    BUTTERFLY3  m4, m3, m2, m5
-    SPILL 4, 12
-
-    BUTTERFLY3  m7, m3, m2, m5
-    SPILL 7, 13
-
-    UNSPILL 5, 10
-    BUTTERFLY3  m5, m3, m2, m7
-    SPILL 5, 10
-
-    UNSPILL 4, 11
-    BUTTERFLY3  m4, m3, m2, m7
-    SPILL 4, 11
-
-    BUTTERFLY3  m6, m3, m2, m7
-    SPILL 6, 9
-
-    BUTTERFLY3  m0, m3, m2, m7
-    SPILL 0, 15
-%endmacro
-%endif
-
-
-; void ff_dct32_float_sse(FFTSample *out, const FFTSample *in)
-%macro DCT32_FUNC 0
-cglobal dct32_float, 2, 3, 16, out, in, tmp
-    ; pass 1
-
-    movaps      m0, [inq+0]
-    LOAD_INV    m1, [inq+112]
-    BUTTERFLY   m0, m1, [ps_cos_vec], m3
-
-    movaps      m7, [inq+64]
-    LOAD_INV    m4, [inq+48]
-    BUTTERFLY   m7, m4, [ps_cos_vec+32], m3
-
-    ; pass 2
-    movaps      m2, [ps_cos_vec+64]
-    BUTTERFLY   m1, m4, m2, m3
-    SPILL 1, 11
-    SPILL 4, 8
-
-    ; pass 1
-    movaps      m1, [inq+16]
-    LOAD_INV    m6, [inq+96]
-    BUTTERFLY   m1, m6, [ps_cos_vec+16], m3
-
-    movaps      m4, [inq+80]
-    LOAD_INV    m5, [inq+32]
-    BUTTERFLY   m4, m5, [ps_cos_vec+48], m3
-
-    ; pass 2
-    BUTTERFLY   m0, m7, m2, m3
-
-    movaps      m2, [ps_cos_vec+80]
-    BUTTERFLY   m6, m5, m2, m3
-
-    BUTTERFLY   m1, m4, m2, m3
-
-    ; pass 3
-    movaps      m2, [ps_cos_vec+96]
-    shufps      m1, m1, 0x1b
-    BUTTERFLY   m0, m1, m2, m3
-    SPILL 0, 15
-    SPILL 1, 14
-
-    UNSPILL 0, 8
-    shufps      m5, m5, 0x1b
-    BUTTERFLY   m0, m5, m2, m3
-
-    UNSPILL 1, 11
-    shufps      m6, m6, 0x1b
-    BUTTERFLY   m1, m6, m2, m3
-    SPILL 1, 11
-
-    shufps      m4, m4, 0x1b
-    BUTTERFLY   m7, m4, m2, m3
-
-    ; pass 4
-    movaps      m3, [ps_p1p1m1m1+0]
-    movaps      m2, [ps_cos_vec+128]
-
-    BUTTERFLY2  m5, m3, m2, m1
-
-    BUTTERFLY2  m0, m3, m2, m1
-    SPILL 0, 9
-
-    BUTTERFLY2  m6, m3, m2, m1
-    SPILL 6, 10
-
-    UNSPILL 0, 11
-    BUTTERFLY2  m0, m3, m2, m1
-    SPILL 0, 11
-
-    BUTTERFLY2  m4, m3, m2, m1
-
-    BUTTERFLY2  m7, m3, m2, m1
-
-    UNSPILL 6, 14
-    BUTTERFLY2  m6, m3, m2, m1
-
-    UNSPILL 0, 15
-    BUTTERFLY2  m0, m3, m2, m1
-
-    PASS5
-    PASS6
-    RET
-%endmacro
-
-%macro LOAD_INV 2
-%if cpuflag(sse2)
-    pshufd      %1, %2, 0x1b
-%elif cpuflag(sse)
-    movaps      %1, %2
-    shufps      %1, %1, 0x1b
-%endif
-%endmacro
-
-%if ARCH_X86_32
-INIT_XMM sse
-DCT32_FUNC
-%endif
-
-INIT_XMM sse2
-DCT32_FUNC
diff -uparN ffmpeg-4.1/libavcodec/x86/diracdsp.asm ffmpeg-y/libavcodec/x86/diracdsp.asm
--- ffmpeg-4.1/libavcodec/x86/diracdsp.asm	2018-11-02 02:34:25.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/diracdsp.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,347 +0,0 @@
-;******************************************************************************
-;* Copyright (c) 2010 David Conrad
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* 51, Inc., Foundation Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-pw_7: times 8 dw 7
-convert_to_unsigned_10bit: times 4 dd 0x200
-clip_10bit:                times 8 dw 0x3ff
-
-cextern pw_3
-cextern pw_16
-cextern pw_32
-cextern pb_80
-
-SECTION .text
-
-%macro UNPACK_ADD 6
-    mov%5   %1, %3
-    mov%6   m5, %4
-    mova    m4, %1
-    mova    %2, m5
-    punpcklbw %1, m7
-    punpcklbw m5, m7
-    punpckhbw m4, m7
-    punpckhbw %2, m7
-    paddw   %1, m5
-    paddw   %2, m4
-%endmacro
-
-%macro HPEL_FILTER 1
-; dirac_hpel_filter_v_sse2(uint8_t *dst, uint8_t *src, int stride, int width);
-cglobal dirac_hpel_filter_v_%1, 4,6,8, dst, src, stride, width, src0, stridex3
-    mov     src0q, srcq
-    lea     stridex3q, [3*strideq]
-    sub     src0q, stridex3q
-    pxor    m7, m7
-.loop:
-    ; 7*(src[0] + src[1])
-    UNPACK_ADD m0, m1, [srcq], [srcq + strideq], a,a
-    pmullw  m0, [pw_7]
-    pmullw  m1, [pw_7]
-
-    ; 3*( ... + src[-2] + src[3])
-    UNPACK_ADD m2, m3, [src0q + strideq], [srcq + stridex3q], a,a
-    paddw   m0, m2
-    paddw   m1, m3
-    pmullw  m0, [pw_3]
-    pmullw  m1, [pw_3]
-
-    ; ... - 7*(src[-1] + src[2])
-    UNPACK_ADD m2, m3, [src0q + strideq*2], [srcq + strideq*2], a,a
-    pmullw  m2, [pw_7]
-    pmullw  m3, [pw_7]
-    psubw   m0, m2
-    psubw   m1, m3
-
-    ; ... - (src[-3] + src[4])
-    UNPACK_ADD m2, m3, [src0q], [srcq + strideq*4], a,a
-    psubw   m0, m2
-    psubw   m1, m3
-
-    paddw   m0, [pw_16]
-    paddw   m1, [pw_16]
-    psraw   m0, 5
-    psraw   m1, 5
-    packuswb m0, m1
-    mova    [dstq], m0
-    add     dstq, mmsize
-    add     srcq, mmsize
-    add     src0q, mmsize
-    sub     widthd, mmsize
-    jg      .loop
-    RET
-
-; dirac_hpel_filter_h_sse2(uint8_t *dst, uint8_t *src, int width);
-cglobal dirac_hpel_filter_h_%1, 3,3,8, dst, src, width
-    dec     widthd
-    pxor    m7, m7
-    and     widthd, ~(mmsize-1)
-.loop:
-    ; 7*(src[0] + src[1])
-    UNPACK_ADD m0, m1, [srcq + widthq], [srcq + widthq + 1], u,u
-    pmullw  m0, [pw_7]
-    pmullw  m1, [pw_7]
-
-    ; 3*( ... + src[-2] + src[3])
-    UNPACK_ADD m2, m3, [srcq + widthq - 2], [srcq + widthq + 3], u,u
-    paddw   m0, m2
-    paddw   m1, m3
-    pmullw  m0, [pw_3]
-    pmullw  m1, [pw_3]
-
-    ; ... - 7*(src[-1] + src[2])
-    UNPACK_ADD m2, m3, [srcq + widthq - 1], [srcq + widthq + 2], u,u
-    pmullw  m2, [pw_7]
-    pmullw  m3, [pw_7]
-    psubw   m0, m2
-    psubw   m1, m3
-
-    ; ... - (src[-3] + src[4])
-    UNPACK_ADD m2, m3, [srcq + widthq - 3], [srcq + widthq + 4], u,u
-    psubw   m0, m2
-    psubw   m1, m3
-
-    paddw   m0, [pw_16]
-    paddw   m1, [pw_16]
-    psraw   m0, 5
-    psraw   m1, 5
-    packuswb m0, m1
-    mova    [dstq + widthq], m0
-    sub     widthd, mmsize
-    jge     .loop
-    RET
-%endmacro
-
-%macro PUT_RECT 1
-; void put_rect_clamped(uint8_t *dst, int dst_stride, int16_t *src, int src_stride, int width, int height)
-cglobal put_signed_rect_clamped_%1, 5,9,3, dst, dst_stride, src, src_stride, w, dst2, src2
-    mova    m0, [pb_80]
-    add     wd, (mmsize-1)
-    and     wd, ~(mmsize-1)
-
-%if ARCH_X86_64
-    movsxd   dst_strideq, dst_strided
-    movsxd   src_strideq, src_strided
-    mov   r7d, r5m
-    mov   r8d, wd
-    %define wspill r8d
-    %define hd r7d
-%else
-    mov    r4m, wd
-    %define wspill r4m
-    %define hd r5mp
-%endif
-
-.loopy:
-    lea     src2q, [srcq+src_strideq]
-    lea     dst2q, [dstq+dst_strideq]
-.loopx:
-    sub      wd, mmsize
-    mova     m1, [srcq +2*wq]
-    mova     m2, [src2q+2*wq]
-    packsswb m1, [srcq +2*wq+mmsize]
-    packsswb m2, [src2q+2*wq+mmsize]
-    paddb    m1, m0
-    paddb    m2, m0
-    mova    [dstq +wq], m1
-    mova    [dst2q+wq], m2
-    jg      .loopx
-
-    lea   srcq, [srcq+src_strideq*2]
-    lea   dstq, [dstq+dst_strideq*2]
-    sub     hd, 2
-    mov     wd, wspill
-    jg      .loopy
-    RET
-%endm
-
-%macro ADD_RECT 1
-; void add_rect_clamped(uint8_t *dst, uint16_t *src, int stride, int16_t *idwt, int idwt_stride, int width, int height)
-cglobal add_rect_clamped_%1, 7,9,3, dst, src, stride, idwt, idwt_stride, w, h
-    mova    m0, [pw_32]
-    add     wd, (mmsize-1)
-    and     wd, ~(mmsize-1)
-
-%if ARCH_X86_64
-    movsxd   strideq, strided
-    movsxd   idwt_strideq, idwt_strided
-    mov   r8d, wd
-    %define wspill r8d
-%else
-    mov    r5m, wd
-    %define wspill r5m
-%endif
-
-.loop:
-    sub     wd, mmsize
-    movu    m1, [srcq +2*wq] ; FIXME: ensure alignment
-    paddw   m1, m0
-    psraw   m1, 6
-    movu    m2, [srcq +2*wq+mmsize] ; FIXME: ensure alignment
-    paddw   m2, m0
-    psraw   m2, 6
-    paddw   m1, [idwtq+2*wq]
-    paddw   m2, [idwtq+2*wq+mmsize]
-    packuswb m1, m2
-    mova    [dstq +wq], m1
-    jg      .loop
-
-    lea   srcq, [srcq + 2*strideq]
-    add   dstq, strideq
-    lea  idwtq, [idwtq+ 2*idwt_strideq]
-    sub     hd, 1
-    mov     wd, wspill
-    jg      .loop
-    RET
-%endm
-
-%macro ADD_OBMC 2
-; void add_obmc(uint16_t *dst, uint8_t *src, int stride, uint8_t *obmc_weight, int yblen)
-cglobal add_dirac_obmc%1_%2, 6,6,5, dst, src, stride, obmc, yblen
-    pxor        m4, m4
-.loop:
-%assign i 0
-%rep %1 / mmsize
-    mova        m0, [srcq+i]
-    mova        m1, m0
-    punpcklbw   m0, m4
-    punpckhbw   m1, m4
-    mova        m2, [obmcq+i]
-    mova        m3, m2
-   punpcklbw   m2, m4
-    punpckhbw   m3, m4
-    pmullw      m0, m2
-    pmullw      m1, m3
-    movu        m2, [dstq+2*i]
-    movu        m3, [dstq+2*i+mmsize]
-    paddw       m0, m2
-    paddw       m1, m3
-    movu        [dstq+2*i], m0
-    movu        [dstq+2*i+mmsize], m1
-%assign i i+mmsize
-%endrep
-    lea         srcq, [srcq+strideq]
-    lea         dstq, [dstq+2*strideq]
-    add         obmcq, 32
-    sub         yblend, 1
-    jg          .loop
-    RET
-%endm
-
-INIT_MMX
-%if ARCH_X86_64 == 0
-PUT_RECT mmx
-ADD_RECT mmx
-
-HPEL_FILTER mmx
-ADD_OBMC 32, mmx
-ADD_OBMC 16, mmx
-%endif
-ADD_OBMC 8, mmx
-
-INIT_XMM
-PUT_RECT sse2
-ADD_RECT sse2
-
-HPEL_FILTER sse2
-ADD_OBMC 32, sse2
-ADD_OBMC 16, sse2
-
-INIT_XMM sse4
-
-; void dequant_subband_32(uint8_t *src, uint8_t *dst, ptrdiff_t stride, const int qf, const int qs, int tot_v, int tot_h)
-cglobal dequant_subband_32, 7, 7, 4, src, dst, stride, qf, qs, tot_v, tot_h
-    movd   m2, qfd
-    movd   m3, qsd
-    SPLATD m2
-    SPLATD m3
-    mov    r4, tot_hq
-    mov    r3, dstq
-
-    .loop_v:
-    mov    tot_hq, r4
-    mov    dstq,   r3
-
-    .loop_h:
-    movu   m0, [srcq]
-
-    pabsd  m1, m0
-    pmulld m1, m2
-    paddd  m1, m3
-    psrld  m1,  2
-    psignd m1, m0
-
-    movu   [dstq], m1
-
-    add    srcq, mmsize
-    add    dstq, mmsize
-    sub    tot_hd, 4
-    jg     .loop_h
-
-    add    r3, strideq
-    dec    tot_vd
-    jg     .loop_v
-
-    RET
-
-INIT_XMM sse4
-; void put_signed_rect_clamped_10(uint8_t *dst, int dst_stride, const uint8_t *src, int src_stride, int width, int height)
-%if ARCH_X86_64
-cglobal put_signed_rect_clamped_10, 6, 8, 5, dst, dst_stride, src, src_stride, w, h, t1, t2
-%else
-cglobal put_signed_rect_clamped_10, 5, 7, 5, dst, dst_stride, src, src_stride, w, t1, t2
-    %define  hd  r5mp
-%endif
-    shl      wd, 2
-    add    srcq, wq
-    neg      wq
-    mov     t2q, dstq
-    mov     t1q, wq
-    pxor     m2, m2
-    mova     m3, [clip_10bit]
-    mova     m4, [convert_to_unsigned_10bit]
-
-    .loop_h:
-    mov    dstq, t2q
-    mov      wq, t1q
-
-    .loop_w:
-    movu     m0, [srcq+wq+0*mmsize]
-    movu     m1, [srcq+wq+1*mmsize]
-
-    paddd    m0, m4
-    paddd    m1, m4
-    packusdw m0, m0, m1
-    CLIPW    m0, m2, m3 ; packusdw saturates so it's fine
-
-    movu     [dstq], m0
-
-    add      dstq, 1*mmsize
-    add      wq,   2*mmsize
-    jl       .loop_w
-
-    add    srcq, src_strideq
-    add     t2q, dst_strideq
-    sub      hd, 1
-    jg       .loop_h
-
-    RET
diff -uparN ffmpeg-4.1/libavcodec/x86/dirac_dwt.asm ffmpeg-y/libavcodec/x86/dirac_dwt.asm
--- ffmpeg-4.1/libavcodec/x86/dirac_dwt.asm	2018-11-02 02:34:25.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/dirac_dwt.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,307 +0,0 @@
-;******************************************************************************
-;* x86 optimized discrete wavelet trasnform
-;* Copyright (c) 2010 David Conrad
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* 51, Inc., Foundation Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-pw_1991: times 4 dw 9,-1
-
-cextern pw_1
-cextern pw_2
-cextern pw_8
-cextern pw_16
-
-SECTION .text
-
-; %1 -= (%2 + %3 + 2)>>2     %4 is pw_2
-%macro COMPOSE_53iL0 4
-    paddw   %2, %3
-    paddw   %2, %4
-    psraw   %2, 2
-    psubw   %1, %2
-%endm
-
-; m1 = %1 + (-m0 + 9*m1 + 9*%2 -%3 + 8)>>4
-; if %4 is supplied, %1 is loaded unaligned from there
-; m2: clobbered  m3: pw_8  m4: pw_1991
-%macro COMPOSE_DD97iH0 3-4
-    paddw   m0, %3
-    paddw   m1, %2
-    psubw   m0, m3
-    mova    m2, m1
-    punpcklwd m1, m0
-    punpckhwd m2, m0
-    pmaddwd m1, m4
-    pmaddwd m2, m4
-%if %0 > 3
-    movu    %1, %4
-%endif
-    psrad   m1, 4
-    psrad   m2, 4
-    packssdw m1, m2
-    paddw   m1, %1
-%endm
-
-%macro COMPOSE_VERTICAL 1
-; void vertical_compose53iL0(IDWTELEM *b0, IDWTELEM *b1, IDWTELEM *b2,
-;                                  int width)
-cglobal vertical_compose53iL0_%1, 4,4,1, b0, b1, b2, width
-    mova    m2, [pw_2]
-%if ARCH_X86_64
-    mov     widthd, widthd
-%endif
-.loop:
-    sub     widthq, mmsize/2
-    mova    m1, [b0q+2*widthq]
-    mova    m0, [b1q+2*widthq]
-    COMPOSE_53iL0 m0, m1, [b2q+2*widthq], m2
-    mova    [b1q+2*widthq], m0
-    jg      .loop
-    REP_RET
-
-; void vertical_compose_dirac53iH0(IDWTELEM *b0, IDWTELEM *b1, IDWTELEM *b2,
-;                                  int width)
-cglobal vertical_compose_dirac53iH0_%1, 4,4,1, b0, b1, b2, width
-    mova    m1, [pw_1]
-%if ARCH_X86_64
-    mov     widthd, widthd
-%endif
-.loop:
-    sub     widthq, mmsize/2
-    mova    m0, [b0q+2*widthq]
-    paddw   m0, [b2q+2*widthq]
-    paddw   m0, m1
-    psraw   m0, 1
-    paddw   m0, [b1q+2*widthq]
-    mova    [b1q+2*widthq], m0
-    jg      .loop
-    REP_RET
-
-; void vertical_compose_dd97iH0(IDWTELEM *b0, IDWTELEM *b1, IDWTELEM *b2,
-;                               IDWTELEM *b3, IDWTELEM *b4, int width)
-cglobal vertical_compose_dd97iH0_%1, 6,6,5, b0, b1, b2, b3, b4, width
-    mova    m3, [pw_8]
-    mova    m4, [pw_1991]
-%if ARCH_X86_64
-    mov     widthd, widthd
-%endif
-.loop:
-    sub     widthq, mmsize/2
-    mova    m0, [b0q+2*widthq]
-    mova    m1, [b1q+2*widthq]
-    COMPOSE_DD97iH0 [b2q+2*widthq], [b3q+2*widthq], [b4q+2*widthq]
-    mova    [b2q+2*widthq], m1
-    jg      .loop
-    REP_RET
-
-; void vertical_compose_dd137iL0(IDWTELEM *b0, IDWTELEM *b1, IDWTELEM *b2,
-;                                IDWTELEM *b3, IDWTELEM *b4, int width)
-cglobal vertical_compose_dd137iL0_%1, 6,6,6, b0, b1, b2, b3, b4, width
-    mova    m3, [pw_16]
-    mova    m4, [pw_1991]
-%if ARCH_X86_64
-    mov     widthd, widthd
-%endif
-.loop:
-    sub     widthq, mmsize/2
-    mova    m0, [b0q+2*widthq]
-    mova    m1, [b1q+2*widthq]
-    mova    m5, [b2q+2*widthq]
-    paddw   m0, [b4q+2*widthq]
-    paddw   m1, [b3q+2*widthq]
-    psubw   m0, m3
-    mova    m2, m1
-    punpcklwd m1, m0
-    punpckhwd m2, m0
-    pmaddwd m1, m4
-    pmaddwd m2, m4
-    psrad   m1, 5
-    psrad   m2, 5
-    packssdw m1, m2
-    psubw   m5, m1
-    mova    [b2q+2*widthq], m5
-    jg      .loop
-    REP_RET
-
-; void vertical_compose_haar(IDWTELEM *b0, IDWTELEM *b1, int width)
-cglobal vertical_compose_haar_%1, 3,4,3, b0, b1, width
-    mova    m3, [pw_1]
-%if ARCH_X86_64
-    mov     widthd, widthd
-%endif
-.loop:
-    sub     widthq, mmsize/2
-    mova    m1, [b1q+2*widthq]
-    mova    m0, [b0q+2*widthq]
-    mova    m2, m1
-    paddw   m1, m3
-    psraw   m1, 1
-    psubw   m0, m1
-    mova    [b0q+2*widthq], m0
-    paddw   m2, m0
-    mova    [b1q+2*widthq], m2
-    jg      .loop
-    REP_RET
-%endmacro
-
-; extend the left and right edges of the tmp array by %1 and %2 respectively
-%macro EDGE_EXTENSION 3
-    mov     %3, [tmpq]
-%assign %%i 1
-%rep %1
-    mov     [tmpq-2*%%i], %3
-    %assign %%i %%i+1
-%endrep
-    mov     %3, [tmpq+2*w2q-2]
-%assign %%i 0
-%rep %2
-    mov     [tmpq+2*w2q+2*%%i], %3
-    %assign %%i %%i+1
-%endrep
-%endmacro
-
-
-%macro HAAR_HORIZONTAL 2
-; void horizontal_compose_haari(IDWTELEM *b, IDWTELEM *tmp, int width)
-cglobal horizontal_compose_haar%2i_%1, 3,6,4, b, tmp, w, x, w2, b_w2
-    mov    w2d, wd
-    xor     xq, xq
-    shr    w2d, 1
-    lea  b_w2q, [bq+wq]
-    mova    m3, [pw_1]
-.lowpass_loop:
-    movu    m1, [b_w2q + 2*xq]
-    mova    m0, [bq    + 2*xq]
-    paddw   m1, m3
-    psraw   m1, 1
-    psubw   m0, m1
-    mova    [tmpq + 2*xq], m0
-    add     xq, mmsize/2
-    cmp     xq, w2q
-    jl      .lowpass_loop
-
-    xor     xq, xq
-    and    w2q, ~(mmsize/2 - 1)
-    cmp    w2q, mmsize/2
-    jl      .end
-
-.highpass_loop:
-    movu    m1, [b_w2q + 2*xq]
-    mova    m0, [tmpq  + 2*xq]
-    paddw   m1, m0
-
-    ; shift and interleave
-%if %2 == 1
-    paddw   m0, m3
-    paddw   m1, m3
-    psraw   m0, 1
-    psraw   m1, 1
-%endif
-    mova    m2, m0
-    punpcklwd m0, m1
-    punpckhwd m2, m1
-    mova    [bq+4*xq], m0
-    mova    [bq+4*xq+mmsize], m2
-
-    add     xq, mmsize/2
-    cmp     xq, w2q
-    jl      .highpass_loop
-.end:
-    REP_RET
-%endmacro
-
-
-INIT_XMM
-; void horizontal_compose_dd97i(IDWTELEM *b, IDWTELEM *tmp, int width)
-cglobal horizontal_compose_dd97i_ssse3, 3,6,8, b, tmp, w, x, w2, b_w2
-    mov    w2d, wd
-    xor     xd, xd
-    shr    w2d, 1
-    lea  b_w2q, [bq+wq]
-    movu    m4, [bq+wq]
-    mova    m7, [pw_2]
-    pslldq  m4, 14
-.lowpass_loop:
-    movu    m1, [b_w2q + 2*xq]
-    mova    m0, [bq    + 2*xq]
-    mova    m2, m1
-    palignr m1, m4, 14
-    mova    m4, m2
-    COMPOSE_53iL0 m0, m1, m2, m7
-    mova    [tmpq + 2*xq], m0
-    add     xd, mmsize/2
-    cmp     xd, w2d
-    jl      .lowpass_loop
-
-    EDGE_EXTENSION 1, 2, xw
-    ; leave the last up to 7 (sse) or 3 (mmx) values for C
-    xor     xd, xd
-    and    w2d, ~(mmsize/2 - 1)
-    cmp    w2d, mmsize/2
-    jl      .end
-
-    mova    m7, [tmpq-mmsize]
-    mova    m0, [tmpq]
-    mova    m5, [pw_1]
-    mova    m3, [pw_8]
-    mova    m4, [pw_1991]
-.highpass_loop:
-    mova    m6, m0
-    palignr m0, m7, 14
-    mova    m7, [tmpq + 2*xq + 16]
-    mova    m1, m7
-    mova    m2, m7
-    palignr m1, m6, 2
-    palignr m2, m6, 4
-    COMPOSE_DD97iH0 m0, m6, m2, [b_w2q + 2*xq]
-    mova    m0, m7
-    mova    m7, m6
-
-    ; shift and interleave
-    paddw   m6, m5
-    paddw   m1, m5
-    psraw   m6, 1
-    psraw   m1, 1
-    mova    m2, m6
-    punpcklwd m6, m1
-    punpckhwd m2, m1
-    mova    [bq+4*xq], m6
-    mova    [bq+4*xq+mmsize], m2
-
-    add     xd, mmsize/2
-    cmp     xd, w2d
-    jl      .highpass_loop
-.end:
-    REP_RET
-
-
-%if ARCH_X86_64 == 0
-INIT_MMX
-COMPOSE_VERTICAL mmx
-HAAR_HORIZONTAL mmx, 0
-HAAR_HORIZONTAL mmx, 1
-%endif
-
-;;INIT_XMM
-INIT_XMM
-COMPOSE_VERTICAL sse2
-HAAR_HORIZONTAL sse2, 0
-HAAR_HORIZONTAL sse2, 1
diff -uparN ffmpeg-4.1/libavcodec/x86/dnxhdenc.asm ffmpeg-y/libavcodec/x86/dnxhdenc.asm
--- ffmpeg-4.1/libavcodec/x86/dnxhdenc.asm	2018-11-02 02:34:25.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/dnxhdenc.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,49 +0,0 @@
-;************************************************************************
-;* VC3/DNxHD SIMD functions
-;* Copyright (c) 2007 Baptiste Coudurier <baptiste dot coudurier at smartjog dot com>
-;* Copyright (c) 2014 Tiancheng "Timothy" Gu <timothygu99@gmail.com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* 51, Inc., Foundation Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION .text
-
-; void get_pixels_8x4_sym_sse2(int16_t *block, const uint8_t *pixels,
-;                              ptrdiff_t line_size)
-INIT_XMM sse2
-cglobal get_pixels_8x4_sym, 3,3,5, block, pixels, linesize
-    pxor      m4,       m4
-    movq      m0,       [pixelsq]
-    add       pixelsq,  linesizeq
-    movq      m1,       [pixelsq]
-    movq      m2,       [pixelsq+linesizeq]
-    movq      m3,       [pixelsq+linesizeq*2]
-    punpcklbw m0,       m4
-    punpcklbw m1,       m4
-    punpcklbw m2,       m4
-    punpcklbw m3,       m4
-    mova  [blockq    ], m0
-    mova  [blockq+16 ], m1
-    mova  [blockq+32 ], m2
-    mova  [blockq+48 ], m3
-    mova  [blockq+64 ], m3
-    mova  [blockq+80 ], m2
-    mova  [blockq+96 ], m1
-    mova  [blockq+112], m0
-    RET
diff -uparN ffmpeg-4.1/libavcodec/x86/exrdsp.asm ffmpeg-y/libavcodec/x86/exrdsp.asm
--- ffmpeg-4.1/libavcodec/x86/exrdsp.asm	2018-11-03 08:17:29.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/exrdsp.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,118 +0,0 @@
-;******************************************************************************
-;* X86 Optimized functions for Open Exr Decoder
-;* Copyright (c) 2006 Industrial Light & Magic, a division of Lucas Digital Ltd. LLC
-;*
-;* reorder_pixels, predictor based on patch by John Loy
-;* port to ASM by Jokyo Images support by CNC - French National Center for Cinema
-;*
-;* predictor AVX/AVX2 by Henrik Gramner
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-cextern pb_15
-cextern pb_80
-
-SECTION .text
-
-;------------------------------------------------------------------------------
-; void ff_reorder_pixels(uint8_t *dst, const uint8_t *src, ptrdiff_t size);
-;------------------------------------------------------------------------------
-
-%macro REORDER_PIXELS 0
-cglobal reorder_pixels, 3,4,3, dst, src1, size, src2
-    lea                              src2q, [src1q+sizeq] ; src2 = src + 2 * half_size
-    add                               dstq, sizeq         ; dst offset by size
-    shr                              sizeq, 1             ; half_size
-    add                              src1q, sizeq         ; offset src by half_size
-    neg                              sizeq                ; size = offset for dst, src1, src2
-.loop:
-
-    mova                                m0, [src1q+sizeq]        ; load first part
-    movu                                m1, [src2q+sizeq]        ; load second part
-    SBUTTERFLY bw, 0, 1, 2                                       ; interleaved
-    mova                 [dstq+2*sizeq   ], xm0                  ; copy to dst
-    mova                 [dstq+2*sizeq+16], xm1
-%if cpuflag(avx2)
-    vperm2i128                          m0, m0, m1, q0301
-    mova                 [dstq+2*sizeq+32], m0
-%endif
-    add     sizeq, mmsize
-    jl .loop
-    RET
-%endmacro
-
-INIT_XMM sse2
-REORDER_PIXELS
-
-%if HAVE_AVX2_EXTERNAL
-INIT_YMM avx2
-REORDER_PIXELS
-%endif
-
-
-;------------------------------------------------------------------------------
-; void ff_predictor(uint8_t *src, ptrdiff_t size);
-;------------------------------------------------------------------------------
-
-%macro PREDICTOR 0
-cglobal predictor, 2,2,5, src, size
-    mova             m0, [pb_80]
-    mova            xm1, [pb_15]
-    mova            xm2, xm0
-    add            srcq, sizeq
-    neg           sizeq
-.loop:
-    pxor             m3, m0, [srcq + sizeq]
-    pslldq           m4, m3, 1
-    paddb            m3, m4
-    pslldq           m4, m3, 2
-    paddb            m3, m4
-    pslldq           m4, m3, 4
-    paddb            m3, m4
-    pslldq           m4, m3, 8
-%if mmsize == 32
-    paddb            m3, m4
-    paddb           xm2, xm3
-    vextracti128    xm4, m3, 1
-    mova [srcq + sizeq], xm2
-    pshufb          xm2, xm1
-    paddb           xm2, xm4
-    mova [srcq + sizeq + 16], xm2
-%else
-    paddb            m2, m3
-    paddb            m2, m4
-    mova [srcq + sizeq], m2
-%endif
-    pshufb          xm2, xm1
-    add           sizeq, mmsize
-    jl .loop
-    RET
-%endmacro
-
-INIT_XMM ssse3
-PREDICTOR
-
-INIT_XMM avx
-PREDICTOR
-
-%if HAVE_AVX2_EXTERNAL
-INIT_YMM avx2
-PREDICTOR
-%endif
diff -uparN ffmpeg-4.1/libavcodec/x86/fft.asm ffmpeg-y/libavcodec/x86/fft.asm
--- ffmpeg-4.1/libavcodec/x86/fft.asm	2018-11-03 08:17:29.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/fft.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,1085 +0,0 @@
-;******************************************************************************
-;* FFT transform with SSE/3DNow optimizations
-;* Copyright (c) 2008 Loren Merritt
-;* Copyright (c) 2011 Vitor Sessak
-;*
-;* This algorithm (though not any of the implementation details) is
-;* based on libdjbfft by D. J. Bernstein.
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-; These functions are not individually interchangeable with the C versions.
-; While C takes arrays of FFTComplex, SSE/3DNow leave intermediate results
-; in blocks as conventient to the vector size.
-; i.e. {4x real, 4x imaginary, 4x real, ...} (or 2x respectively)
-
-%include "libavutil/x86/x86util.asm"
-
-%if ARCH_X86_64
-%define pointer resq
-%else
-%define pointer resd
-%endif
-
-struc FFTContext
-    .nbits:    resd 1
-    .reverse:  resd 1
-    .revtab:   pointer 1
-    .tmpbuf:   pointer 1
-    .mdctsize: resd 1
-    .mdctbits: resd 1
-    .tcos:     pointer 1
-    .tsin:     pointer 1
-    .fftperm:  pointer 1
-    .fftcalc:  pointer 1
-    .imdctcalc:pointer 1
-    .imdcthalf:pointer 1
-endstruc
-
-SECTION_RODATA 32
-
-%define M_SQRT1_2 0.70710678118654752440
-%define M_COS_PI_1_8 0.923879532511287
-%define M_COS_PI_3_8 0.38268343236509
-
-ps_cos16_1: dd 1.0, M_COS_PI_1_8, M_SQRT1_2, M_COS_PI_3_8, 1.0, M_COS_PI_1_8, M_SQRT1_2, M_COS_PI_3_8
-ps_cos16_2: dd 0, M_COS_PI_3_8, M_SQRT1_2, M_COS_PI_1_8, 0, -M_COS_PI_3_8, -M_SQRT1_2, -M_COS_PI_1_8
-
-ps_root2: times 8 dd M_SQRT1_2
-ps_root2mppm: dd -M_SQRT1_2, M_SQRT1_2, M_SQRT1_2, -M_SQRT1_2, -M_SQRT1_2, M_SQRT1_2, M_SQRT1_2, -M_SQRT1_2
-ps_p1p1m1p1: dd 0, 0, 1<<31, 0, 0, 0, 1<<31, 0
-
-perm1: dd 0x00, 0x02, 0x03, 0x01, 0x03, 0x00, 0x02, 0x01
-perm2: dd 0x00, 0x01, 0x02, 0x03, 0x01, 0x00, 0x02, 0x03
-ps_p1p1m1p1root2: dd 1.0, 1.0, -1.0, 1.0, M_SQRT1_2, M_SQRT1_2, M_SQRT1_2, M_SQRT1_2
-ps_m1m1p1m1p1m1m1m1: dd 1<<31, 1<<31, 0, 1<<31, 0, 1<<31, 1<<31, 1<<31
-ps_m1p1: dd 1<<31, 0
-
-cextern ps_neg
-
-%assign i 16
-%rep 14
-cextern cos_ %+ i
-%assign i i<<1
-%endrep
-
-%if ARCH_X86_64
-    %define pointer dq
-%else
-    %define pointer dd
-%endif
-
-%macro IF0 1+
-%endmacro
-%macro IF1 1+
-    %1
-%endmacro
-
-SECTION .text
-
-%macro T2_3DNOW 4 ; z0, z1, mem0, mem1
-    mova     %1, %3
-    mova     %2, %1
-    pfadd    %1, %4
-    pfsub    %2, %4
-%endmacro
-
-%macro T4_3DNOW 6 ; z0, z1, z2, z3, tmp0, tmp1
-    mova     %5, %3
-    pfsub    %3, %4
-    pfadd    %5, %4 ; {t6,t5}
-    pxor     %3, [ps_m1p1] ; {t8,t7}
-    mova     %6, %1
-    movd [r0+12], %3
-    punpckhdq %3, [r0+8]
-    pfadd    %1, %5 ; {r0,i0}
-    pfsub    %6, %5 ; {r2,i2}
-    mova     %4, %2
-    pfadd    %2, %3 ; {r1,i1}
-    pfsub    %4, %3 ; {r3,i3}
-    SWAP     %3, %6
-%endmacro
-
-;  in: %1 = {r0,i0,r2,i2,r4,i4,r6,i6}
-;      %2 = {r1,i1,r3,i3,r5,i5,r7,i7}
-;      %3, %4, %5 tmp
-; out: %1 = {r0,r1,r2,r3,i0,i1,i2,i3}
-;      %2 = {r4,r5,r6,r7,i4,i5,i6,i7}
-%macro T8_AVX 5
-    vsubps     %5, %1, %2       ; v  = %1 - %2
-    vaddps     %3, %1, %2       ; w  = %1 + %2
-    vmulps     %2, %5, [ps_p1p1m1p1root2]  ; v *= vals1
-    vpermilps  %2, %2, [perm1]
-    vblendps   %1, %2, %3, 0x33 ; q = {w1,w2,v4,v2,w5,w6,v7,v6}
-    vshufps    %5, %3, %2, 0x4e ; r = {w3,w4,v1,v3,w7,w8,v8,v5}
-    vsubps     %4, %5, %1       ; s = r - q
-    vaddps     %1, %5, %1       ; u = r + q
-    vpermilps  %1, %1, [perm2]  ; k  = {u1,u2,u3,u4,u6,u5,u7,u8}
-    vshufps    %5, %4, %1, 0xbb
-    vshufps    %3, %4, %1, 0xee
-    vperm2f128 %3, %3, %5, 0x13
-    vxorps     %4, %4, [ps_m1m1p1m1p1m1m1m1]  ; s *= {1,1,-1,-1,1,-1,-1,-1}
-    vshufps    %2, %1, %4, 0xdd
-    vshufps    %1, %1, %4, 0x88
-    vperm2f128 %4, %2, %1, 0x02 ; v  = {k1,k3,s1,s3,k2,k4,s2,s4}
-    vperm2f128 %1, %1, %2, 0x13 ; w  = {k6,k8,s6,s8,k5,k7,s5,s7}
-    vsubps     %5, %1, %3
-    vblendps   %1, %5, %1, 0x55 ; w -= {0,s7,0,k7,0,s8,0,k8}
-    vsubps     %2, %4, %1       ; %2 = v - w
-    vaddps     %1, %4, %1       ; %1 = v + w
-%endmacro
-
-; In SSE mode do one fft4 transforms
-; in:  %1={r0,i0,r2,i2} %2={r1,i1,r3,i3}
-; out: %1={r0,r1,r2,r3} %2={i0,i1,i2,i3}
-;
-; In AVX mode do two fft4 transforms
-; in:  %1={r0,i0,r2,i2,r4,i4,r6,i6} %2={r1,i1,r3,i3,r5,i5,r7,i7}
-; out: %1={r0,r1,r2,r3,r4,r5,r6,r7} %2={i0,i1,i2,i3,i4,i5,i6,i7}
-%macro T4_SSE 3
-    subps    %3, %1, %2       ; {t3,t4,-t8,t7}
-    addps    %1, %1, %2       ; {t1,t2,t6,t5}
-    xorps    %3, %3, [ps_p1p1m1p1]
-    shufps   %2, %1, %3, 0xbe ; {t6,t5,t7,t8}
-    shufps   %1, %1, %3, 0x44 ; {t1,t2,t3,t4}
-    subps    %3, %1, %2       ; {r2,i2,r3,i3}
-    addps    %1, %1, %2       ; {r0,i0,r1,i1}
-    shufps   %2, %1, %3, 0xdd ; {i0,i1,i2,i3}
-    shufps   %1, %1, %3, 0x88 ; {r0,r1,r2,r3}
-%endmacro
-
-; In SSE mode do one FFT8
-; in:  %1={r0,r1,r2,r3} %2={i0,i1,i2,i3} %3={r4,i4,r6,i6} %4={r5,i5,r7,i7}
-; out: %1={r0,r1,r2,r3} %2={i0,i1,i2,i3} %1={r4,r5,r6,r7} %2={i4,i5,i6,i7}
-;
-; In AVX mode do two FFT8
-; in:  %1={r0,i0,r2,i2,r8, i8, r10,i10} %2={r1,i1,r3,i3,r9, i9, r11,i11}
-;      %3={r4,i4,r6,i6,r12,i12,r14,i14} %4={r5,i5,r7,i7,r13,i13,r15,i15}
-; out: %1={r0,r1,r2,r3,r8, r9, r10,r11} %2={i0,i1,i2,i3,i8, i9, i10,i11}
-;      %3={r4,r5,r6,r7,r12,r13,r14,r15} %4={i4,i5,i6,i7,i12,i13,i14,i15}
-%macro T8_SSE 6
-    addps    %6, %3, %4       ; {t1,t2,t3,t4}
-    subps    %3, %3, %4       ; {r5,i5,r7,i7}
-    shufps   %4, %3, %3, 0xb1 ; {i5,r5,i7,r7}
-    mulps    %3, %3, [ps_root2mppm] ; {-r5,i5,r7,-i7}
-    mulps    %4, %4, [ps_root2]
-    addps    %3, %3, %4       ; {t8,t7,ta,t9}
-    shufps   %4, %6, %3, 0x9c ; {t1,t4,t7,ta}
-    shufps   %6, %6, %3, 0x36 ; {t3,t2,t9,t8}
-    subps    %3, %6, %4       ; {t6,t5,tc,tb}
-    addps    %6, %6, %4       ; {t1,t2,t9,ta}
-    shufps   %5, %6, %3, 0x8d ; {t2,ta,t6,tc}
-    shufps   %6, %6, %3, 0xd8 ; {t1,t9,t5,tb}
-    subps    %3, %1, %6       ; {r4,r5,r6,r7}
-    addps    %1, %1, %6       ; {r0,r1,r2,r3}
-    subps    %4, %2, %5       ; {i4,i5,i6,i7}
-    addps    %2, %2, %5       ; {i0,i1,i2,i3}
-%endmacro
-
-%macro INTERL 5
-%if cpuflag(avx)
-    vunpckhps      %3, %2, %1
-    vunpcklps      %2, %2, %1
-    vextractf128   %4(%5), %2, 0
-    vextractf128  %4 %+ H(%5), %3, 0
-    vextractf128   %4(%5 + 1), %2, 1
-    vextractf128  %4 %+ H(%5 + 1), %3, 1
-%elif cpuflag(sse) || cpuflag(3dnow)
-    mova     %3, %2
-    unpcklps %2, %1
-    unpckhps %3, %1
-    mova  %4(%5), %2
-    mova  %4(%5+1), %3
-%endif
-%endmacro
-
-; scheduled for cpu-bound sizes
-%macro PASS_SMALL 3 ; (to load m4-m7), wre, wim
-IF%1 mova    m4, Z(4)
-IF%1 mova    m5, Z(5)
-    mova     m0, %2 ; wre
-    mova     m1, %3 ; wim
-    mulps    m2, m4, m0 ; r2*wre
-IF%1 mova    m6, Z2(6)
-    mulps    m3, m5, m1 ; i2*wim
-IF%1 mova    m7, Z2(7)
-    mulps    m4, m4, m1 ; r2*wim
-    mulps    m5, m5, m0 ; i2*wre
-    addps    m2, m2, m3 ; r2*wre + i2*wim
-    mulps    m3, m1, m7 ; i3*wim
-    subps    m5, m5, m4 ; i2*wre - r2*wim
-    mulps    m1, m1, m6 ; r3*wim
-    mulps    m4, m0, m6 ; r3*wre
-    mulps    m0, m0, m7 ; i3*wre
-    subps    m4, m4, m3 ; r3*wre - i3*wim
-    mova     m3, Z(0)
-    addps    m0, m0, m1 ; i3*wre + r3*wim
-    subps    m1, m4, m2 ; t3
-    addps    m4, m4, m2 ; t5
-    subps    m3, m3, m4 ; r2
-    addps    m4, m4, Z(0) ; r0
-    mova     m6, Z(2)
-    mova   Z(4), m3
-    mova   Z(0), m4
-    subps    m3, m5, m0 ; t4
-    subps    m4, m6, m3 ; r3
-    addps    m3, m3, m6 ; r1
-    mova  Z2(6), m4
-    mova   Z(2), m3
-    mova     m2, Z(3)
-    addps    m3, m5, m0 ; t6
-    subps    m2, m2, m1 ; i3
-    mova     m7, Z(1)
-    addps    m1, m1, Z(3) ; i1
-    mova  Z2(7), m2
-    mova   Z(3), m1
-    subps    m4, m7, m3 ; i2
-    addps    m3, m3, m7 ; i0
-    mova   Z(5), m4
-    mova   Z(1), m3
-%endmacro
-
-; scheduled to avoid store->load aliasing
-%macro PASS_BIG 1 ; (!interleave)
-    mova     m4, Z(4) ; r2
-    mova     m5, Z(5) ; i2
-    mova     m0, [wq] ; wre
-    mova     m1, [wq+o1q] ; wim
-    mulps    m2, m4, m0 ; r2*wre
-    mova     m6, Z2(6) ; r3
-    mulps    m3, m5, m1 ; i2*wim
-    mova     m7, Z2(7) ; i3
-    mulps    m4, m4, m1 ; r2*wim
-    mulps    m5, m5, m0 ; i2*wre
-    addps    m2, m2, m3 ; r2*wre + i2*wim
-    mulps    m3, m1, m7 ; i3*wim
-    mulps    m1, m1, m6 ; r3*wim
-    subps    m5, m5, m4 ; i2*wre - r2*wim
-    mulps    m4, m0, m6 ; r3*wre
-    mulps    m0, m0, m7 ; i3*wre
-    subps    m4, m4, m3 ; r3*wre - i3*wim
-    mova     m3, Z(0)
-    addps    m0, m0, m1 ; i3*wre + r3*wim
-    subps    m1, m4, m2 ; t3
-    addps    m4, m4, m2 ; t5
-    subps    m3, m3, m4 ; r2
-    addps    m4, m4, Z(0) ; r0
-    mova     m6, Z(2)
-    mova   Z(4), m3
-    mova   Z(0), m4
-    subps    m3, m5, m0 ; t4
-    subps    m4, m6, m3 ; r3
-    addps    m3, m3, m6 ; r1
-IF%1 mova Z2(6), m4
-IF%1 mova  Z(2), m3
-    mova     m2, Z(3)
-    addps    m5, m5, m0 ; t6
-    subps    m2, m2, m1 ; i3
-    mova     m7, Z(1)
-    addps    m1, m1, Z(3) ; i1
-IF%1 mova Z2(7), m2
-IF%1 mova  Z(3), m1
-    subps    m6, m7, m5 ; i2
-    addps    m5, m5, m7 ; i0
-IF%1 mova  Z(5), m6
-IF%1 mova  Z(1), m5
-%if %1==0
-    INTERL m1, m3, m7, Z, 2
-    INTERL m2, m4, m0, Z2, 6
-
-    mova     m1, Z(0)
-    mova     m2, Z(4)
-
-    INTERL m5, m1, m3, Z, 0
-    INTERL m6, m2, m7, Z, 4
-%endif
-%endmacro
-
-%macro PUNPCK 3
-    mova      %3, %1
-    punpckldq %1, %2
-    punpckhdq %3, %2
-%endmacro
-
-%define Z(x) [r0+mmsize*x]
-%define Z2(x) [r0+mmsize*x]
-%define ZH(x) [r0+mmsize*x+mmsize/2]
-
-INIT_YMM avx
-
-%if HAVE_AVX_EXTERNAL
-align 16
-fft8_avx:
-    mova      m0, Z(0)
-    mova      m1, Z(1)
-    T8_AVX    m0, m1, m2, m3, m4
-    mova      Z(0), m0
-    mova      Z(1), m1
-    ret
-
-
-align 16
-fft16_avx:
-    mova       m2, Z(2)
-    mova       m3, Z(3)
-    T4_SSE     m2, m3, m7
-
-    mova       m0, Z(0)
-    mova       m1, Z(1)
-    T8_AVX     m0, m1, m4, m5, m7
-
-    mova       m4, [ps_cos16_1]
-    mova       m5, [ps_cos16_2]
-    vmulps     m6, m2, m4
-    vmulps     m7, m3, m5
-    vaddps     m7, m7, m6
-    vmulps     m2, m2, m5
-    vmulps     m3, m3, m4
-    vsubps     m3, m3, m2
-    vblendps   m2, m7, m3, 0xf0
-    vperm2f128 m3, m7, m3, 0x21
-    vaddps     m4, m2, m3
-    vsubps     m2, m3, m2
-    vperm2f128 m2, m2, m2, 0x01
-    vsubps     m3, m1, m2
-    vaddps     m1, m1, m2
-    vsubps     m5, m0, m4
-    vaddps     m0, m0, m4
-    vextractf128   Z(0), m0, 0
-    vextractf128  ZH(0), m1, 0
-    vextractf128   Z(1), m0, 1
-    vextractf128  ZH(1), m1, 1
-    vextractf128   Z(2), m5, 0
-    vextractf128  ZH(2), m3, 0
-    vextractf128   Z(3), m5, 1
-    vextractf128  ZH(3), m3, 1
-    ret
-
-align 16
-fft32_avx:
-    call fft16_avx
-
-    mova m0, Z(4)
-    mova m1, Z(5)
-
-    T4_SSE      m0, m1, m4
-
-    mova m2, Z(6)
-    mova m3, Z(7)
-
-    T8_SSE      m0, m1, m2, m3, m4, m6
-    ; m0={r0,r1,r2,r3,r8, r9, r10,r11} m1={i0,i1,i2,i3,i8, i9, i10,i11}
-    ; m2={r4,r5,r6,r7,r12,r13,r14,r15} m3={i4,i5,i6,i7,i12,i13,i14,i15}
-
-    vperm2f128  m4, m0, m2, 0x20
-    vperm2f128  m5, m1, m3, 0x20
-    vperm2f128  m6, m0, m2, 0x31
-    vperm2f128  m7, m1, m3, 0x31
-
-    PASS_SMALL 0, [cos_32], [cos_32+32]
-
-    ret
-
-fft32_interleave_avx:
-    call fft32_avx
-    mov r2d, 32
-.deint_loop:
-    mova     m2, Z(0)
-    mova     m3, Z(1)
-    vunpcklps      m0, m2, m3
-    vunpckhps      m1, m2, m3
-    vextractf128   Z(0), m0, 0
-    vextractf128  ZH(0), m1, 0
-    vextractf128   Z(1), m0, 1
-    vextractf128  ZH(1), m1, 1
-    add r0, mmsize*2
-    sub r2d, mmsize/4
-    jg .deint_loop
-    ret
-
-%endif
-
-INIT_XMM sse
-
-align 16
-fft4_avx:
-fft4_sse:
-    mova     m0, Z(0)
-    mova     m1, Z(1)
-    T4_SSE   m0, m1, m2
-    mova   Z(0), m0
-    mova   Z(1), m1
-    ret
-
-align 16
-fft8_sse:
-    mova     m0, Z(0)
-    mova     m1, Z(1)
-    T4_SSE   m0, m1, m2
-    mova     m2, Z(2)
-    mova     m3, Z(3)
-    T8_SSE   m0, m1, m2, m3, m4, m5
-    mova   Z(0), m0
-    mova   Z(1), m1
-    mova   Z(2), m2
-    mova   Z(3), m3
-    ret
-
-align 16
-fft16_sse:
-    mova     m0, Z(0)
-    mova     m1, Z(1)
-    T4_SSE   m0, m1, m2
-    mova     m2, Z(2)
-    mova     m3, Z(3)
-    T8_SSE   m0, m1, m2, m3, m4, m5
-    mova     m4, Z(4)
-    mova     m5, Z(5)
-    mova   Z(0), m0
-    mova   Z(1), m1
-    mova   Z(2), m2
-    mova   Z(3), m3
-    T4_SSE   m4, m5, m6
-    mova     m6, Z2(6)
-    mova     m7, Z2(7)
-    T4_SSE   m6, m7, m0
-    PASS_SMALL 0, [cos_16], [cos_16+16]
-    ret
-
-
-%macro FFT48_3DNOW 0
-align 16
-fft4 %+ SUFFIX:
-    T2_3DNOW m0, m1, Z(0), Z(1)
-    mova     m2, Z(2)
-    mova     m3, Z(3)
-    T4_3DNOW m0, m1, m2, m3, m4, m5
-    PUNPCK   m0, m1, m4
-    PUNPCK   m2, m3, m5
-    mova   Z(0), m0
-    mova   Z(1), m4
-    mova   Z(2), m2
-    mova   Z(3), m5
-    ret
-
-align 16
-fft8 %+ SUFFIX:
-    T2_3DNOW m0, m1, Z(0), Z(1)
-    mova     m2, Z(2)
-    mova     m3, Z(3)
-    T4_3DNOW m0, m1, m2, m3, m4, m5
-    mova   Z(0), m0
-    mova   Z(2), m2
-    T2_3DNOW m4, m5,  Z(4),  Z(5)
-    T2_3DNOW m6, m7, Z2(6), Z2(7)
-    PSWAPD   m0, m5
-    PSWAPD   m2, m7
-    pxor     m0, [ps_m1p1]
-    pxor     m2, [ps_m1p1]
-    pfsub    m5, m0
-    pfadd    m7, m2
-    pfmul    m5, [ps_root2]
-    pfmul    m7, [ps_root2]
-    T4_3DNOW m1, m3, m5, m7, m0, m2
-    mova   Z(5), m5
-    mova  Z2(7), m7
-    mova     m0, Z(0)
-    mova     m2, Z(2)
-    T4_3DNOW m0, m2, m4, m6, m5, m7
-    PUNPCK   m0, m1, m5
-    PUNPCK   m2, m3, m7
-    mova   Z(0), m0
-    mova   Z(1), m5
-    mova   Z(2), m2
-    mova   Z(3), m7
-    PUNPCK   m4,  Z(5), m5
-    PUNPCK   m6, Z2(7), m7
-    mova   Z(4), m4
-    mova   Z(5), m5
-    mova  Z2(6), m6
-    mova  Z2(7), m7
-    ret
-%endmacro
-
-%if ARCH_X86_32
-INIT_MMX 3dnowext
-FFT48_3DNOW
-
-INIT_MMX 3dnow
-FFT48_3DNOW
-%endif
-
-%define Z(x) [zcq + o1q*(x&6) + mmsize*(x&1)]
-%define Z2(x) [zcq + o3q + mmsize*(x&1)]
-%define ZH(x) [zcq + o1q*(x&6) + mmsize*(x&1) + mmsize/2]
-%define Z2H(x) [zcq + o3q + mmsize*(x&1) + mmsize/2]
-
-%macro DECL_PASS 2+ ; name, payload
-align 16
-%1:
-DEFINE_ARGS zc, w, n, o1, o3
-    lea o3q, [nq*3]
-    lea o1q, [nq*8]
-    shl o3q, 4
-.loop:
-    %2
-    add zcq, mmsize*2
-    add  wq, mmsize
-    sub  nd, mmsize/8
-    jg .loop
-    rep ret
-%endmacro
-
-%macro FFT_DISPATCH 2; clobbers 5 GPRs, 8 XMMs
-    lea r2, [dispatch_tab%1]
-    mov r2, [r2 + (%2q-2)*gprsize]
-%ifdef PIC
-    lea r3, [$$]
-    add r2, r3
-%endif
-    call r2
-%endmacro ; FFT_DISPATCH
-
-INIT_YMM avx
-
-%if HAVE_AVX_EXTERNAL
-DECL_PASS pass_avx, PASS_BIG 1
-DECL_PASS pass_interleave_avx, PASS_BIG 0
-
-cglobal fft_calc, 2,5,8
-    mov     r3d, [r0 + FFTContext.nbits]
-    mov     r0, r1
-    mov     r1, r3
-    FFT_DISPATCH _interleave %+ SUFFIX, r1
-    REP_RET
-
-%endif
-
-INIT_XMM sse
-
-DECL_PASS pass_sse, PASS_BIG 1
-DECL_PASS pass_interleave_sse, PASS_BIG 0
-
-%macro FFT_CALC_FUNC 0
-cglobal fft_calc, 2,5,8
-    mov     r3d, [r0 + FFTContext.nbits]
-    PUSH    r1
-    PUSH    r3
-    mov     r0, r1
-    mov     r1, r3
-    FFT_DISPATCH _interleave %+ SUFFIX, r1
-    POP     rcx
-    POP     r4
-    cmp     rcx, 3+(mmsize/16)
-    jg      .end
-    mov     r2, -1
-    add     rcx, 3
-    shl     r2, cl
-    sub     r4, r2
-.loop:
-%if mmsize == 8
-    PSWAPD  m0, [r4 + r2 + 4]
-    mova [r4 + r2 + 4], m0
-%else
-    movaps   xmm0, [r4 + r2]
-    movaps   xmm1, xmm0
-    unpcklps xmm0, [r4 + r2 + 16]
-    unpckhps xmm1, [r4 + r2 + 16]
-    movaps   [r4 + r2],      xmm0
-    movaps   [r4 + r2 + 16], xmm1
-%endif
-    add      r2, mmsize*2
-    jl       .loop
-.end:
-%if cpuflag(3dnow)
-    femms
-    RET
-%else
-    REP_RET
-%endif
-%endmacro
-
-%if ARCH_X86_32
-INIT_MMX 3dnow
-FFT_CALC_FUNC
-INIT_MMX 3dnowext
-FFT_CALC_FUNC
-%endif
-INIT_XMM sse
-FFT_CALC_FUNC
-
-cglobal fft_permute, 2,7,1
-    mov     r4,  [r0 + FFTContext.revtab]
-    mov     r5,  [r0 + FFTContext.tmpbuf]
-    mov     ecx, [r0 + FFTContext.nbits]
-    mov     r2, 1
-    shl     r2, cl
-    xor     r0, r0
-%if ARCH_X86_32
-    mov     r1, r1m
-%endif
-.loop:
-    movaps  xmm0, [r1 + 8*r0]
-    movzx   r6, word [r4 + 2*r0]
-    movzx   r3, word [r4 + 2*r0 + 2]
-    movlps  [r5 + 8*r6], xmm0
-    movhps  [r5 + 8*r3], xmm0
-    add     r0, 2
-    cmp     r0, r2
-    jl      .loop
-    shl     r2, 3
-    add     r1, r2
-    add     r5, r2
-    neg     r2
-; nbits >= 2 (FFT4) and sizeof(FFTComplex)=8 => at least 32B
-.loopcopy:
-    movaps  xmm0, [r5 + r2]
-    movaps  xmm1, [r5 + r2 + 16]
-    movaps  [r1 + r2], xmm0
-    movaps  [r1 + r2 + 16], xmm1
-    add     r2, 32
-    jl      .loopcopy
-    REP_RET
-
-%macro IMDCT_CALC_FUNC 0
-cglobal imdct_calc, 3,5,3
-    mov     r3d, [r0 + FFTContext.mdctsize]
-    mov     r4,  [r0 + FFTContext.imdcthalf]
-    add     r1,  r3
-    PUSH    r3
-    PUSH    r1
-%if ARCH_X86_32
-    push    r2
-    push    r1
-    push    r0
-%else
-    sub     rsp, 8+32*WIN64 ; allocate win64 shadow space
-%endif
-    call    r4
-%if ARCH_X86_32
-    add     esp, 12
-%else
-    add     rsp, 8+32*WIN64
-%endif
-    POP     r1
-    POP     r3
-    lea     r0, [r1 + 2*r3]
-    mov     r2, r3
-    sub     r3, mmsize
-    neg     r2
-    mova    m2, [ps_neg]
-.loop:
-%if mmsize == 8
-    PSWAPD  m0, [r1 + r3]
-    PSWAPD  m1, [r0 + r2]
-    pxor    m0, m2
-%else
-    mova    m0, [r1 + r3]
-    mova    m1, [r0 + r2]
-    shufps  m0, m0, 0x1b
-    shufps  m1, m1, 0x1b
-    xorps   m0, m2
-%endif
-    mova [r0 + r3], m1
-    mova [r1 + r2], m0
-    sub     r3, mmsize
-    add     r2, mmsize
-    jl      .loop
-%if cpuflag(3dnow)
-    femms
-    RET
-%else
-    REP_RET
-%endif
-%endmacro
-
-%if ARCH_X86_32
-INIT_MMX 3dnow
-IMDCT_CALC_FUNC
-INIT_MMX 3dnowext
-IMDCT_CALC_FUNC
-%endif
-
-INIT_XMM sse
-IMDCT_CALC_FUNC
-
-%if ARCH_X86_32
-INIT_MMX 3dnow
-%define mulps pfmul
-%define addps pfadd
-%define subps pfsub
-%define unpcklps punpckldq
-%define unpckhps punpckhdq
-DECL_PASS pass_3dnow, PASS_SMALL 1, [wq], [wq+o1q]
-DECL_PASS pass_interleave_3dnow, PASS_BIG 0
-%define pass_3dnowext pass_3dnow
-%define pass_interleave_3dnowext pass_interleave_3dnow
-%endif
-
-%ifdef PIC
-%define SECTION_REL - $$
-%else
-%define SECTION_REL
-%endif
-
-%macro DECL_FFT 1-2 ; nbits, suffix
-%ifidn %0, 1
-%xdefine fullsuffix SUFFIX
-%else
-%xdefine fullsuffix %2 %+ SUFFIX
-%endif
-%xdefine list_of_fft fft4 %+ SUFFIX SECTION_REL, fft8 %+ SUFFIX SECTION_REL
-%if %1>=5
-%xdefine list_of_fft list_of_fft, fft16 %+ SUFFIX SECTION_REL
-%endif
-%if %1>=6
-%xdefine list_of_fft list_of_fft, fft32 %+ fullsuffix SECTION_REL
-%endif
-
-%assign n 1<<%1
-%rep 18-%1
-%assign n2 n/2
-%assign n4 n/4
-%xdefine list_of_fft list_of_fft, fft %+ n %+ fullsuffix SECTION_REL
-
-align 16
-fft %+ n %+ fullsuffix:
-    call fft %+ n2 %+ SUFFIX
-    add r0, n*4 - (n&(-2<<%1))
-    call fft %+ n4 %+ SUFFIX
-    add r0, n*2 - (n2&(-2<<%1))
-    call fft %+ n4 %+ SUFFIX
-    sub r0, n*6 + (n2&(-2<<%1))
-    lea r1, [cos_ %+ n]
-    mov r2d, n4/2
-    jmp pass %+ fullsuffix
-
-%assign n n*2
-%endrep
-%undef n
-
-align 8
-dispatch_tab %+ fullsuffix: pointer list_of_fft
-%endmacro ; DECL_FFT
-
-%if HAVE_AVX_EXTERNAL
-INIT_YMM avx
-DECL_FFT 6
-DECL_FFT 6, _interleave
-%endif
-INIT_XMM sse
-DECL_FFT 5
-DECL_FFT 5, _interleave
-%if ARCH_X86_32
-INIT_MMX 3dnow
-DECL_FFT 4
-DECL_FFT 4, _interleave
-INIT_MMX 3dnowext
-DECL_FFT 4
-DECL_FFT 4, _interleave
-%endif
-
-INIT_XMM sse
-%undef mulps
-%undef addps
-%undef subps
-%undef unpcklps
-%undef unpckhps
-
-%macro PREROTATER 5 ;-2*k, 2*k, input+n4, tcos+n8, tsin+n8
-%if mmsize == 8 ; j*2+2-n4, n4-2-j*2, input+n4, tcos+n8, tsin+n8
-    PSWAPD     m0, [%3+%2*4]
-    movq       m2, [%3+%1*4-8]
-    movq       m3, m0
-    punpckldq  m0, m2
-    punpckhdq  m2, m3
-    movd       m1, [%4+%1*2-4] ; tcos[j]
-    movd       m3, [%4+%2*2]   ; tcos[n4-j-1]
-    punpckldq  m1, [%5+%1*2-4] ; tsin[j]
-    punpckldq  m3, [%5+%2*2]   ; tsin[n4-j-1]
-
-    mova       m4, m0
-    PSWAPD     m5, m1
-    pfmul      m0, m1
-    pfmul      m4, m5
-    mova       m6, m2
-    PSWAPD     m5, m3
-    pfmul      m2, m3
-    pfmul      m6, m5
-%if cpuflag(3dnowext)
-    pfpnacc    m0, m4
-    pfpnacc    m2, m6
-%else
-    SBUTTERFLY dq, 0, 4, 1
-    SBUTTERFLY dq, 2, 6, 3
-    pxor       m4, m7
-    pxor       m6, m7
-    pfadd      m0, m4
-    pfadd      m2, m6
-%endif
-%else
-    movaps   xmm0, [%3+%2*4]
-    movaps   xmm1, [%3+%1*4-0x10]
-    movaps   xmm2, xmm0
-    shufps   xmm0, xmm1, 0x88
-    shufps   xmm1, xmm2, 0x77
-    movlps   xmm4, [%4+%2*2]
-    movlps   xmm5, [%5+%2*2+0x0]
-    movhps   xmm4, [%4+%1*2-0x8]
-    movhps   xmm5, [%5+%1*2-0x8]
-    movaps   xmm2, xmm0
-    movaps   xmm3, xmm1
-    mulps    xmm0, xmm5
-    mulps    xmm1, xmm4
-    mulps    xmm2, xmm4
-    mulps    xmm3, xmm5
-    subps    xmm1, xmm0
-    addps    xmm2, xmm3
-    movaps   xmm0, xmm1
-    unpcklps xmm1, xmm2
-    unpckhps xmm0, xmm2
-%endif
-%endmacro
-
-%macro CMUL 6 ;j, xmm0, xmm1, 3, 4, 5
-%if cpuflag(sse)
-    mulps      m6, %3, [%5+%1]
-    mulps      m7, %2, [%5+%1]
-    mulps      %2, %2, [%6+%1]
-    mulps      %3, %3, [%6+%1]
-    subps      %2, %2, m6
-    addps      %3, %3, m7
-%elif cpuflag(3dnow)
-    mova       m6, [%1+%2*2]
-    mova       %3, [%1+%2*2+8]
-    mova       %4, m6
-    mova       m7, %3
-    pfmul      m6, [%5+%2]
-    pfmul      %3, [%6+%2]
-    pfmul      %4, [%6+%2]
-    pfmul      m7, [%5+%2]
-    pfsub      %3, m6
-    pfadd      %4, m7
-%endif
-%endmacro
-
-%macro POSROTATESHUF 5 ;j, k, z+n8, tcos+n8, tsin+n8
-.post:
-%if cpuflag(avx)
-    vmovaps      ymm1,   [%3+%1*2]
-    vmovaps      ymm0,   [%3+%1*2+0x20]
-    vmovaps      ymm3,   [%3+%2*2]
-    vmovaps      ymm2,   [%3+%2*2+0x20]
-
-    CMUL         %1, ymm0, ymm1, %3, %4, %5
-    CMUL         %2, ymm2, ymm3, %3, %4, %5
-    vshufps      ymm1, ymm1, ymm1, 0x1b
-    vshufps      ymm3, ymm3, ymm3, 0x1b
-    vperm2f128   ymm1, ymm1, ymm1, 0x01
-    vperm2f128   ymm3, ymm3, ymm3, 0x01
-    vunpcklps    ymm6, ymm2, ymm1
-    vunpckhps    ymm4, ymm2, ymm1
-    vunpcklps    ymm7, ymm0, ymm3
-    vunpckhps    ymm5, ymm0, ymm3
-
-    vextractf128 [%3+%1*2],      ymm7, 0
-    vextractf128 [%3+%1*2+0x10], ymm5, 0
-    vextractf128 [%3+%1*2+0x20], ymm7, 1
-    vextractf128 [%3+%1*2+0x30], ymm5, 1
-
-    vextractf128 [%3+%2*2],      ymm6, 0
-    vextractf128 [%3+%2*2+0x10], ymm4, 0
-    vextractf128 [%3+%2*2+0x20], ymm6, 1
-    vextractf128 [%3+%2*2+0x30], ymm4, 1
-    sub      %2,   0x20
-    add      %1,   0x20
-    jl       .post
-%elif cpuflag(sse)
-    movaps   xmm1, [%3+%1*2]
-    movaps   xmm0, [%3+%1*2+0x10]
-    CMUL     %1,   xmm0, xmm1, %3, %4, %5
-    movaps   xmm5, [%3+%2*2]
-    movaps   xmm4, [%3+%2*2+0x10]
-    CMUL     %2,   xmm4, xmm5, %3, %4, %5
-    shufps   xmm1, xmm1, 0x1b
-    shufps   xmm5, xmm5, 0x1b
-    movaps   xmm6, xmm4
-    unpckhps xmm4, xmm1
-    unpcklps xmm6, xmm1
-    movaps   xmm2, xmm0
-    unpcklps xmm0, xmm5
-    unpckhps xmm2, xmm5
-    movaps   [%3+%2*2],      xmm6
-    movaps   [%3+%2*2+0x10], xmm4
-    movaps   [%3+%1*2],      xmm0
-    movaps   [%3+%1*2+0x10], xmm2
-    sub      %2,   0x10
-    add      %1,   0x10
-    jl       .post
-%elif cpuflag(3dnow)
-    CMUL  %3, %1, m0, m1, %4, %5
-    CMUL  %3, %2, m2, m3, %4, %5
-    movd  [%3+%1*2+ 0], m0
-    movd  [%3+%2*2+12], m1
-    movd  [%3+%2*2+ 0], m2
-    movd  [%3+%1*2+12], m3
-    psrlq      m0, 32
-    psrlq      m1, 32
-    psrlq      m2, 32
-    psrlq      m3, 32
-    movd  [%3+%1*2+ 8], m0
-    movd  [%3+%2*2+ 4], m1
-    movd  [%3+%2*2+ 8], m2
-    movd  [%3+%1*2+ 4], m3
-    sub        %2, 8
-    add        %1, 8
-    jl         .post
-%endif
-%endmacro
-
-%macro DECL_IMDCT 0
-cglobal imdct_half, 3,12,8; FFTContext *s, FFTSample *output, const FFTSample *input
-%if ARCH_X86_64
-%define rrevtab r7
-%define rtcos   r8
-%define rtsin   r9
-%else
-%define rrevtab r6
-%define rtsin   r6
-%define rtcos   r5
-%endif
-    mov   r3d, [r0+FFTContext.mdctsize]
-    add   r2, r3
-    shr   r3, 1
-    mov   rtcos, [r0+FFTContext.tcos]
-    mov   rtsin, [r0+FFTContext.tsin]
-    add   rtcos, r3
-    add   rtsin, r3
-%if ARCH_X86_64 == 0
-    push  rtcos
-    push  rtsin
-%endif
-    shr   r3, 1
-    mov   rrevtab, [r0+FFTContext.revtab]
-    add   rrevtab, r3
-%if ARCH_X86_64 == 0
-    push  rrevtab
-%endif
-
-%if mmsize == 8
-    sub   r3, 2
-%else
-    sub   r3, 4
-%endif
-%if ARCH_X86_64 || mmsize == 8
-    xor   r4, r4
-    sub   r4, r3
-%endif
-%if notcpuflag(3dnowext) && mmsize == 8
-    movd  m7, [ps_neg]
-%endif
-.pre:
-%if ARCH_X86_64 == 0
-;unspill
-%if mmsize != 8
-    xor   r4, r4
-    sub   r4, r3
-%endif
-    mov   rtcos, [esp+8]
-    mov   rtsin, [esp+4]
-%endif
-
-    PREROTATER r4, r3, r2, rtcos, rtsin
-%if mmsize == 8
-    mov    r6, [esp]                ; rrevtab = ptr+n8
-    movzx  r5,  word [rrevtab+r4-2] ; rrevtab[j]
-    movzx  r6,  word [rrevtab+r3]   ; rrevtab[n4-j-1]
-    mova [r1+r5*8], m0
-    mova [r1+r6*8], m2
-    add    r4, 2
-    sub    r3, 2
-%else
-%if ARCH_X86_64
-    movzx  r5,  word [rrevtab+r4-4]
-    movzx  r6,  word [rrevtab+r4-2]
-    movzx  r10, word [rrevtab+r3]
-    movzx  r11, word [rrevtab+r3+2]
-    movlps [r1+r5 *8], xmm0
-    movhps [r1+r6 *8], xmm0
-    movlps [r1+r10*8], xmm1
-    movhps [r1+r11*8], xmm1
-    add    r4, 4
-%else
-    mov    r6, [esp]
-    movzx  r5, word [r6+r4-4]
-    movzx  r4, word [r6+r4-2]
-    movlps [r1+r5*8], xmm0
-    movhps [r1+r4*8], xmm0
-    movzx  r5, word [r6+r3]
-    movzx  r4, word [r6+r3+2]
-    movlps [r1+r5*8], xmm1
-    movhps [r1+r4*8], xmm1
-%endif
-    sub    r3, 4
-%endif
-    jns    .pre
-
-    mov  r5, r0
-    mov  r6, r1
-    mov  r0, r1
-    mov  r1d, [r5+FFTContext.nbits]
-
-    FFT_DISPATCH SUFFIX, r1
-
-    mov  r0d, [r5+FFTContext.mdctsize]
-    add  r6, r0
-    shr  r0, 1
-%if ARCH_X86_64 == 0
-%define rtcos r2
-%define rtsin r3
-    mov  rtcos, [esp+8]
-    mov  rtsin, [esp+4]
-%endif
-    neg  r0
-    mov  r1, -mmsize
-    sub  r1, r0
-    POSROTATESHUF r0, r1, r6, rtcos, rtsin
-%if ARCH_X86_64 == 0
-    add esp, 12
-%endif
-%if mmsize == 8
-    femms
-%endif
-    RET
-%endmacro
-
-DECL_IMDCT
-
-%if ARCH_X86_32
-INIT_MMX 3dnow
-DECL_IMDCT
-
-INIT_MMX 3dnowext
-DECL_IMDCT
-%endif
-
-INIT_YMM avx
-
-%if HAVE_AVX_EXTERNAL
-DECL_IMDCT
-%endif
diff -uparN ffmpeg-4.1/libavcodec/x86/flacdsp.asm ffmpeg-y/libavcodec/x86/flacdsp.asm
--- ffmpeg-4.1/libavcodec/x86/flacdsp.asm	2018-01-01 06:35:48.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/flacdsp.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,313 +0,0 @@
-;******************************************************************************
-;* FLAC DSP SIMD optimizations
-;*
-;* Copyright (C) 2014 Loren Merritt
-;* Copyright (C) 2014 James Almer
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION .text
-
-%macro PMACSDQL 5
-%if cpuflag(xop)
-    pmacsdql %1, %2, %3, %1
-%else
-    pmuldq   %2, %3
-    paddq    %1, %2
-%endif
-%endmacro
-
-%macro LPC_32 1
-INIT_XMM %1
-cglobal flac_lpc_32, 5,6,5, decoded, coeffs, pred_order, qlevel, len, j
-    sub    lend, pred_orderd
-    jle .ret
-    lea    decodedq, [decodedq+pred_orderq*4-8]
-    lea    coeffsq, [coeffsq+pred_orderq*4]
-    neg    pred_orderq
-    movd   m4, qlevelm
-ALIGN 16
-.loop_sample:
-    movd   m0, [decodedq+pred_orderq*4+8]
-    add    decodedq, 8
-    movd   m1, [coeffsq+pred_orderq*4]
-    pxor   m2, m2
-    pxor   m3, m3
-    lea    jq, [pred_orderq+1]
-    test   jq, jq
-    jz .end_order
-.loop_order:
-    PMACSDQL m2, m0, m1, m2, m0
-    movd   m0, [decodedq+jq*4]
-    PMACSDQL m3, m1, m0, m3, m1
-    movd   m1, [coeffsq+jq*4]
-    inc    jq
-    jl .loop_order
-.end_order:
-    PMACSDQL m2, m0, m1, m2, m0
-    psrlq  m2, m4
-    movd   m0, [decodedq]
-    paddd  m0, m2
-    movd   [decodedq], m0
-    sub  lend, 2
-    jl .ret
-    PMACSDQL m3, m1, m0, m3, m1
-    psrlq  m3, m4
-    movd   m1, [decodedq+4]
-    paddd  m1, m3
-    movd   [decodedq+4], m1
-    jg .loop_sample
-.ret:
-    REP_RET
-%endmacro
-
-%if HAVE_XOP_EXTERNAL
-LPC_32 xop
-%endif
-LPC_32 sse4
-
-;----------------------------------------------------------------------------------
-;void ff_flac_decorrelate_[lrm]s_16_sse2(uint8_t **out, int32_t **in, int channels,
-;                                                   int len, int shift);
-;----------------------------------------------------------------------------------
-%macro FLAC_DECORRELATE_16 3-4
-cglobal flac_decorrelate_%1_16, 2, 4, 4, out, in0, in1, len
-%if ARCH_X86_32
-    mov      lend, lenm
-%endif
-    movd       m3, r4m
-    shl      lend, 2
-    mov      in1q, [in0q + gprsize]
-    mov      in0q, [in0q]
-    mov      outq, [outq]
-    add      in1q, lenq
-    add      in0q, lenq
-    add      outq, lenq
-    neg      lenq
-
-align 16
-.loop:
-    mova       m0, [in0q + lenq]
-    mova       m1, [in1q + lenq]
-%ifidn %1, ms
-    psrad      m2, m1, 1
-    psubd      m0, m2
-%endif
-%ifnidn %1, indep2
-    p%4d       m2, m0, m1
-%endif
-    packssdw  m%2, m%2
-    packssdw  m%3, m%3
-    punpcklwd m%2, m%3
-    psllw     m%2, m3
-    mova [outq + lenq], m%2
-    add      lenq, 16
-    jl .loop
-    REP_RET
-%endmacro
-
-INIT_XMM sse2
-FLAC_DECORRELATE_16 ls, 0, 2, sub
-FLAC_DECORRELATE_16 rs, 2, 1, add
-FLAC_DECORRELATE_16 ms, 2, 0, add
-
-;----------------------------------------------------------------------------------
-;void ff_flac_decorrelate_[lrm]s_32_sse2(uint8_t **out, int32_t **in, int channels,
-;                                        int len, int shift);
-;----------------------------------------------------------------------------------
-%macro FLAC_DECORRELATE_32 5
-cglobal flac_decorrelate_%1_32, 2, 4, 4, out, in0, in1, len
-%if ARCH_X86_32
-    mov      lend, lenm
-%endif
-    movd       m3, r4m
-    mov      in1q, [in0q + gprsize]
-    mov      in0q, [in0q]
-    mov      outq, [outq]
-    sub      in1q, in0q
-
-align 16
-.loop:
-    mova       m0, [in0q]
-    mova       m1, [in0q + in1q]
-%ifidn %1, ms
-    psrad      m2, m1, 1
-    psubd      m0, m2
-%endif
-    p%5d       m2, m0, m1
-    pslld     m%2, m3
-    pslld     m%3, m3
-
-    SBUTTERFLY dq, %2, %3, %4
-
-    mova  [outq         ], m%2
-    mova  [outq + mmsize], m%3
-
-    add      in0q, mmsize
-    add      outq, mmsize*2
-    sub      lend, mmsize/4
-    jg .loop
-    REP_RET
-%endmacro
-
-INIT_XMM sse2
-FLAC_DECORRELATE_32 ls, 0, 2, 1, sub
-FLAC_DECORRELATE_32 rs, 2, 1, 0, add
-FLAC_DECORRELATE_32 ms, 2, 0, 1, add
-
-;-----------------------------------------------------------------------------------------
-;void ff_flac_decorrelate_indep<ch>_<bps>_<opt>(uint8_t **out, int32_t **in, int channels,
-;                                            int len, int shift);
-;-----------------------------------------------------------------------------------------
-;%1 = bps
-;%2 = channels
-;%3 = last xmm reg used
-;%4 = word/dword (shift instruction)
-%macro FLAC_DECORRELATE_INDEP 4
-%define REPCOUNT %2/(32/%1) ; 16bits = channels / 2; 32bits = channels
-cglobal flac_decorrelate_indep%2_%1, 2, %2+2, %3+1, out, in0, in1, len, in2, in3, in4, in5, in6, in7
-%if ARCH_X86_32
-%if %2 == 6
-    DEFINE_ARGS out, in0, in1, in2, in3, in4, in5
-    %define  lend  dword r3m
-%else
-    mov      lend, lenm
-%endif
-%endif
-    movd      m%3, r4m
-
-%assign %%i 1
-%rep %2-1
-    mov      in %+ %%i %+ q, [in0q+%%i*gprsize]
-%assign %%i %%i+1
-%endrep
-
-    mov      in0q, [in0q]
-    mov      outq, [outq]
-
-%assign %%i 1
-%rep %2-1
-    sub      in %+ %%i %+ q, in0q
-%assign %%i %%i+1
-%endrep
-
-align 16
-.loop:
-    mova       m0, [in0q]
-
-%assign %%i 1
-%rep REPCOUNT-1
-    mova     m %+ %%i, [in0q + in %+ %%i %+ q]
-%assign %%i %%i+1
-%endrep
-
-%if %1 == 32
-
-%if %2 == 8
-    TRANSPOSE8x4D 0, 1, 2, 3, 4, 5, 6, 7, 8
-%elif %2 == 6
-    SBUTTERFLY dq, 0, 1, 6
-    SBUTTERFLY dq, 2, 3, 6
-    SBUTTERFLY dq, 4, 5, 6
-
-    punpcklqdq m6, m0, m2
-    punpckhqdq m2, m4
-    shufps     m4, m0, 0xe4
-    punpcklqdq m0, m1, m3
-    punpckhqdq m3, m5
-    shufps     m5, m1, 0xe4
-    SWAP 0,6,1,4,5,3
-%elif %2 == 4
-    TRANSPOSE4x4D 0, 1, 2, 3, 4
-%else ; %2 == 2
-    SBUTTERFLY dq, 0, 1, 2
-%endif
-
-%else ; %1 == 16
-
-%if %2 == 8
-    packssdw   m0, [in0q + in4q]
-    packssdw   m1, [in0q + in5q]
-    packssdw   m2, [in0q + in6q]
-    packssdw   m3, [in0q + in7q]
-    TRANSPOSE2x4x4W 0, 1, 2, 3, 4
-%elif %2 == 6
-    packssdw   m0, [in0q + in3q]
-    packssdw   m1, [in0q + in4q]
-    packssdw   m2, [in0q + in5q]
-    pshufd     m3, m0,     q1032
-    punpcklwd  m0, m1
-    punpckhwd  m1, m2
-    punpcklwd  m2, m3
-
-    shufps     m3, m0, m2, q2020
-    shufps     m0, m1,     q2031
-    shufps     m2, m1,     q3131
-    shufps     m1, m2, m3, q3120
-    shufps     m3, m0,     q0220
-    shufps     m0, m2,     q3113
-    SWAP 2, 0, 3
-%else ; %2 == 4
-    packssdw   m0, [in0q + in2q]
-    packssdw   m1, [in0q + in3q]
-    SBUTTERFLY wd, 0, 1, 2
-    SBUTTERFLY dq, 0, 1, 2
-%endif
-
-%endif
-
-%assign %%i 0
-%rep REPCOUNT
-    psll%4   m %+ %%i, m%3
-%assign %%i %%i+1
-%endrep
-
-%assign %%i 0
-%rep REPCOUNT
-    mova [outq + %%i*mmsize], m %+ %%i
-%assign %%i %%i+1
-%endrep
-
-    add      in0q, mmsize
-    add      outq, mmsize*REPCOUNT
-    sub      lend, mmsize/4
-    jg .loop
-    REP_RET
-%endmacro
-
-INIT_XMM sse2
-FLAC_DECORRELATE_16 indep2, 0, 1 ; Reuse stereo 16bits macro
-FLAC_DECORRELATE_INDEP 32, 2, 3, d
-FLAC_DECORRELATE_INDEP 16, 4, 3, w
-FLAC_DECORRELATE_INDEP 32, 4, 5, d
-FLAC_DECORRELATE_INDEP 16, 6, 4, w
-FLAC_DECORRELATE_INDEP 32, 6, 7, d
-%if ARCH_X86_64
-FLAC_DECORRELATE_INDEP 16, 8, 5, w
-FLAC_DECORRELATE_INDEP 32, 8, 9, d
-%endif
-
-INIT_XMM avx
-FLAC_DECORRELATE_INDEP 32, 4, 5, d
-FLAC_DECORRELATE_INDEP 32, 6, 7, d
-%if ARCH_X86_64
-FLAC_DECORRELATE_INDEP 16, 8, 5, w
-FLAC_DECORRELATE_INDEP 32, 8, 9, d
-%endif
diff -uparN ffmpeg-4.1/libavcodec/x86/flac_dsp_gpl.asm ffmpeg-y/libavcodec/x86/flac_dsp_gpl.asm
--- ffmpeg-4.1/libavcodec/x86/flac_dsp_gpl.asm	2018-01-01 06:35:48.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/flac_dsp_gpl.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,101 +0,0 @@
-;******************************************************************************
-;* FLAC DSP functions
-;*
-;* Copyright (c) 2014 James Darnley <james.darnley@gmail.com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or modify
-;* it under the terms of the GNU General Public License as published by
-;* the Free Software Foundation; either version 2 of the License, or
-;* (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
-;* GNU General Public License for more details.
-;*
-;* You should have received a copy of the GNU General Public License along
-;* with FFmpeg; if not, write to the Free Software Foundation, Inc.,
-;* 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION .text
-
-INIT_XMM sse4
-%if ARCH_X86_64
-    cglobal flac_enc_lpc_16, 5, 7, 8, 0, res, smp, len, order, coefs
-    DECLARE_REG_TMP 5, 6
-    %define length r2d
-
-    movsxd orderq, orderd
-%else
-    cglobal flac_enc_lpc_16, 5, 6, 8, 0, res, smp, len, order, coefs
-    DECLARE_REG_TMP 2, 5
-    %define length r2mp
-%endif
-
-; Here we assume that the maximum order value is 32.  This means that we only
-; need to copy a maximum of 32 samples.  Therefore we let the preprocessor
-; unroll this loop and copy all 32.
-%assign iter 0
-%rep 32/(mmsize/4)
-    movu  m0,         [smpq+iter]
-    movu [resq+iter],  m0
-    %assign iter iter+mmsize
-%endrep
-
-lea  resq,   [resq+orderq*4]
-lea  smpq,   [smpq+orderq*4]
-lea  coefsq, [coefsq+orderq*4]
-sub  length,  orderd
-movd m3,      r5m
-neg  orderq
-
-%define posj t0q
-%define negj t1q
-
-.looplen:
-    pxor m0,   m0
-    pxor m4,   m4
-    pxor m6,   m6
-    mov  posj, orderq
-    xor  negj, negj
-
-    .looporder:
-        movd   m2, [coefsq+posj*4] ; c = coefs[j]
-        SPLATD m2
-        movu   m1, [smpq+negj*4-4] ; s = smp[i-j-1]
-        movu   m5, [smpq+negj*4-4+mmsize]
-        movu   m7, [smpq+negj*4-4+mmsize*2]
-        pmulld m1,  m2
-        pmulld m5,  m2
-        pmulld m7,  m2
-        paddd  m0,  m1             ; p += c * s
-        paddd  m4,  m5
-        paddd  m6,  m7
-
-        dec    negj
-        inc    posj
-    jnz .looporder
-
-    psrad  m0,     m3              ; p >>= shift
-    psrad  m4,     m3
-    psrad  m6,     m3
-    movu   m1,    [smpq]
-    movu   m5,    [smpq+mmsize]
-    movu   m7,    [smpq+mmsize*2]
-    psubd  m1,     m0              ; smp[i] - p
-    psubd  m5,     m4
-    psubd  m7,     m6
-    movu  [resq],  m1              ; res[i] = smp[i] - (p >> shift)
-    movu  [resq+mmsize], m5
-    movu  [resq+mmsize*2], m7
-
-    add resq,    3*mmsize
-    add smpq,    3*mmsize
-    sub length, (3*mmsize)/4
-jg .looplen
-RET
diff -uparN ffmpeg-4.1/libavcodec/x86/fmtconvert.asm ffmpeg-y/libavcodec/x86/fmtconvert.asm
--- ffmpeg-4.1/libavcodec/x86/fmtconvert.asm	2018-07-17 17:27:41.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/fmtconvert.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,124 +0,0 @@
-;******************************************************************************
-;* x86 optimized Format Conversion Utils
-;* Copyright (c) 2008 Loren Merritt
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION .text
-
-;------------------------------------------------------------------------------
-; void ff_int32_to_float_fmul_scalar(float *dst, const int32_t *src, float mul,
-;                                    int len);
-;------------------------------------------------------------------------------
-%macro INT32_TO_FLOAT_FMUL_SCALAR 1
-%if UNIX64
-cglobal int32_to_float_fmul_scalar, 3, 3, %1, dst, src, len
-%else
-cglobal int32_to_float_fmul_scalar, 4, 4, %1, dst, src, mul, len
-%endif
-%if WIN64
-    SWAP 0, 2
-%elif ARCH_X86_32
-    movss   m0, mulm
-%endif
-    SPLATD  m0
-    shl     lend, 2
-    add     srcq, lenq
-    add     dstq, lenq
-    neg     lenq
-.loop:
-%if cpuflag(sse2)
-    cvtdq2ps  m1, [srcq+lenq   ]
-    cvtdq2ps  m2, [srcq+lenq+16]
-%else
-    cvtpi2ps  m1, [srcq+lenq   ]
-    cvtpi2ps  m3, [srcq+lenq+ 8]
-    cvtpi2ps  m2, [srcq+lenq+16]
-    cvtpi2ps  m4, [srcq+lenq+24]
-    movlhps   m1, m3
-    movlhps   m2, m4
-%endif
-    mulps     m1, m0
-    mulps     m2, m0
-    mova  [dstq+lenq   ], m1
-    mova  [dstq+lenq+16], m2
-    add     lenq, 32
-    jl .loop
-%if notcpuflag(sse2)
-    ;; cvtpi2ps switches to MMX even if the source is a memory location
-    ;; possible an error in documentation since every tested CPU disagrees with
-    ;; that. Use emms anyway since the vast majority of machines will use the
-    ;; SSE2 variant
-    emms
-%endif
-    RET
-%endmacro
-
-INIT_XMM sse
-INT32_TO_FLOAT_FMUL_SCALAR 5
-INIT_XMM sse2
-INT32_TO_FLOAT_FMUL_SCALAR 3
-
-;------------------------------------------------------------------------------
-; void ff_int32_to_float_fmul_array8(FmtConvertContext *c, float *dst, const int32_t *src,
-;                                    const float *mul, int len);
-;------------------------------------------------------------------------------
-%macro INT32_TO_FLOAT_FMUL_ARRAY8 0
-cglobal int32_to_float_fmul_array8, 5, 5, 5, c, dst, src, mul, len
-    shl     lend, 2
-    add     srcq, lenq
-    add     dstq, lenq
-    neg     lenq
-.loop:
-    movss     m0, [mulq]
-    SPLATD    m0
-%if cpuflag(sse2)
-    cvtdq2ps  m1, [srcq+lenq   ]
-    cvtdq2ps  m2, [srcq+lenq+16]
-%else
-    cvtpi2ps  m1, [srcq+lenq   ]
-    cvtpi2ps  m3, [srcq+lenq+ 8]
-    cvtpi2ps  m2, [srcq+lenq+16]
-    cvtpi2ps  m4, [srcq+lenq+24]
-    movlhps   m1, m3
-    movlhps   m2, m4
-%endif
-    mulps     m1, m0
-    mulps     m2, m0
-    mova  [dstq+lenq   ], m1
-    mova  [dstq+lenq+16], m2
-    add     mulq, 4
-    add     lenq, 32
-    jl .loop
-%if notcpuflag(sse2)
-    ;; cvtpi2ps switches to MMX even if the source is a memory location
-    ;; possible an error in documentation since every tested CPU disagrees with
-    ;; that. Use emms anyway since the vast majority of machines will use the
-    ;; SSE2 variant
-    emms
-%endif
-    RET
-%endmacro
-
-INIT_XMM sse
-INT32_TO_FLOAT_FMUL_ARRAY8
-INIT_XMM sse2
-INT32_TO_FLOAT_FMUL_ARRAY8
-
diff -uparN ffmpeg-4.1/libavcodec/x86/fpel.asm ffmpeg-y/libavcodec/x86/fpel.asm
--- ffmpeg-4.1/libavcodec/x86/fpel.asm	2018-11-02 02:34:25.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/fpel.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,106 +0,0 @@
-;******************************************************************************
-;* SIMD-optimized fullpel functions
-;* Copyright (c) 2008 Loren Merritt
-;* Copyright (c) 2003-2013 Michael Niedermayer
-;* Copyright (c) 2013 Daniel Kang
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION .text
-
-%macro PAVGB_MMX 4
-    LOAD   %3, %1
-    por    %3, %2
-    pxor   %2, %1
-    pand   %2, %4
-    psrlq  %2, 1
-    psubb  %3, %2
-    SWAP   %2, %3
-%endmacro
-
-; void ff_put/avg_pixels(uint8_t *block, const uint8_t *pixels,
-;                        ptrdiff_t line_size, int h)
-%macro OP_PIXELS 2
-%if %2 == mmsize/2
-%define LOAD movh
-%define SAVE movh
-%define LEN  mmsize
-%else
-%define LOAD movu
-%define SAVE mova
-%define LEN  %2
-%endif
-cglobal %1_pixels%2, 4,5,4
-    lea          r4, [r2*3]
-%ifidn %1, avg
-%if notcpuflag(mmxext)
-    pcmpeqd      m6, m6
-    paddb        m6, m6
-%endif
-%endif
-.loop:
-%assign %%i 0
-%rep LEN/mmsize
-    LOAD         m0, [r1 + %%i]
-    LOAD         m1, [r1+r2 + %%i]
-    LOAD         m2, [r1+r2*2 + %%i]
-    LOAD         m3, [r1+r4 + %%i]
-%ifidn %1, avg
-%if notcpuflag(mmxext)
-    PAVGB_MMX    [r0 + %%i], m0, m4, m6
-    PAVGB_MMX    [r0+r2 + %%i], m1, m5, m6
-    PAVGB_MMX    [r0+r2*2 + %%i], m2, m4, m6
-    PAVGB_MMX    [r0+r4 + %%i], m3, m5, m6
-%else
-    pavgb        m0, [r0 + %%i]
-    pavgb        m1, [r0+r2 + %%i]
-    pavgb        m2, [r0+r2*2 + %%i]
-    pavgb        m3, [r0+r4 + %%i]
-%endif
-%endif
-    SAVE       [r0 + %%i], m0
-    SAVE    [r0+r2 + %%i], m1
-    SAVE  [r0+r2*2 + %%i], m2
-    SAVE    [r0+r4 + %%i], m3
-%assign %%i %%i+mmsize
-%endrep
-    sub         r3d, 4
-    lea          r1, [r1+r2*4]
-    lea          r0, [r0+r2*4]
-    jne       .loop
-    RET
-%endmacro
-
-INIT_MMX mmx
-OP_PIXELS put, 4
-OP_PIXELS avg, 4
-OP_PIXELS put, 8
-OP_PIXELS avg, 8
-OP_PIXELS put, 16
-OP_PIXELS avg, 16
-
-INIT_MMX mmxext
-OP_PIXELS avg, 4
-OP_PIXELS avg, 8
-OP_PIXELS avg, 16
-
-INIT_XMM sse2
-OP_PIXELS put, 16
-OP_PIXELS avg, 16
diff -uparN ffmpeg-4.1/libavcodec/x86/g722dsp.asm ffmpeg-y/libavcodec/x86/g722dsp.asm
--- ffmpeg-4.1/libavcodec/x86/g722dsp.asm	2018-01-01 06:35:48.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/g722dsp.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,54 +0,0 @@
-;******************************************************************************
-;* SIMD optimized DSP functions for G722 coding
-;*
-;* Copyright (c) 2014 James Almer
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-
-pw_qmf_coeffs:  dw   3, -210,  -11, -805,  -11,  951,  53, 3876
-pw_qmf_coeffs2: dw  12, 3876, -156,  951,   32, -805, 362, -210
-pw_qmf_coeffs3: dw 362,    0 ,  32,    0, -156,    0,  12,    0
-pw_qmf_coeffs4: dw  53,    0,  -11,    0,  -11,    0,   3,    0
-
-SECTION .text
-
-INIT_XMM sse2
-cglobal g722_apply_qmf, 2, 2, 5, prev, out
-    movu m0, [prevq+mmsize*0]
-    movu m1, [prevq+mmsize*1]
-    movu m2, [prevq+mmsize*2]
-    punpcklwd m3, m0, m1
-    punpckhwd m0, m1
-    punpcklwd m4, m2, m2
-    punpckhwd m2, m2
-    pmaddwd   m3, [pw_qmf_coeffs ]
-    pmaddwd   m0, [pw_qmf_coeffs2]
-    pmaddwd   m4, [pw_qmf_coeffs3]
-    pmaddwd   m2, [pw_qmf_coeffs4]
-    paddd     m0, m3
-    paddd     m2, m4
-    paddd     m0, m2
-    pshufd    m2, m0, q0032
-    paddd     m0, m2
-    pshufd    m0, m0, q0001
-    movq  [outq], m0
-    RET
diff -uparN ffmpeg-4.1/libavcodec/x86/h263_loopfilter.asm ffmpeg-y/libavcodec/x86/h263_loopfilter.asm
--- ffmpeg-4.1/libavcodec/x86/h263_loopfilter.asm	2018-01-01 06:35:48.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/h263_loopfilter.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,189 +0,0 @@
-;******************************************************************************
-;* MMX-optimized H.263 loop filter
-;* Copyright (c) 2003-2013 Michael Niedermayer
-;* Copyright (c) 2013 Daniel Kang
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-cextern pb_FC
-cextern h263_loop_filter_strength
-
-SECTION .text
-
-%macro H263_LOOP_FILTER 5
-    pxor         m7, m7
-    mova         m0, [%1]
-    mova         m1, [%1]
-    mova         m2, [%4]
-    mova         m3, [%4]
-    punpcklbw    m0, m7
-    punpckhbw    m1, m7
-    punpcklbw    m2, m7
-    punpckhbw    m3, m7
-    psubw        m0, m2
-    psubw        m1, m3
-    mova         m2, [%2]
-    mova         m3, [%2]
-    mova         m4, [%3]
-    mova         m5, [%3]
-    punpcklbw    m2, m7
-    punpckhbw    m3, m7
-    punpcklbw    m4, m7
-    punpckhbw    m5, m7
-    psubw        m4, m2
-    psubw        m5, m3
-    psllw        m4, 2
-    psllw        m5, 2
-    paddw        m4, m0
-    paddw        m5, m1
-    pxor         m6, m6
-    pcmpgtw      m6, m4
-    pcmpgtw      m7, m5
-    pxor         m4, m6
-    pxor         m5, m7
-    psubw        m4, m6
-    psubw        m5, m7
-    psrlw        m4, 3
-    psrlw        m5, 3
-    packuswb     m4, m5
-    packsswb     m6, m7
-    pxor         m7, m7
-    movd         m2, %5
-    punpcklbw    m2, m2
-    punpcklbw    m2, m2
-    punpcklbw    m2, m2
-    psubusb      m2, m4
-    mova         m3, m2
-    psubusb      m3, m4
-    psubb        m2, m3
-    mova         m3, [%2]
-    mova         m4, [%3]
-    pxor         m3, m6
-    pxor         m4, m6
-    paddusb      m3, m2
-    psubusb      m4, m2
-    pxor         m3, m6
-    pxor         m4, m6
-    paddusb      m2, m2
-    packsswb     m0, m1
-    pcmpgtb      m7, m0
-    pxor         m0, m7
-    psubb        m0, m7
-    mova         m1, m0
-    psubusb      m0, m2
-    psubb        m1, m0
-    pand         m1, [pb_FC]
-    psrlw        m1, 2
-    pxor         m1, m7
-    psubb        m1, m7
-    mova         m5, [%1]
-    mova         m6, [%4]
-    psubb        m5, m1
-    paddb        m6, m1
-%endmacro
-
-INIT_MMX mmx
-; void ff_h263_v_loop_filter_mmx(uint8_t *src, int stride, int qscale)
-cglobal h263_v_loop_filter, 3,5
-    movsxdifnidn r1, r1d
-    movsxdifnidn r2, r2d
-
-    lea          r4, [h263_loop_filter_strength]
-    movzx       r3d, BYTE [r4+r2]
-    movsx        r2, r3b
-    shl          r2, 1
-
-    mov          r3, r0
-    sub          r3, r1
-    mov          r4, r3
-    sub          r4, r1
-    H263_LOOP_FILTER r4, r3, r0, r0+r1, r2d
-
-    mova       [r3], m3
-    mova       [r0], m4
-    mova       [r4], m5
-    mova    [r0+r1], m6
-    RET
-
-%macro TRANSPOSE4X4 2
-    movd      m0, [%1]
-    movd      m1, [%1+r1]
-    movd      m2, [%1+r1*2]
-    movd      m3, [%1+r3]
-    punpcklbw m0, m1
-    punpcklbw m2, m3
-    mova      m1, m0
-    punpcklwd m0, m2
-    punpckhwd m1, m2
-    movd [%2+ 0], m0
-    punpckhdq m0, m0
-    movd [%2+ 8], m0
-    movd [%2+16], m1
-    punpckhdq m1, m1
-    movd [%2+24], m1
-%endmacro
-
-
-; void ff_h263_h_loop_filter_mmx(uint8_t *src, int stride, int qscale)
-INIT_MMX mmx
-cglobal h263_h_loop_filter, 3,5,0,32
-    movsxdifnidn r1, r1d
-    movsxdifnidn r2, r2d
-
-    lea          r4, [h263_loop_filter_strength]
-    movzx       r3d, BYTE [r4+r2]
-    movsx        r2, r3b
-    shl          r2, 1
-
-    sub          r0, 2
-    lea          r3, [r1*3]
-
-    TRANSPOSE4X4 r0, rsp
-    lea          r4, [r0+r1*4]
-    TRANSPOSE4X4 r4, rsp+4
-
-    H263_LOOP_FILTER rsp, rsp+8, rsp+16, rsp+24, r2d
-
-    mova         m1, m5
-    mova         m0, m4
-    punpcklbw    m5, m3
-    punpcklbw    m4, m6
-    punpckhbw    m1, m3
-    punpckhbw    m0, m6
-    mova         m3, m5
-    mova         m6, m1
-    punpcklwd    m5, m4
-    punpcklwd    m1, m0
-    punpckhwd    m3, m4
-    punpckhwd    m6, m0
-    movd       [r0], m5
-    punpckhdq    m5, m5
-    movd  [r0+r1*1], m5
-    movd  [r0+r1*2], m3
-    punpckhdq    m3, m3
-    movd    [r0+r3], m3
-    movd       [r4], m1
-    punpckhdq    m1, m1
-    movd  [r4+r1*1], m1
-    movd  [r4+r1*2], m6
-    punpckhdq    m6, m6
-    movd    [r4+r3], m6
-    RET
diff -uparN ffmpeg-4.1/libavcodec/x86/h264_chromamc_10bit.asm ffmpeg-y/libavcodec/x86/h264_chromamc_10bit.asm
--- ffmpeg-4.1/libavcodec/x86/h264_chromamc_10bit.asm	2018-11-02 02:34:25.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/h264_chromamc_10bit.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,269 +0,0 @@
-;*****************************************************************************
-;* MMX/SSE2/AVX-optimized 10-bit H.264 chroma MC code
-;*****************************************************************************
-;* Copyright (C) 2005-2011 x264 project
-;*
-;* Authors: Daniel Kang <daniel.d.kang@gmail.com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-
-cextern pw_4
-cextern pw_8
-cextern pw_32
-cextern pw_64
-
-SECTION .text
-
-
-%macro MV0_PIXELS_MC8 0
-    lea           r4, [r2*3   ]
-    lea           r5, [r2*4   ]
-.next4rows:
-    movu          m0, [r1     ]
-    movu          m1, [r1+r2  ]
-    CHROMAMC_AVG  m0, [r0     ]
-    CHROMAMC_AVG  m1, [r0+r2  ]
-    mova   [r0     ], m0
-    mova   [r0+r2  ], m1
-    movu          m0, [r1+r2*2]
-    movu          m1, [r1+r4  ]
-    CHROMAMC_AVG  m0, [r0+r2*2]
-    CHROMAMC_AVG  m1, [r0+r4  ]
-    mova   [r0+r2*2], m0
-    mova   [r0+r4  ], m1
-    add           r1, r5
-    add           r0, r5
-    sub          r3d, 4
-    jne .next4rows
-%endmacro
-
-;-----------------------------------------------------------------------------
-; void ff_put/avg_h264_chroma_mc8(pixel *dst, pixel *src, ptrdiff_t stride,
-;                                 int h, int mx, int my)
-;-----------------------------------------------------------------------------
-%macro CHROMA_MC8 1
-cglobal %1_h264_chroma_mc8_10, 6,7,8
-    mov          r6d, r5d
-    or           r6d, r4d
-    jne .at_least_one_non_zero
-    ; mx == 0 AND my == 0 - no filter needed
-    MV0_PIXELS_MC8
-    REP_RET
-
-.at_least_one_non_zero:
-    mov          r6d, 2
-    test         r5d, r5d
-    je .x_interpolation
-    mov           r6, r2        ; dxy = x ? 1 : stride
-    test         r4d, r4d
-    jne .xy_interpolation
-.x_interpolation:
-    ; mx == 0 XOR my == 0 - 1 dimensional filter only
-    or           r4d, r5d       ; x + y
-    movd          m5, r4d
-    mova          m4, [pw_8]
-    mova          m6, [pw_4]    ; mm6 = rnd >> 3
-    SPLATW        m5, m5        ; mm5 = B = x
-    psubw         m4, m5        ; mm4 = A = 8-x
-
-.next1drow:
-    movu          m0, [r1   ]   ; mm0 = src[0..7]
-    movu          m2, [r1+r6]   ; mm2 = src[1..8]
-
-    pmullw        m0, m4        ; mm0 = A * src[0..7]
-    pmullw        m2, m5        ; mm2 = B * src[1..8]
-
-    paddw         m0, m6
-    paddw         m0, m2
-    psrlw         m0, 3
-    CHROMAMC_AVG  m0, [r0]
-    mova        [r0], m0        ; dst[0..7] = (A * src[0..7] + B * src[1..8] + (rnd >> 3)) >> 3
-
-    add           r0, r2
-    add           r1, r2
-    dec           r3d
-    jne .next1drow
-    REP_RET
-
-.xy_interpolation: ; general case, bilinear
-    movd          m4, r4m         ; x
-    movd          m6, r5m         ; y
-
-    SPLATW        m4, m4          ; mm4 = x words
-    SPLATW        m6, m6          ; mm6 = y words
-    psllw         m5, m4, 3       ; mm5 = 8x
-    pmullw        m4, m6          ; mm4 = x * y
-    psllw         m6, 3           ; mm6 = 8y
-    paddw         m1, m5, m6      ; mm7 = 8x+8y
-    mova          m7, m4          ; DD = x * y
-    psubw         m5, m4          ; mm5 = B = 8x - xy
-    psubw         m6, m4          ; mm6 = C = 8y - xy
-    paddw         m4, [pw_64]
-    psubw         m4, m1          ; mm4 = A = xy - (8x+8y) + 64
-
-    movu          m0, [r1  ]      ; mm0 = src[0..7]
-    movu          m1, [r1+2]      ; mm1 = src[1..8]
-.next2drow:
-    add           r1, r2
-
-    pmullw        m2, m0, m4
-    pmullw        m1, m5
-    paddw         m2, m1          ; mm2 = A * src[0..7] + B * src[1..8]
-
-    movu          m0, [r1]
-    movu          m1, [r1+2]
-    pmullw        m3, m0, m6
-    paddw         m2, m3          ; mm2 += C * src[0..7+strde]
-    pmullw        m3, m1, m7
-    paddw         m2, m3          ; mm2 += D * src[1..8+strde]
-
-    paddw         m2, [pw_32]
-    psrlw         m2, 6
-    CHROMAMC_AVG  m2, [r0]
-    mova        [r0], m2          ; dst[0..7] = (mm2 + 32) >> 6
-
-    add           r0, r2
-    dec          r3d
-    jne .next2drow
-    REP_RET
-%endmacro
-
-;-----------------------------------------------------------------------------
-; void ff_put/avg_h264_chroma_mc4(pixel *dst, pixel *src, ptrdiff_t stride,
-;                                 int h, int mx, int my)
-;-----------------------------------------------------------------------------
-;TODO: xmm mc4
-%macro MC4_OP 2
-    movq          %1, [r1  ]
-    movq          m1, [r1+2]
-    add           r1, r2
-    pmullw        %1, m4
-    pmullw        m1, m2
-    paddw         m1, %1
-    mova          %1, m1
-
-    pmullw        %2, m5
-    pmullw        m1, m3
-    paddw         %2, [pw_32]
-    paddw         m1, %2
-    psrlw         m1, 6
-    CHROMAMC_AVG  m1, %2, [r0]
-    movq        [r0], m1
-    add           r0, r2
-%endmacro
-
-%macro CHROMA_MC4 1
-cglobal %1_h264_chroma_mc4_10, 6,6,7
-    movd          m2, r4m         ; x
-    movd          m3, r5m         ; y
-    mova          m4, [pw_8]
-    mova          m5, m4
-    SPLATW        m2, m2
-    SPLATW        m3, m3
-    psubw         m4, m2
-    psubw         m5, m3
-
-    movq          m0, [r1  ]
-    movq          m6, [r1+2]
-    add           r1, r2
-    pmullw        m0, m4
-    pmullw        m6, m2
-    paddw         m6, m0
-
-.next2rows:
-    MC4_OP m0, m6
-    MC4_OP m6, m0
-    sub   r3d, 2
-    jnz .next2rows
-    REP_RET
-%endmacro
-
-;-----------------------------------------------------------------------------
-; void ff_put/avg_h264_chroma_mc2(pixel *dst, pixel *src, ptrdiff_t stride,
-;                                 int h, int mx, int my)
-;-----------------------------------------------------------------------------
-%macro CHROMA_MC2 1
-cglobal %1_h264_chroma_mc2_10, 6,7
-    mov          r6d, r4d
-    shl          r4d, 16
-    sub          r4d, r6d
-    add          r4d, 8
-    imul         r5d, r4d         ; x*y<<16 | y*(8-x)
-    shl          r4d, 3
-    sub          r4d, r5d         ; x*(8-y)<<16 | (8-x)*(8-y)
-
-    movd          m5, r4d
-    movd          m6, r5d
-    punpckldq     m5, m5          ; mm5 = {A,B,A,B}
-    punpckldq     m6, m6          ; mm6 = {C,D,C,D}
-    pxor          m7, m7
-    pshufw        m2, [r1], 0x94    ; mm0 = src[0,1,1,2]
-
-.nextrow:
-    add           r1, r2
-    movq          m1, m2
-    pmaddwd       m1, m5          ; mm1 = A * src[0,1] + B * src[1,2]
-    pshufw        m0, [r1], 0x94    ; mm0 = src[0,1,1,2]
-    movq          m2, m0
-    pmaddwd       m0, m6
-    paddw         m1, [pw_32]
-    paddw         m1, m0          ; mm1 += C * src[0,1] + D * src[1,2]
-    psrlw         m1, 6
-    packssdw      m1, m7
-    CHROMAMC_AVG  m1, m3, [r0]
-    movd        [r0], m1
-    add           r0, r2
-    dec          r3d
-    jnz .nextrow
-    REP_RET
-%endmacro
-
-%macro NOTHING 2-3
-%endmacro
-%macro AVG 2-3
-%if %0==3
-    movq          %2, %3
-%endif
-    pavgw         %1, %2
-%endmacro
-
-%define CHROMAMC_AVG  NOTHING
-INIT_XMM sse2
-CHROMA_MC8 put
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-CHROMA_MC8 put
-%endif
-INIT_MMX mmxext
-CHROMA_MC4 put
-CHROMA_MC2 put
-
-%define CHROMAMC_AVG  AVG
-INIT_XMM sse2
-CHROMA_MC8 avg
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-CHROMA_MC8 avg
-%endif
-INIT_MMX mmxext
-CHROMA_MC4 avg
-CHROMA_MC2 avg
diff -uparN ffmpeg-4.1/libavcodec/x86/h264_chromamc.asm ffmpeg-y/libavcodec/x86/h264_chromamc.asm
--- ffmpeg-4.1/libavcodec/x86/h264_chromamc.asm	2018-11-02 02:34:25.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/h264_chromamc.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,663 +0,0 @@
-;******************************************************************************
-;* MMX/SSSE3-optimized functions for H.264 chroma MC
-;* Copyright (c) 2005 Zoltan Hidvegi <hzoli -a- hzoli -d- com>,
-;*               2005-2008 Loren Merritt
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-
-rnd_rv40_2d_tbl: times 4 dw  0
-                 times 4 dw 16
-                 times 4 dw 32
-                 times 4 dw 16
-                 times 4 dw 32
-                 times 4 dw 28
-                 times 4 dw 32
-                 times 4 dw 28
-                 times 4 dw  0
-                 times 4 dw 32
-                 times 4 dw 16
-                 times 4 dw 32
-                 times 4 dw 32
-                 times 4 dw 28
-                 times 4 dw 32
-                 times 4 dw 28
-rnd_rv40_1d_tbl: times 4 dw  0
-                 times 4 dw  2
-                 times 4 dw  4
-                 times 4 dw  2
-                 times 4 dw  4
-                 times 4 dw  3
-                 times 4 dw  4
-                 times 4 dw  3
-                 times 4 dw  0
-                 times 4 dw  4
-                 times 4 dw  2
-                 times 4 dw  4
-                 times 4 dw  4
-                 times 4 dw  3
-                 times 4 dw  4
-                 times 4 dw  3
-
-cextern pw_3
-cextern pw_4
-cextern pw_8
-pw_28: times 8 dw 28
-cextern pw_32
-cextern pw_64
-
-SECTION .text
-
-%macro mv0_pixels_mc8 0
-    lea           r4, [r2*2 ]
-.next4rows:
-    movq         mm0, [r1   ]
-    movq         mm1, [r1+r2]
-    add           r1, r4
-    CHROMAMC_AVG mm0, [r0   ]
-    CHROMAMC_AVG mm1, [r0+r2]
-    movq     [r0   ], mm0
-    movq     [r0+r2], mm1
-    add           r0, r4
-    movq         mm0, [r1   ]
-    movq         mm1, [r1+r2]
-    add           r1, r4
-    CHROMAMC_AVG mm0, [r0   ]
-    CHROMAMC_AVG mm1, [r0+r2]
-    movq     [r0   ], mm0
-    movq     [r0+r2], mm1
-    add           r0, r4
-    sub          r3d, 4
-    jne .next4rows
-%endmacro
-
-%macro chroma_mc8_mmx_func 2-3
-%ifidn %2, rv40
-%ifdef PIC
-%define rnd_1d_rv40 r8
-%define rnd_2d_rv40 r8
-%define extra_regs 2
-%else ; no-PIC
-%define rnd_1d_rv40 rnd_rv40_1d_tbl
-%define rnd_2d_rv40 rnd_rv40_2d_tbl
-%define extra_regs 1
-%endif ; PIC
-%else
-%define extra_regs 0
-%endif ; rv40
-; void ff_put/avg_h264_chroma_mc8_*(uint8_t *dst /* align 8 */,
-;                                   uint8_t *src /* align 1 */,
-;                                   ptrdiff_t stride, int h, int mx, int my)
-cglobal %1_%2_chroma_mc8%3, 6, 7 + extra_regs, 0
-    mov          r6d, r5d
-    or           r6d, r4d
-    jne .at_least_one_non_zero
-    ; mx == 0 AND my == 0 - no filter needed
-    mv0_pixels_mc8
-    REP_RET
-
-.at_least_one_non_zero:
-%ifidn %2, rv40
-%if ARCH_X86_64
-    mov           r7, r5
-    and           r7, 6         ; &~1 for mx/my=[0,7]
-    lea           r7, [r7*4+r4]
-    sar          r7d, 1
-%define rnd_bias r7
-%define dest_reg r0
-%else ; x86-32
-    mov           r0, r5
-    and           r0, 6         ; &~1 for mx/my=[0,7]
-    lea           r0, [r0*4+r4]
-    sar          r0d, 1
-%define rnd_bias r0
-%define dest_reg r5
-%endif
-%else ; vc1, h264
-%define rnd_bias  0
-%define dest_reg r0
-%endif
-
-    test         r5d, r5d
-    mov           r6, 1
-    je .my_is_zero
-    test         r4d, r4d
-    mov           r6, r2        ; dxy = x ? 1 : stride
-    jne .both_non_zero
-.my_is_zero:
-    ; mx == 0 XOR my == 0 - 1 dimensional filter only
-    or           r4d, r5d       ; x + y
-
-%ifidn %2, rv40
-%ifdef PIC
-    lea           r8, [rnd_rv40_1d_tbl]
-%endif
-%if ARCH_X86_64 == 0
-    mov           r5, r0m
-%endif
-%endif
-
-    movd          m5, r4d
-    movq          m4, [pw_8]
-    movq          m6, [rnd_1d_%2+rnd_bias*8] ; mm6 = rnd >> 3
-    punpcklwd     m5, m5
-    punpckldq     m5, m5        ; mm5 = B = x
-    pxor          m7, m7
-    psubw         m4, m5        ; mm4 = A = 8-x
-
-.next1drow:
-    movq          m0, [r1   ]   ; mm0 = src[0..7]
-    movq          m2, [r1+r6]   ; mm1 = src[1..8]
-
-    movq          m1, m0
-    movq          m3, m2
-    punpcklbw     m0, m7
-    punpckhbw     m1, m7
-    punpcklbw     m2, m7
-    punpckhbw     m3, m7
-    pmullw        m0, m4        ; [mm0,mm1] = A * src[0..7]
-    pmullw        m1, m4
-    pmullw        m2, m5        ; [mm2,mm3] = B * src[1..8]
-    pmullw        m3, m5
-
-    paddw         m0, m6
-    paddw         m1, m6
-    paddw         m0, m2
-    paddw         m1, m3
-    psrlw         m0, 3
-    psrlw         m1, 3
-    packuswb      m0, m1
-    CHROMAMC_AVG  m0, [dest_reg]
-    movq  [dest_reg], m0        ; dst[0..7] = (A * src[0..7] + B * src[1..8] + (rnd >> 3)) >> 3
-
-    add     dest_reg, r2
-    add           r1, r2
-    dec           r3d
-    jne .next1drow
-    REP_RET
-
-.both_non_zero: ; general case, bilinear
-    movd          m4, r4d         ; x
-    movd          m6, r5d         ; y
-%ifidn %2, rv40
-%ifdef PIC
-    lea           r8, [rnd_rv40_2d_tbl]
-%endif
-%if ARCH_X86_64 == 0
-    mov           r5, r0m
-%endif
-%endif
-    mov           r6, rsp         ; backup stack pointer
-    and          rsp, ~(mmsize-1) ; align stack
-    sub          rsp, 16          ; AA and DD
-
-    punpcklwd     m4, m4
-    punpcklwd     m6, m6
-    punpckldq     m4, m4          ; mm4 = x words
-    punpckldq     m6, m6          ; mm6 = y words
-    movq          m5, m4
-    pmullw        m4, m6          ; mm4 = x * y
-    psllw         m5, 3
-    psllw         m6, 3
-    movq          m7, m5
-    paddw         m7, m6
-    movq     [rsp+8], m4          ; DD = x * y
-    psubw         m5, m4          ; mm5 = B = 8x - xy
-    psubw         m6, m4          ; mm6 = C = 8y - xy
-    paddw         m4, [pw_64]
-    psubw         m4, m7          ; mm4 = A = xy - (8x+8y) + 64
-    pxor          m7, m7
-    movq     [rsp  ], m4
-
-    movq          m0, [r1  ]      ; mm0 = src[0..7]
-    movq          m1, [r1+1]      ; mm1 = src[1..8]
-.next2drow:
-    add           r1, r2
-
-    movq          m2, m0
-    movq          m3, m1
-    punpckhbw     m0, m7
-    punpcklbw     m1, m7
-    punpcklbw     m2, m7
-    punpckhbw     m3, m7
-    pmullw        m0, [rsp]
-    pmullw        m2, [rsp]
-    pmullw        m1, m5
-    pmullw        m3, m5
-    paddw         m2, m1          ; mm2 = A * src[0..3] + B * src[1..4]
-    paddw         m3, m0          ; mm3 = A * src[4..7] + B * src[5..8]
-
-    movq          m0, [r1]
-    movq          m1, m0
-    punpcklbw     m0, m7
-    punpckhbw     m1, m7
-    pmullw        m0, m6
-    pmullw        m1, m6
-    paddw         m2, m0
-    paddw         m3, m1          ; [mm2,mm3] += C * src[0..7]
-
-    movq          m1, [r1+1]
-    movq          m0, m1
-    movq          m4, m1
-    punpcklbw     m0, m7
-    punpckhbw     m4, m7
-    pmullw        m0, [rsp+8]
-    pmullw        m4, [rsp+8]
-    paddw         m2, m0
-    paddw         m3, m4          ; [mm2,mm3] += D * src[1..8]
-    movq          m0, [r1]
-
-    paddw         m2, [rnd_2d_%2+rnd_bias*8]
-    paddw         m3, [rnd_2d_%2+rnd_bias*8]
-    psrlw         m2, 6
-    psrlw         m3, 6
-    packuswb      m2, m3
-    CHROMAMC_AVG  m2, [dest_reg]
-    movq  [dest_reg], m2          ; dst[0..7] = ([mm2,mm3] + rnd) >> 6
-
-    add     dest_reg, r2
-    dec          r3d
-    jne .next2drow
-    mov          rsp, r6          ; restore stack pointer
-    RET
-%endmacro
-
-%macro chroma_mc4_mmx_func 2
-%define extra_regs 0
-%ifidn %2, rv40
-%ifdef PIC
-%define extra_regs 1
-%endif ; PIC
-%endif ; rv40
-cglobal %1_%2_chroma_mc4, 6, 6 + extra_regs, 0
-    pxor          m7, m7
-    movd          m2, r4d         ; x
-    movd          m3, r5d         ; y
-    movq          m4, [pw_8]
-    movq          m5, [pw_8]
-    punpcklwd     m2, m2
-    punpcklwd     m3, m3
-    punpcklwd     m2, m2
-    punpcklwd     m3, m3
-    psubw         m4, m2
-    psubw         m5, m3
-
-%ifidn %2, rv40
-%ifdef PIC
-   lea            r6, [rnd_rv40_2d_tbl]
-%define rnd_2d_rv40 r6
-%else
-%define rnd_2d_rv40 rnd_rv40_2d_tbl
-%endif
-    and           r5, 6         ; &~1 for mx/my=[0,7]
-    lea           r5, [r5*4+r4]
-    sar          r5d, 1
-%define rnd_bias r5
-%else ; vc1, h264
-%define rnd_bias 0
-%endif
-
-    movd          m0, [r1  ]
-    movd          m6, [r1+1]
-    add           r1, r2
-    punpcklbw     m0, m7
-    punpcklbw     m6, m7
-    pmullw        m0, m4
-    pmullw        m6, m2
-    paddw         m6, m0
-
-.next2rows:
-    movd          m0, [r1  ]
-    movd          m1, [r1+1]
-    add           r1, r2
-    punpcklbw     m0, m7
-    punpcklbw     m1, m7
-    pmullw        m0, m4
-    pmullw        m1, m2
-    paddw         m1, m0
-    movq          m0, m1
-
-    pmullw        m6, m5
-    pmullw        m1, m3
-    paddw         m6, [rnd_2d_%2+rnd_bias*8]
-    paddw         m1, m6
-    psrlw         m1, 6
-    packuswb      m1, m1
-    CHROMAMC_AVG4 m1, m6, [r0]
-    movd        [r0], m1
-    add           r0, r2
-
-    movd          m6, [r1  ]
-    movd          m1, [r1+1]
-    add           r1, r2
-    punpcklbw     m6, m7
-    punpcklbw     m1, m7
-    pmullw        m6, m4
-    pmullw        m1, m2
-    paddw         m1, m6
-    movq          m6, m1
-    pmullw        m0, m5
-    pmullw        m1, m3
-    paddw         m0, [rnd_2d_%2+rnd_bias*8]
-    paddw         m1, m0
-    psrlw         m1, 6
-    packuswb      m1, m1
-    CHROMAMC_AVG4 m1, m0, [r0]
-    movd        [r0], m1
-    add           r0, r2
-    sub          r3d, 2
-    jnz .next2rows
-    REP_RET
-%endmacro
-
-%macro chroma_mc2_mmx_func 2
-cglobal %1_%2_chroma_mc2, 6, 7, 0
-    mov          r6d, r4d
-    shl          r4d, 16
-    sub          r4d, r6d
-    add          r4d, 8
-    imul         r5d, r4d         ; x*y<<16 | y*(8-x)
-    shl          r4d, 3
-    sub          r4d, r5d         ; x*(8-y)<<16 | (8-x)*(8-y)
-
-    movd          m5, r4d
-    movd          m6, r5d
-    punpckldq     m5, m5          ; mm5 = {A,B,A,B}
-    punpckldq     m6, m6          ; mm6 = {C,D,C,D}
-    pxor          m7, m7
-    movd          m2, [r1]
-    punpcklbw     m2, m7
-    pshufw        m2, m2, 0x94    ; mm0 = src[0,1,1,2]
-
-.nextrow:
-    add           r1, r2
-    movq          m1, m2
-    pmaddwd       m1, m5          ; mm1 = A * src[0,1] + B * src[1,2]
-    movd          m0, [r1]
-    punpcklbw     m0, m7
-    pshufw        m0, m0, 0x94    ; mm0 = src[0,1,1,2]
-    movq          m2, m0
-    pmaddwd       m0, m6
-    paddw         m1, [rnd_2d_%2]
-    paddw         m1, m0          ; mm1 += C * src[0,1] + D * src[1,2]
-    psrlw         m1, 6
-    packssdw      m1, m7
-    packuswb      m1, m7
-    CHROMAMC_AVG4 m1, m3, [r0]
-    movd         r5d, m1
-    mov         [r0], r5w
-    add           r0, r2
-    sub          r3d, 1
-    jnz .nextrow
-    REP_RET
-%endmacro
-
-%define rnd_1d_h264 pw_4
-%define rnd_2d_h264 pw_32
-%define rnd_1d_vc1  pw_3
-%define rnd_2d_vc1  pw_28
-
-%macro NOTHING 2-3
-%endmacro
-%macro DIRECT_AVG 2
-    PAVGB         %1, %2
-%endmacro
-%macro COPY_AVG 3
-    movd          %2, %3
-    PAVGB         %1, %2
-%endmacro
-
-INIT_MMX mmx
-%define CHROMAMC_AVG  NOTHING
-%define CHROMAMC_AVG4 NOTHING
-chroma_mc8_mmx_func put, h264, _rnd
-chroma_mc8_mmx_func put, vc1,  _nornd
-chroma_mc8_mmx_func put, rv40
-chroma_mc4_mmx_func put, h264
-chroma_mc4_mmx_func put, rv40
-
-INIT_MMX mmxext
-chroma_mc2_mmx_func put, h264
-
-%define CHROMAMC_AVG  DIRECT_AVG
-%define CHROMAMC_AVG4 COPY_AVG
-chroma_mc8_mmx_func avg, h264, _rnd
-chroma_mc8_mmx_func avg, vc1,  _nornd
-chroma_mc8_mmx_func avg, rv40
-chroma_mc4_mmx_func avg, h264
-chroma_mc4_mmx_func avg, rv40
-chroma_mc2_mmx_func avg, h264
-
-INIT_MMX 3dnow
-chroma_mc8_mmx_func avg, h264, _rnd
-chroma_mc8_mmx_func avg, vc1,  _nornd
-chroma_mc8_mmx_func avg, rv40
-chroma_mc4_mmx_func avg, h264
-chroma_mc4_mmx_func avg, rv40
-
-%macro chroma_mc8_ssse3_func 2-3
-cglobal %1_%2_chroma_mc8%3, 6, 7, 8
-    mov          r6d, r5d
-    or           r6d, r4d
-    jne .at_least_one_non_zero
-    ; mx == 0 AND my == 0 - no filter needed
-    mv0_pixels_mc8
-    REP_RET
-
-.at_least_one_non_zero:
-    test         r5d, r5d
-    je .my_is_zero
-    test         r4d, r4d
-    je .mx_is_zero
-
-    ; general case, bilinear
-    mov          r6d, r4d
-    shl          r4d, 8
-    sub           r4, r6
-    mov           r6, 8
-    add           r4, 8           ; x*288+8 = x<<8 | (8-x)
-    sub          r6d, r5d
-    imul          r6, r4          ; (8-y)*(x*255+8) = (8-y)*x<<8 | (8-y)*(8-x)
-    imul         r4d, r5d         ;    y *(x*255+8) =    y *x<<8 |    y *(8-x)
-
-    movd          m7, r6d
-    movd          m6, r4d
-    movdqa        m5, [rnd_2d_%2]
-    movq          m0, [r1  ]
-    movq          m1, [r1+1]
-    pshuflw       m7, m7, 0
-    pshuflw       m6, m6, 0
-    punpcklbw     m0, m1
-    movlhps       m7, m7
-    movlhps       m6, m6
-
-.next2rows:
-    movq          m1, [r1+r2*1   ]
-    movq          m2, [r1+r2*1+1]
-    movq          m3, [r1+r2*2  ]
-    movq          m4, [r1+r2*2+1]
-    lea           r1, [r1+r2*2]
-    punpcklbw     m1, m2
-    movdqa        m2, m1
-    punpcklbw     m3, m4
-    movdqa        m4, m3
-    pmaddubsw     m0, m7
-    pmaddubsw     m1, m6
-    pmaddubsw     m2, m7
-    pmaddubsw     m3, m6
-    paddw         m0, m5
-    paddw         m2, m5
-    paddw         m1, m0
-    paddw         m3, m2
-    psrlw         m1, 6
-    movdqa        m0, m4
-    psrlw         m3, 6
-%ifidn %1, avg
-    movq          m2, [r0   ]
-    movhps        m2, [r0+r2]
-%endif
-    packuswb      m1, m3
-    CHROMAMC_AVG  m1, m2
-    movq     [r0   ], m1
-    movhps   [r0+r2], m1
-    sub          r3d, 2
-    lea           r0, [r0+r2*2]
-    jg .next2rows
-    REP_RET
-
-.my_is_zero:
-    mov          r5d, r4d
-    shl          r4d, 8
-    add           r4, 8
-    sub           r4, r5          ; 255*x+8 = x<<8 | (8-x)
-    movd          m7, r4d
-    movdqa        m6, [rnd_1d_%2]
-    pshuflw       m7, m7, 0
-    movlhps       m7, m7
-
-.next2xrows:
-    movq          m0, [r1     ]
-    movq          m1, [r1   +1]
-    movq          m2, [r1+r2  ]
-    movq          m3, [r1+r2+1]
-    punpcklbw     m0, m1
-    punpcklbw     m2, m3
-    pmaddubsw     m0, m7
-    pmaddubsw     m2, m7
-%ifidn %1, avg
-    movq          m4, [r0   ]
-    movhps        m4, [r0+r2]
-%endif
-    paddw         m0, m6
-    paddw         m2, m6
-    psrlw         m0, 3
-    psrlw         m2, 3
-    packuswb      m0, m2
-    CHROMAMC_AVG  m0, m4
-    movq     [r0   ], m0
-    movhps   [r0+r2], m0
-    sub          r3d, 2
-    lea           r0, [r0+r2*2]
-    lea           r1, [r1+r2*2]
-    jg .next2xrows
-    REP_RET
-
-.mx_is_zero:
-    mov          r4d, r5d
-    shl          r5d, 8
-    add           r5, 8
-    sub           r5, r4          ; 255*y+8 = y<<8 | (8-y)
-    movd          m7, r5d
-    movdqa        m6, [rnd_1d_%2]
-    pshuflw       m7, m7, 0
-    movlhps       m7, m7
-
-.next2yrows:
-    movq          m0, [r1     ]
-    movq          m1, [r1+r2  ]
-    movdqa        m2, m1
-    movq          m3, [r1+r2*2]
-    lea           r1, [r1+r2*2]
-    punpcklbw     m0, m1
-    punpcklbw     m2, m3
-    pmaddubsw     m0, m7
-    pmaddubsw     m2, m7
-%ifidn %1, avg
-    movq          m4, [r0   ]
-    movhps        m4, [r0+r2]
-%endif
-    paddw         m0, m6
-    paddw         m2, m6
-    psrlw         m0, 3
-    psrlw         m2, 3
-    packuswb      m0, m2
-    CHROMAMC_AVG  m0, m4
-    movq     [r0   ], m0
-    movhps   [r0+r2], m0
-    sub          r3d, 2
-    lea           r0, [r0+r2*2]
-    jg .next2yrows
-    REP_RET
-%endmacro
-
-%macro chroma_mc4_ssse3_func 2
-cglobal %1_%2_chroma_mc4, 6, 7, 0
-    mov           r6, r4
-    shl          r4d, 8
-    sub          r4d, r6d
-    mov           r6, 8
-    add          r4d, 8           ; x*288+8
-    sub          r6d, r5d
-    imul         r6d, r4d         ; (8-y)*(x*255+8) = (8-y)*x<<8 | (8-y)*(8-x)
-    imul         r4d, r5d         ;    y *(x*255+8) =    y *x<<8 |    y *(8-x)
-
-    movd          m7, r6d
-    movd          m6, r4d
-    movq          m5, [pw_32]
-    movd          m0, [r1  ]
-    pshufw        m7, m7, 0
-    punpcklbw     m0, [r1+1]
-    pshufw        m6, m6, 0
-
-.next2rows:
-    movd          m1, [r1+r2*1  ]
-    movd          m3, [r1+r2*2  ]
-    punpcklbw     m1, [r1+r2*1+1]
-    punpcklbw     m3, [r1+r2*2+1]
-    lea           r1, [r1+r2*2]
-    movq          m2, m1
-    movq          m4, m3
-    pmaddubsw     m0, m7
-    pmaddubsw     m1, m6
-    pmaddubsw     m2, m7
-    pmaddubsw     m3, m6
-    paddw         m0, m5
-    paddw         m2, m5
-    paddw         m1, m0
-    paddw         m3, m2
-    psrlw         m1, 6
-    movq          m0, m4
-    psrlw         m3, 6
-    packuswb      m1, m1
-    packuswb      m3, m3
-    CHROMAMC_AVG  m1, [r0  ]
-    CHROMAMC_AVG  m3, [r0+r2]
-    movd     [r0   ], m1
-    movd     [r0+r2], m3
-    sub          r3d, 2
-    lea           r0, [r0+r2*2]
-    jg .next2rows
-    REP_RET
-%endmacro
-
-%define CHROMAMC_AVG NOTHING
-INIT_XMM ssse3
-chroma_mc8_ssse3_func put, h264, _rnd
-chroma_mc8_ssse3_func put, vc1,  _nornd
-INIT_MMX ssse3
-chroma_mc4_ssse3_func put, h264
-
-%define CHROMAMC_AVG DIRECT_AVG
-INIT_XMM ssse3
-chroma_mc8_ssse3_func avg, h264, _rnd
-chroma_mc8_ssse3_func avg, vc1,  _nornd
-INIT_MMX ssse3
-chroma_mc4_ssse3_func avg, h264
diff -uparN ffmpeg-4.1/libavcodec/x86/h264_deblock_10bit.asm ffmpeg-y/libavcodec/x86/h264_deblock_10bit.asm
--- ffmpeg-4.1/libavcodec/x86/h264_deblock_10bit.asm	2018-11-02 02:34:25.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/h264_deblock_10bit.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,1080 +0,0 @@
-;*****************************************************************************
-;* MMX/SSE2/AVX-optimized 10-bit H.264 deblocking code
-;*****************************************************************************
-;* Copyright (C) 2005-2011 x264 project
-;*
-;* Authors: Oskar Arvidsson <oskar@irock.se>
-;*          Loren Merritt <lorenm@u.washington.edu>
-;*          Fiona Glaser <fiona@x264.com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION .text
-
-cextern pw_2
-cextern pw_3
-cextern pw_4
-cextern pw_1023
-%define pw_pixel_max pw_1023
-
-; out: %4 = |%1-%2|-%3
-; clobbers: %5
-%macro ABS_SUB 5
-    psubusw %5, %2, %1
-    psubusw %4, %1, %2
-    por     %4, %5
-    psubw   %4, %3
-%endmacro
-
-; out: %4 = |%1-%2|<%3
-%macro DIFF_LT   5
-    psubusw %4, %2, %1
-    psubusw %5, %1, %2
-    por     %5, %4 ; |%1-%2|
-    pxor    %4, %4
-    psubw   %5, %3 ; |%1-%2|-%3
-    pcmpgtw %4, %5 ; 0 > |%1-%2|-%3
-%endmacro
-
-%macro LOAD_AB 4
-    movd       %1, %3
-    movd       %2, %4
-    SPLATW     %1, %1
-    SPLATW     %2, %2
-%endmacro
-
-; in:  %2=tc reg
-; out: %1=splatted tc
-%macro LOAD_TC 2
-    movd        %1, [%2]
-    punpcklbw   %1, %1
-%if mmsize == 8
-    pshufw      %1, %1, 0
-%else
-    pshuflw     %1, %1, 01010000b
-    pshufd      %1, %1, 01010000b
-%endif
-    psraw       %1, 6
-%endmacro
-
-; in: %1=p1, %2=p0, %3=q0, %4=q1
-;     %5=alpha, %6=beta, %7-%9=tmp
-; out: %7=mask
-%macro LOAD_MASK 9
-    ABS_SUB     %2, %3, %5, %8, %7 ; |p0-q0| - alpha
-    ABS_SUB     %1, %2, %6, %9, %7 ; |p1-p0| - beta
-    pand        %8, %9
-    ABS_SUB     %3, %4, %6, %9, %7 ; |q1-q0| - beta
-    pxor        %7, %7
-    pand        %8, %9
-    pcmpgtw     %7, %8
-%endmacro
-
-; in: %1=p0, %2=q0, %3=p1, %4=q1, %5=mask, %6=tmp, %7=tmp
-; out: %1=p0', m2=q0'
-%macro DEBLOCK_P0_Q0 7
-    psubw   %3, %4
-    pxor    %7, %7
-    paddw   %3, [pw_4]
-    psubw   %7, %5
-    psubw   %6, %2, %1
-    psllw   %6, 2
-    paddw   %3, %6
-    psraw   %3, 3
-    mova    %6, [pw_pixel_max]
-    CLIPW   %3, %7, %5
-    pxor    %7, %7
-    paddw   %1, %3
-    psubw   %2, %3
-    CLIPW   %1, %7, %6
-    CLIPW   %2, %7, %6
-%endmacro
-
-; in: %1=x2, %2=x1, %3=p0, %4=q0 %5=mask&tc, %6=tmp
-%macro LUMA_Q1 6
-    pavgw       %6, %3, %4      ; (p0+q0+1)>>1
-    paddw       %1, %6
-    pxor        %6, %6
-    psraw       %1, 1
-    psubw       %6, %5
-    psubw       %1, %2
-    CLIPW       %1, %6, %5
-    paddw       %1, %2
-%endmacro
-
-%macro LUMA_DEBLOCK_ONE 3
-    DIFF_LT     m5, %1, bm, m4, m6
-    pxor        m6, m6
-    mova        %3, m4
-    pcmpgtw     m6, tcm
-    pand        m4, tcm
-    pandn       m6, m7
-    pand        m4, m6
-    LUMA_Q1 m5, %2, m1, m2, m4, m6
-%endmacro
-
-%macro LUMA_H_STORE 2
-%if mmsize == 8
-    movq        [r0-4], m0
-    movq        [r0+r1-4], m1
-    movq        [r0+r1*2-4], m2
-    movq        [r0+%2-4], m3
-%else
-    movq        [r0-4], m0
-    movhps      [r0+r1-4], m0
-    movq        [r0+r1*2-4], m1
-    movhps      [%1-4], m1
-    movq        [%1+r1-4], m2
-    movhps      [%1+r1*2-4], m2
-    movq        [%1+%2-4], m3
-    movhps      [%1+r1*4-4], m3
-%endif
-%endmacro
-
-%macro DEBLOCK_LUMA 0
-;-----------------------------------------------------------------------------
-; void ff_deblock_v_luma_10(uint16_t *pix, int stride, int alpha, int beta,
-;                           int8_t *tc0)
-;-----------------------------------------------------------------------------
-cglobal deblock_v_luma_10, 5,5,8*(mmsize/16)
-    %assign pad 5*mmsize+12-(stack_offset&15)
-    %define tcm [rsp]
-    %define ms1 [rsp+mmsize]
-    %define ms2 [rsp+mmsize*2]
-    %define am  [rsp+mmsize*3]
-    %define bm  [rsp+mmsize*4]
-    SUB        rsp, pad
-    shl        r2d, 2
-    shl        r3d, 2
-    LOAD_AB     m4, m5, r2d, r3d
-    mov         r3, 32/mmsize
-    mov         r2, r0
-    sub         r0, r1
-    mova        am, m4
-    sub         r0, r1
-    mova        bm, m5
-    sub         r0, r1
-.loop:
-    mova        m0, [r0+r1]
-    mova        m1, [r0+r1*2]
-    mova        m2, [r2]
-    mova        m3, [r2+r1]
-
-    LOAD_MASK   m0, m1, m2, m3, am, bm, m7, m4, m6
-    LOAD_TC     m6, r4
-    mova       tcm, m6
-
-    mova        m5, [r0]
-    LUMA_DEBLOCK_ONE m1, m0, ms1
-    mova   [r0+r1], m5
-
-    mova        m5, [r2+r1*2]
-    LUMA_DEBLOCK_ONE m2, m3, ms2
-    mova   [r2+r1], m5
-
-    pxor        m5, m5
-    mova        m6, tcm
-    pcmpgtw     m5, tcm
-    psubw       m6, ms1
-    pandn       m5, m7
-    psubw       m6, ms2
-    pand        m5, m6
-    DEBLOCK_P0_Q0 m1, m2, m0, m3, m5, m7, m6
-    mova [r0+r1*2], m1
-    mova      [r2], m2
-
-    add         r0, mmsize
-    add         r2, mmsize
-    add         r4, mmsize/8
-    dec         r3
-    jg .loop
-    ADD         rsp, pad
-    RET
-
-cglobal deblock_h_luma_10, 5,6,8*(mmsize/16)
-    %assign pad 7*mmsize+12-(stack_offset&15)
-    %define tcm [rsp]
-    %define ms1 [rsp+mmsize]
-    %define ms2 [rsp+mmsize*2]
-    %define p1m [rsp+mmsize*3]
-    %define p2m [rsp+mmsize*4]
-    %define am  [rsp+mmsize*5]
-    %define bm  [rsp+mmsize*6]
-    SUB        rsp, pad
-    shl        r2d, 2
-    shl        r3d, 2
-    LOAD_AB     m4, m5, r2d, r3d
-    mov         r3, r1
-    mova        am, m4
-    add         r3, r1
-    mov         r5, 32/mmsize
-    mova        bm, m5
-    add         r3, r1
-%if mmsize == 16
-    mov         r2, r0
-    add         r2, r3
-%endif
-.loop:
-%if mmsize == 8
-    movq        m2, [r0-8]     ; y q2 q1 q0
-    movq        m7, [r0+0]
-    movq        m5, [r0+r1-8]
-    movq        m3, [r0+r1+0]
-    movq        m0, [r0+r1*2-8]
-    movq        m6, [r0+r1*2+0]
-    movq        m1, [r0+r3-8]
-    TRANSPOSE4x4W 2, 5, 0, 1, 4
-    SWAP         2, 7
-    movq        m7, [r0+r3]
-    TRANSPOSE4x4W 2, 3, 6, 7, 4
-%else
-    movu        m5, [r0-8]     ; y q2 q1 q0 p0 p1 p2 x
-    movu        m0, [r0+r1-8]
-    movu        m2, [r0+r1*2-8]
-    movu        m3, [r2-8]
-    TRANSPOSE4x4W 5, 0, 2, 3, 6
-    mova       tcm, m3
-
-    movu        m4, [r2+r1-8]
-    movu        m1, [r2+r1*2-8]
-    movu        m3, [r2+r3-8]
-    movu        m7, [r2+r1*4-8]
-    TRANSPOSE4x4W 4, 1, 3, 7, 6
-
-    mova        m6, tcm
-    punpcklqdq  m6, m7
-    punpckhqdq  m5, m4
-    SBUTTERFLY qdq, 0, 1, 7
-    SBUTTERFLY qdq, 2, 3, 7
-%endif
-
-    mova       p2m, m6
-    LOAD_MASK   m0, m1, m2, m3, am, bm, m7, m4, m6
-    LOAD_TC     m6, r4
-    mova       tcm, m6
-
-    LUMA_DEBLOCK_ONE m1, m0, ms1
-    mova       p1m, m5
-
-    mova        m5, p2m
-    LUMA_DEBLOCK_ONE m2, m3, ms2
-    mova       p2m, m5
-
-    pxor        m5, m5
-    mova        m6, tcm
-    pcmpgtw     m5, tcm
-    psubw       m6, ms1
-    pandn       m5, m7
-    psubw       m6, ms2
-    pand        m5, m6
-    DEBLOCK_P0_Q0 m1, m2, m0, m3, m5, m7, m6
-    mova        m0, p1m
-    mova        m3, p2m
-    TRANSPOSE4x4W 0, 1, 2, 3, 4
-    LUMA_H_STORE r2, r3
-
-    add         r4, mmsize/8
-    lea         r0, [r0+r1*(mmsize/2)]
-    lea         r2, [r2+r1*(mmsize/2)]
-    dec         r5
-    jg .loop
-    ADD        rsp, pad
-    RET
-%endmacro
-
-%if ARCH_X86_64
-; in:  m0=p1, m1=p0, m2=q0, m3=q1, m8=p2, m9=q2
-;      m12=alpha, m13=beta
-; out: m0=p1', m3=q1', m1=p0', m2=q0'
-; clobbers: m4, m5, m6, m7, m10, m11, m14
-%macro DEBLOCK_LUMA_INTER_SSE2 0
-    LOAD_MASK   m0, m1, m2, m3, m12, m13, m7, m4, m6
-    LOAD_TC     m6, r4
-    DIFF_LT     m8, m1, m13, m10, m4
-    DIFF_LT     m9, m2, m13, m11, m4
-    pand        m6, m7
-
-    mova       m14, m6
-    pxor        m4, m4
-    pcmpgtw     m6, m4
-    pand        m6, m14
-
-    mova        m5, m10
-    pand        m5, m6
-    LUMA_Q1 m8, m0, m1, m2, m5, m4
-
-    mova        m5, m11
-    pand        m5, m6
-    LUMA_Q1 m9, m3, m1, m2, m5, m4
-
-    pxor        m4, m4
-    psubw       m6, m10
-    pcmpgtw     m4, m14
-    pandn       m4, m7
-    psubw       m6, m11
-    pand        m4, m6
-    DEBLOCK_P0_Q0 m1, m2, m0, m3, m4, m5, m6
-
-    SWAP         0, 8
-    SWAP         3, 9
-%endmacro
-
-%macro DEBLOCK_LUMA_64 0
-cglobal deblock_v_luma_10, 5,5,15
-    %define p2 m8
-    %define p1 m0
-    %define p0 m1
-    %define q0 m2
-    %define q1 m3
-    %define q2 m9
-    %define mask0 m7
-    %define mask1 m10
-    %define mask2 m11
-    shl        r2d, 2
-    shl        r3d, 2
-    LOAD_AB    m12, m13, r2d, r3d
-    mov         r2, r0
-    sub         r0, r1
-    sub         r0, r1
-    sub         r0, r1
-    mov         r3, 2
-.loop:
-    mova        p2, [r0]
-    mova        p1, [r0+r1]
-    mova        p0, [r0+r1*2]
-    mova        q0, [r2]
-    mova        q1, [r2+r1]
-    mova        q2, [r2+r1*2]
-    DEBLOCK_LUMA_INTER_SSE2
-    mova   [r0+r1], p1
-    mova [r0+r1*2], p0
-    mova      [r2], q0
-    mova   [r2+r1], q1
-    add         r0, mmsize
-    add         r2, mmsize
-    add         r4, 2
-    dec         r3
-    jg .loop
-    REP_RET
-
-cglobal deblock_h_luma_10, 5,7,15
-    shl        r2d, 2
-    shl        r3d, 2
-    LOAD_AB    m12, m13, r2d, r3d
-    mov         r2, r1
-    add         r2, r1
-    add         r2, r1
-    mov         r5, r0
-    add         r5, r2
-    mov         r6, 2
-.loop:
-    movu        m8, [r0-8]     ; y q2 q1 q0 p0 p1 p2 x
-    movu        m0, [r0+r1-8]
-    movu        m2, [r0+r1*2-8]
-    movu        m9, [r5-8]
-    movu        m5, [r5+r1-8]
-    movu        m1, [r5+r1*2-8]
-    movu        m3, [r5+r2-8]
-    movu        m7, [r5+r1*4-8]
-
-    TRANSPOSE4x4W 8, 0, 2, 9, 10
-    TRANSPOSE4x4W 5, 1, 3, 7, 10
-
-    punpckhqdq  m8, m5
-    SBUTTERFLY qdq, 0, 1, 10
-    SBUTTERFLY qdq, 2, 3, 10
-    punpcklqdq  m9, m7
-
-    DEBLOCK_LUMA_INTER_SSE2
-
-    TRANSPOSE4x4W 0, 1, 2, 3, 4
-    LUMA_H_STORE r5, r2
-    add         r4, 2
-    lea         r0, [r0+r1*8]
-    lea         r5, [r5+r1*8]
-    dec         r6
-    jg .loop
-    REP_RET
-%endmacro
-
-INIT_XMM sse2
-DEBLOCK_LUMA_64
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-DEBLOCK_LUMA_64
-%endif
-%endif
-
-%macro SWAPMOVA 2
-%ifid %1
-    SWAP %1, %2
-%else
-    mova %1, %2
-%endif
-%endmacro
-
-; in: t0-t2: tmp registers
-;     %1=p0 %2=p1 %3=p2 %4=p3 %5=q0 %6=q1 %7=mask0
-;     %8=mask1p %9=2 %10=p0' %11=p1' %12=p2'
-%macro LUMA_INTRA_P012 12 ; p0..p3 in memory
-%if ARCH_X86_64
-    paddw     t0, %3, %2
-    mova      t2, %4
-    paddw     t2, %3
-%else
-    mova      t0, %3
-    mova      t2, %4
-    paddw     t0, %2
-    paddw     t2, %3
-%endif
-    paddw     t0, %1
-    paddw     t2, t2
-    paddw     t0, %5
-    paddw     t2, %9
-    paddw     t0, %9    ; (p2 + p1 + p0 + q0 + 2)
-    paddw     t2, t0    ; (2*p3 + 3*p2 + p1 + p0 + q0 + 4)
-
-    psrlw     t2, 3
-    psrlw     t1, t0, 2
-    psubw     t2, %3
-    psubw     t1, %2
-    pand      t2, %8
-    pand      t1, %8
-    paddw     t2, %3
-    paddw     t1, %2
-    SWAPMOVA %11, t1
-
-    psubw     t1, t0, %3
-    paddw     t0, t0
-    psubw     t1, %5
-    psubw     t0, %3
-    paddw     t1, %6
-    paddw     t1, %2
-    paddw     t0, %6
-    psrlw     t1, 2     ; (2*p1 + p0 + q1 + 2)/4
-    psrlw     t0, 3     ; (p2 + 2*p1 + 2*p0 + 2*q0 + q1 + 4)>>3
-
-    pxor      t0, t1
-    pxor      t1, %1
-    pand      t0, %8
-    pand      t1, %7
-    pxor      t0, t1
-    pxor      t0, %1
-    SWAPMOVA %10, t0
-    SWAPMOVA %12, t2
-%endmacro
-
-%macro LUMA_INTRA_INIT 1
-    %xdefine pad %1*mmsize+((gprsize*3) % mmsize)-(stack_offset&15)
-    %define t0 m4
-    %define t1 m5
-    %define t2 m6
-    %define t3 m7
-    %assign i 4
-%rep %1
-    CAT_XDEFINE t, i, [rsp+mmsize*(i-4)]
-    %assign i i+1
-%endrep
-    SUB    rsp, pad
-%endmacro
-
-; in: %1-%3=tmp, %4=p2, %5=q2
-%macro LUMA_INTRA_INTER 5
-    LOAD_AB t0, t1, r2d, r3d
-    mova    %1, t0
-    LOAD_MASK m0, m1, m2, m3, %1, t1, t0, t2, t3
-%if ARCH_X86_64
-    mova    %2, t0        ; mask0
-    psrlw   t3, %1, 2
-%else
-    mova    t3, %1
-    mova    %2, t0        ; mask0
-    psrlw   t3, 2
-%endif
-    paddw   t3, [pw_2]    ; alpha/4+2
-    DIFF_LT m1, m2, t3, t2, t0 ; t2 = |p0-q0| < alpha/4+2
-    pand    t2, %2
-    mova    t3, %5        ; q2
-    mova    %1, t2        ; mask1
-    DIFF_LT t3, m2, t1, t2, t0 ; t2 = |q2-q0| < beta
-    pand    t2, %1
-    mova    t3, %4        ; p2
-    mova    %3, t2        ; mask1q
-    DIFF_LT t3, m1, t1, t2, t0 ; t2 = |p2-p0| < beta
-    pand    t2, %1
-    mova    %1, t2        ; mask1p
-%endmacro
-
-%macro LUMA_H_INTRA_LOAD 0
-%if mmsize == 8
-    movu    t0, [r0-8]
-    movu    t1, [r0+r1-8]
-    movu    m0, [r0+r1*2-8]
-    movu    m1, [r0+r4-8]
-    TRANSPOSE4x4W 4, 5, 0, 1, 2
-    mova    t4, t0        ; p3
-    mova    t5, t1        ; p2
-
-    movu    m2, [r0]
-    movu    m3, [r0+r1]
-    movu    t0, [r0+r1*2]
-    movu    t1, [r0+r4]
-    TRANSPOSE4x4W 2, 3, 4, 5, 6
-    mova    t6, t0        ; q2
-    mova    t7, t1        ; q3
-%else
-    movu    t0, [r0-8]
-    movu    t1, [r0+r1-8]
-    movu    m0, [r0+r1*2-8]
-    movu    m1, [r0+r5-8]
-    movu    m2, [r4-8]
-    movu    m3, [r4+r1-8]
-    movu    t2, [r4+r1*2-8]
-    movu    t3, [r4+r5-8]
-    TRANSPOSE8x8W 4, 5, 0, 1, 2, 3, 6, 7, t4, t5
-    mova    t4, t0        ; p3
-    mova    t5, t1        ; p2
-    mova    t6, t2        ; q2
-    mova    t7, t3        ; q3
-%endif
-%endmacro
-
-; in: %1=q3 %2=q2' %3=q1' %4=q0' %5=p0' %6=p1' %7=p2' %8=p3 %9=tmp
-%macro LUMA_H_INTRA_STORE 9
-%if mmsize == 8
-    TRANSPOSE4x4W %1, %2, %3, %4, %9
-    movq       [r0-8], m%1
-    movq       [r0+r1-8], m%2
-    movq       [r0+r1*2-8], m%3
-    movq       [r0+r4-8], m%4
-    movq       m%1, %8
-    TRANSPOSE4x4W %5, %6, %7, %1, %9
-    movq       [r0], m%5
-    movq       [r0+r1], m%6
-    movq       [r0+r1*2], m%7
-    movq       [r0+r4], m%1
-%else
-    TRANSPOSE2x4x4W %1, %2, %3, %4, %9
-    movq       [r0-8], m%1
-    movq       [r0+r1-8], m%2
-    movq       [r0+r1*2-8], m%3
-    movq       [r0+r5-8], m%4
-    movhps     [r4-8], m%1
-    movhps     [r4+r1-8], m%2
-    movhps     [r4+r1*2-8], m%3
-    movhps     [r4+r5-8], m%4
-%ifnum %8
-    SWAP       %1, %8
-%else
-    mova       m%1, %8
-%endif
-    TRANSPOSE2x4x4W %5, %6, %7, %1, %9
-    movq       [r0], m%5
-    movq       [r0+r1], m%6
-    movq       [r0+r1*2], m%7
-    movq       [r0+r5], m%1
-    movhps     [r4], m%5
-    movhps     [r4+r1], m%6
-    movhps     [r4+r1*2], m%7
-    movhps     [r4+r5], m%1
-%endif
-%endmacro
-
-%if ARCH_X86_64
-;-----------------------------------------------------------------------------
-; void ff_deblock_v_luma_intra_10(uint16_t *pix, int stride, int alpha,
-;                                 int beta)
-;-----------------------------------------------------------------------------
-%macro DEBLOCK_LUMA_INTRA_64 0
-cglobal deblock_v_luma_intra_10, 4,7,16
-    %define t0 m1
-    %define t1 m2
-    %define t2 m4
-    %define p2 m8
-    %define p1 m9
-    %define p0 m10
-    %define q0 m11
-    %define q1 m12
-    %define q2 m13
-    %define aa m5
-    %define bb m14
-    lea     r4, [r1*4]
-    lea     r5, [r1*3] ; 3*stride
-    neg     r4
-    add     r4, r0     ; pix-4*stride
-    mov     r6, 2
-    mova    m0, [pw_2]
-    shl    r2d, 2
-    shl    r3d, 2
-    LOAD_AB aa, bb, r2d, r3d
-.loop:
-    mova    p2, [r4+r1]
-    mova    p1, [r4+2*r1]
-    mova    p0, [r4+r5]
-    mova    q0, [r0]
-    mova    q1, [r0+r1]
-    mova    q2, [r0+2*r1]
-
-    LOAD_MASK p1, p0, q0, q1, aa, bb, m3, t0, t1
-    mova    t2, aa
-    psrlw   t2, 2
-    paddw   t2, m0 ; alpha/4+2
-    DIFF_LT p0, q0, t2, m6, t0 ; m6 = |p0-q0| < alpha/4+2
-    DIFF_LT p2, p0, bb, t1, t0 ; m7 = |p2-p0| < beta
-    DIFF_LT q2, q0, bb, m7, t0 ; t1 = |q2-q0| < beta
-    pand    m6, m3
-    pand    m7, m6
-    pand    m6, t1
-    LUMA_INTRA_P012 p0, p1, p2, [r4], q0, q1, m3, m6, m0, [r4+r5], [r4+2*r1], [r4+r1]
-    LUMA_INTRA_P012 q0, q1, q2, [r0+r5], p0, p1, m3, m7, m0, [r0], [r0+r1], [r0+2*r1]
-    add     r0, mmsize
-    add     r4, mmsize
-    dec     r6
-    jg .loop
-    REP_RET
-
-;-----------------------------------------------------------------------------
-; void ff_deblock_h_luma_intra_10(uint16_t *pix, int stride, int alpha,
-;                                 int beta)
-;-----------------------------------------------------------------------------
-cglobal deblock_h_luma_intra_10, 4,7,16
-    %define t0 m15
-    %define t1 m14
-    %define t2 m2
-    %define q3 m5
-    %define q2 m8
-    %define q1 m9
-    %define q0 m10
-    %define p0 m11
-    %define p1 m12
-    %define p2 m13
-    %define p3 m4
-    %define spill [rsp]
-    %assign pad 24-(stack_offset&15)
-    SUB     rsp, pad
-    lea     r4, [r1*4]
-    lea     r5, [r1*3] ; 3*stride
-    add     r4, r0     ; pix+4*stride
-    mov     r6, 2
-    mova    m0, [pw_2]
-    shl    r2d, 2
-    shl    r3d, 2
-.loop:
-    movu    q3, [r0-8]
-    movu    q2, [r0+r1-8]
-    movu    q1, [r0+r1*2-8]
-    movu    q0, [r0+r5-8]
-    movu    p0, [r4-8]
-    movu    p1, [r4+r1-8]
-    movu    p2, [r4+r1*2-8]
-    movu    p3, [r4+r5-8]
-    TRANSPOSE8x8W 5, 8, 9, 10, 11, 12, 13, 4, 1
-
-    LOAD_AB m1, m2, r2d, r3d
-    LOAD_MASK q1, q0, p0, p1, m1, m2, m3, t0, t1
-    psrlw   m1, 2
-    paddw   m1, m0 ; alpha/4+2
-    DIFF_LT p0, q0, m1, m6, t0 ; m6 = |p0-q0| < alpha/4+2
-    DIFF_LT q2, q0, m2, t1, t0 ; t1 = |q2-q0| < beta
-    DIFF_LT p0, p2, m2, m7, t0 ; m7 = |p2-p0| < beta
-    pand    m6, m3
-    pand    m7, m6
-    pand    m6, t1
-
-    mova spill, q3
-    LUMA_INTRA_P012 q0, q1, q2, q3, p0, p1, m3, m6, m0, m5, m1, q2
-    LUMA_INTRA_P012 p0, p1, p2, p3, q0, q1, m3, m7, m0, p0, m6, p2
-    mova    m7, spill
-
-    LUMA_H_INTRA_STORE 7, 8, 1, 5, 11, 6, 13, 4, 14
-
-    lea     r0, [r0+r1*8]
-    lea     r4, [r4+r1*8]
-    dec     r6
-    jg .loop
-    ADD    rsp, pad
-    RET
-%endmacro
-
-INIT_XMM sse2
-DEBLOCK_LUMA_INTRA_64
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-DEBLOCK_LUMA_INTRA_64
-%endif
-
-%endif
-
-%macro DEBLOCK_LUMA_INTRA 0
-;-----------------------------------------------------------------------------
-; void ff_deblock_v_luma_intra_10(uint16_t *pix, int stride, int alpha,
-;                                 int beta)
-;-----------------------------------------------------------------------------
-cglobal deblock_v_luma_intra_10, 4,7,8*(mmsize/16)
-    LUMA_INTRA_INIT 3
-    lea     r4, [r1*4]
-    lea     r5, [r1*3]
-    neg     r4
-    add     r4, r0
-    mov     r6, 32/mmsize
-    shl    r2d, 2
-    shl    r3d, 2
-.loop:
-    mova    m0, [r4+r1*2] ; p1
-    mova    m1, [r4+r5]   ; p0
-    mova    m2, [r0]      ; q0
-    mova    m3, [r0+r1]   ; q1
-    LUMA_INTRA_INTER t4, t5, t6, [r4+r1], [r0+r1*2]
-    LUMA_INTRA_P012 m1, m0, t3, [r4], m2, m3, t5, t4, [pw_2], [r4+r5], [r4+2*r1], [r4+r1]
-    mova    t3, [r0+r1*2] ; q2
-    LUMA_INTRA_P012 m2, m3, t3, [r0+r5], m1, m0, t5, t6, [pw_2], [r0], [r0+r1], [r0+2*r1]
-    add     r0, mmsize
-    add     r4, mmsize
-    dec     r6
-    jg .loop
-    ADD    rsp, pad
-    RET
-
-;-----------------------------------------------------------------------------
-; void ff_deblock_h_luma_intra_10(uint16_t *pix, int stride, int alpha,
-;                                 int beta)
-;-----------------------------------------------------------------------------
-cglobal deblock_h_luma_intra_10, 4,7,8*(mmsize/16)
-    LUMA_INTRA_INIT 8
-%if mmsize == 8
-    lea     r4, [r1*3]
-    mov     r5, 32/mmsize
-%else
-    lea     r4, [r1*4]
-    lea     r5, [r1*3] ; 3*stride
-    add     r4, r0     ; pix+4*stride
-    mov     r6, 32/mmsize
-%endif
-    shl    r2d, 2
-    shl    r3d, 2
-.loop:
-    LUMA_H_INTRA_LOAD
-    LUMA_INTRA_INTER t8, t9, t10, t5, t6
-
-    LUMA_INTRA_P012 m1, m0, t3, t4, m2, m3, t9, t8, [pw_2], t8, t5, t11
-    mova    t3, t6     ; q2
-    LUMA_INTRA_P012 m2, m3, t3, t7, m1, m0, t9, t10, [pw_2], m4, t6, m5
-
-    mova    m2, t4
-    mova    m0, t11
-    mova    m1, t5
-    mova    m3, t8
-    mova    m6, t6
-
-    LUMA_H_INTRA_STORE 2, 0, 1, 3, 4, 6, 5, t7, 7
-
-    lea     r0, [r0+r1*(mmsize/2)]
-%if mmsize == 8
-    dec     r5
-%else
-    lea     r4, [r4+r1*(mmsize/2)]
-    dec     r6
-%endif
-    jg .loop
-    ADD    rsp, pad
-    RET
-%endmacro
-
-%if ARCH_X86_64 == 0
-INIT_MMX mmxext
-DEBLOCK_LUMA
-DEBLOCK_LUMA_INTRA
-INIT_XMM sse2
-DEBLOCK_LUMA
-DEBLOCK_LUMA_INTRA
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-DEBLOCK_LUMA
-DEBLOCK_LUMA_INTRA
-%endif
-%endif
-
-; in: %1=p0, %2=q0, %3=p1, %4=q1, %5=mask, %6=tmp, %7=tmp
-; out: %1=p0', %2=q0'
-%macro CHROMA_DEBLOCK_P0_Q0_INTRA 7
-    mova    %6, [pw_2]
-    paddw   %6, %3
-    paddw   %6, %4
-    paddw   %7, %6, %2
-    paddw   %6, %1
-    paddw   %6, %3
-    paddw   %7, %4
-    psraw   %6, 2
-    psraw   %7, 2
-    psubw   %6, %1
-    psubw   %7, %2
-    pand    %6, %5
-    pand    %7, %5
-    paddw   %1, %6
-    paddw   %2, %7
-%endmacro
-
-%macro CHROMA_V_LOAD 1
-    mova        m0, [r0]    ; p1
-    mova        m1, [r0+r1] ; p0
-    mova        m2, [%1]    ; q0
-    mova        m3, [%1+r1] ; q1
-%endmacro
-
-%macro CHROMA_V_STORE 0
-    mova [r0+1*r1], m1
-    mova [r0+2*r1], m2
-%endmacro
-
-; in: 8 rows of 4 words in %4..%11
-; out: 4 rows of 8 words in m0..m3
-%macro TRANSPOSE4x8W_LOAD 8
-    movq             m0, %1
-    movq             m2, %2
-    movq             m1, %3
-    movq             m3, %4
-
-    punpcklwd        m0, m2
-    punpcklwd        m1, m3
-    punpckhdq        m2, m0, m1
-    punpckldq        m0, m1
-
-    movq             m4, %5
-    movq             m6, %6
-    movq             m5, %7
-    movq             m3, %8
-
-    punpcklwd        m4, m6
-    punpcklwd        m5, m3
-    punpckhdq        m6, m4, m5
-    punpckldq        m4, m5
-
-    punpckhqdq       m1, m0, m4
-    punpcklqdq       m0, m4
-    punpckhqdq       m3, m2, m6
-    punpcklqdq       m2, m6
-%endmacro
-
-; in: 4 rows of 8 words in m0..m3
-; out: 8 rows of 4 words in %1..%8
-%macro TRANSPOSE8x4W_STORE 8
-    TRANSPOSE4x4W     0, 1, 2, 3, 4
-    movq             %1, m0
-    movhps           %2, m0
-    movq             %3, m1
-    movhps           %4, m1
-    movq             %5, m2
-    movhps           %6, m2
-    movq             %7, m3
-    movhps           %8, m3
-%endmacro
-
-; %1 = base + 3*stride
-; %2 = 3*stride (unused on mmx)
-; %3, %4 = place to store p1 and q1 values
-%macro CHROMA_H_LOAD 4
-    %if mmsize == 8
-        movq m0, [pix_q - 4]
-        movq m1, [pix_q +   stride_q - 4]
-        movq m2, [pix_q + 2*stride_q - 4]
-        movq m3, [%1 - 4]
-        TRANSPOSE4x4W 0, 1, 2, 3, 4
-    %else
-        TRANSPOSE4x8W_LOAD PASS8ROWS(pix_q-4, %1-4, stride_q, %2)
-    %endif
-    mova %3, m0
-    mova %4, m3
-%endmacro
-
-; %1 = base + 3*stride
-; %2 = 3*stride (unused on mmx)
-; %3, %4 = place to load p1 and q1 values
-%macro CHROMA_H_STORE 4
-    mova m0, %3
-    mova m3, %4
-    %if mmsize == 8
-        TRANSPOSE4x4W 0, 1, 2, 3, 4
-        movq [pix_q - 4],              m0
-        movq [pix_q +   stride_q - 4], m1
-        movq [pix_q + 2*stride_q - 4], m2
-        movq [%1 - 4],                 m3
-    %else
-        TRANSPOSE8x4W_STORE PASS8ROWS(pix_q-4, %1-4, stride_q, %2)
-    %endif
-%endmacro
-
-%macro CHROMA_V_LOAD_TC 2
-    movd        %1, [%2]
-    punpcklbw   %1, %1
-    punpcklwd   %1, %1
-    psraw       %1, 6
-%endmacro
-
-%macro DEBLOCK_CHROMA 0
-;-----------------------------------------------------------------------------
-; void ff_deblock_v_chroma_10(uint16_t *pix, int stride, int alpha, int beta,
-;                             int8_t *tc0)
-;-----------------------------------------------------------------------------
-cglobal deblock_v_chroma_10, 5,7-(mmsize/16),8*(mmsize/16)
-    mov         r5, r0
-    sub         r0, r1
-    sub         r0, r1
-    shl        r2d, 2
-    shl        r3d, 2
-%if mmsize < 16
-    mov         r6, 16/mmsize
-.loop:
-%endif
-    CHROMA_V_LOAD r5
-    LOAD_AB     m4, m5, r2d, r3d
-    LOAD_MASK   m0, m1, m2, m3, m4, m5, m7, m6, m4
-    pxor        m4, m4
-    CHROMA_V_LOAD_TC m6, r4
-    psubw       m6, [pw_3]
-    pmaxsw      m6, m4
-    pand        m7, m6
-    DEBLOCK_P0_Q0 m1, m2, m0, m3, m7, m5, m6
-    CHROMA_V_STORE
-%if mmsize < 16
-    add         r0, mmsize
-    add         r5, mmsize
-    add         r4, mmsize/4
-    dec         r6
-    jg .loop
-    REP_RET
-%else
-    RET
-%endif
-
-;-----------------------------------------------------------------------------
-; void ff_deblock_v_chroma_intra_10(uint16_t *pix, int stride, int alpha,
-;                                   int beta)
-;-----------------------------------------------------------------------------
-cglobal deblock_v_chroma_intra_10, 4,6-(mmsize/16),8*(mmsize/16)
-    mov         r4, r0
-    sub         r0, r1
-    sub         r0, r1
-    shl        r2d, 2
-    shl        r3d, 2
-%if mmsize < 16
-    mov         r5, 16/mmsize
-.loop:
-%endif
-    CHROMA_V_LOAD r4
-    LOAD_AB     m4, m5, r2d, r3d
-    LOAD_MASK   m0, m1, m2, m3, m4, m5, m7, m6, m4
-    CHROMA_DEBLOCK_P0_Q0_INTRA m1, m2, m0, m3, m7, m5, m6
-    CHROMA_V_STORE
-%if mmsize < 16
-    add         r0, mmsize
-    add         r4, mmsize
-    dec         r5
-    jg .loop
-    REP_RET
-%else
-    RET
-%endif
-
-;-----------------------------------------------------------------------------
-; void ff_deblock_h_chroma_10(uint16_t *pix, int stride, int alpha, int beta,
-;                             int8_t *tc0)
-;-----------------------------------------------------------------------------
-cglobal deblock_h_chroma_10, 5, 7, 8, 0-2*mmsize, pix_, stride_, alpha_, beta_, tc0_
-    shl alpha_d,  2
-    shl beta_d,   2
-    mov r5,       pix_q
-    lea r6,      [3*stride_q]
-    add r5,       r6
-%if mmsize == 8
-    mov r6d,      2
-    .loop:
-%endif
-
-        CHROMA_H_LOAD r5, r6, [rsp], [rsp + mmsize]
-        LOAD_AB          m4,  m5, alpha_d, beta_d
-        LOAD_MASK        m0,  m1, m2, m3, m4, m5, m7, m6, m4
-        pxor             m4,  m4
-        CHROMA_V_LOAD_TC m6,  tc0_q
-        psubw            m6, [pw_3]
-        pmaxsw           m6,  m4
-        pand             m7,  m6
-        DEBLOCK_P0_Q0    m1,  m2, m0, m3, m7, m5, m6
-        CHROMA_H_STORE r5, r6, [rsp], [rsp + mmsize]
-
-%if mmsize == 8
-        lea pix_q, [pix_q + 4*stride_q]
-        lea r5,    [r5 + 4*stride_q]
-        add tc0_q,  2
-        dec r6d
-    jg .loop
-%endif
-RET
-
-;-----------------------------------------------------------------------------
-; void ff_deblock_h_chroma422_10(uint16_t *pix, int stride, int alpha, int beta,
-;                                int8_t *tc0)
-;-----------------------------------------------------------------------------
-cglobal deblock_h_chroma422_10, 5, 7, 8, 0-3*mmsize, pix_, stride_, alpha_, beta_, tc0_
-    shl alpha_d,  2
-    shl beta_d,   2
-
-    movd m0, [tc0_q]
-    punpcklbw m0, m0
-    psraw m0, 6
-    movq [rsp], m0
-
-    mov r5,       pix_q
-    lea r6,      [3*stride_q]
-    add r5,       r6
-
-    mov r4, -8
-    .loop:
-
-        CHROMA_H_LOAD r5, r6, [rsp + 1*mmsize], [rsp + 2*mmsize]
-        LOAD_AB          m4,  m5, alpha_d, beta_d
-        LOAD_MASK        m0,  m1, m2, m3, m4, m5, m7, m6, m4
-        pxor             m4,  m4
-        movd             m6, [rsp + r4 + 8]
-        punpcklwd        m6,  m6
-        punpcklwd        m6,  m6
-        psubw            m6, [pw_3]
-        pmaxsw           m6,  m4
-        pand             m7,  m6
-        DEBLOCK_P0_Q0    m1,  m2, m0, m3, m7, m5, m6
-        CHROMA_H_STORE r5, r6, [rsp + 1*mmsize], [rsp + 2*mmsize]
-
-        lea pix_q, [pix_q + (mmsize/2)*stride_q]
-        lea r5,    [r5 +    (mmsize/2)*stride_q]
-        add r4, (mmsize/4)
-    jl .loop
-RET
-
-%endmacro
-
-%if ARCH_X86_64 == 0
-INIT_MMX mmxext
-DEBLOCK_CHROMA
-%endif
-INIT_XMM sse2
-DEBLOCK_CHROMA
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-DEBLOCK_CHROMA
-%endif
diff -uparN ffmpeg-4.1/libavcodec/x86/h264_deblock.asm ffmpeg-y/libavcodec/x86/h264_deblock.asm
--- ffmpeg-4.1/libavcodec/x86/h264_deblock.asm	2018-11-02 02:34:25.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/h264_deblock.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,1420 +0,0 @@
-;*****************************************************************************
-;* MMX/SSE2/AVX-optimized H.264 deblocking code
-;*****************************************************************************
-;* Copyright (C) 2005-2011 x264 project
-;*
-;* Authors: Loren Merritt <lorenm@u.washington.edu>
-;*          Fiona Glaser <fiona@x264.com>
-;*          Oskar Arvidsson <oskar@irock.se>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-
-pb_A1: times 16 db 0xA1
-pb_3_1: times 4 db 3, 1
-
-SECTION .text
-
-cextern pb_0
-cextern pb_1
-cextern pb_3
-
-%define PASS8ROWS(base, base3, stride, stride3, offset) \
-    PASS8ROWS(base+offset, base3+offset, stride, stride3)
-
-; in: 8 rows of 4 bytes in %4..%11
-; out: 4 rows of 8 bytes in m0..m3
-%macro TRANSPOSE4x8_LOAD 11
-    movh       m0, %4
-    movh       m2, %5
-    movh       m1, %6
-    movh       m3, %7
-    punpckl%1  m0, m2
-    punpckl%1  m1, m3
-    mova       m2, m0
-    punpckl%2  m0, m1
-    punpckh%2  m2, m1
-
-    movh       m4, %8
-    movh       m6, %9
-    movh       m5, %10
-    movh       m7, %11
-    punpckl%1  m4, m6
-    punpckl%1  m5, m7
-    mova       m6, m4
-    punpckl%2  m4, m5
-    punpckh%2  m6, m5
-
-    punpckh%3  m1, m0, m4
-    punpckh%3  m3, m2, m6
-    punpckl%3  m0, m4
-    punpckl%3  m2, m6
-%endmacro
-
-; in: 4 rows of 8 bytes in m0..m3
-; out: 8 rows of 4 bytes in %1..%8
-%macro TRANSPOSE8x4B_STORE 8
-    punpckhdq  m4, m0, m0
-    punpckhdq  m5, m1, m1
-    punpckhdq  m6, m2, m2
-
-    punpcklbw  m0, m1
-    punpcklbw  m2, m3
-    punpcklwd  m1, m0, m2
-    punpckhwd  m0, m2
-    movh       %1, m1
-    punpckhdq  m1, m1
-    movh       %2, m1
-    movh       %3, m0
-    punpckhdq  m0, m0
-    movh       %4, m0
-
-    punpckhdq  m3, m3
-    punpcklbw  m4, m5
-    punpcklbw  m6, m3
-    punpcklwd  m5, m4, m6
-    punpckhwd  m4, m6
-    movh       %5, m5
-    punpckhdq  m5, m5
-    movh       %6, m5
-    movh       %7, m4
-    punpckhdq  m4, m4
-    movh       %8, m4
-%endmacro
-
-%macro TRANSPOSE4x8B_LOAD 8
-    TRANSPOSE4x8_LOAD bw, wd, dq, %1, %2, %3, %4, %5, %6, %7, %8
-%endmacro
-
-%macro SBUTTERFLY3 4
-    punpckh%1  %4, %2, %3
-    punpckl%1  %2, %3
-%endmacro
-
-; in: 8 rows of 8 (only the middle 6 pels are used) in %1..%8
-; out: 6 rows of 8 in [%9+0*16] .. [%9+5*16]
-%macro TRANSPOSE6x8_MEM 9
-    RESET_MM_PERMUTATION
-    movq  m0, %1
-    movq  m1, %2
-    movq  m2, %3
-    movq  m3, %4
-    movq  m4, %5
-    movq  m5, %6
-    movq  m6, %7
-    SBUTTERFLY bw, 0, 1, 7
-    SBUTTERFLY bw, 2, 3, 7
-    SBUTTERFLY bw, 4, 5, 7
-    movq  [%9+0x10], m3
-    SBUTTERFLY3 bw, m6, %8, m7
-    SBUTTERFLY wd, 0, 2, 3
-    SBUTTERFLY wd, 4, 6, 3
-    punpckhdq m0, m4
-    movq  [%9+0x00], m0
-    SBUTTERFLY3 wd, m1, [%9+0x10], m3
-    SBUTTERFLY wd, 5, 7, 0
-    SBUTTERFLY dq, 1, 5, 0
-    SBUTTERFLY dq, 2, 6, 0
-    punpckldq m3, m7
-    movq  [%9+0x10], m2
-    movq  [%9+0x20], m6
-    movq  [%9+0x30], m1
-    movq  [%9+0x40], m5
-    movq  [%9+0x50], m3
-    RESET_MM_PERMUTATION
-%endmacro
-
-; in: 8 rows of 8 in %1..%8
-; out: 8 rows of 8 in %9..%16
-%macro TRANSPOSE8x8_MEM 16
-    RESET_MM_PERMUTATION
-    movq  m0, %1
-    movq  m1, %2
-    movq  m2, %3
-    movq  m3, %4
-    movq  m4, %5
-    movq  m5, %6
-    movq  m6, %7
-    SBUTTERFLY bw, 0, 1, 7
-    SBUTTERFLY bw, 2, 3, 7
-    SBUTTERFLY bw, 4, 5, 7
-    SBUTTERFLY3 bw, m6, %8, m7
-    movq  %9,  m5
-    SBUTTERFLY wd, 0, 2, 5
-    SBUTTERFLY wd, 4, 6, 5
-    SBUTTERFLY wd, 1, 3, 5
-    movq  %11, m6
-    movq  m6,  %9
-    SBUTTERFLY wd, 6, 7, 5
-    SBUTTERFLY dq, 0, 4, 5
-    SBUTTERFLY dq, 1, 6, 5
-    movq  %9,  m0
-    movq  %10, m4
-    movq  %13, m1
-    movq  %14, m6
-    SBUTTERFLY3 dq, m2, %11, m0
-    SBUTTERFLY dq, 3, 7, 4
-    movq  %11, m2
-    movq  %12, m0
-    movq  %15, m3
-    movq  %16, m7
-    RESET_MM_PERMUTATION
-%endmacro
-
-; out: %4 = |%1-%2|>%3
-; clobbers: %5
-%macro DIFF_GT 5
-%if avx_enabled == 0
-    mova    %5, %2
-    mova    %4, %1
-    psubusb %5, %1
-    psubusb %4, %2
-%else
-    psubusb %5, %2, %1
-    psubusb %4, %1, %2
-%endif
-    por     %4, %5
-    psubusb %4, %3
-%endmacro
-
-; out: %4 = |%1-%2|>%3
-; clobbers: %5
-%macro DIFF_GT2 5
-%if ARCH_X86_64
-    psubusb %5, %2, %1
-    psubusb %4, %1, %2
-%else
-    mova    %5, %2
-    mova    %4, %1
-    psubusb %5, %1
-    psubusb %4, %2
-%endif
-    psubusb %5, %3
-    psubusb %4, %3
-    pcmpeqb %4, %5
-%endmacro
-
-; in: m0=p1 m1=p0 m2=q0 m3=q1 %1=alpha-1 %2=beta-1
-; out: m5=beta-1, m7=mask, %3=alpha-1
-; clobbers: m4,m6
-%macro LOAD_MASK 2-3
-    movd     m4, %1
-    movd     m5, %2
-    SPLATW   m4, m4
-    SPLATW   m5, m5
-    packuswb m4, m4  ; 16x alpha-1
-    packuswb m5, m5  ; 16x beta-1
-%if %0>2
-    mova     %3, m4
-%endif
-    DIFF_GT  m1, m2, m4, m7, m6 ; |p0-q0| > alpha-1
-    DIFF_GT  m0, m1, m5, m4, m6 ; |p1-p0| > beta-1
-    por      m7, m4
-    DIFF_GT  m3, m2, m5, m4, m6 ; |q1-q0| > beta-1
-    por      m7, m4
-    pxor     m6, m6
-    pcmpeqb  m7, m6
-%endmacro
-
-; in: m0=p1 m1=p0 m2=q0 m3=q1 m7=(tc&mask)
-; out: m1=p0' m2=q0'
-; clobbers: m0,3-6
-%macro DEBLOCK_P0_Q0 0
-    pcmpeqb m4, m4
-    pxor    m5, m1, m2   ; p0^q0
-    pxor    m3, m4
-    pand    m5, [pb_1]   ; (p0^q0)&1
-    pavgb   m3, m0       ; (p1 - q1 + 256)>>1
-    pxor    m4, m1
-    pavgb   m3, [pb_3]   ; (((p1 - q1 + 256)>>1)+4)>>1 = 64+2+(p1-q1)>>2
-    pavgb   m4, m2       ; (q0 - p0 + 256)>>1
-    pavgb   m3, m5
-    mova    m6, [pb_A1]
-    paddusb m3, m4       ; d+128+33
-    psubusb m6, m3
-    psubusb m3, [pb_A1]
-    pminub  m6, m7
-    pminub  m3, m7
-    psubusb m1, m6
-    psubusb m2, m3
-    paddusb m1, m3
-    paddusb m2, m6
-%endmacro
-
-; in: m1=p0 m2=q0
-;     %1=p1 %2=q2 %3=[q2] %4=[q1] %5=tc0 %6=tmp
-; out: [q1] = clip( (q2+((p0+q0+1)>>1))>>1, q1-tc0, q1+tc0 )
-; clobbers: q2, tmp, tc0
-%macro LUMA_Q1 6
-    pavgb   %6, m1, m2
-    pavgb   %2, %6       ; avg(p2,avg(p0,q0))
-    pxor    %6, %3
-    pand    %6, [pb_1]   ; (p2^avg(p0,q0))&1
-    psubusb %2, %6       ; (p2+((p0+q0+1)>>1))>>1
-    psubusb %6, %1, %5
-    paddusb %5, %1
-    pmaxub  %2, %6
-    pminub  %2, %5
-    mova    %4, %2
-%endmacro
-
-%if ARCH_X86_64
-;-----------------------------------------------------------------------------
-; void ff_deblock_v_luma(uint8_t *pix, int stride, int alpha, int beta,
-;                        int8_t *tc0)
-;-----------------------------------------------------------------------------
-%macro DEBLOCK_LUMA 0
-cglobal deblock_v_luma_8, 5,5,10, pix_, stride_, alpha_, beta_, base3_
-    movd    m8, [r4] ; tc0
-    lea     r4, [stride_q*3]
-    dec     alpha_d        ; alpha-1
-    neg     r4
-    dec     beta_d        ; beta-1
-    add     base3_q, pix_q     ; pix-3*stride
-
-    mova    m0, [base3_q + stride_q]   ; p1
-    mova    m1, [base3_q + 2*stride_q] ; p0
-    mova    m2, [pix_q]      ; q0
-    mova    m3, [pix_q + stride_q]   ; q1
-    LOAD_MASK r2d, r3d
-
-    punpcklbw m8, m8
-    punpcklbw m8, m8 ; tc = 4x tc0[3], 4x tc0[2], 4x tc0[1], 4x tc0[0]
-    pcmpeqb m9, m9
-    pcmpeqb m9, m8
-    pandn   m9, m7
-    pand    m8, m9
-
-    movdqa  m3, [base3_q] ; p2
-    DIFF_GT2 m1, m3, m5, m6, m7 ; |p2-p0| > beta-1
-    pand    m6, m9
-    psubb   m7, m8, m6
-    pand    m6, m8
-    LUMA_Q1 m0, m3, [base3_q], [base3_q + stride_q], m6, m4
-
-    movdqa  m4, [pix_q + 2*stride_q] ; q2
-    DIFF_GT2 m2, m4, m5, m6, m3 ; |q2-q0| > beta-1
-    pand    m6, m9
-    pand    m8, m6
-    psubb   m7, m6
-    mova    m3, [pix_q + stride_q]
-    LUMA_Q1 m3, m4, [pix_q + 2*stride_q], [pix_q + stride_q], m8, m6
-
-    DEBLOCK_P0_Q0
-    mova    [base3_q + 2*stride_q], m1
-    mova    [pix_q], m2
-    RET
-
-;-----------------------------------------------------------------------------
-; void ff_deblock_h_luma(uint8_t *pix, int stride, int alpha, int beta,
-;                        int8_t *tc0)
-;-----------------------------------------------------------------------------
-INIT_MMX cpuname
-cglobal deblock_h_luma_8, 5,9,0,0x60+16*WIN64
-    movsxd r7,  r1d
-    lea    r8,  [r7+r7*2]
-    lea    r6,  [r0-4]
-    lea    r5,  [r0-4+r8]
-%if WIN64
-    %define pix_tmp rsp+0x30 ; shadow space + r4
-%else
-    %define pix_tmp rsp
-%endif
-
-    ; transpose 6x16 -> tmp space
-    TRANSPOSE6x8_MEM  PASS8ROWS(r6, r5, r7, r8), pix_tmp
-    lea    r6, [r6+r7*8]
-    lea    r5, [r5+r7*8]
-    TRANSPOSE6x8_MEM  PASS8ROWS(r6, r5, r7, r8), pix_tmp+8
-
-    ; vertical filter
-    ; alpha, beta, tc0 are still in r2d, r3d, r4
-    ; don't backup r6, r5, r7, r8 because deblock_v_luma_sse2 doesn't use them
-    lea    r0, [pix_tmp+0x30]
-    mov    r1d, 0x10
-%if WIN64
-    mov    [rsp+0x20], r4
-%endif
-    call   deblock_v_luma_8
-
-    ; transpose 16x4 -> original space  (only the middle 4 rows were changed by the filter)
-    add    r6, 2
-    add    r5, 2
-    movq   m0, [pix_tmp+0x18]
-    movq   m1, [pix_tmp+0x28]
-    movq   m2, [pix_tmp+0x38]
-    movq   m3, [pix_tmp+0x48]
-    TRANSPOSE8x4B_STORE  PASS8ROWS(r6, r5, r7, r8)
-
-    shl    r7,  3
-    sub    r6,  r7
-    sub    r5,  r7
-    shr    r7,  3
-    movq   m0, [pix_tmp+0x10]
-    movq   m1, [pix_tmp+0x20]
-    movq   m2, [pix_tmp+0x30]
-    movq   m3, [pix_tmp+0x40]
-    TRANSPOSE8x4B_STORE  PASS8ROWS(r6, r5, r7, r8)
-
-    RET
-%endmacro
-
-%macro DEBLOCK_H_LUMA_MBAFF 0
-
-cglobal deblock_h_luma_mbaff_8, 5, 9, 10, 8*16, pix_, stride_, alpha_, beta_, tc0_, base3_, stride3_
-    movsxd stride_q,   stride_d
-    dec    alpha_d
-    dec    beta_d
-    mov    base3_q,    pix_q
-    lea    stride3_q, [3*stride_q]
-    add    base3_q,    stride3_q
-
-    movq m0, [pix_q - 4]
-    movq m1, [pix_q + stride_q - 4]
-    movq m2, [pix_q + 2*stride_q - 4]
-    movq m3, [base3_q - 4]
-    movq m4, [base3_q + stride_q - 4]
-    movq m5, [base3_q + 2*stride_q - 4]
-    movq m6, [base3_q + stride3_q - 4]
-    movq m7, [base3_q + 4*stride_q - 4]
-
-    TRANSPOSE_8X8B 0,1,2,3,4,5,6,7
-
-    %assign i 0
-    %rep 8
-        movq [rsp + 16*i], m %+ i
-        %assign i i+1
-    %endrep
-
-    ; p2 = m1 [rsp + 16]
-    ; p1 = m2 [rsp + 32]
-    ; p0 = m3 [rsp + 48]
-    ; q0 = m4 [rsp + 64]
-    ; q1 = m5 [rsp + 80]
-    ; q2 = m6 [rsp + 96]
-
-    SWAP 0, 2
-    SWAP 1, 3
-    SWAP 2, 4
-    SWAP 3, 5
-
-    LOAD_MASK alpha_d, beta_d
-    movd m8, [tc0_q]
-    punpcklbw m8, m8
-    pcmpeqb m9, m9
-    pcmpeqb m9, m8
-    pandn   m9, m7
-    pand    m8, m9
-
-    movdqa  m3, [rsp + 16] ; p2
-    DIFF_GT2 m1, m3, m5, m6, m7 ; |p2-p0| > beta-1
-    pand    m6, m9
-    psubb   m7, m8, m6
-    pand    m6, m8
-    LUMA_Q1 m0, m3, [rsp + 16], [rsp + 32], m6, m4
-
-    movdqa  m4, [rsp + 96] ; q2
-    DIFF_GT2 m2, m4, m5, m6, m3 ; |q2-q0| > beta-1
-    pand    m6, m9
-    pand    m8, m6
-    psubb   m7, m6
-    mova    m3, [rsp + 80]
-    LUMA_Q1 m3, m4, [rsp + 96], [rsp + 80], m8, m6
-
-    DEBLOCK_P0_Q0
-    SWAP 1, 3
-    SWAP 2, 4
-    movq m0, [rsp]
-    movq m1, [rsp + 16]
-    movq m2, [rsp + 32]
-    movq m5, [rsp + 80]
-    movq m6, [rsp + 96]
-    movq m7, [rsp + 112]
-
-    TRANSPOSE_8X8B 0,1,2,3,4,5,6,7
-    movq [pix_q - 4], m0
-    movq [pix_q + stride_q - 4], m1
-    movq [pix_q + 2*stride_q - 4], m2
-    movq [base3_q - 4], m3
-    movq [base3_q + stride_q - 4], m4
-    movq [base3_q + 2*stride_q - 4], m5
-    movq [base3_q + stride3_q - 4], m6
-    movq [base3_q + 4*stride_q - 4], m7
-
-RET
-
-%endmacro
-
-INIT_XMM sse2
-DEBLOCK_H_LUMA_MBAFF
-DEBLOCK_LUMA
-
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-DEBLOCK_H_LUMA_MBAFF
-DEBLOCK_LUMA
-%endif
-
-%else
-
-%macro DEBLOCK_LUMA 2
-;-----------------------------------------------------------------------------
-; void ff_deblock_v8_luma(uint8_t *pix, int stride, int alpha, int beta,
-;                         int8_t *tc0)
-;-----------------------------------------------------------------------------
-cglobal deblock_%1_luma_8, 5,5,8,2*%2
-    lea     r4, [r1*3]
-    dec     r2     ; alpha-1
-    neg     r4
-    dec     r3     ; beta-1
-    add     r4, r0 ; pix-3*stride
-
-    mova    m0, [r4+r1]   ; p1
-    mova    m1, [r4+2*r1] ; p0
-    mova    m2, [r0]      ; q0
-    mova    m3, [r0+r1]   ; q1
-    LOAD_MASK r2, r3
-
-    mov     r3, r4mp
-    pcmpeqb m3, m3
-    movd    m4, [r3] ; tc0
-    punpcklbw m4, m4
-    punpcklbw m4, m4 ; tc = 4x tc0[3], 4x tc0[2], 4x tc0[1], 4x tc0[0]
-    mova   [esp+%2], m4 ; tc
-    pcmpgtb m4, m3
-    mova    m3, [r4] ; p2
-    pand    m4, m7
-    mova   [esp], m4 ; mask
-
-    DIFF_GT2 m1, m3, m5, m6, m7 ; |p2-p0| > beta-1
-    pand    m6, m4
-    pand    m4, [esp+%2] ; tc
-    psubb   m7, m4, m6
-    pand    m6, m4
-    LUMA_Q1 m0, m3, [r4], [r4+r1], m6, m4
-
-    mova    m4, [r0+2*r1] ; q2
-    DIFF_GT2 m2, m4, m5, m6, m3 ; |q2-q0| > beta-1
-    pand    m6, [esp] ; mask
-    mova    m5, [esp+%2] ; tc
-    psubb   m7, m6
-    pand    m5, m6
-    mova    m3, [r0+r1]
-    LUMA_Q1 m3, m4, [r0+2*r1], [r0+r1], m5, m6
-
-    DEBLOCK_P0_Q0
-    mova    [r4+2*r1], m1
-    mova    [r0], m2
-    RET
-
-;-----------------------------------------------------------------------------
-; void ff_deblock_h_luma(uint8_t *pix, int stride, int alpha, int beta,
-;                        int8_t *tc0)
-;-----------------------------------------------------------------------------
-INIT_MMX cpuname
-cglobal deblock_h_luma_8, 0,5,8,0x60+12
-    mov    r0, r0mp
-    mov    r3, r1m
-    lea    r4, [r3*3]
-    sub    r0, 4
-    lea    r1, [r0+r4]
-%define pix_tmp esp+12
-
-    ; transpose 6x16 -> tmp space
-    TRANSPOSE6x8_MEM  PASS8ROWS(r0, r1, r3, r4), pix_tmp
-    lea    r0, [r0+r3*8]
-    lea    r1, [r1+r3*8]
-    TRANSPOSE6x8_MEM  PASS8ROWS(r0, r1, r3, r4), pix_tmp+8
-
-    ; vertical filter
-    lea    r0, [pix_tmp+0x30]
-    PUSH   dword r4m
-    PUSH   dword r3m
-    PUSH   dword r2m
-    PUSH   dword 16
-    PUSH   dword r0
-    call   deblock_%1_luma_8
-%ifidn %1, v8
-    add    dword [esp   ], 8 ; pix_tmp+0x38
-    add    dword [esp+16], 2 ; tc0+2
-    call   deblock_%1_luma_8
-%endif
-    ADD    esp, 20
-
-    ; transpose 16x4 -> original space  (only the middle 4 rows were changed by the filter)
-    mov    r0, r0mp
-    sub    r0, 2
-
-    movq   m0, [pix_tmp+0x10]
-    movq   m1, [pix_tmp+0x20]
-    lea    r1, [r0+r4]
-    movq   m2, [pix_tmp+0x30]
-    movq   m3, [pix_tmp+0x40]
-    TRANSPOSE8x4B_STORE  PASS8ROWS(r0, r1, r3, r4)
-
-    lea    r0, [r0+r3*8]
-    lea    r1, [r1+r3*8]
-    movq   m0, [pix_tmp+0x18]
-    movq   m1, [pix_tmp+0x28]
-    movq   m2, [pix_tmp+0x38]
-    movq   m3, [pix_tmp+0x48]
-    TRANSPOSE8x4B_STORE  PASS8ROWS(r0, r1, r3, r4)
-
-    RET
-%endmacro ; DEBLOCK_LUMA
-
-INIT_MMX mmxext
-DEBLOCK_LUMA v8, 8
-INIT_XMM sse2
-DEBLOCK_LUMA v, 16
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-DEBLOCK_LUMA v, 16
-%endif
-
-%endif ; ARCH
-
-
-
-%macro LUMA_INTRA_P012 4 ; p0..p3 in memory
-%if ARCH_X86_64
-    pavgb t0, p2, p1
-    pavgb t1, p0, q0
-%else
-    mova  t0, p2
-    mova  t1, p0
-    pavgb t0, p1
-    pavgb t1, q0
-%endif
-    pavgb t0, t1 ; ((p2+p1+1)/2 + (p0+q0+1)/2 + 1)/2
-    mova  t5, t1
-%if ARCH_X86_64
-    paddb t2, p2, p1
-    paddb t3, p0, q0
-%else
-    mova  t2, p2
-    mova  t3, p0
-    paddb t2, p1
-    paddb t3, q0
-%endif
-    paddb t2, t3
-    mova  t3, t2
-    mova  t4, t2
-    psrlw t2, 1
-    pavgb t2, mpb_0
-    pxor  t2, t0
-    pand  t2, mpb_1
-    psubb t0, t2 ; p1' = (p2+p1+p0+q0+2)/4;
-
-%if ARCH_X86_64
-    pavgb t1, p2, q1
-    psubb t2, p2, q1
-%else
-    mova  t1, p2
-    mova  t2, p2
-    pavgb t1, q1
-    psubb t2, q1
-%endif
-    paddb t3, t3
-    psubb t3, t2 ; p2+2*p1+2*p0+2*q0+q1
-    pand  t2, mpb_1
-    psubb t1, t2
-    pavgb t1, p1
-    pavgb t1, t5 ; (((p2+q1)/2 + p1+1)/2 + (p0+q0+1)/2 + 1)/2
-    psrlw t3, 2
-    pavgb t3, mpb_0
-    pxor  t3, t1
-    pand  t3, mpb_1
-    psubb t1, t3 ; p0'a = (p2+2*p1+2*p0+2*q0+q1+4)/8
-
-    pxor  t3, p0, q1
-    pavgb t2, p0, q1
-    pand  t3, mpb_1
-    psubb t2, t3
-    pavgb t2, p1 ; p0'b = (2*p1+p0+q0+2)/4
-
-    pxor  t1, t2
-    pxor  t2, p0
-    pand  t1, mask1p
-    pand  t2, mask0
-    pxor  t1, t2
-    pxor  t1, p0
-    mova  %1, t1 ; store p0
-
-    mova  t1, %4 ; p3
-    paddb t2, t1, p2
-    pavgb t1, p2
-    pavgb t1, t0 ; (p3+p2+1)/2 + (p2+p1+p0+q0+2)/4
-    paddb t2, t2
-    paddb t2, t4 ; 2*p3+3*p2+p1+p0+q0
-    psrlw t2, 2
-    pavgb t2, mpb_0
-    pxor  t2, t1
-    pand  t2, mpb_1
-    psubb t1, t2 ; p2' = (2*p3+3*p2+p1+p0+q0+4)/8
-
-    pxor  t0, p1
-    pxor  t1, p2
-    pand  t0, mask1p
-    pand  t1, mask1p
-    pxor  t0, p1
-    pxor  t1, p2
-    mova  %2, t0 ; store p1
-    mova  %3, t1 ; store p2
-%endmacro
-
-%macro LUMA_INTRA_SWAP_PQ 0
-    %define q1 m0
-    %define q0 m1
-    %define p0 m2
-    %define p1 m3
-    %define p2 q2
-    %define mask1p mask1q
-%endmacro
-
-%macro DEBLOCK_LUMA_INTRA 1
-    %define p1 m0
-    %define p0 m1
-    %define q0 m2
-    %define q1 m3
-    %define t0 m4
-    %define t1 m5
-    %define t2 m6
-    %define t3 m7
-%if ARCH_X86_64
-    %define p2 m8
-    %define q2 m9
-    %define t4 m10
-    %define t5 m11
-    %define mask0 m12
-    %define mask1p m13
-%if WIN64
-    %define mask1q [rsp]
-%else
-    %define mask1q [rsp-24]
-%endif
-    %define mpb_0 m14
-    %define mpb_1 m15
-%else
-    %define spill(x) [esp+16*x]
-    %define p2 [r4+r1]
-    %define q2 [r0+2*r1]
-    %define t4 spill(0)
-    %define t5 spill(1)
-    %define mask0 spill(2)
-    %define mask1p spill(3)
-    %define mask1q spill(4)
-    %define mpb_0 [pb_0]
-    %define mpb_1 [pb_1]
-%endif
-
-;-----------------------------------------------------------------------------
-; void ff_deblock_v_luma_intra(uint8_t *pix, int stride, int alpha, int beta)
-;-----------------------------------------------------------------------------
-%if WIN64
-cglobal deblock_%1_luma_intra_8, 4,6,16,0x10
-%else
-cglobal deblock_%1_luma_intra_8, 4,6,16,ARCH_X86_64*0x50-0x50
-%endif
-    lea     r4, [r1*4]
-    lea     r5, [r1*3] ; 3*stride
-    dec     r2d        ; alpha-1
-    jl .end
-    neg     r4
-    dec     r3d        ; beta-1
-    jl .end
-    add     r4, r0     ; pix-4*stride
-    mova    p1, [r4+2*r1]
-    mova    p0, [r4+r5]
-    mova    q0, [r0]
-    mova    q1, [r0+r1]
-%if ARCH_X86_64
-    pxor    mpb_0, mpb_0
-    mova    mpb_1, [pb_1]
-    LOAD_MASK r2d, r3d, t5 ; m5=beta-1, t5=alpha-1, m7=mask0
-    SWAP    7, 12 ; m12=mask0
-    pavgb   t5, mpb_0
-    pavgb   t5, mpb_1 ; alpha/4+1
-    movdqa  p2, [r4+r1]
-    movdqa  q2, [r0+2*r1]
-    DIFF_GT2 p0, q0, t5, t0, t3 ; t0 = |p0-q0| > alpha/4+1
-    DIFF_GT2 p0, p2, m5, t2, t5 ; mask1 = |p2-p0| > beta-1
-    DIFF_GT2 q0, q2, m5, t4, t5 ; t4 = |q2-q0| > beta-1
-    pand    t0, mask0
-    pand    t4, t0
-    pand    t2, t0
-    mova    mask1q, t4
-    mova    mask1p, t2
-%else
-    LOAD_MASK r2d, r3d, t5 ; m5=beta-1, t5=alpha-1, m7=mask0
-    mova    m4, t5
-    mova    mask0, m7
-    pavgb   m4, [pb_0]
-    pavgb   m4, [pb_1] ; alpha/4+1
-    DIFF_GT2 p0, q0, m4, m6, m7 ; m6 = |p0-q0| > alpha/4+1
-    pand    m6, mask0
-    DIFF_GT2 p0, p2, m5, m4, m7 ; m4 = |p2-p0| > beta-1
-    pand    m4, m6
-    mova    mask1p, m4
-    DIFF_GT2 q0, q2, m5, m4, m7 ; m4 = |q2-q0| > beta-1
-    pand    m4, m6
-    mova    mask1q, m4
-%endif
-    LUMA_INTRA_P012 [r4+r5], [r4+2*r1], [r4+r1], [r4]
-    LUMA_INTRA_SWAP_PQ
-    LUMA_INTRA_P012 [r0], [r0+r1], [r0+2*r1], [r0+r5]
-.end:
-    RET
-
-INIT_MMX cpuname
-%if ARCH_X86_64
-;-----------------------------------------------------------------------------
-; void ff_deblock_h_luma_intra(uint8_t *pix, int stride, int alpha, int beta)
-;-----------------------------------------------------------------------------
-cglobal deblock_h_luma_intra_8, 4,9,0,0x80
-    movsxd r7,  r1d
-    lea    r8,  [r7*3]
-    lea    r6,  [r0-4]
-    lea    r5,  [r0-4+r8]
-%if WIN64
-    %define pix_tmp rsp+0x20 ; shadow space
-%else
-    %define pix_tmp rsp
-%endif
-
-    ; transpose 8x16 -> tmp space
-    TRANSPOSE8x8_MEM  PASS8ROWS(r6, r5, r7, r8), PASS8ROWS(pix_tmp, pix_tmp+0x30, 0x10, 0x30)
-    lea    r6, [r6+r7*8]
-    lea    r5, [r5+r7*8]
-    TRANSPOSE8x8_MEM  PASS8ROWS(r6, r5, r7, r8), PASS8ROWS(pix_tmp+8, pix_tmp+0x38, 0x10, 0x30)
-
-    lea    r0,  [pix_tmp+0x40]
-    mov    r1,  0x10
-    call   deblock_v_luma_intra_8
-
-    ; transpose 16x6 -> original space (but we can't write only 6 pixels, so really 16x8)
-    lea    r5, [r6+r8]
-    TRANSPOSE8x8_MEM  PASS8ROWS(pix_tmp+8, pix_tmp+0x38, 0x10, 0x30), PASS8ROWS(r6, r5, r7, r8)
-    shl    r7,  3
-    sub    r6,  r7
-    sub    r5,  r7
-    shr    r7,  3
-    TRANSPOSE8x8_MEM  PASS8ROWS(pix_tmp, pix_tmp+0x30, 0x10, 0x30), PASS8ROWS(r6, r5, r7, r8)
-    RET
-%else
-cglobal deblock_h_luma_intra_8, 2,4,8,0x80
-    lea    r3,  [r1*3]
-    sub    r0,  4
-    lea    r2,  [r0+r3]
-    %define pix_tmp rsp
-
-    ; transpose 8x16 -> tmp space
-    TRANSPOSE8x8_MEM  PASS8ROWS(r0, r2, r1, r3), PASS8ROWS(pix_tmp, pix_tmp+0x30, 0x10, 0x30)
-    lea    r0,  [r0+r1*8]
-    lea    r2,  [r2+r1*8]
-    TRANSPOSE8x8_MEM  PASS8ROWS(r0, r2, r1, r3), PASS8ROWS(pix_tmp+8, pix_tmp+0x38, 0x10, 0x30)
-
-    lea    r0,  [pix_tmp+0x40]
-    PUSH   dword r3m
-    PUSH   dword r2m
-    PUSH   dword 16
-    PUSH   r0
-    call   deblock_%1_luma_intra_8
-%ifidn %1, v8
-    add    dword [rsp], 8 ; pix_tmp+8
-    call   deblock_%1_luma_intra_8
-%endif
-    ADD    esp, 16
-
-    mov    r1,  r1m
-    mov    r0,  r0mp
-    lea    r3,  [r1*3]
-    sub    r0,  4
-    lea    r2,  [r0+r3]
-    ; transpose 16x6 -> original space (but we can't write only 6 pixels, so really 16x8)
-    TRANSPOSE8x8_MEM  PASS8ROWS(pix_tmp, pix_tmp+0x30, 0x10, 0x30), PASS8ROWS(r0, r2, r1, r3)
-    lea    r0,  [r0+r1*8]
-    lea    r2,  [r2+r1*8]
-    TRANSPOSE8x8_MEM  PASS8ROWS(pix_tmp+8, pix_tmp+0x38, 0x10, 0x30), PASS8ROWS(r0, r2, r1, r3)
-    RET
-%endif ; ARCH_X86_64
-%endmacro ; DEBLOCK_LUMA_INTRA
-
-INIT_XMM sse2
-DEBLOCK_LUMA_INTRA v
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-DEBLOCK_LUMA_INTRA v
-%endif
-%if ARCH_X86_64 == 0
-INIT_MMX mmxext
-DEBLOCK_LUMA_INTRA v8
-%endif
-
-INIT_MMX mmxext
-
-%macro CHROMA_V_START 0
-    dec    r2d      ; alpha-1
-    dec    r3d      ; beta-1
-    mov    t5, r0
-    sub    t5, r1
-    sub    t5, r1
-%endmacro
-
-%macro CHROMA_H_START 0
-    dec    r2d
-    dec    r3d
-    sub    r0, 2
-    lea    t6, [r1*3]
-    mov    t5, r0
-    add    r0, t6
-%endmacro
-
-%define t5 r5
-%define t6 r6
-
-;-----------------------------------------------------------------------------
-; void ff_deblock_v_chroma(uint8_t *pix, int stride, int alpha, int beta,
-;                          int8_t *tc0)
-;-----------------------------------------------------------------------------
-cglobal deblock_v_chroma_8, 5,6
-    CHROMA_V_START
-    movq  m0, [t5]
-    movq  m1, [t5+r1]
-    movq  m2, [r0]
-    movq  m3, [r0+r1]
-    call ff_chroma_inter_body_mmxext
-    movq  [t5+r1], m1
-    movq  [r0], m2
-    RET
-
-;-----------------------------------------------------------------------------
-; void ff_deblock_h_chroma(uint8_t *pix, int stride, int alpha, int beta,
-;                          int8_t *tc0)
-;-----------------------------------------------------------------------------
-cglobal deblock_h_chroma_8, 5,7
-%if ARCH_X86_64
-    ; This could use the red zone on 64 bit unix to avoid the stack pointer
-    ; readjustment, but valgrind assumes the red zone is clobbered on
-    ; function calls and returns.
-    sub   rsp, 16
-    %define buf0 [rsp]
-    %define buf1 [rsp+8]
-%else
-    %define buf0 r0m
-    %define buf1 r2m
-%endif
-    CHROMA_H_START
-    TRANSPOSE4x8_LOAD  bw, wd, dq, PASS8ROWS(t5, r0, r1, t6)
-    movq  buf0, m0
-    movq  buf1, m3
-    LOAD_MASK  r2d, r3d
-    movd       m6, [r4] ; tc0
-    punpcklbw  m6, m6
-    pand       m7, m6
-    DEBLOCK_P0_Q0
-    movq  m0, buf0
-    movq  m3, buf1
-    TRANSPOSE8x4B_STORE PASS8ROWS(t5, r0, r1, t6)
-%if ARCH_X86_64
-    add   rsp, 16
-%endif
-    RET
-
-ALIGN 16
-ff_chroma_inter_body_mmxext:
-    LOAD_MASK  r2d, r3d
-    movd       m6, [r4] ; tc0
-    punpcklbw  m6, m6
-    pand       m7, m6
-    DEBLOCK_P0_Q0
-    ret
-
-%define t5 r4
-%define t6 r5
-
-cglobal deblock_h_chroma422_8, 5, 6
-    SUB rsp, (1+ARCH_X86_64*2)*mmsize
-    %if ARCH_X86_64
-        %define buf0 [rsp+16]
-        %define buf1 [rsp+8]
-    %else
-        %define buf0 r0m
-        %define buf1 r2m
-    %endif
-
-    movd m6, [r4]
-    punpcklbw m6, m6
-    movq [rsp], m6
-    CHROMA_H_START
-
-    TRANSPOSE4x8B_LOAD PASS8ROWS(t5, r0, r1, t6)
-    movq buf0, m0
-    movq buf1, m3
-    LOAD_MASK r2d, r3d
-    movd m6, [rsp]
-    punpcklwd m6, m6
-    pand m7, m6
-    DEBLOCK_P0_Q0
-    movq m0, buf0
-    movq m3, buf1
-    TRANSPOSE8x4B_STORE PASS8ROWS(t5, r0, r1, t6)
-
-    lea r0, [r0+r1*8]
-    lea t5, [t5+r1*8]
-
-    TRANSPOSE4x8B_LOAD PASS8ROWS(t5, r0, r1, t6)
-    movq buf0, m0
-    movq buf1, m3
-    LOAD_MASK r2d, r3d
-    movd m6, [rsp+4]
-    punpcklwd m6, m6
-    pand m7, m6
-    DEBLOCK_P0_Q0
-    movq m0, buf0
-    movq m3, buf1
-    TRANSPOSE8x4B_STORE PASS8ROWS(t5, r0, r1, t6)
-    ADD rsp, (1+ARCH_X86_64*2)*mmsize
-RET
-
-; in: %1=p0 %2=p1 %3=q1
-; out: p0 = (p0 + q1 + 2*p1 + 2) >> 2
-%macro CHROMA_INTRA_P0 3
-    movq    m4, %1
-    pxor    m4, %3
-    pand    m4, [pb_1] ; m4 = (p0^q1)&1
-    pavgb   %1, %3
-    psubusb %1, m4
-    pavgb   %1, %2             ; dst = avg(p1, avg(p0,q1) - ((p0^q1)&1))
-%endmacro
-
-;------------------------------------------------------------------------------
-; void ff_deblock_v_chroma_intra(uint8_t *pix, int stride, int alpha, int beta)
-;------------------------------------------------------------------------------
-cglobal deblock_v_chroma_intra_8, 4,5
-    CHROMA_V_START
-    movq  m0, [t5]
-    movq  m1, [t5+r1]
-    movq  m2, [r0]
-    movq  m3, [r0+r1]
-    call ff_chroma_intra_body_mmxext
-    movq  [t5+r1], m1
-    movq  [r0], m2
-    RET
-
-;------------------------------------------------------------------------------
-; void ff_deblock_h_chroma_intra(uint8_t *pix, int stride, int alpha, int beta)
-;------------------------------------------------------------------------------
-cglobal deblock_h_chroma_intra_8, 4,6
-    CHROMA_H_START
-    TRANSPOSE4x8_LOAD  bw, wd, dq, PASS8ROWS(t5, r0, r1, t6)
-    call ff_chroma_intra_body_mmxext
-    TRANSPOSE8x4B_STORE PASS8ROWS(t5, r0, r1, t6)
-    RET
-
-cglobal deblock_h_chroma422_intra_8, 4, 6
-    CHROMA_H_START
-    TRANSPOSE4x8_LOAD  bw, wd, dq, PASS8ROWS(t5, r0, r1, t6)
-    call ff_chroma_intra_body_mmxext
-    TRANSPOSE8x4B_STORE PASS8ROWS(t5, r0, r1, t6)
-
-    lea r0, [r0+r1*8]
-    lea t5, [t5+r1*8]
-
-    TRANSPOSE4x8_LOAD  bw, wd, dq, PASS8ROWS(t5, r0, r1, t6)
-    call ff_chroma_intra_body_mmxext
-    TRANSPOSE8x4B_STORE PASS8ROWS(t5, r0, r1, t6)
-RET
-
-ALIGN 16
-ff_chroma_intra_body_mmxext:
-    LOAD_MASK r2d, r3d
-    movq   m5, m1
-    movq   m6, m2
-    CHROMA_INTRA_P0  m1, m0, m3
-    CHROMA_INTRA_P0  m2, m3, m0
-    psubb  m1, m5
-    psubb  m2, m6
-    pand   m1, m7
-    pand   m2, m7
-    paddb  m1, m5
-    paddb  m2, m6
-    ret
-
-%macro LOAD_8_ROWS 8
-    movd m0, %1
-    movd m1, %2
-    movd m2, %3
-    movd m3, %4
-    movd m4, %5
-    movd m5, %6
-    movd m6, %7
-    movd m7, %8
-%endmacro
-
-%macro STORE_8_ROWS 8
-    movd %1, m0
-    movd %2, m1
-    movd %3, m2
-    movd %4, m3
-    movd %5, m4
-    movd %6, m5
-    movd %7, m6
-    movd %8, m7
-%endmacro
-
-%macro TRANSPOSE_8x4B_XMM 0
-    punpcklbw m0, m1
-    punpcklbw m2, m3
-    punpcklbw m4, m5
-    punpcklbw m6, m7
-    punpcklwd m0, m2
-    punpcklwd m4, m6
-    punpckhdq m2, m0, m4
-    punpckldq m0, m4
-    MOVHL m1, m0
-    MOVHL m3, m2
-%endmacro
-
-%macro TRANSPOSE_4x8B_XMM 0
-    punpcklbw m0, m1
-    punpcklbw m2, m3
-    punpckhwd m4, m0, m2
-    punpcklwd m0, m2
-    MOVHL m6, m4
-    MOVHL m2, m0
-    pshufd m1, m0, 1
-    pshufd m3, m2, 1
-    pshufd m5, m4, 1
-    pshufd m7, m6, 1
-%endmacro
-
-%macro CHROMA_INTER_BODY_XMM 1
-    LOAD_MASK alpha_d, beta_d
-    movd m6, [tc0_q]
-    %rep %1
-        punpcklbw m6, m6
-    %endrep
-    pand m7, m6
-    DEBLOCK_P0_Q0
-%endmacro
-
-%macro CHROMA_INTRA_BODY_XMM 0
-    LOAD_MASK alpha_d, beta_d
-    mova    m5,  m1
-    mova    m6,  m2
-    pxor    m4,  m1, m3
-    pand    m4, [pb_1]
-    pavgb   m1,  m3
-    psubusb m1,  m4
-    pavgb   m1,  m0
-    pxor    m4,  m2, m0
-    pand    m4, [pb_1]
-    pavgb   m2,  m0
-    psubusb m2,  m4
-    pavgb   m2,  m3
-    psubb   m1,  m5
-    psubb   m2,  m6
-    pand    m1,  m7
-    pand    m2,  m7
-    paddb   m1,  m5
-    paddb   m2,  m6
-%endmacro
-
-%macro CHROMA_V_START_XMM 1
-    movsxdifnidn stride_q, stride_d
-    dec alpha_d
-    dec beta_d
-    mov %1, pix_q
-    sub %1, stride_q
-    sub %1, stride_q
-%endmacro
-
-%macro CHROMA_H_START_XMM 2
-    movsxdifnidn stride_q, stride_d
-    dec alpha_d
-    dec beta_d
-    lea %2, [3*stride_q]
-    mov %1,  pix_q
-    add %1,  %2
-%endmacro
-
-%macro DEBLOCK_CHROMA_XMM 1
-
-INIT_XMM %1
-
-cglobal deblock_v_chroma_8, 5, 6, 8, pix_, stride_, alpha_, beta_, tc0_
-    CHROMA_V_START_XMM r5
-    movq m0, [r5]
-    movq m1, [r5 + stride_q]
-    movq m2, [pix_q]
-    movq m3, [pix_q + stride_q]
-    CHROMA_INTER_BODY_XMM 1
-    movq [r5 + stride_q], m1
-    movq [pix_q], m2
-RET
-
-cglobal deblock_h_chroma_8, 5, 7, 8, 0-16, pix_, stride_, alpha_, beta_, tc0_
-    CHROMA_H_START_XMM r5, r6
-    LOAD_8_ROWS PASS8ROWS(pix_q - 2, r5 - 2, stride_q, r6)
-    TRANSPOSE_8x4B_XMM
-    movq [rsp], m0
-    movq [rsp + 8], m3
-    CHROMA_INTER_BODY_XMM 1
-    movq m0, [rsp]
-    movq m3, [rsp + 8]
-    TRANSPOSE_4x8B_XMM
-    STORE_8_ROWS PASS8ROWS(pix_q - 2, r5 - 2, stride_q, r6)
-RET
-
-cglobal deblock_h_chroma422_8, 5, 7, 8, 0-16, pix_, stride_, alpha_, beta_, tc0_,
-    CHROMA_H_START_XMM r5, r6
-    LOAD_8_ROWS PASS8ROWS(pix_q - 2, r5 - 2, stride_q, r6)
-    TRANSPOSE_8x4B_XMM
-    movq [rsp], m0
-    movq [rsp + 8], m3
-    CHROMA_INTER_BODY_XMM 2
-    movq m0, [rsp]
-    movq m3, [rsp + 8]
-    TRANSPOSE_4x8B_XMM
-    STORE_8_ROWS PASS8ROWS(pix_q - 2, r5 - 2, stride_q, r6)
-
-    lea pix_q, [pix_q + 8*stride_q]
-    lea r5,    [r5    + 8*stride_q]
-    add tc0_q,  2
-
-    LOAD_8_ROWS PASS8ROWS(pix_q - 2, r5 - 2, stride_q, r6)
-    TRANSPOSE_8x4B_XMM
-    movq [rsp], m0
-    movq [rsp + 8], m3
-    CHROMA_INTER_BODY_XMM 2
-    movq m0, [rsp]
-    movq m3, [rsp + 8]
-    TRANSPOSE_4x8B_XMM
-    STORE_8_ROWS PASS8ROWS(pix_q - 2, r5 - 2, stride_q, r6)
-RET
-
-cglobal deblock_v_chroma_intra_8, 4, 5, 8, pix_, stride_, alpha_, beta_
-    CHROMA_V_START_XMM r4
-    movq m0, [r4]
-    movq m1, [r4 + stride_q]
-    movq m2, [pix_q]
-    movq m3, [pix_q + stride_q]
-    CHROMA_INTRA_BODY_XMM
-    movq [r4 + stride_q], m1
-    movq [pix_q], m2
-RET
-
-cglobal deblock_h_chroma_intra_8, 4, 6, 8, pix_, stride_, alpha_, beta_
-    CHROMA_H_START_XMM r4, r5
-    LOAD_8_ROWS PASS8ROWS(pix_q - 2, r4 - 2, stride_q, r5)
-    TRANSPOSE_8x4B_XMM
-    CHROMA_INTRA_BODY_XMM
-    TRANSPOSE_4x8B_XMM
-    STORE_8_ROWS PASS8ROWS(pix_q - 2, r4 - 2, stride_q, r5)
-RET
-
-cglobal deblock_h_chroma422_intra_8, 4, 6, 8, pix_, stride_, alpha_, beta_
-    CHROMA_H_START_XMM r4, r5
-    LOAD_8_ROWS PASS8ROWS(pix_q - 2, r4 - 2, stride_q, r5)
-    TRANSPOSE_8x4B_XMM
-    CHROMA_INTRA_BODY_XMM
-    TRANSPOSE_4x8B_XMM
-    STORE_8_ROWS PASS8ROWS(pix_q - 2, r4 - 2, stride_q, r5)
-
-    lea pix_q, [pix_q + 8*stride_q]
-    lea r4,    [r4    + 8*stride_q]
-
-    LOAD_8_ROWS PASS8ROWS(pix_q - 2, r4 - 2, stride_q, r5)
-    TRANSPOSE_8x4B_XMM
-    CHROMA_INTRA_BODY_XMM
-    TRANSPOSE_4x8B_XMM
-    STORE_8_ROWS PASS8ROWS(pix_q - 2, r4 - 2, stride_q, r5)
-RET
-
-%endmacro ; DEBLOCK_CHROMA_XMM
-
-DEBLOCK_CHROMA_XMM sse2
-DEBLOCK_CHROMA_XMM avx
-
-;-----------------------------------------------------------------------------
-; void ff_h264_loop_filter_strength(int16_t bs[2][4][4], uint8_t nnz[40],
-;                                   int8_t ref[2][40], int16_t mv[2][40][2],
-;                                   int bidir,    int edges,    int step,
-;                                   int mask_mv0, int mask_mv1, int field);
-;
-; bidir    is 0 or 1
-; edges    is 1 or 4
-; step     is 1 or 2
-; mask_mv0 is 0 or 3
-; mask_mv1 is 0 or 1
-; field    is 0 or 1
-;-----------------------------------------------------------------------------
-%macro loop_filter_strength_iteration 7 ; edges, step, mask_mv,
-                                        ; dir, d_idx, mask_dir, bidir
-%define edgesd    %1
-%define stepd     %2
-%define mask_mvd  %3
-%define dir       %4
-%define d_idx     %5
-%define mask_dir  %6
-%define bidir     %7
-    xor          b_idxd, b_idxd ; for (b_idx = 0; b_idx < edges; b_idx += step)
-%%.b_idx_loop:
-%if mask_dir == 0
-    pxor             m0, m0
-%endif
-    test         b_idxd, dword mask_mvd
-    jnz %%.skip_loop_iter                       ; if (!(b_idx & mask_mv))
-%if bidir == 1
-    movd             m2, [refq+b_idxq+d_idx+12] ; { ref0[bn] }
-    punpckldq        m2, [refq+b_idxq+d_idx+52] ; { ref0[bn], ref1[bn] }
-    pshufw           m0, [refq+b_idxq+12], 0x44 ; { ref0[b],  ref0[b]  }
-    pshufw           m1, [refq+b_idxq+52], 0x44 ; { ref1[b],  ref1[b]  }
-    pshufw           m3, m2, 0x4E               ; { ref1[bn], ref0[bn] }
-    psubb            m0, m2                     ; { ref0[b] != ref0[bn],
-                                                ;   ref0[b] != ref1[bn] }
-    psubb            m1, m3                     ; { ref1[b] != ref1[bn],
-                                                ;   ref1[b] != ref0[bn] }
-
-    por              m0, m1
-    mova             m1, [mvq+b_idxq*4+(d_idx+12)*4]
-    mova             m2, [mvq+b_idxq*4+(d_idx+12)*4+mmsize]
-    mova             m3, m1
-    mova             m4, m2
-    psubw            m1, [mvq+b_idxq*4+12*4]
-    psubw            m2, [mvq+b_idxq*4+12*4+mmsize]
-    psubw            m3, [mvq+b_idxq*4+52*4]
-    psubw            m4, [mvq+b_idxq*4+52*4+mmsize]
-    packsswb         m1, m2
-    packsswb         m3, m4
-    paddb            m1, m6
-    paddb            m3, m6
-    psubusb          m1, m5 ; abs(mv[b] - mv[bn]) >= limit
-    psubusb          m3, m5
-    packsswb         m1, m3
-
-    por              m0, m1
-    mova             m1, [mvq+b_idxq*4+(d_idx+52)*4]
-    mova             m2, [mvq+b_idxq*4+(d_idx+52)*4+mmsize]
-    mova             m3, m1
-    mova             m4, m2
-    psubw            m1, [mvq+b_idxq*4+12*4]
-    psubw            m2, [mvq+b_idxq*4+12*4+mmsize]
-    psubw            m3, [mvq+b_idxq*4+52*4]
-    psubw            m4, [mvq+b_idxq*4+52*4+mmsize]
-    packsswb         m1, m2
-    packsswb         m3, m4
-    paddb            m1, m6
-    paddb            m3, m6
-    psubusb          m1, m5 ; abs(mv[b] - mv[bn]) >= limit
-    psubusb          m3, m5
-    packsswb         m1, m3
-
-    pshufw           m1, m1, 0x4E
-    por              m0, m1
-    pshufw           m1, m0, 0x4E
-    pminub           m0, m1
-%else ; bidir == 0
-    movd             m0, [refq+b_idxq+12]
-    psubb            m0, [refq+b_idxq+d_idx+12] ; ref[b] != ref[bn]
-
-    mova             m1, [mvq+b_idxq*4+12*4]
-    mova             m2, [mvq+b_idxq*4+12*4+mmsize]
-    psubw            m1, [mvq+b_idxq*4+(d_idx+12)*4]
-    psubw            m2, [mvq+b_idxq*4+(d_idx+12)*4+mmsize]
-    packsswb         m1, m2
-    paddb            m1, m6
-    psubusb          m1, m5 ; abs(mv[b] - mv[bn]) >= limit
-    packsswb         m1, m1
-    por              m0, m1
-%endif ; bidir == 1/0
-
-%%.skip_loop_iter:
-    movd             m1, [nnzq+b_idxq+12]
-    por              m1, [nnzq+b_idxq+d_idx+12] ; nnz[b] || nnz[bn]
-
-    pminub           m1, m7
-    pminub           m0, m7
-    psllw            m1, 1
-    pxor             m2, m2
-    pmaxub           m1, m0
-    punpcklbw        m1, m2
-    movq [bsq+b_idxq+32*dir], m1
-
-    add          b_idxd, dword stepd
-    cmp          b_idxd, dword edgesd
-    jl %%.b_idx_loop
-%endmacro
-
-INIT_MMX mmxext
-cglobal h264_loop_filter_strength, 9, 9, 0, bs, nnz, ref, mv, bidir, edges, \
-                                            step, mask_mv0, mask_mv1, field
-%define b_idxq bidirq
-%define b_idxd bidird
-    cmp    dword fieldm, 0
-    mova             m7, [pb_1]
-    mova             m5, [pb_3]
-    je .nofield
-    mova             m5, [pb_3_1]
-.nofield:
-    mova             m6, m5
-    paddb            m5, m5
-
-    shl     dword stepd, 3
-    shl    dword edgesd, 3
-%if ARCH_X86_32
-%define mask_mv0d mask_mv0m
-%define mask_mv1d mask_mv1m
-%endif
-    shl dword mask_mv1d, 3
-    shl dword mask_mv0d, 3
-
-    cmp    dword bidird, 0
-    jne .bidir
-    loop_filter_strength_iteration edgesd, stepd, mask_mv1d, 1, -8,  0, 0
-    loop_filter_strength_iteration     32,     8, mask_mv0d, 0, -1, -1, 0
-
-    mova             m0, [bsq+mmsize*0]
-    mova             m1, [bsq+mmsize*1]
-    mova             m2, [bsq+mmsize*2]
-    mova             m3, [bsq+mmsize*3]
-    TRANSPOSE4x4W 0, 1, 2, 3, 4
-    mova  [bsq+mmsize*0], m0
-    mova  [bsq+mmsize*1], m1
-    mova  [bsq+mmsize*2], m2
-    mova  [bsq+mmsize*3], m3
-    RET
-
-.bidir:
-    loop_filter_strength_iteration edgesd, stepd, mask_mv1d, 1, -8,  0, 1
-    loop_filter_strength_iteration     32,     8, mask_mv0d, 0, -1, -1, 1
-
-    mova             m0, [bsq+mmsize*0]
-    mova             m1, [bsq+mmsize*1]
-    mova             m2, [bsq+mmsize*2]
-    mova             m3, [bsq+mmsize*3]
-    TRANSPOSE4x4W 0, 1, 2, 3, 4
-    mova  [bsq+mmsize*0], m0
-    mova  [bsq+mmsize*1], m1
-    mova  [bsq+mmsize*2], m2
-    mova  [bsq+mmsize*3], m3
-    RET
diff -uparN ffmpeg-4.1/libavcodec/x86/h264_idct_10bit.asm ffmpeg-y/libavcodec/x86/h264_idct_10bit.asm
--- ffmpeg-4.1/libavcodec/x86/h264_idct_10bit.asm	2018-11-02 02:34:25.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/h264_idct_10bit.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,657 +0,0 @@
-;*****************************************************************************
-;* MMX/SSE2/AVX-optimized 10-bit H.264 iDCT code
-;*****************************************************************************
-;* Copyright (C) 2005-2011 x264 project
-;*
-;* Authors: Daniel Kang <daniel.d.kang@gmail.com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION .text
-
-cextern pw_1023
-%define pw_pixel_max pw_1023
-cextern pd_32
-
-;-----------------------------------------------------------------------------
-; void ff_h264_idct_add_10(pixel *dst, int16_t *block, int stride)
-;-----------------------------------------------------------------------------
-%macro STORE_DIFFx2 6
-    psrad       %1, 6
-    psrad       %2, 6
-    packssdw    %1, %2
-    movq        %3, [%5]
-    movhps      %3, [%5+%6]
-    paddsw      %1, %3
-    CLIPW       %1, %4, [pw_pixel_max]
-    movq      [%5], %1
-    movhps [%5+%6], %1
-%endmacro
-
-%macro STORE_DIFF16 5
-    psrad       %1, 6
-    psrad       %2, 6
-    packssdw    %1, %2
-    paddsw      %1, [%5]
-    CLIPW       %1, %3, %4
-    mova      [%5], %1
-%endmacro
-
-;dst, in, stride
-%macro IDCT4_ADD_10 3
-    mova  m0, [%2+ 0]
-    mova  m1, [%2+16]
-    mova  m2, [%2+32]
-    mova  m3, [%2+48]
-    IDCT4_1D d,0,1,2,3,4,5
-    TRANSPOSE4x4D 0,1,2,3,4
-    paddd m0, [pd_32]
-    IDCT4_1D d,0,1,2,3,4,5
-    pxor  m5, m5
-    mova [%2+ 0], m5
-    mova [%2+16], m5
-    mova [%2+32], m5
-    mova [%2+48], m5
-    STORE_DIFFx2 m0, m1, m4, m5, %1, %3
-    lea   %1, [%1+%3*2]
-    STORE_DIFFx2 m2, m3, m4, m5, %1, %3
-%endmacro
-
-%macro IDCT_ADD_10 0
-cglobal h264_idct_add_10, 3,3
-    movsxdifnidn r2, r2d
-    IDCT4_ADD_10 r0, r1, r2
-    RET
-%endmacro
-
-INIT_XMM sse2
-IDCT_ADD_10
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-IDCT_ADD_10
-%endif
-
-;-----------------------------------------------------------------------------
-; void ff_h264_idct_add16_10(pixel *dst, const int *block_offset,
-;                            int16_t *block, int stride,
-;                            const uint8_t nnzc[6*8])
-;-----------------------------------------------------------------------------
-;;;;;;; NO FATE SAMPLES TRIGGER THIS
-%macro ADD4x4IDCT 0
-add4x4_idct %+ SUFFIX:
-    add   r5, r0
-    mova  m0, [r2+ 0]
-    mova  m1, [r2+16]
-    mova  m2, [r2+32]
-    mova  m3, [r2+48]
-    IDCT4_1D d,0,1,2,3,4,5
-    TRANSPOSE4x4D 0,1,2,3,4
-    paddd m0, [pd_32]
-    IDCT4_1D d,0,1,2,3,4,5
-    pxor  m5, m5
-    mova  [r2+ 0], m5
-    mova  [r2+16], m5
-    mova  [r2+32], m5
-    mova  [r2+48], m5
-    STORE_DIFFx2 m0, m1, m4, m5, r5, r3
-    lea   r5, [r5+r3*2]
-    STORE_DIFFx2 m2, m3, m4, m5, r5, r3
-    ret
-%endmacro
-
-INIT_XMM sse2
-ALIGN 16
-ADD4x4IDCT
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-ALIGN 16
-ADD4x4IDCT
-%endif
-
-%macro ADD16_OP 2
-    cmp          byte [r4+%2], 0
-    jz .skipblock%1
-    mov         r5d, [r1+%1*4]
-    call add4x4_idct %+ SUFFIX
-.skipblock%1:
-%if %1<15
-    add          r2, 64
-%endif
-%endmacro
-
-%macro IDCT_ADD16_10 0
-cglobal h264_idct_add16_10, 5,6
-    movsxdifnidn r3, r3d
-    ADD16_OP 0, 4+1*8
-    ADD16_OP 1, 5+1*8
-    ADD16_OP 2, 4+2*8
-    ADD16_OP 3, 5+2*8
-    ADD16_OP 4, 6+1*8
-    ADD16_OP 5, 7+1*8
-    ADD16_OP 6, 6+2*8
-    ADD16_OP 7, 7+2*8
-    ADD16_OP 8, 4+3*8
-    ADD16_OP 9, 5+3*8
-    ADD16_OP 10, 4+4*8
-    ADD16_OP 11, 5+4*8
-    ADD16_OP 12, 6+3*8
-    ADD16_OP 13, 7+3*8
-    ADD16_OP 14, 6+4*8
-    ADD16_OP 15, 7+4*8
-    REP_RET
-%endmacro
-
-INIT_XMM sse2
-IDCT_ADD16_10
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-IDCT_ADD16_10
-%endif
-
-;-----------------------------------------------------------------------------
-; void ff_h264_idct_dc_add_10(pixel *dst, int16_t *block, int stride)
-;-----------------------------------------------------------------------------
-%macro IDCT_DC_ADD_OP_10 3
-    pxor      m5, m5
-%if avx_enabled
-    paddw     m1, m0, [%1+0   ]
-    paddw     m2, m0, [%1+%2  ]
-    paddw     m3, m0, [%1+%2*2]
-    paddw     m4, m0, [%1+%3  ]
-%else
-    mova      m1, [%1+0   ]
-    mova      m2, [%1+%2  ]
-    mova      m3, [%1+%2*2]
-    mova      m4, [%1+%3  ]
-    paddw     m1, m0
-    paddw     m2, m0
-    paddw     m3, m0
-    paddw     m4, m0
-%endif
-    CLIPW     m1, m5, m6
-    CLIPW     m2, m5, m6
-    CLIPW     m3, m5, m6
-    CLIPW     m4, m5, m6
-    mova [%1+0   ], m1
-    mova [%1+%2  ], m2
-    mova [%1+%2*2], m3
-    mova [%1+%3  ], m4
-%endmacro
-
-INIT_MMX mmxext
-cglobal h264_idct_dc_add_10,3,3
-    movsxdifnidn r2, r2d
-    movd      m0, [r1]
-    mov dword [r1], 0
-    paddd     m0, [pd_32]
-    psrad     m0, 6
-    lea       r1, [r2*3]
-    pshufw    m0, m0, 0
-    mova      m6, [pw_pixel_max]
-    IDCT_DC_ADD_OP_10 r0, r2, r1
-    RET
-
-;-----------------------------------------------------------------------------
-; void ff_h264_idct8_dc_add_10(pixel *dst, int16_t *block, int stride)
-;-----------------------------------------------------------------------------
-%macro IDCT8_DC_ADD 0
-cglobal h264_idct8_dc_add_10,3,4,7
-    movsxdifnidn r2, r2d
-    movd      m0, [r1]
-    mov dword[r1], 0
-    paddd     m0, [pd_32]
-    psrad     m0, 6
-    lea       r1, [r2*3]
-    SPLATW    m0, m0, 0
-    mova      m6, [pw_pixel_max]
-    IDCT_DC_ADD_OP_10 r0, r2, r1
-    lea       r0, [r0+r2*4]
-    IDCT_DC_ADD_OP_10 r0, r2, r1
-    RET
-%endmacro
-
-INIT_XMM sse2
-IDCT8_DC_ADD
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-IDCT8_DC_ADD
-%endif
-
-;-----------------------------------------------------------------------------
-; void ff_h264_idct_add16intra_10(pixel *dst, const int *block_offset,
-;                                 int16_t *block, int stride,
-;                                 const uint8_t nnzc[6*8])
-;-----------------------------------------------------------------------------
-%macro AC 1
-.ac%1:
-    mov  r5d, [r1+(%1+0)*4]
-    call add4x4_idct %+ SUFFIX
-    mov  r5d, [r1+(%1+1)*4]
-    add  r2, 64
-    call add4x4_idct %+ SUFFIX
-    add  r2, 64
-    jmp .skipadd%1
-%endmacro
-
-%assign last_block 16
-%macro ADD16_OP_INTRA 2
-    cmp      word [r4+%2], 0
-    jnz .ac%1
-    mov      r5d, [r2+ 0]
-    or       r5d, [r2+64]
-    jz .skipblock%1
-    mov      r5d, [r1+(%1+0)*4]
-    call idct_dc_add %+ SUFFIX
-.skipblock%1:
-%if %1<last_block-2
-    add       r2, 128
-%endif
-.skipadd%1:
-%endmacro
-
-%macro IDCT_ADD16INTRA_10 0
-idct_dc_add %+ SUFFIX:
-    add       r5, r0
-    movq      m0, [r2+ 0]
-    movhps    m0, [r2+64]
-    mov dword [r2+ 0], 0
-    mov dword [r2+64], 0
-    paddd     m0, [pd_32]
-    psrad     m0, 6
-    pshufhw   m0, m0, 0
-    pshuflw   m0, m0, 0
-    lea       r6, [r3*3]
-    mova      m6, [pw_pixel_max]
-    IDCT_DC_ADD_OP_10 r5, r3, r6
-    ret
-
-cglobal h264_idct_add16intra_10,5,7,8
-    movsxdifnidn r3, r3d
-    ADD16_OP_INTRA 0, 4+1*8
-    ADD16_OP_INTRA 2, 4+2*8
-    ADD16_OP_INTRA 4, 6+1*8
-    ADD16_OP_INTRA 6, 6+2*8
-    ADD16_OP_INTRA 8, 4+3*8
-    ADD16_OP_INTRA 10, 4+4*8
-    ADD16_OP_INTRA 12, 6+3*8
-    ADD16_OP_INTRA 14, 6+4*8
-    REP_RET
-    AC 8
-    AC 10
-    AC 12
-    AC 14
-    AC 0
-    AC 2
-    AC 4
-    AC 6
-%endmacro
-
-INIT_XMM sse2
-IDCT_ADD16INTRA_10
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-IDCT_ADD16INTRA_10
-%endif
-
-%assign last_block 36
-;-----------------------------------------------------------------------------
-; void ff_h264_idct_add8_10(pixel **dst, const int *block_offset,
-;                           int16_t *block, int stride,
-;                           const uint8_t nnzc[6*8])
-;-----------------------------------------------------------------------------
-%macro IDCT_ADD8 0
-cglobal h264_idct_add8_10,5,8,7
-    movsxdifnidn r3, r3d
-%if ARCH_X86_64
-    mov      r7, r0
-%endif
-    add      r2, 1024
-    mov      r0, [r0]
-    ADD16_OP_INTRA 16, 4+ 6*8
-    ADD16_OP_INTRA 18, 4+ 7*8
-    add      r2, 1024-128*2
-%if ARCH_X86_64
-    mov      r0, [r7+gprsize]
-%else
-    mov      r0, r0m
-    mov      r0, [r0+gprsize]
-%endif
-    ADD16_OP_INTRA 32, 4+11*8
-    ADD16_OP_INTRA 34, 4+12*8
-    REP_RET
-    AC 16
-    AC 18
-    AC 32
-    AC 34
-
-%endmacro ; IDCT_ADD8
-
-INIT_XMM sse2
-IDCT_ADD8
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-IDCT_ADD8
-%endif
-
-;-----------------------------------------------------------------------------
-; void ff_h264_idct_add8_422_10(pixel **dst, const int *block_offset,
-;                               int16_t *block, int stride,
-;                               const uint8_t nnzc[6*8])
-;-----------------------------------------------------------------------------
-%assign last_block 44
-
-%macro IDCT_ADD8_422 0
-
-cglobal h264_idct_add8_422_10, 5, 8, 7
-    movsxdifnidn r3, r3d
-%if ARCH_X86_64
-    mov      r7, r0
-%endif
-
-    add      r2, 1024
-    mov      r0, [r0]
-    ADD16_OP_INTRA 16, 4+ 6*8
-    ADD16_OP_INTRA 18, 4+ 7*8
-    ADD16_OP_INTRA 24, 4+ 8*8 ; i+4
-    ADD16_OP_INTRA 26, 4+ 9*8 ; i+4
-    add      r2, 1024-128*4
-
-%if ARCH_X86_64
-    mov      r0, [r7+gprsize]
-%else
-    mov      r0, r0m
-    mov      r0, [r0+gprsize]
-%endif
-
-    ADD16_OP_INTRA 32, 4+11*8
-    ADD16_OP_INTRA 34, 4+12*8
-    ADD16_OP_INTRA 40, 4+13*8 ; i+4
-    ADD16_OP_INTRA 42, 4+14*8 ; i+4
-REP_RET
-    AC 16
-    AC 18
-    AC 24 ; i+4
-    AC 26 ; i+4
-    AC 32
-    AC 34
-    AC 40 ; i+4
-    AC 42 ; i+4
-
-%endmacro
-
-INIT_XMM sse2
-IDCT_ADD8_422
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-IDCT_ADD8_422
-%endif
-
-;-----------------------------------------------------------------------------
-; void ff_h264_idct8_add_10(pixel *dst, int16_t *block, int stride)
-;-----------------------------------------------------------------------------
-%macro IDCT8_1D 2
-    SWAP      0, 1
-    psrad     m4, m5, 1
-    psrad     m1, m0, 1
-    paddd     m4, m5
-    paddd     m1, m0
-    paddd     m4, m7
-    paddd     m1, m5
-    psubd     m4, m0
-    paddd     m1, m3
-
-    psubd     m0, m3
-    psubd     m5, m3
-    paddd     m0, m7
-    psubd     m5, m7
-    psrad     m3, 1
-    psrad     m7, 1
-    psubd     m0, m3
-    psubd     m5, m7
-
-    SWAP      1, 7
-    psrad     m1, m7, 2
-    psrad     m3, m4, 2
-    paddd     m3, m0
-    psrad     m0, 2
-    paddd     m1, m5
-    psrad     m5, 2
-    psubd     m0, m4
-    psubd     m7, m5
-
-    SWAP      5, 6
-    psrad     m4, m2, 1
-    psrad     m6, m5, 1
-    psubd     m4, m5
-    paddd     m6, m2
-
-    mova      m2, %1
-    mova      m5, %2
-    SUMSUB_BA d, 5, 2
-    SUMSUB_BA d, 6, 5
-    SUMSUB_BA d, 4, 2
-    SUMSUB_BA d, 7, 6
-    SUMSUB_BA d, 0, 4
-    SUMSUB_BA d, 3, 2
-    SUMSUB_BA d, 1, 5
-    SWAP      7, 6, 4, 5, 2, 3, 1, 0 ; 70315246 -> 01234567
-%endmacro
-
-%macro IDCT8_1D_FULL 1
-    mova         m7, [%1+112*2]
-    mova         m6, [%1+ 96*2]
-    mova         m5, [%1+ 80*2]
-    mova         m3, [%1+ 48*2]
-    mova         m2, [%1+ 32*2]
-    mova         m1, [%1+ 16*2]
-    IDCT8_1D   [%1], [%1+ 64*2]
-%endmacro
-
-; %1=int16_t *block, %2=int16_t *dstblock
-%macro IDCT8_ADD_SSE_START 2
-    IDCT8_1D_FULL %1
-%if ARCH_X86_64
-    TRANSPOSE4x4D  0,1,2,3,8
-    mova    [%2    ], m0
-    TRANSPOSE4x4D  4,5,6,7,8
-    mova    [%2+8*2], m4
-%else
-    mova         [%1], m7
-    TRANSPOSE4x4D   0,1,2,3,7
-    mova           m7, [%1]
-    mova    [%2     ], m0
-    mova    [%2+16*2], m1
-    mova    [%2+32*2], m2
-    mova    [%2+48*2], m3
-    TRANSPOSE4x4D   4,5,6,7,3
-    mova    [%2+ 8*2], m4
-    mova    [%2+24*2], m5
-    mova    [%2+40*2], m6
-    mova    [%2+56*2], m7
-%endif
-%endmacro
-
-; %1=uint8_t *dst, %2=int16_t *block, %3=int stride
-%macro IDCT8_ADD_SSE_END 3
-    IDCT8_1D_FULL %2
-    mova  [%2     ], m6
-    mova  [%2+16*2], m7
-
-    pxor         m7, m7
-    STORE_DIFFx2 m0, m1, m6, m7, %1, %3
-    lea          %1, [%1+%3*2]
-    STORE_DIFFx2 m2, m3, m6, m7, %1, %3
-    mova         m0, [%2     ]
-    mova         m1, [%2+16*2]
-    lea          %1, [%1+%3*2]
-    STORE_DIFFx2 m4, m5, m6, m7, %1, %3
-    lea          %1, [%1+%3*2]
-    STORE_DIFFx2 m0, m1, m6, m7, %1, %3
-%endmacro
-
-%macro IDCT8_ADD 0
-cglobal h264_idct8_add_10, 3,4,16
-    movsxdifnidn r2, r2d
-%if UNIX64 == 0
-    %assign pad 16-gprsize-(stack_offset&15)
-    sub  rsp, pad
-    call h264_idct8_add1_10 %+ SUFFIX
-    add  rsp, pad
-    RET
-%endif
-
-ALIGN 16
-; TODO: does not need to use stack
-h264_idct8_add1_10 %+ SUFFIX:
-%assign pad 256+16-gprsize
-    sub          rsp, pad
-    add   dword [r1], 32
-
-%if ARCH_X86_64
-    IDCT8_ADD_SSE_START r1, rsp
-    SWAP 1,  9
-    SWAP 2, 10
-    SWAP 3, 11
-    SWAP 5, 13
-    SWAP 6, 14
-    SWAP 7, 15
-    IDCT8_ADD_SSE_START r1+16, rsp+128
-    PERMUTE 1,9, 2,10, 3,11, 5,1, 6,2, 7,3, 9,13, 10,14, 11,15, 13,5, 14,6, 15,7
-    IDCT8_1D [rsp], [rsp+128]
-    SWAP 0,  8
-    SWAP 1,  9
-    SWAP 2, 10
-    SWAP 3, 11
-    SWAP 4, 12
-    SWAP 5, 13
-    SWAP 6, 14
-    SWAP 7, 15
-    IDCT8_1D [rsp+16], [rsp+144]
-    psrad         m8, 6
-    psrad         m0, 6
-    packssdw      m8, m0
-    paddsw        m8, [r0]
-    pxor          m0, m0
-    mova    [r1+  0], m0
-    mova    [r1+ 16], m0
-    mova    [r1+ 32], m0
-    mova    [r1+ 48], m0
-    mova    [r1+ 64], m0
-    mova    [r1+ 80], m0
-    mova    [r1+ 96], m0
-    mova    [r1+112], m0
-    mova    [r1+128], m0
-    mova    [r1+144], m0
-    mova    [r1+160], m0
-    mova    [r1+176], m0
-    mova    [r1+192], m0
-    mova    [r1+208], m0
-    mova    [r1+224], m0
-    mova    [r1+240], m0
-    CLIPW         m8, m0, [pw_pixel_max]
-    mova        [r0], m8
-    mova          m8, [pw_pixel_max]
-    STORE_DIFF16  m9, m1, m0, m8, r0+r2
-    lea           r0, [r0+r2*2]
-    STORE_DIFF16 m10, m2, m0, m8, r0
-    STORE_DIFF16 m11, m3, m0, m8, r0+r2
-    lea           r0, [r0+r2*2]
-    STORE_DIFF16 m12, m4, m0, m8, r0
-    STORE_DIFF16 m13, m5, m0, m8, r0+r2
-    lea           r0, [r0+r2*2]
-    STORE_DIFF16 m14, m6, m0, m8, r0
-    STORE_DIFF16 m15, m7, m0, m8, r0+r2
-%else
-    IDCT8_ADD_SSE_START r1,    rsp
-    IDCT8_ADD_SSE_START r1+16, rsp+128
-    lea           r3, [r0+8]
-    IDCT8_ADD_SSE_END r0, rsp,    r2
-    IDCT8_ADD_SSE_END r3, rsp+16, r2
-    mova    [r1+  0], m7
-    mova    [r1+ 16], m7
-    mova    [r1+ 32], m7
-    mova    [r1+ 48], m7
-    mova    [r1+ 64], m7
-    mova    [r1+ 80], m7
-    mova    [r1+ 96], m7
-    mova    [r1+112], m7
-    mova    [r1+128], m7
-    mova    [r1+144], m7
-    mova    [r1+160], m7
-    mova    [r1+176], m7
-    mova    [r1+192], m7
-    mova    [r1+208], m7
-    mova    [r1+224], m7
-    mova    [r1+240], m7
-%endif ; ARCH_X86_64
-
-    add          rsp, pad
-    ret
-%endmacro
-
-INIT_XMM sse2
-IDCT8_ADD
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-IDCT8_ADD
-%endif
-
-;-----------------------------------------------------------------------------
-; void ff_h264_idct8_add4_10(pixel **dst, const int *block_offset,
-;                            int16_t *block, int stride,
-;                            const uint8_t nnzc[6*8])
-;-----------------------------------------------------------------------------
-;;;;;;; NO FATE SAMPLES TRIGGER THIS
-%macro IDCT8_ADD4_OP 2
-    cmp       byte [r4+%2], 0
-    jz .skipblock%1
-    mov      r0d, [r6+%1*4]
-    add       r0, r5
-    call h264_idct8_add1_10 %+ SUFFIX
-.skipblock%1:
-%if %1<12
-    add       r1, 256
-%endif
-%endmacro
-
-%macro IDCT8_ADD4 0
-cglobal h264_idct8_add4_10, 0,7,16
-    movsxdifnidn r3, r3d
-    %assign pad 16-gprsize-(stack_offset&15)
-    SUB      rsp, pad
-    mov       r5, r0mp
-    mov       r6, r1mp
-    mov       r1, r2mp
-    mov      r2d, r3m
-    movifnidn r4, r4mp
-    IDCT8_ADD4_OP  0, 4+1*8
-    IDCT8_ADD4_OP  4, 6+1*8
-    IDCT8_ADD4_OP  8, 4+3*8
-    IDCT8_ADD4_OP 12, 6+3*8
-    ADD       rsp, pad
-    RET
-%endmacro ; IDCT8_ADD4
-
-INIT_XMM sse2
-IDCT8_ADD4
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-IDCT8_ADD4
-%endif
diff -uparN ffmpeg-4.1/libavcodec/x86/h264_idct.asm ffmpeg-y/libavcodec/x86/h264_idct.asm
--- ffmpeg-4.1/libavcodec/x86/h264_idct.asm	2018-11-03 08:17:29.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/h264_idct.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,1199 +0,0 @@
-;*****************************************************************************
-;* MMX/SSE2-optimized H.264 iDCT
-;*****************************************************************************
-;* Copyright (C) 2004-2005 Michael Niedermayer, Loren Merritt
-;* Copyright (C) 2003-2008 x264 project
-;*
-;* Authors: Laurent Aimar <fenrir@via.ecp.fr>
-;*          Loren Merritt <lorenm@u.washington.edu>
-;*          Holger Lubitz <hal@duncan.ol.sub.de>
-;*          Min Chen <chenm001.163.com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;*****************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-
-scan8_mem: db  4+ 1*8, 5+ 1*8, 4+ 2*8, 5+ 2*8
-           db  6+ 1*8, 7+ 1*8, 6+ 2*8, 7+ 2*8
-           db  4+ 3*8, 5+ 3*8, 4+ 4*8, 5+ 4*8
-           db  6+ 3*8, 7+ 3*8, 6+ 4*8, 7+ 4*8
-           db  4+ 6*8, 5+ 6*8, 4+ 7*8, 5+ 7*8
-           db  6+ 6*8, 7+ 6*8, 6+ 7*8, 7+ 7*8
-           db  4+ 8*8, 5+ 8*8, 4+ 9*8, 5+ 9*8
-           db  6+ 8*8, 7+ 8*8, 6+ 9*8, 7+ 9*8
-           db  4+11*8, 5+11*8, 4+12*8, 5+12*8
-           db  6+11*8, 7+11*8, 6+12*8, 7+12*8
-           db  4+13*8, 5+13*8, 4+14*8, 5+14*8
-           db  6+13*8, 7+13*8, 6+14*8, 7+14*8
-%ifdef PIC
-%define npicregs 1
-%define scan8 picregq
-%else
-%define npicregs 0
-%define scan8 scan8_mem
-%endif
-
-cextern pw_32
-cextern pw_1
-
-SECTION .text
-
-; %1=uint8_t *dst, %2=int16_t *block, %3=int stride
-%macro IDCT4_ADD 3
-    ; Load dct coeffs
-    movq         m0, [%2]
-    movq         m1, [%2+8]
-    movq         m2, [%2+16]
-    movq         m3, [%2+24]
-
-    IDCT4_1D      w, 0, 1, 2, 3, 4, 5
-    mova         m6, [pw_32]
-    %if mmsize == 8
-        TRANSPOSE4x4W 0, 1, 2, 3, 4
-    %else
-        punpcklwd m0, m1
-        punpcklwd m2, m3
-        SBUTTERFLY dq, 0, 2, 4
-        MOVHL m1, m0
-        MOVHL m3, m2
-    %endif
-    paddw        m0, m6
-    IDCT4_1D      w, 0, 1, 2, 3, 4, 5
-    pxor         m7, m7
-    movq    [%2+ 0], m7
-    movq    [%2+ 8], m7
-    movq    [%2+16], m7
-    movq    [%2+24], m7
-
-    STORE_DIFFx2 m0, m1, m4, m5, m7, 6, %1, %3
-    lea          %1, [%1+%3*2]
-    STORE_DIFFx2 m2, m3, m4, m5, m7, 6, %1, %3
-%endmacro
-
-INIT_MMX mmx
-; void ff_h264_idct_add_8_mmx(uint8_t *dst, int16_t *block, int stride)
-cglobal h264_idct_add_8, 3, 3, 0
-    movsxdifnidn r2, r2d
-    IDCT4_ADD    r0, r1, r2
-    RET
-
-%macro IDCT8_1D 2
-    psraw        m0, m1, 1
-    SWAP 0, 1
-    psraw        m4, m5, 1
-    paddw        m4, m5
-    paddw        m1, m0
-    paddw        m4, m7
-    paddw        m1, m5
-    psubw        m4, m0
-    paddw        m1, m3
-
-    psubw        m0, m3
-    psubw        m5, m3
-    psraw        m3, 1
-    paddw        m0, m7
-    psubw        m5, m7
-    psraw        m7, 1
-    psubw        m0, m3
-    psubw        m5, m7
-
-    psraw        m7, m1, 2
-    SWAP 7,1
-    psraw        m3, m4, 2
-    paddw        m3, m0
-    psraw        m0, 2
-    paddw        m1, m5
-    psraw        m5, 2
-    psubw        m0, m4
-    psubw        m7, m5
-
-    psraw        m5, m6, 1
-    SWAP 5,6
-    psraw        m4, m2, 1
-    paddw        m6, m2
-    psubw        m4, m5
-
-    mova         m2, %1
-    mova         m5, %2
-    SUMSUB_BA    w, 5, 2
-    SUMSUB_BA    w, 6, 5
-    SUMSUB_BA    w, 4, 2
-    SUMSUB_BA    w, 7, 6
-    SUMSUB_BA    w, 0, 4
-    SUMSUB_BA    w, 3, 2
-    SUMSUB_BA    w, 1, 5
-    SWAP         7, 6, 4, 5, 2, 3, 1, 0 ; 70315246 -> 01234567
-%endmacro
-
-%macro IDCT8_1D_FULL 1
-    mova         m7, [%1+112]
-    mova         m6, [%1+ 96]
-    mova         m5, [%1+ 80]
-    mova         m3, [%1+ 48]
-    mova         m2, [%1+ 32]
-    mova         m1, [%1+ 16]
-    IDCT8_1D   [%1], [%1+ 64]
-%endmacro
-
-; %1=int16_t *block, %2=int16_t *dstblock
-%macro IDCT8_ADD_MMX_START 2
-    IDCT8_1D_FULL %1
-    mova       [%1], m7
-    TRANSPOSE4x4W 0, 1, 2, 3, 7
-    mova         m7, [%1]
-    mova    [%2   ], m0
-    mova    [%2+16], m1
-    mova    [%2+32], m2
-    mova    [%2+48], m3
-    TRANSPOSE4x4W 4, 5, 6, 7, 3
-    mova    [%2+ 8], m4
-    mova    [%2+24], m5
-    mova    [%2+40], m6
-    mova    [%2+56], m7
-%endmacro
-
-; %1=uint8_t *dst, %2=int16_t *block, %3=int stride
-%macro IDCT8_ADD_MMX_END 3-4
-    IDCT8_1D_FULL %2
-    mova    [%2   ], m5
-    mova    [%2+16], m6
-    mova    [%2+32], m7
-
-    pxor         m7, m7
-%if %0 == 4
-    movq   [%4+  0], m7
-    movq   [%4+  8], m7
-    movq   [%4+ 16], m7
-    movq   [%4+ 24], m7
-    movq   [%4+ 32], m7
-    movq   [%4+ 40], m7
-    movq   [%4+ 48], m7
-    movq   [%4+ 56], m7
-    movq   [%4+ 64], m7
-    movq   [%4+ 72], m7
-    movq   [%4+ 80], m7
-    movq   [%4+ 88], m7
-    movq   [%4+ 96], m7
-    movq   [%4+104], m7
-    movq   [%4+112], m7
-    movq   [%4+120], m7
-%endif
-    STORE_DIFFx2 m0, m1, m5, m6, m7, 6, %1, %3
-    lea          %1, [%1+%3*2]
-    STORE_DIFFx2 m2, m3, m5, m6, m7, 6, %1, %3
-    mova         m0, [%2   ]
-    mova         m1, [%2+16]
-    mova         m2, [%2+32]
-    lea          %1, [%1+%3*2]
-    STORE_DIFFx2 m4, m0, m5, m6, m7, 6, %1, %3
-    lea          %1, [%1+%3*2]
-    STORE_DIFFx2 m1, m2, m5, m6, m7, 6, %1, %3
-%endmacro
-
-INIT_MMX mmx
-; void ff_h264_idct8_add_8_mmx(uint8_t *dst, int16_t *block, int stride)
-cglobal h264_idct8_add_8, 3, 4, 0
-    movsxdifnidn r2, r2d
-    %assign pad 128+4-(stack_offset&7)
-    SUB         rsp, pad
-
-    add   word [r1], 32
-    IDCT8_ADD_MMX_START r1  , rsp
-    IDCT8_ADD_MMX_START r1+8, rsp+64
-    lea          r3, [r0+4]
-    IDCT8_ADD_MMX_END   r0  , rsp,   r2, r1
-    IDCT8_ADD_MMX_END   r3  , rsp+8, r2
-
-    ADD         rsp, pad
-    RET
-
-; %1=uint8_t *dst, %2=int16_t *block, %3=int stride
-%macro IDCT8_ADD_SSE 4
-    IDCT8_1D_FULL %2
-%if ARCH_X86_64
-    TRANSPOSE8x8W 0, 1, 2, 3, 4, 5, 6, 7, 8
-%else
-    TRANSPOSE8x8W 0, 1, 2, 3, 4, 5, 6, 7, [%2], [%2+16]
-%endif
-    paddw        m0, [pw_32]
-
-%if ARCH_X86_64 == 0
-    mova    [%2   ], m0
-    mova    [%2+16], m4
-    IDCT8_1D   [%2], [%2+ 16]
-    mova    [%2   ], m6
-    mova    [%2+16], m7
-%else
-    SWAP          0, 8
-    SWAP          4, 9
-    IDCT8_1D     m8, m9
-    SWAP          6, 8
-    SWAP          7, 9
-%endif
-
-    pxor         m7, m7
-    lea          %4, [%3*3]
-    STORE_DIFF   m0, m6, m7, [%1     ]
-    STORE_DIFF   m1, m6, m7, [%1+%3  ]
-    STORE_DIFF   m2, m6, m7, [%1+%3*2]
-    STORE_DIFF   m3, m6, m7, [%1+%4  ]
-%if ARCH_X86_64 == 0
-    mova         m0, [%2   ]
-    mova         m1, [%2+16]
-%else
-    SWAP          0, 8
-    SWAP          1, 9
-%endif
-    mova   [%2+  0], m7
-    mova   [%2+ 16], m7
-    mova   [%2+ 32], m7
-    mova   [%2+ 48], m7
-    mova   [%2+ 64], m7
-    mova   [%2+ 80], m7
-    mova   [%2+ 96], m7
-    mova   [%2+112], m7
-    lea          %1, [%1+%3*4]
-    STORE_DIFF   m4, m6, m7, [%1     ]
-    STORE_DIFF   m5, m6, m7, [%1+%3  ]
-    STORE_DIFF   m0, m6, m7, [%1+%3*2]
-    STORE_DIFF   m1, m6, m7, [%1+%4  ]
-%endmacro
-
-INIT_XMM sse2
-; void ff_h264_idct8_add_8_sse2(uint8_t *dst, int16_t *block, int stride)
-cglobal h264_idct8_add_8, 3, 4, 10
-    movsxdifnidn  r2, r2d
-    IDCT8_ADD_SSE r0, r1, r2, r3
-    RET
-
-%macro DC_ADD_MMXEXT_INIT 2
-    add          %1, 32
-    sar          %1, 6
-    movd         m0, %1d
-    lea          %1, [%2*3]
-    pshufw       m0, m0, 0
-    pxor         m1, m1
-    psubw        m1, m0
-    packuswb     m0, m0
-    packuswb     m1, m1
-%endmacro
-
-%macro DC_ADD_MMXEXT_OP 4
-    %1           m2, [%2     ]
-    %1           m3, [%2+%3  ]
-    %1           m4, [%2+%3*2]
-    %1           m5, [%2+%4  ]
-    paddusb      m2, m0
-    paddusb      m3, m0
-    paddusb      m4, m0
-    paddusb      m5, m0
-    psubusb      m2, m1
-    psubusb      m3, m1
-    psubusb      m4, m1
-    psubusb      m5, m1
-    %1    [%2     ], m2
-    %1    [%2+%3  ], m3
-    %1    [%2+%3*2], m4
-    %1    [%2+%4  ], m5
-%endmacro
-
-INIT_MMX mmxext
-; void ff_h264_idct_dc_add_8_mmxext(uint8_t *dst, int16_t *block, int stride)
-%if ARCH_X86_64
-cglobal h264_idct_dc_add_8, 3, 4, 0
-    movsxd       r2, r2d
-    movsx        r3, word [r1]
-    mov  dword [r1], 0
-    DC_ADD_MMXEXT_INIT r3, r2
-    DC_ADD_MMXEXT_OP movh, r0, r2, r3
-    RET
-
-; void ff_h264_idct8_dc_add_8_mmxext(uint8_t *dst, int16_t *block, int stride)
-cglobal h264_idct8_dc_add_8, 3, 4, 0
-    movsxd       r2, r2d
-    movsx        r3, word [r1]
-    mov  dword [r1], 0
-    DC_ADD_MMXEXT_INIT r3, r2
-    DC_ADD_MMXEXT_OP mova, r0, r2, r3
-    lea          r0, [r0+r2*4]
-    DC_ADD_MMXEXT_OP mova, r0, r2, r3
-    RET
-%else
-; void ff_h264_idct_dc_add_8_mmxext(uint8_t *dst, int16_t *block, int stride)
-cglobal h264_idct_dc_add_8, 2, 3, 0
-    movsx        r2, word [r1]
-    mov  dword [r1], 0
-    mov          r1, r2m
-    DC_ADD_MMXEXT_INIT r2, r1
-    DC_ADD_MMXEXT_OP movh, r0, r1, r2
-    RET
-
-; void ff_h264_idct8_dc_add_8_mmxext(uint8_t *dst, int16_t *block, int stride)
-cglobal h264_idct8_dc_add_8, 2, 3, 0
-    movsx        r2, word [r1]
-    mov  dword [r1], 0
-    mov          r1, r2m
-    DC_ADD_MMXEXT_INIT r2, r1
-    DC_ADD_MMXEXT_OP mova, r0, r1, r2
-    lea          r0, [r0+r1*4]
-    DC_ADD_MMXEXT_OP mova, r0, r1, r2
-    RET
-%endif
-
-INIT_MMX mmx
-; void ff_h264_idct_add16_8_mmx(uint8_t *dst, const int *block_offset,
-;                               int16_t *block, int stride,
-;                               const uint8_t nnzc[6 * 8])
-cglobal h264_idct_add16_8, 5, 7 + npicregs, 0, dst, block_offset, block, stride, nnzc, cntr, coeff, picreg
-    movsxdifnidn r3, r3d
-    xor          r5, r5
-%ifdef PIC
-    lea     picregq, [scan8_mem]
-%endif
-.nextblock:
-    movzx        r6, byte [scan8+r5]
-    movzx        r6, byte [r4+r6]
-    test         r6, r6
-    jz .skipblock
-    mov         r6d, dword [r1+r5*4]
-    lea          r6, [r0+r6]
-    IDCT4_ADD    r6, r2, r3
-.skipblock:
-    inc          r5
-    add          r2, 32
-    cmp          r5, 16
-    jl .nextblock
-    REP_RET
-
-; void ff_h264_idct8_add4_8_mmx(uint8_t *dst, const int *block_offset,
-;                               int16_t *block, int stride,
-;                               const uint8_t nnzc[6 * 8])
-cglobal h264_idct8_add4_8, 5, 7 + npicregs, 0, dst, block_offset, block, stride, nnzc, cntr, coeff, picreg
-    movsxdifnidn r3, r3d
-    %assign pad 128+4-(stack_offset&7)
-    SUB         rsp, pad
-
-    xor          r5, r5
-%ifdef PIC
-    lea     picregq, [scan8_mem]
-%endif
-.nextblock:
-    movzx        r6, byte [scan8+r5]
-    movzx        r6, byte [r4+r6]
-    test         r6, r6
-    jz .skipblock
-    mov         r6d, dword [r1+r5*4]
-    add          r6, r0
-    add   word [r2], 32
-    IDCT8_ADD_MMX_START r2  , rsp
-    IDCT8_ADD_MMX_START r2+8, rsp+64
-    IDCT8_ADD_MMX_END   r6  , rsp,   r3, r2
-    mov         r6d, dword [r1+r5*4]
-    lea          r6, [r0+r6+4]
-    IDCT8_ADD_MMX_END   r6  , rsp+8, r3
-.skipblock:
-    add          r5, 4
-    add          r2, 128
-    cmp          r5, 16
-    jl .nextblock
-    ADD         rsp, pad
-    RET
-
-INIT_MMX mmxext
-; void ff_h264_idct_add16_8_mmxext(uint8_t *dst, const int *block_offset,
-;                                  int16_t *block, int stride,
-;                                  const uint8_t nnzc[6 * 8])
-cglobal h264_idct_add16_8, 5, 8 + npicregs, 0, dst1, block_offset, block, stride, nnzc, cntr, coeff, dst2, picreg
-    movsxdifnidn r3, r3d
-    xor          r5, r5
-%ifdef PIC
-    lea     picregq, [scan8_mem]
-%endif
-.nextblock:
-    movzx        r6, byte [scan8+r5]
-    movzx        r6, byte [r4+r6]
-    test         r6, r6
-    jz .skipblock
-    cmp          r6, 1
-    jnz .no_dc
-    movsx        r6, word [r2]
-    test         r6, r6
-    jz .no_dc
-    mov   word [r2], 0
-    DC_ADD_MMXEXT_INIT r6, r3
-%if ARCH_X86_64 == 0
-%define dst2q r1
-%define dst2d r1d
-%endif
-    mov       dst2d, dword [r1+r5*4]
-    lea       dst2q, [r0+dst2q]
-    DC_ADD_MMXEXT_OP movh, dst2q, r3, r6
-%if ARCH_X86_64 == 0
-    mov          r1, r1m
-%endif
-    inc          r5
-    add          r2, 32
-    cmp          r5, 16
-    jl .nextblock
-    REP_RET
-.no_dc:
-    mov         r6d, dword [r1+r5*4]
-    add          r6, r0
-    IDCT4_ADD    r6, r2, r3
-.skipblock:
-    inc          r5
-    add          r2, 32
-    cmp          r5, 16
-    jl .nextblock
-    REP_RET
-
-INIT_MMX mmx
-; void ff_h264_idct_add16intra_8_mmx(uint8_t *dst, const int *block_offset,
-;                                    int16_t *block, int stride,
-;                                    const uint8_t nnzc[6 * 8])
-cglobal h264_idct_add16intra_8, 5, 7 + npicregs, 0, dst, block_offset, block, stride, nnzc, cntr, coeff, picreg
-    movsxdifnidn r3, r3d
-    xor          r5, r5
-%ifdef PIC
-    lea     picregq, [scan8_mem]
-%endif
-.nextblock:
-    movzx        r6, byte [scan8+r5]
-    movzx        r6, byte [r4+r6]
-    or          r6w, word [r2]
-    test         r6, r6
-    jz .skipblock
-    mov         r6d, dword [r1+r5*4]
-    add          r6, r0
-    IDCT4_ADD    r6, r2, r3
-.skipblock:
-    inc          r5
-    add          r2, 32
-    cmp          r5, 16
-    jl .nextblock
-    REP_RET
-
-INIT_MMX mmxext
-; void ff_h264_idct_add16intra_8_mmxext(uint8_t *dst, const int *block_offset,
-;                                       int16_t *block, int stride,
-;                                       const uint8_t nnzc[6 * 8])
-cglobal h264_idct_add16intra_8, 5, 8 + npicregs, 0, dst1, block_offset, block, stride, nnzc, cntr, coeff, dst2, picreg
-    movsxdifnidn r3, r3d
-    xor          r5, r5
-%ifdef PIC
-    lea     picregq, [scan8_mem]
-%endif
-.nextblock:
-    movzx        r6, byte [scan8+r5]
-    movzx        r6, byte [r4+r6]
-    test         r6, r6
-    jz .try_dc
-    mov         r6d, dword [r1+r5*4]
-    lea          r6, [r0+r6]
-    IDCT4_ADD    r6, r2, r3
-    inc          r5
-    add          r2, 32
-    cmp          r5, 16
-    jl .nextblock
-    REP_RET
-.try_dc:
-    movsx        r6, word [r2]
-    test         r6, r6
-    jz .skipblock
-    mov   word [r2], 0
-    DC_ADD_MMXEXT_INIT r6, r3
-%if ARCH_X86_64 == 0
-%define dst2q r1
-%define dst2d r1d
-%endif
-    mov       dst2d, dword [r1+r5*4]
-    add       dst2q, r0
-    DC_ADD_MMXEXT_OP movh, dst2q, r3, r6
-%if ARCH_X86_64 == 0
-    mov          r1, r1m
-%endif
-.skipblock:
-    inc          r5
-    add          r2, 32
-    cmp          r5, 16
-    jl .nextblock
-    REP_RET
-
-; void ff_h264_idct8_add4_8_mmxext(uint8_t *dst, const int *block_offset,
-;                                  int16_t *block, int stride,
-;                                  const uint8_t nnzc[6 * 8])
-cglobal h264_idct8_add4_8, 5, 8 + npicregs, 0, dst1, block_offset, block, stride, nnzc, cntr, coeff, dst2, picreg
-    movsxdifnidn r3, r3d
-    %assign pad 128+4-(stack_offset&7)
-    SUB         rsp, pad
-
-    xor          r5, r5
-%ifdef PIC
-    lea     picregq, [scan8_mem]
-%endif
-.nextblock:
-    movzx        r6, byte [scan8+r5]
-    movzx        r6, byte [r4+r6]
-    test         r6, r6
-    jz .skipblock
-    cmp          r6, 1
-    jnz .no_dc
-    movsx        r6, word [r2]
-    test         r6, r6
-    jz .no_dc
-    mov   word [r2], 0
-    DC_ADD_MMXEXT_INIT r6, r3
-%if ARCH_X86_64 == 0
-%define dst2q r1
-%define dst2d r1d
-%endif
-    mov       dst2d, dword [r1+r5*4]
-    lea       dst2q, [r0+dst2q]
-    DC_ADD_MMXEXT_OP mova, dst2q, r3, r6
-    lea       dst2q, [dst2q+r3*4]
-    DC_ADD_MMXEXT_OP mova, dst2q, r3, r6
-%if ARCH_X86_64 == 0
-    mov          r1, r1m
-%endif
-    add          r5, 4
-    add          r2, 128
-    cmp          r5, 16
-    jl .nextblock
-
-    ADD         rsp, pad
-    RET
-.no_dc:
-    mov         r6d, dword [r1+r5*4]
-    add          r6, r0
-    add   word [r2], 32
-    IDCT8_ADD_MMX_START r2  , rsp
-    IDCT8_ADD_MMX_START r2+8, rsp+64
-    IDCT8_ADD_MMX_END   r6  , rsp,   r3, r2
-    mov         r6d, dword [r1+r5*4]
-    lea          r6, [r0+r6+4]
-    IDCT8_ADD_MMX_END   r6  , rsp+8, r3
-.skipblock:
-    add          r5, 4
-    add          r2, 128
-    cmp          r5, 16
-    jl .nextblock
-
-    ADD         rsp, pad
-    RET
-
-INIT_XMM sse2
-; void ff_h264_idct8_add4_8_sse2(uint8_t *dst, const int *block_offset,
-;                                int16_t *block, int stride,
-;                                const uint8_t nnzc[6 * 8])
-cglobal h264_idct8_add4_8, 5, 8 + npicregs, 10, dst1, block_offset, block, stride, nnzc, cntr, coeff, dst2, picreg
-    movsxdifnidn r3, r3d
-    xor          r5, r5
-%ifdef PIC
-    lea     picregq, [scan8_mem]
-%endif
-.nextblock:
-    movzx        r6, byte [scan8+r5]
-    movzx        r6, byte [r4+r6]
-    test         r6, r6
-    jz .skipblock
-    cmp          r6, 1
-    jnz .no_dc
-    movsx        r6, word [r2]
-    test         r6, r6
-    jz .no_dc
-INIT_MMX cpuname
-    mov   word [r2], 0
-    DC_ADD_MMXEXT_INIT r6, r3
-%if ARCH_X86_64 == 0
-%define dst2q r1
-%define dst2d r1d
-%endif
-    mov       dst2d, dword [r1+r5*4]
-    add       dst2q, r0
-    DC_ADD_MMXEXT_OP mova, dst2q, r3, r6
-    lea       dst2q, [dst2q+r3*4]
-    DC_ADD_MMXEXT_OP mova, dst2q, r3, r6
-%if ARCH_X86_64 == 0
-    mov          r1, r1m
-%endif
-    add          r5, 4
-    add          r2, 128
-    cmp          r5, 16
-    jl .nextblock
-    REP_RET
-.no_dc:
-INIT_XMM cpuname
-    mov       dst2d, dword [r1+r5*4]
-    add       dst2q, r0
-    IDCT8_ADD_SSE dst2q, r2, r3, r6
-%if ARCH_X86_64 == 0
-    mov          r1, r1m
-%endif
-.skipblock:
-    add          r5, 4
-    add          r2, 128
-    cmp          r5, 16
-    jl .nextblock
-    REP_RET
-
-INIT_MMX mmx
-h264_idct_add8_mmx_plane:
-    movsxdifnidn r3, r3d
-.nextblock:
-    movzx        r6, byte [scan8+r5]
-    movzx        r6, byte [r4+r6]
-    or          r6w, word [r2]
-    test         r6, r6
-    jz .skipblock
-%if ARCH_X86_64
-    mov         r0d, dword [r1+r5*4]
-    add          r0, [dst2q]
-%else
-    mov          r0, r1m ; XXX r1m here is actually r0m of the calling func
-    mov          r0, [r0]
-    add          r0, dword [r1+r5*4]
-%endif
-    IDCT4_ADD    r0, r2, r3
-.skipblock:
-    inc          r5
-    add          r2, 32
-    test         r5, 3
-    jnz .nextblock
-    rep ret
-
-; void ff_h264_idct_add8_8_mmx(uint8_t **dest, const int *block_offset,
-;                              int16_t *block, int stride,
-;                              const uint8_t nnzc[6 * 8])
-cglobal h264_idct_add8_8, 5, 8 + npicregs, 0, dst1, block_offset, block, stride, nnzc, cntr, coeff, dst2, picreg
-    movsxdifnidn r3, r3d
-    mov          r5, 16
-    add          r2, 512
-%ifdef PIC
-    lea     picregq, [scan8_mem]
-%endif
-%if ARCH_X86_64
-    mov       dst2q, r0
-%endif
-    call         h264_idct_add8_mmx_plane
-    mov          r5, 32
-    add          r2, 384
-%if ARCH_X86_64
-    add       dst2q, gprsize
-%else
-    add        r0mp, gprsize
-%endif
-    call         h264_idct_add8_mmx_plane
-    RET ; TODO: check rep ret after a function call
-
-cglobal h264_idct_add8_422_8, 5, 8 + npicregs, 0, dst1, block_offset, block, stride, nnzc, cntr, coeff, dst2, picreg
-; dst1, block_offset, block, stride, nnzc, cntr, coeff, dst2, picreg
-    movsxdifnidn r3, r3d
-%ifdef PIC
-    lea     picregq, [scan8_mem]
-%endif
-%if ARCH_X86_64
-    mov       dst2q, r0
-%endif
-
-    mov          r5, 16  ; i
-    add          r2, 512 ; i * 16 * sizeof(dctcoef) ; #define dctcoef int16_t
-
-    call         h264_idct_add8_mmx_plane
-    add r5, 4
-    call         h264_idct_add8_mmx_plane
-
-%if ARCH_X86_64
-    add       dst2q, gprsize ; dest[1]
-%else
-    add        r0mp, gprsize
-%endif
-
-    add r5, 4   ; set to 32
-    add r2, 256 ; set to i * 16 * sizeof(dctcoef)
-
-    call         h264_idct_add8_mmx_plane
-    add r5, 4
-    call         h264_idct_add8_mmx_plane
-
-    RET ; TODO: check rep ret after a function call
-
-h264_idct_add8_mmxext_plane:
-    movsxdifnidn r3, r3d
-.nextblock:
-    movzx        r6, byte [scan8+r5]
-    movzx        r6, byte [r4+r6]
-    test         r6, r6
-    jz .try_dc
-%if ARCH_X86_64
-    mov         r0d, dword [r1+r5*4]
-    add          r0, [dst2q]
-%else
-    mov          r0, r1m ; XXX r1m here is actually r0m of the calling func
-    mov          r0, [r0]
-    add          r0, dword [r1+r5*4]
-%endif
-    IDCT4_ADD    r0, r2, r3
-    inc          r5
-    add          r2, 32
-    test         r5, 3
-    jnz .nextblock
-    rep ret
-.try_dc:
-    movsx        r6, word [r2]
-    test         r6, r6
-    jz .skipblock
-    mov   word [r2], 0
-    DC_ADD_MMXEXT_INIT r6, r3
-%if ARCH_X86_64
-    mov         r0d, dword [r1+r5*4]
-    add          r0, [dst2q]
-%else
-    mov          r0, r1m ; XXX r1m here is actually r0m of the calling func
-    mov          r0, [r0]
-    add          r0, dword [r1+r5*4]
-%endif
-    DC_ADD_MMXEXT_OP movh, r0, r3, r6
-.skipblock:
-    inc          r5
-    add          r2, 32
-    test         r5, 3
-    jnz .nextblock
-    rep ret
-
-INIT_MMX mmxext
-; void ff_h264_idct_add8_8_mmxext(uint8_t **dest, const int *block_offset,
-;                                 int16_t *block, int stride,
-;                                 const uint8_t nnzc[6 * 8])
-cglobal h264_idct_add8_8, 5, 8 + npicregs, 0, dst1, block_offset, block, stride, nnzc, cntr, coeff, dst2, picreg
-    movsxdifnidn r3, r3d
-    mov          r5, 16
-    add          r2, 512
-%if ARCH_X86_64
-    mov       dst2q, r0
-%endif
-%ifdef PIC
-    lea     picregq, [scan8_mem]
-%endif
-    call h264_idct_add8_mmxext_plane
-    mov          r5, 32
-    add          r2, 384
-%if ARCH_X86_64
-    add       dst2q, gprsize
-%else
-    add        r0mp, gprsize
-%endif
-    call h264_idct_add8_mmxext_plane
-    RET ; TODO: check rep ret after a function call
-
-; r0 = uint8_t *dst, r2 = int16_t *block, r3 = int stride, r6=clobbered
-h264_idct_dc_add8_mmxext:
-    movsxdifnidn r3, r3d
-    movd         m0, [r2   ]          ;  0 0 X D
-    mov word [r2+ 0], 0
-    punpcklwd    m0, [r2+32]          ;  x X d D
-    mov word [r2+32], 0
-    paddsw       m0, [pw_32]
-    psraw        m0, 6
-    punpcklwd    m0, m0               ;  d d D D
-    pxor         m1, m1               ;  0 0 0 0
-    psubw        m1, m0               ; -d-d-D-D
-    packuswb     m0, m1               ; -d-d-D-D d d D D
-    pshufw       m1, m0, 0xFA         ; -d-d-d-d-D-D-D-D
-    punpcklwd    m0, m0               ;  d d d d D D D D
-    lea          r6, [r3*3]
-    DC_ADD_MMXEXT_OP movq, r0, r3, r6
-    ret
-
-ALIGN 16
-INIT_XMM sse2
-; r0 = uint8_t *dst (clobbered), r2 = int16_t *block, r3 = int stride
-h264_add8x4_idct_sse2:
-    movsxdifnidn r3, r3d
-    movq   m0, [r2+ 0]
-    movq   m1, [r2+ 8]
-    movq   m2, [r2+16]
-    movq   m3, [r2+24]
-    movhps m0, [r2+32]
-    movhps m1, [r2+40]
-    movhps m2, [r2+48]
-    movhps m3, [r2+56]
-    IDCT4_1D w,0,1,2,3,4,5
-    TRANSPOSE2x4x4W 0,1,2,3,4
-    paddw m0, [pw_32]
-    IDCT4_1D w,0,1,2,3,4,5
-    pxor  m7, m7
-    mova [r2+ 0], m7
-    mova [r2+16], m7
-    mova [r2+32], m7
-    mova [r2+48], m7
-    STORE_DIFFx2 m0, m1, m4, m5, m7, 6, r0, r3
-    lea   r0, [r0+r3*2]
-    STORE_DIFFx2 m2, m3, m4, m5, m7, 6, r0, r3
-    ret
-
-%macro add16_sse2_cycle 2
-    movzx       r0, word [r4+%2]
-    test        r0, r0
-    jz .cycle%1end
-    mov        r0d, dword [r1+%1*8]
-%if ARCH_X86_64
-    add         r0, r5
-%else
-    add         r0, r0m
-%endif
-    call        h264_add8x4_idct_sse2
-.cycle%1end:
-%if %1 < 7
-    add         r2, 64
-%endif
-%endmacro
-
-; void ff_h264_idct_add16_8_sse2(uint8_t *dst, const int *block_offset,
-;                                int16_t *block, int stride,
-;                                const uint8_t nnzc[6 * 8])
-cglobal h264_idct_add16_8, 5, 5 + ARCH_X86_64, 8
-    movsxdifnidn r3, r3d
-%if ARCH_X86_64
-    mov         r5, r0
-%endif
-    ; unrolling of the loop leads to an average performance gain of
-    ; 20-25%
-    add16_sse2_cycle 0, 0xc
-    add16_sse2_cycle 1, 0x14
-    add16_sse2_cycle 2, 0xe
-    add16_sse2_cycle 3, 0x16
-    add16_sse2_cycle 4, 0x1c
-    add16_sse2_cycle 5, 0x24
-    add16_sse2_cycle 6, 0x1e
-    add16_sse2_cycle 7, 0x26
-REP_RET
-
-%macro add16intra_sse2_cycle 2
-    movzx       r0, word [r4+%2]
-    test        r0, r0
-    jz .try%1dc
-    mov        r0d, dword [r1+%1*8]
-%if ARCH_X86_64
-    add         r0, r7
-%else
-    add         r0, r0m
-%endif
-    call        h264_add8x4_idct_sse2
-    jmp .cycle%1end
-.try%1dc:
-    movsx       r0, word [r2   ]
-    or         r0w, word [r2+32]
-    jz .cycle%1end
-    mov        r0d, dword [r1+%1*8]
-%if ARCH_X86_64
-    add         r0, r7
-%else
-    add         r0, r0m
-%endif
-    call        h264_idct_dc_add8_mmxext
-.cycle%1end:
-%if %1 < 7
-    add         r2, 64
-%endif
-%endmacro
-
-; void ff_h264_idct_add16intra_8_sse2(uint8_t *dst, const int *block_offset,
-;                                     int16_t *block, int stride,
-;                                     const uint8_t nnzc[6 * 8])
-cglobal h264_idct_add16intra_8, 5, 7 + ARCH_X86_64, 8
-    movsxdifnidn r3, r3d
-%if ARCH_X86_64
-    mov         r7, r0
-%endif
-    add16intra_sse2_cycle 0, 0xc
-    add16intra_sse2_cycle 1, 0x14
-    add16intra_sse2_cycle 2, 0xe
-    add16intra_sse2_cycle 3, 0x16
-    add16intra_sse2_cycle 4, 0x1c
-    add16intra_sse2_cycle 5, 0x24
-    add16intra_sse2_cycle 6, 0x1e
-    add16intra_sse2_cycle 7, 0x26
-REP_RET
-
-%macro add8_sse2_cycle 2
-    movzx       r0, word [r4+%2]
-    test        r0, r0
-    jz .try%1dc
-%if ARCH_X86_64
-    mov        r0d, dword [r1+(%1&1)*8+64*(1+(%1>>1))]
-    add         r0, [r7]
-%else
-    mov         r0, r0m
-    mov         r0, [r0]
-    add         r0, dword [r1+(%1&1)*8+64*(1+(%1>>1))]
-%endif
-    call        h264_add8x4_idct_sse2
-    jmp .cycle%1end
-.try%1dc:
-    movsx       r0, word [r2   ]
-    or         r0w, word [r2+32]
-    jz .cycle%1end
-%if ARCH_X86_64
-    mov        r0d, dword [r1+(%1&1)*8+64*(1+(%1>>1))]
-    add         r0, [r7]
-%else
-    mov         r0, r0m
-    mov         r0, [r0]
-    add         r0, dword [r1+(%1&1)*8+64*(1+(%1>>1))]
-%endif
-    call        h264_idct_dc_add8_mmxext
-.cycle%1end:
-%if %1 == 1
-    add         r2, 384+64
-%elif %1 < 3
-    add         r2, 64
-%endif
-%endmacro
-
-; void ff_h264_idct_add8_8_sse2(uint8_t **dest, const int *block_offset,
-;                               int16_t *block, int stride,
-;                               const uint8_t nnzc[6 * 8])
-cglobal h264_idct_add8_8, 5, 7 + ARCH_X86_64, 8
-    movsxdifnidn r3, r3d
-    add          r2, 512
-%if ARCH_X86_64
-    mov          r7, r0
-%endif
-    add8_sse2_cycle 0, 0x34
-    add8_sse2_cycle 1, 0x3c
-%if ARCH_X86_64
-    add          r7, gprsize
-%else
-    add        r0mp, gprsize
-%endif
-    add8_sse2_cycle 2, 0x5c
-    add8_sse2_cycle 3, 0x64
-REP_RET
-
-;void ff_h264_luma_dc_dequant_idct_mmx(int16_t *output, int16_t *input, int qmul)
-
-%macro WALSH4_1D 5
-    SUMSUB_BADC w, %4, %3, %2, %1, %5
-    SUMSUB_BADC w, %4, %2, %3, %1, %5
-    SWAP %1, %4, %3
-%endmacro
-
-%macro DEQUANT 1-3
-%if cpuflag(sse2)
-    movd      xmm4, t3d
-    movq      xmm5, [pw_1]
-    pshufd    xmm4, xmm4, 0
-    movq2dq   xmm0, m0
-    movq2dq   xmm1, m1
-    movq2dq   xmm2, m2
-    movq2dq   xmm3, m3
-    punpcklwd xmm0, xmm5
-    punpcklwd xmm1, xmm5
-    punpcklwd xmm2, xmm5
-    punpcklwd xmm3, xmm5
-    pmaddwd   xmm0, xmm4
-    pmaddwd   xmm1, xmm4
-    pmaddwd   xmm2, xmm4
-    pmaddwd   xmm3, xmm4
-    psrad     xmm0, %1
-    psrad     xmm1, %1
-    psrad     xmm2, %1
-    psrad     xmm3, %1
-    packssdw  xmm0, xmm1
-    packssdw  xmm2, xmm3
-%else
-    mova        m7, [pw_1]
-    mova        m4, %1
-    punpcklwd   %1, m7
-    punpckhwd   m4, m7
-    mova        m5, %2
-    punpcklwd   %2, m7
-    punpckhwd   m5, m7
-    movd        m7, t3d
-    punpckldq   m7, m7
-    pmaddwd     %1, m7
-    pmaddwd     %2, m7
-    pmaddwd     m4, m7
-    pmaddwd     m5, m7
-    psrad       %1, %3
-    psrad       %2, %3
-    psrad       m4, %3
-    psrad       m5, %3
-    packssdw    %1, m4
-    packssdw    %2, m5
-%endif
-%endmacro
-
-%macro STORE_WORDS 5-9
-%if cpuflag(sse)
-    movd  t0d, %1
-    psrldq  %1, 4
-    movd  t1d, %1
-    psrldq  %1, 4
-    mov [t2+%2*32], t0w
-    mov [t2+%4*32], t1w
-    shr   t0d, 16
-    shr   t1d, 16
-    mov [t2+%3*32], t0w
-    mov [t2+%5*32], t1w
-    movd  t0d, %1
-    psrldq  %1, 4
-    movd  t1d, %1
-    mov [t2+%6*32], t0w
-    mov [t2+%8*32], t1w
-    shr   t0d, 16
-    shr   t1d, 16
-    mov [t2+%7*32], t0w
-    mov [t2+%9*32], t1w
-%else
-    movd  t0d, %1
-    psrlq  %1, 32
-    movd  t1d, %1
-    mov [t2+%2*32], t0w
-    mov [t2+%4*32], t1w
-    shr   t0d, 16
-    shr   t1d, 16
-    mov [t2+%3*32], t0w
-    mov [t2+%5*32], t1w
-%endif
-%endmacro
-
-%macro DEQUANT_STORE 1
-%if cpuflag(sse2)
-    DEQUANT     %1
-    STORE_WORDS xmm0,  0,  1,  4,  5,  2,  3,  6,  7
-    STORE_WORDS xmm2,  8,  9, 12, 13, 10, 11, 14, 15
-%else
-    DEQUANT     m0, m1, %1
-    STORE_WORDS m0,  0,  1,  4,  5
-    STORE_WORDS m1,  2,  3,  6,  7
-
-    DEQUANT     m2, m3, %1
-    STORE_WORDS m2,  8,  9, 12, 13
-    STORE_WORDS m3, 10, 11, 14, 15
-%endif
-%endmacro
-
-%macro IDCT_DC_DEQUANT 1
-cglobal h264_luma_dc_dequant_idct, 3, 4, %1
-    ; manually spill XMM registers for Win64 because
-    ; the code here is initialized with INIT_MMX
-    WIN64_SPILL_XMM %1
-    movq        m3, [r1+24]
-    movq        m2, [r1+16]
-    movq        m1, [r1+ 8]
-    movq        m0, [r1+ 0]
-    WALSH4_1D    0,1,2,3,4
-    TRANSPOSE4x4W 0,1,2,3,4
-    WALSH4_1D    0,1,2,3,4
-
-; shift, tmp, output, qmul
-%if WIN64
-    DECLARE_REG_TMP 0,3,1,2
-    ; we can't avoid this, because r0 is the shift register (ecx) on win64
-    xchg        r0, t2
-%elif ARCH_X86_64
-    DECLARE_REG_TMP 3,1,0,2
-%else
-    DECLARE_REG_TMP 1,3,0,2
-%endif
-
-    cmp        t3d, 32767
-    jg .big_qmul
-    add        t3d, 128 << 16
-    DEQUANT_STORE 8
-    RET
-.big_qmul:
-    bsr        t0d, t3d
-    add        t3d, 128 << 16
-    mov        t1d, 7
-    cmp        t0d, t1d
-    cmovg      t0d, t1d
-    inc        t1d
-    shr        t3d, t0b
-    sub        t1d, t0d
-%if cpuflag(sse2)
-    movd      xmm6, t1d
-    DEQUANT_STORE xmm6
-%else
-    movd        m6, t1d
-    DEQUANT_STORE m6
-%endif
-    RET
-%endmacro
-
-INIT_MMX mmx
-IDCT_DC_DEQUANT 0
-INIT_MMX sse2
-IDCT_DC_DEQUANT 7
-
-%ifdef __NASM_VER__
-%if __NASM_MAJOR__ >= 2 && __NASM_MINOR__ >= 4
-%unmacro STORE_DIFFx2 8 ; remove macro from x86util.asm but yasm doesn't have this yet
-%endif
-%endif
-%macro STORE_DIFFx2 8 ; add1, add2, reg1, reg2, zero, shift, source, stride
-    movd       %3, [%7]
-    movd       %4, [%7+%8]
-    psraw      %1, %6
-    psraw      %2, %6
-    punpcklbw  %3, %5
-    punpcklbw  %4, %5
-    paddw      %3, %1
-    paddw      %4, %2
-    packuswb   %3, %5
-    packuswb   %4, %5
-    movd     [%7], %3
-    movd  [%7+%8], %4
-%endmacro
-
-%macro DC_ADD_INIT 1
-    add      %1d, 32
-    sar      %1d, 6
-    movd     m0, %1d
-    pshuflw  m0, m0, 0
-    lea      %1, [3*stride_q]
-    pxor     m1, m1
-    psubw    m1, m0
-    packuswb m0, m0
-    packuswb m1, m1
-%endmacro
-
-%macro IDCT_XMM 1
-
-INIT_XMM %1
-
-cglobal h264_idct_add_8, 3, 3, 8, dst_, block_, stride_
-    movsxdifnidn stride_q, stride_d
-    IDCT4_ADD    dst_q, block_q, stride_q
-RET
-
-cglobal h264_idct_dc_add_8, 3, 4, 6, dst_, block_, stride_
-    movsxdifnidn stride_q, stride_d
-    movsx             r3d, word [block_q]
-    mov   dword [block_q], 0
-    DC_ADD_INIT r3
-    DC_ADD_MMXEXT_OP movd, dst_q, stride_q, r3
-RET
-
-%endmacro
-
-IDCT_XMM sse2
-IDCT_XMM avx
diff -uparN ffmpeg-4.1/libavcodec/x86/h264_intrapred_10bit.asm ffmpeg-y/libavcodec/x86/h264_intrapred_10bit.asm
--- ffmpeg-4.1/libavcodec/x86/h264_intrapred_10bit.asm	2018-11-02 02:34:25.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/h264_intrapred_10bit.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,1199 +0,0 @@
-;*****************************************************************************
-;* MMX/SSE2/AVX-optimized 10-bit H.264 intra prediction code
-;*****************************************************************************
-;* Copyright (C) 2005-2011 x264 project
-;*
-;* Authors: Daniel Kang <daniel.d.kang@gmail.com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-
-cextern pw_1023
-%define pw_pixel_max pw_1023
-cextern pw_512
-cextern pw_16
-cextern pw_8
-cextern pw_4
-cextern pw_2
-cextern pw_1
-cextern pd_16
-
-pw_m32101234: dw -3, -2, -1, 0, 1, 2, 3, 4
-pw_m3:        times 8 dw -3
-pd_17:        times 4 dd 17
-
-SECTION .text
-
-; dest, left, right, src
-; output: %1 = (t[n-1] + t[n]*2 + t[n+1] + 2) >> 2
-%macro PRED4x4_LOWPASS 4
-    paddw       %2, %3
-    psrlw       %2, 1
-    pavgw       %1, %4, %2
-%endmacro
-
-;-----------------------------------------------------------------------------
-; void ff_pred4x4_down_right_10(pixel *src, const pixel *topright,
-;                               ptrdiff_t stride)
-;-----------------------------------------------------------------------------
-%macro PRED4x4_DR 0
-cglobal pred4x4_down_right_10, 3, 3
-    sub       r0, r2
-    lea       r1, [r0+r2*2]
-    movhps    m1, [r1-8]
-    movhps    m2, [r0+r2*1-8]
-    movhps    m4, [r0-8]
-    punpckhwd m2, m4
-    movq      m3, [r0]
-    punpckhdq m1, m2
-    PALIGNR   m3, m1, 10, m1
-    movhps    m4, [r1+r2*1-8]
-    PALIGNR   m0, m3, m4, 14, m4
-    movhps    m4, [r1+r2*2-8]
-    PALIGNR   m2, m0, m4, 14, m4
-    PRED4x4_LOWPASS m0, m2, m3, m0
-    movq      [r1+r2*2], m0
-    psrldq    m0, 2
-    movq      [r1+r2*1], m0
-    psrldq    m0, 2
-    movq      [r0+r2*2], m0
-    psrldq    m0, 2
-    movq      [r0+r2*1], m0
-    RET
-%endmacro
-
-INIT_XMM sse2
-PRED4x4_DR
-INIT_XMM ssse3
-PRED4x4_DR
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-PRED4x4_DR
-%endif
-
-;------------------------------------------------------------------------------
-; void ff_pred4x4_vertical_right_10(pixel *src, const pixel *topright,
-;                                   ptrdiff_t stride)
-;------------------------------------------------------------------------------
-%macro PRED4x4_VR 0
-cglobal pred4x4_vertical_right_10, 3, 3, 6
-    sub     r0, r2
-    lea     r1, [r0+r2*2]
-    movq    m5, [r0]            ; ........t3t2t1t0
-    movhps  m1, [r0-8]
-    PALIGNR m0, m5, m1, 14, m1  ; ......t3t2t1t0lt
-    pavgw   m5, m0
-    movhps  m1, [r0+r2*1-8]
-    PALIGNR m0, m1, 14, m1      ; ....t3t2t1t0ltl0
-    movhps  m2, [r0+r2*2-8]
-    PALIGNR m1, m0, m2, 14, m2  ; ..t3t2t1t0ltl0l1
-    movhps  m3, [r1+r2*1-8]
-    PALIGNR m2, m1, m3, 14, m3  ; t3t2t1t0ltl0l1l2
-    PRED4x4_LOWPASS m1, m0, m2, m1
-    pslldq  m0, m1, 12
-    psrldq  m1, 4
-    movq    [r0+r2*1], m5
-    movq    [r0+r2*2], m1
-    PALIGNR m5, m0, 14, m2
-    pslldq  m0, 2
-    movq    [r1+r2*1], m5
-    PALIGNR m1, m0, 14, m0
-    movq    [r1+r2*2], m1
-    RET
-%endmacro
-
-INIT_XMM sse2
-PRED4x4_VR
-INIT_XMM ssse3
-PRED4x4_VR
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-PRED4x4_VR
-%endif
-
-;-------------------------------------------------------------------------------
-; void ff_pred4x4_horizontal_down_10(pixel *src, const pixel *topright,
-;                                    ptrdiff_t stride)
-;-------------------------------------------------------------------------------
-%macro PRED4x4_HD 0
-cglobal pred4x4_horizontal_down_10, 3, 3
-    sub        r0, r2
-    lea        r1, [r0+r2*2]
-    movq       m0, [r0-8]      ; lt ..
-    movhps     m0, [r0]
-    pslldq     m0, 2           ; t2 t1 t0 lt .. .. .. ..
-    movq       m1, [r1+r2*2-8] ; l3
-    movq       m3, [r1+r2*1-8]
-    punpcklwd  m1, m3          ; l2 l3
-    movq       m2, [r0+r2*2-8] ; l1
-    movq       m3, [r0+r2*1-8]
-    punpcklwd  m2, m3          ; l0 l1
-    punpckhdq  m1, m2          ; l0 l1 l2 l3
-    punpckhqdq m1, m0          ; t2 t1 t0 lt l0 l1 l2 l3
-    psrldq     m0, m1, 4       ; .. .. t2 t1 t0 lt l0 l1
-    psrldq     m3, m1, 2       ; .. t2 t1 t0 lt l0 l1 l2
-    pavgw      m5, m1, m3
-    PRED4x4_LOWPASS m3, m1, m0, m3
-    punpcklwd  m5, m3
-    psrldq     m3, 8
-    PALIGNR    m3, m5, 12, m4
-    movq       [r1+r2*2], m5
-    movhps     [r0+r2*2], m5
-    psrldq     m5, 4
-    movq       [r1+r2*1], m5
-    movq       [r0+r2*1], m3
-    RET
-%endmacro
-
-INIT_XMM sse2
-PRED4x4_HD
-INIT_XMM ssse3
-PRED4x4_HD
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-PRED4x4_HD
-%endif
-
-;-----------------------------------------------------------------------------
-; void ff_pred4x4_dc_10(pixel *src, const pixel *topright, ptrdiff_t stride)
-;-----------------------------------------------------------------------------
-
-INIT_MMX mmxext
-cglobal pred4x4_dc_10, 3, 3
-    sub    r0, r2
-    lea    r1, [r0+r2*2]
-    movq   m2, [r0+r2*1-8]
-    paddw  m2, [r0+r2*2-8]
-    paddw  m2, [r1+r2*1-8]
-    paddw  m2, [r1+r2*2-8]
-    psrlq  m2, 48
-    movq   m0, [r0]
-    HADDW  m0, m1
-    paddw  m0, [pw_4]
-    paddw  m0, m2
-    psrlw  m0, 3
-    SPLATW m0, m0, 0
-    movq   [r0+r2*1], m0
-    movq   [r0+r2*2], m0
-    movq   [r1+r2*1], m0
-    movq   [r1+r2*2], m0
-    RET
-
-;-----------------------------------------------------------------------------
-; void ff_pred4x4_down_left_10(pixel *src, const pixel *topright,
-;                              ptrdiff_t stride)
-;-----------------------------------------------------------------------------
-%macro PRED4x4_DL 0
-cglobal pred4x4_down_left_10, 3, 3
-    sub        r0, r2
-    movq       m0, [r0]
-    movhps     m0, [r1]
-    psrldq     m2, m0, 2
-    pslldq     m3, m0, 2
-    pshufhw    m2, m2, 10100100b
-    PRED4x4_LOWPASS m0, m3, m2, m0
-    lea        r1, [r0+r2*2]
-    movhps     [r1+r2*2], m0
-    psrldq     m0, 2
-    movq       [r0+r2*1], m0
-    psrldq     m0, 2
-    movq       [r0+r2*2], m0
-    psrldq     m0, 2
-    movq       [r1+r2*1], m0
-    RET
-%endmacro
-
-INIT_XMM sse2
-PRED4x4_DL
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-PRED4x4_DL
-%endif
-
-;-----------------------------------------------------------------------------
-; void ff_pred4x4_vertical_left_10(pixel *src, const pixel *topright,
-;                                  ptrdiff_t stride)
-;-----------------------------------------------------------------------------
-%macro PRED4x4_VL 0
-cglobal pred4x4_vertical_left_10, 3, 3
-    sub        r0, r2
-    movu       m1, [r0]
-    movhps     m1, [r1]
-    psrldq     m0, m1, 2
-    psrldq     m2, m1, 4
-    pavgw      m4, m0, m1
-    PRED4x4_LOWPASS m0, m1, m2, m0
-    lea        r1, [r0+r2*2]
-    movq       [r0+r2*1], m4
-    movq       [r0+r2*2], m0
-    psrldq     m4, 2
-    psrldq     m0, 2
-    movq       [r1+r2*1], m4
-    movq       [r1+r2*2], m0
-    RET
-%endmacro
-
-INIT_XMM sse2
-PRED4x4_VL
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-PRED4x4_VL
-%endif
-
-;-----------------------------------------------------------------------------
-; void ff_pred4x4_horizontal_up_10(pixel *src, const pixel *topright,
-;                                  ptrdiff_t stride)
-;-----------------------------------------------------------------------------
-INIT_MMX mmxext
-cglobal pred4x4_horizontal_up_10, 3, 3
-    sub       r0, r2
-    lea       r1, [r0+r2*2]
-    movq      m0, [r0+r2*1-8]
-    punpckhwd m0, [r0+r2*2-8]
-    movq      m1, [r1+r2*1-8]
-    punpckhwd m1, [r1+r2*2-8]
-    punpckhdq m0, m1
-    pshufw    m1, m1, 0xFF
-    movq      [r1+r2*2], m1
-    movd      [r1+r2*1+4], m1
-    pshufw    m2, m0, 11111001b
-    movq      m1, m2
-    pavgw     m2, m0
-
-    pshufw    m5, m0, 11111110b
-    PRED4x4_LOWPASS m1, m0, m5, m1
-    movq      m6, m2
-    punpcklwd m6, m1
-    movq      [r0+r2*1], m6
-    psrlq     m2, 16
-    psrlq     m1, 16
-    punpcklwd m2, m1
-    movq      [r0+r2*2], m2
-    psrlq     m2, 32
-    movd      [r1+r2*1], m2
-    RET
-
-
-
-;-----------------------------------------------------------------------------
-; void ff_pred8x8_vertical_10(pixel *src, ptrdiff_t stride)
-;-----------------------------------------------------------------------------
-INIT_XMM sse2
-cglobal pred8x8_vertical_10, 2, 2
-    sub  r0, r1
-    mova m0, [r0]
-%rep 3
-    mova [r0+r1*1], m0
-    mova [r0+r1*2], m0
-    lea  r0, [r0+r1*2]
-%endrep
-    mova [r0+r1*1], m0
-    mova [r0+r1*2], m0
-    RET
-
-;-----------------------------------------------------------------------------
-; void ff_pred8x8_horizontal_10(pixel *src, ptrdiff_t stride)
-;-----------------------------------------------------------------------------
-INIT_XMM sse2
-cglobal pred8x8_horizontal_10, 2, 3
-    mov         r2d, 4
-.loop:
-    movq         m0, [r0+r1*0-8]
-    movq         m1, [r0+r1*1-8]
-    pshuflw      m0, m0, 0xff
-    pshuflw      m1, m1, 0xff
-    punpcklqdq   m0, m0
-    punpcklqdq   m1, m1
-    mova  [r0+r1*0], m0
-    mova  [r0+r1*1], m1
-    lea          r0, [r0+r1*2]
-    dec          r2d
-    jg .loop
-    REP_RET
-
-;-----------------------------------------------------------------------------
-; void ff_predict_8x8_dc_10(pixel *src, ptrdiff_t stride)
-;-----------------------------------------------------------------------------
-%macro MOV8 2-3
-; sort of a hack, but it works
-%if mmsize==8
-    movq    [%1+0], %2
-    movq    [%1+8], %3
-%else
-    movdqa    [%1], %2
-%endif
-%endmacro
-
-%macro PRED8x8_DC 1
-cglobal pred8x8_dc_10, 2, 6
-    sub         r0, r1
-    pxor        m4, m4
-    movq        m0, [r0+0]
-    movq        m1, [r0+8]
-%if mmsize==16
-    punpcklwd   m0, m1
-    movhlps     m1, m0
-    paddw       m0, m1
-%else
-    pshufw      m2, m0, 00001110b
-    pshufw      m3, m1, 00001110b
-    paddw       m0, m2
-    paddw       m1, m3
-    punpcklwd   m0, m1
-%endif
-    %1          m2, m0, 00001110b
-    paddw       m0, m2
-
-    lea         r5, [r1*3]
-    lea         r4, [r0+r1*4]
-    movzx      r2d, word [r0+r1*1-2]
-    movzx      r3d, word [r0+r1*2-2]
-    add        r2d, r3d
-    movzx      r3d, word [r0+r5*1-2]
-    add        r2d, r3d
-    movzx      r3d, word [r4-2]
-    add        r2d, r3d
-    movd        m2, r2d            ; s2
-
-    movzx      r2d, word [r4+r1*1-2]
-    movzx      r3d, word [r4+r1*2-2]
-    add        r2d, r3d
-    movzx      r3d, word [r4+r5*1-2]
-    add        r2d, r3d
-    movzx      r3d, word [r4+r1*4-2]
-    add        r2d, r3d
-    movd        m3, r2d            ; s3
-
-    punpcklwd   m2, m3
-    punpckldq   m0, m2            ; s0, s1, s2, s3
-    %1          m3, m0, 11110110b ; s2, s1, s3, s3
-    %1          m0, m0, 01110100b ; s0, s1, s3, s1
-    paddw       m0, m3
-    psrlw       m0, 2
-    pavgw       m0, m4            ; s0+s2, s1, s3, s1+s3
-%if mmsize==16
-    punpcklwd   m0, m0
-    pshufd      m3, m0, 11111010b
-    punpckldq   m0, m0
-    SWAP         0,1
-%else
-    pshufw      m1, m0, 0x00
-    pshufw      m2, m0, 0x55
-    pshufw      m3, m0, 0xaa
-    pshufw      m4, m0, 0xff
-%endif
-    MOV8   r0+r1*1, m1, m2
-    MOV8   r0+r1*2, m1, m2
-    MOV8   r0+r5*1, m1, m2
-    MOV8   r0+r1*4, m1, m2
-    MOV8   r4+r1*1, m3, m4
-    MOV8   r4+r1*2, m3, m4
-    MOV8   r4+r5*1, m3, m4
-    MOV8   r4+r1*4, m3, m4
-    RET
-%endmacro
-
-INIT_MMX mmxext
-PRED8x8_DC pshufw
-INIT_XMM sse2
-PRED8x8_DC pshuflw
-
-;-----------------------------------------------------------------------------
-; void ff_pred8x8_top_dc_10(pixel *src, ptrdiff_t stride)
-;-----------------------------------------------------------------------------
-INIT_XMM sse2
-cglobal pred8x8_top_dc_10, 2, 4
-    sub         r0, r1
-    mova        m0, [r0]
-    pshuflw     m1, m0, 0x4e
-    pshufhw     m1, m1, 0x4e
-    paddw       m0, m1
-    pshuflw     m1, m0, 0xb1
-    pshufhw     m1, m1, 0xb1
-    paddw       m0, m1
-    lea         r2, [r1*3]
-    lea         r3, [r0+r1*4]
-    paddw       m0, [pw_2]
-    psrlw       m0, 2
-    mova [r0+r1*1], m0
-    mova [r0+r1*2], m0
-    mova [r0+r2*1], m0
-    mova [r0+r1*4], m0
-    mova [r3+r1*1], m0
-    mova [r3+r1*2], m0
-    mova [r3+r2*1], m0
-    mova [r3+r1*4], m0
-    RET
-
-;-----------------------------------------------------------------------------
-; void ff_pred8x8_plane_10(pixel *src, ptrdiff_t stride)
-;-----------------------------------------------------------------------------
-INIT_XMM sse2
-cglobal pred8x8_plane_10, 2, 7, 7
-    sub       r0, r1
-    lea       r2, [r1*3]
-    lea       r3, [r0+r1*4]
-    mova      m2, [r0]
-    pmaddwd   m2, [pw_m32101234]
-    HADDD     m2, m1
-    movd      m0, [r0-4]
-    psrld     m0, 14
-    psubw     m2, m0               ; H
-    movd      m0, [r3+r1*4-4]
-    movd      m1, [r0+12]
-    paddw     m0, m1
-    psllw     m0, 4                ; 16*(src[7*stride-1] + src[-stride+7])
-    movzx    r4d, word [r3+r1*1-2] ; src[4*stride-1]
-    movzx    r5d, word [r0+r2*1-2] ; src[2*stride-1]
-    sub      r4d, r5d
-    movzx    r6d, word [r3+r1*2-2] ; src[5*stride-1]
-    movzx    r5d, word [r0+r1*2-2] ; src[1*stride-1]
-    sub      r6d, r5d
-    lea      r4d, [r4+r6*2]
-    movzx    r5d, word [r3+r2*1-2] ; src[6*stride-1]
-    movzx    r6d, word [r0+r1*1-2] ; src[0*stride-1]
-    sub      r5d, r6d
-    lea      r5d, [r5*3]
-    add      r4d, r5d
-    movzx    r6d, word [r3+r1*4-2] ; src[7*stride-1]
-    movzx    r5d, word [r0+r1*0-2] ; src[ -stride-1]
-    sub      r6d, r5d
-    lea      r4d, [r4+r6*4]
-    movd      m3, r4d              ; V
-    punpckldq m2, m3
-    pmaddwd   m2, [pd_17]
-    paddd     m2, [pd_16]
-    psrad     m2, 5                ; b, c
-
-    mova      m3, [pw_pixel_max]
-    pxor      m1, m1
-    SPLATW    m0, m0, 1
-    SPLATW    m4, m2, 2
-    SPLATW    m2, m2, 0
-    pmullw    m2, [pw_m32101234]   ; b
-    pmullw    m5, m4, [pw_m3]      ; c
-    paddw     m5, [pw_16]
-    mov      r2d, 8
-    add       r0, r1
-.loop:
-    paddsw    m6, m2, m5
-    paddsw    m6, m0
-    psraw     m6, 5
-    CLIPW     m6, m1, m3
-    mova    [r0], m6
-    paddw     m5, m4
-    add       r0, r1
-    dec r2d
-    jg .loop
-    REP_RET
-
-
-;-----------------------------------------------------------------------------
-; void ff_pred8x8l_128_dc_10(pixel *src, int has_topleft, int has_topright,
-;                            ptrdiff_t stride)
-;-----------------------------------------------------------------------------
-%macro PRED8x8L_128_DC 0
-cglobal pred8x8l_128_dc_10, 4, 4
-    mova      m0, [pw_512] ; (1<<(BIT_DEPTH-1))
-    lea       r1, [r3*3]
-    lea       r2, [r0+r3*4]
-    MOV8 r0+r3*0, m0, m0
-    MOV8 r0+r3*1, m0, m0
-    MOV8 r0+r3*2, m0, m0
-    MOV8 r0+r1*1, m0, m0
-    MOV8 r2+r3*0, m0, m0
-    MOV8 r2+r3*1, m0, m0
-    MOV8 r2+r3*2, m0, m0
-    MOV8 r2+r1*1, m0, m0
-    RET
-%endmacro
-
-INIT_MMX mmxext
-PRED8x8L_128_DC
-INIT_XMM sse2
-PRED8x8L_128_DC
-
-;-----------------------------------------------------------------------------
-; void ff_pred8x8l_top_dc_10(pixel *src, int has_topleft, int has_topright,
-;                            ptrdiff_t stride)
-;-----------------------------------------------------------------------------
-%macro PRED8x8L_TOP_DC 0
-cglobal pred8x8l_top_dc_10, 4, 4, 6
-    sub         r0, r3
-    mova        m0, [r0]
-    shr        r1d, 14
-    shr        r2d, 13
-    neg         r1
-    pslldq      m1, m0, 2
-    psrldq      m2, m0, 2
-    pinsrw      m1, [r0+r1], 0
-    pinsrw      m2, [r0+r2+14], 7
-    lea         r1, [r3*3]
-    lea         r2, [r0+r3*4]
-    PRED4x4_LOWPASS m0, m2, m1, m0
-    HADDW       m0, m1
-    paddw       m0, [pw_4]
-    psrlw       m0, 3
-    SPLATW      m0, m0, 0
-    mova [r0+r3*1], m0
-    mova [r0+r3*2], m0
-    mova [r0+r1*1], m0
-    mova [r0+r3*4], m0
-    mova [r2+r3*1], m0
-    mova [r2+r3*2], m0
-    mova [r2+r1*1], m0
-    mova [r2+r3*4], m0
-    RET
-%endmacro
-
-INIT_XMM sse2
-PRED8x8L_TOP_DC
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-PRED8x8L_TOP_DC
-%endif
-
-;-------------------------------------------------------------------------------
-; void ff_pred8x8l_dc_10(pixel *src, int has_topleft, int has_topright,
-;                        ptrdiff_t stride)
-;-------------------------------------------------------------------------------
-;TODO: see if scalar is faster
-%macro PRED8x8L_DC 0
-cglobal pred8x8l_dc_10, 4, 6, 6
-    sub         r0, r3
-    lea         r4, [r0+r3*4]
-    lea         r5, [r3*3]
-    mova        m0, [r0+r3*2-16]
-    punpckhwd   m0, [r0+r3*1-16]
-    mova        m1, [r4+r3*0-16]
-    punpckhwd   m1, [r0+r5*1-16]
-    punpckhdq   m1, m0
-    mova        m2, [r4+r3*2-16]
-    punpckhwd   m2, [r4+r3*1-16]
-    mova        m3, [r4+r3*4-16]
-    punpckhwd   m3, [r4+r5*1-16]
-    punpckhdq   m3, m2
-    punpckhqdq  m3, m1
-    mova        m0, [r0]
-    shr        r1d, 14
-    shr        r2d, 13
-    neg         r1
-    pslldq      m1, m0, 2
-    psrldq      m2, m0, 2
-    pinsrw      m1, [r0+r1], 0
-    pinsrw      m2, [r0+r2+14], 7
-    not         r1
-    and         r1, r3
-    pslldq      m4, m3, 2
-    psrldq      m5, m3, 2
-    pshuflw     m4, m4, 11100101b
-    pinsrw      m5, [r0+r1-2], 7
-    PRED4x4_LOWPASS m3, m4, m5, m3
-    PRED4x4_LOWPASS m0, m2, m1, m0
-    paddw       m0, m3
-    HADDW       m0, m1
-    paddw       m0, [pw_8]
-    psrlw       m0, 4
-    SPLATW      m0, m0
-    mova [r0+r3*1], m0
-    mova [r0+r3*2], m0
-    mova [r0+r5*1], m0
-    mova [r0+r3*4], m0
-    mova [r4+r3*1], m0
-    mova [r4+r3*2], m0
-    mova [r4+r5*1], m0
-    mova [r4+r3*4], m0
-    RET
-%endmacro
-
-INIT_XMM sse2
-PRED8x8L_DC
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-PRED8x8L_DC
-%endif
-
-;-----------------------------------------------------------------------------
-; void ff_pred8x8l_vertical_10(pixel *src, int has_topleft, int has_topright,
-;                              ptrdiff_t stride)
-;-----------------------------------------------------------------------------
-%macro PRED8x8L_VERTICAL 0
-cglobal pred8x8l_vertical_10, 4, 4, 6
-    sub         r0, r3
-    mova        m0, [r0]
-    shr        r1d, 14
-    shr        r2d, 13
-    neg         r1
-    pslldq      m1, m0, 2
-    psrldq      m2, m0, 2
-    pinsrw      m1, [r0+r1], 0
-    pinsrw      m2, [r0+r2+14], 7
-    lea         r1, [r3*3]
-    lea         r2, [r0+r3*4]
-    PRED4x4_LOWPASS m0, m2, m1, m0
-    mova [r0+r3*1], m0
-    mova [r0+r3*2], m0
-    mova [r0+r1*1], m0
-    mova [r0+r3*4], m0
-    mova [r2+r3*1], m0
-    mova [r2+r3*2], m0
-    mova [r2+r1*1], m0
-    mova [r2+r3*4], m0
-    RET
-%endmacro
-
-INIT_XMM sse2
-PRED8x8L_VERTICAL
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-PRED8x8L_VERTICAL
-%endif
-
-;-----------------------------------------------------------------------------
-; void ff_pred8x8l_horizontal_10(uint8_t *src, int has_topleft,
-;                                int has_topright, ptrdiff_t stride)
-;-----------------------------------------------------------------------------
-%macro PRED8x8L_HORIZONTAL 0
-cglobal pred8x8l_horizontal_10, 4, 4, 5
-    mova        m0, [r0-16]
-    shr        r1d, 14
-    dec         r1
-    and         r1, r3
-    sub         r1, r3
-    punpckhwd   m0, [r0+r1-16]
-    mova        m1, [r0+r3*2-16]
-    punpckhwd   m1, [r0+r3*1-16]
-    lea         r2, [r0+r3*4]
-    lea         r1, [r3*3]
-    punpckhdq   m1, m0
-    mova        m2, [r2+r3*0-16]
-    punpckhwd   m2, [r0+r1-16]
-    mova        m3, [r2+r3*2-16]
-    punpckhwd   m3, [r2+r3*1-16]
-    punpckhdq   m3, m2
-    punpckhqdq  m3, m1
-    PALIGNR     m4, m3, [r2+r1-16], 14, m0
-    pslldq      m0, m4, 2
-    pshuflw     m0, m0, 11100101b
-    PRED4x4_LOWPASS m4, m3, m0, m4
-    punpckhwd   m3, m4, m4
-    punpcklwd   m4, m4
-    pshufd      m0, m3, 0xff
-    pshufd      m1, m3, 0xaa
-    pshufd      m2, m3, 0x55
-    pshufd      m3, m3, 0x00
-    mova [r0+r3*0], m0
-    mova [r0+r3*1], m1
-    mova [r0+r3*2], m2
-    mova [r0+r1*1], m3
-    pshufd      m0, m4, 0xff
-    pshufd      m1, m4, 0xaa
-    pshufd      m2, m4, 0x55
-    pshufd      m3, m4, 0x00
-    mova [r2+r3*0], m0
-    mova [r2+r3*1], m1
-    mova [r2+r3*2], m2
-    mova [r2+r1*1], m3
-    RET
-%endmacro
-
-INIT_XMM sse2
-PRED8x8L_HORIZONTAL
-INIT_XMM ssse3
-PRED8x8L_HORIZONTAL
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-PRED8x8L_HORIZONTAL
-%endif
-
-;-----------------------------------------------------------------------------
-; void ff_pred8x8l_down_left_10(pixel *src, int has_topleft, int has_topright,
-;                               ptrdiff_t stride)
-;-----------------------------------------------------------------------------
-%macro PRED8x8L_DOWN_LEFT 0
-cglobal pred8x8l_down_left_10, 4, 4, 7
-    sub         r0, r3
-    mova        m3, [r0]
-    shr        r1d, 14
-    neg         r1
-    shr        r2d, 13
-    pslldq      m1, m3, 2
-    psrldq      m2, m3, 2
-    pinsrw      m1, [r0+r1], 0
-    pinsrw      m2, [r0+r2+14], 7
-    PRED4x4_LOWPASS m6, m2, m1, m3
-    jz .fix_tr ; flags from shr r2d
-    mova        m1, [r0+16]
-    psrldq      m5, m1, 2
-    PALIGNR     m2, m1, m3, 14, m3
-    pshufhw     m5, m5, 10100100b
-    PRED4x4_LOWPASS m1, m2, m5, m1
-.do_topright:
-    lea         r1, [r3*3]
-    psrldq      m5, m1, 14
-    lea         r2, [r0+r3*4]
-    PALIGNR     m2, m1, m6,  2, m0
-    PALIGNR     m3, m1, m6, 14, m0
-    PALIGNR     m5, m1,  2, m0
-    pslldq      m4, m6, 2
-    PRED4x4_LOWPASS m6, m4, m2, m6
-    PRED4x4_LOWPASS m1, m3, m5, m1
-    mova [r2+r3*4], m1
-    PALIGNR     m1, m6, 14, m2
-    pslldq      m6, 2
-    mova [r2+r1*1], m1
-    PALIGNR     m1, m6, 14, m2
-    pslldq      m6, 2
-    mova [r2+r3*2], m1
-    PALIGNR     m1, m6, 14, m2
-    pslldq      m6, 2
-    mova [r2+r3*1], m1
-    PALIGNR     m1, m6, 14, m2
-    pslldq      m6, 2
-    mova [r0+r3*4], m1
-    PALIGNR     m1, m6, 14, m2
-    pslldq      m6, 2
-    mova [r0+r1*1], m1
-    PALIGNR     m1, m6, 14, m2
-    pslldq      m6, 2
-    mova [r0+r3*2], m1
-    PALIGNR     m1, m6, 14, m6
-    mova [r0+r3*1], m1
-    RET
-.fix_tr:
-    punpckhwd   m3, m3
-    pshufd      m1, m3, 0xFF
-    jmp .do_topright
-%endmacro
-
-INIT_XMM sse2
-PRED8x8L_DOWN_LEFT
-INIT_XMM ssse3
-PRED8x8L_DOWN_LEFT
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-PRED8x8L_DOWN_LEFT
-%endif
-
-;-----------------------------------------------------------------------------
-; void ff_pred8x8l_down_right_10(pixel *src, int has_topleft,
-;                                int has_topright, ptrdiff_t stride)
-;-----------------------------------------------------------------------------
-%macro PRED8x8L_DOWN_RIGHT 0
-; standard forbids this when has_topleft is false
-; no need to check
-cglobal pred8x8l_down_right_10, 4, 5, 8
-    sub         r0, r3
-    lea         r4, [r0+r3*4]
-    lea         r1, [r3*3]
-    mova        m0, [r0+r3*1-16]
-    punpckhwd   m0, [r0+r3*0-16]
-    mova        m1, [r0+r1*1-16]
-    punpckhwd   m1, [r0+r3*2-16]
-    punpckhdq   m1, m0
-    mova        m2, [r4+r3*1-16]
-    punpckhwd   m2, [r4+r3*0-16]
-    mova        m3, [r4+r1*1-16]
-    punpckhwd   m3, [r4+r3*2-16]
-    punpckhdq   m3, m2
-    punpckhqdq  m3, m1
-    mova        m0, [r4+r3*4-16]
-    mova        m1, [r0]
-    PALIGNR     m4, m3, m0, 14, m0
-    PALIGNR     m1, m3,  2, m2
-    pslldq      m0, m4, 2
-    pshuflw     m0, m0, 11100101b
-    PRED4x4_LOWPASS m6, m1, m4, m3
-    PRED4x4_LOWPASS m4, m3, m0, m4
-    mova        m3, [r0]
-    shr        r2d, 13
-    pslldq      m1, m3, 2
-    psrldq      m2, m3, 2
-    pinsrw      m1, [r0-2], 0
-    pinsrw      m2, [r0+r2+14], 7
-    PRED4x4_LOWPASS m3, m2, m1, m3
-    PALIGNR     m2, m3, m6,  2, m0
-    PALIGNR     m5, m3, m6, 14, m0
-    psrldq      m7, m3, 2
-    PRED4x4_LOWPASS m6, m4, m2, m6
-    PRED4x4_LOWPASS m3, m5, m7, m3
-    mova [r4+r3*4], m6
-    PALIGNR     m3, m6, 14, m2
-    pslldq      m6, 2
-    mova [r0+r3*1], m3
-    PALIGNR     m3, m6, 14, m2
-    pslldq      m6, 2
-    mova [r0+r3*2], m3
-    PALIGNR     m3, m6, 14, m2
-    pslldq      m6, 2
-    mova [r0+r1*1], m3
-    PALIGNR     m3, m6, 14, m2
-    pslldq      m6, 2
-    mova [r0+r3*4], m3
-    PALIGNR     m3, m6, 14, m2
-    pslldq      m6, 2
-    mova [r4+r3*1], m3
-    PALIGNR     m3, m6, 14, m2
-    pslldq      m6, 2
-    mova [r4+r3*2], m3
-    PALIGNR     m3, m6, 14, m6
-    mova [r4+r1*1], m3
-    RET
-%endmacro
-
-INIT_XMM sse2
-PRED8x8L_DOWN_RIGHT
-INIT_XMM ssse3
-PRED8x8L_DOWN_RIGHT
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-PRED8x8L_DOWN_RIGHT
-%endif
-
-;-----------------------------------------------------------------------------
-; void ff_pred8x8l_vertical_right_10(pixel *src, int has_topleft,
-;                                    int has_topright, ptrdiff_t stride)
-;-----------------------------------------------------------------------------
-%macro PRED8x8L_VERTICAL_RIGHT 0
-; likewise with 8x8l_down_right
-cglobal pred8x8l_vertical_right_10, 4, 5, 7
-    sub         r0, r3
-    lea         r4, [r0+r3*4]
-    lea         r1, [r3*3]
-    mova        m0, [r0+r3*1-16]
-    punpckhwd   m0, [r0+r3*0-16]
-    mova        m1, [r0+r1*1-16]
-    punpckhwd   m1, [r0+r3*2-16]
-    punpckhdq   m1, m0
-    mova        m2, [r4+r3*1-16]
-    punpckhwd   m2, [r4+r3*0-16]
-    mova        m3, [r4+r1*1-16]
-    punpckhwd   m3, [r4+r3*2-16]
-    punpckhdq   m3, m2
-    punpckhqdq  m3, m1
-    mova        m0, [r4+r3*4-16]
-    mova        m1, [r0]
-    PALIGNR     m4, m3, m0, 14, m0
-    PALIGNR     m1, m3,  2, m2
-    PRED4x4_LOWPASS m3, m1, m4, m3
-    mova        m2, [r0]
-    shr        r2d, 13
-    pslldq      m1, m2, 2
-    psrldq      m5, m2, 2
-    pinsrw      m1, [r0-2], 0
-    pinsrw      m5, [r0+r2+14], 7
-    PRED4x4_LOWPASS m2, m5, m1, m2
-    PALIGNR     m6, m2, m3, 12, m1
-    PALIGNR     m5, m2, m3, 14, m0
-    PRED4x4_LOWPASS m0, m6, m2, m5
-    pavgw       m2, m5
-    mova [r0+r3*2], m0
-    mova [r0+r3*1], m2
-    pslldq      m6, m3, 4
-    pslldq      m1, m3, 2
-    PRED4x4_LOWPASS m1, m3, m6, m1
-    PALIGNR     m2, m1, 14, m4
-    mova [r0+r1*1], m2
-    pslldq      m1, 2
-    PALIGNR     m0, m1, 14, m3
-    mova [r0+r3*4], m0
-    pslldq      m1, 2
-    PALIGNR     m2, m1, 14, m4
-    mova [r4+r3*1], m2
-    pslldq      m1, 2
-    PALIGNR     m0, m1, 14, m3
-    mova [r4+r3*2], m0
-    pslldq      m1, 2
-    PALIGNR     m2, m1, 14, m4
-    mova [r4+r1*1], m2
-    pslldq      m1, 2
-    PALIGNR     m0, m1, 14, m1
-    mova [r4+r3*4], m0
-    RET
-%endmacro
-
-INIT_XMM sse2
-PRED8x8L_VERTICAL_RIGHT
-INIT_XMM ssse3
-PRED8x8L_VERTICAL_RIGHT
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-PRED8x8L_VERTICAL_RIGHT
-%endif
-
-;-----------------------------------------------------------------------------
-; void ff_pred8x8l_horizontal_up_10(pixel *src, int has_topleft,
-;                                   int has_topright, ptrdiff_t stride)
-;-----------------------------------------------------------------------------
-%macro PRED8x8L_HORIZONTAL_UP 0
-cglobal pred8x8l_horizontal_up_10, 4, 4, 6
-    mova        m0, [r0+r3*0-16]
-    punpckhwd   m0, [r0+r3*1-16]
-    shr        r1d, 14
-    dec         r1
-    and         r1, r3
-    sub         r1, r3
-    mova        m4, [r0+r1*1-16]
-    lea         r1, [r3*3]
-    lea         r2, [r0+r3*4]
-    mova        m1, [r0+r3*2-16]
-    punpckhwd   m1, [r0+r1*1-16]
-    punpckhdq   m0, m1
-    mova        m2, [r2+r3*0-16]
-    punpckhwd   m2, [r2+r3*1-16]
-    mova        m3, [r2+r3*2-16]
-    punpckhwd   m3, [r2+r1*1-16]
-    punpckhdq   m2, m3
-    punpckhqdq  m0, m2
-    PALIGNR     m1, m0, m4, 14, m4
-    psrldq      m2, m0, 2
-    pshufhw     m2, m2, 10100100b
-    PRED4x4_LOWPASS m0, m1, m2, m0
-    psrldq      m1, m0, 2
-    psrldq      m2, m0, 4
-    pshufhw     m1, m1, 10100100b
-    pshufhw     m2, m2, 01010100b
-    pavgw       m4, m0, m1
-    PRED4x4_LOWPASS m1, m2, m0, m1
-    punpckhwd   m5, m4, m1
-    punpcklwd   m4, m1
-    mova [r2+r3*0], m5
-    mova [r0+r3*0], m4
-    pshufd      m0, m5, 11111001b
-    pshufd      m1, m5, 11111110b
-    pshufd      m2, m5, 11111111b
-    mova [r2+r3*1], m0
-    mova [r2+r3*2], m1
-    mova [r2+r1*1], m2
-    PALIGNR     m2, m5, m4, 4, m0
-    PALIGNR     m3, m5, m4, 8, m1
-    PALIGNR     m5, m5, m4, 12, m4
-    mova [r0+r3*1], m2
-    mova [r0+r3*2], m3
-    mova [r0+r1*1], m5
-    RET
-%endmacro
-
-INIT_XMM sse2
-PRED8x8L_HORIZONTAL_UP
-INIT_XMM ssse3
-PRED8x8L_HORIZONTAL_UP
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-PRED8x8L_HORIZONTAL_UP
-%endif
-
-
-;-----------------------------------------------------------------------------
-; void ff_pred16x16_vertical_10(pixel *src, ptrdiff_t stride)
-;-----------------------------------------------------------------------------
-%macro MOV16 3-5
-    mova [%1+     0], %2
-    mova [%1+mmsize], %3
-%if mmsize==8
-    mova [%1+    16], %4
-    mova [%1+    24], %5
-%endif
-%endmacro
-
-%macro PRED16x16_VERTICAL 0
-cglobal pred16x16_vertical_10, 2, 3
-    sub   r0, r1
-    mov  r2d, 8
-    mova  m0, [r0+ 0]
-    mova  m1, [r0+mmsize]
-%if mmsize==8
-    mova  m2, [r0+16]
-    mova  m3, [r0+24]
-%endif
-.loop:
-    MOV16 r0+r1*1, m0, m1, m2, m3
-    MOV16 r0+r1*2, m0, m1, m2, m3
-    lea   r0, [r0+r1*2]
-    dec   r2d
-    jg .loop
-    REP_RET
-%endmacro
-
-INIT_MMX mmxext
-PRED16x16_VERTICAL
-INIT_XMM sse2
-PRED16x16_VERTICAL
-
-;-----------------------------------------------------------------------------
-; void ff_pred16x16_horizontal_10(pixel *src, ptrdiff_t stride)
-;-----------------------------------------------------------------------------
-%macro PRED16x16_HORIZONTAL 0
-cglobal pred16x16_horizontal_10, 2, 3
-    mov   r2d, 8
-.vloop:
-    movd   m0, [r0+r1*0-4]
-    movd   m1, [r0+r1*1-4]
-    SPLATW m0, m0, 1
-    SPLATW m1, m1, 1
-    MOV16  r0+r1*0, m0, m0, m0, m0
-    MOV16  r0+r1*1, m1, m1, m1, m1
-    lea    r0, [r0+r1*2]
-    dec    r2d
-    jg .vloop
-    REP_RET
-%endmacro
-
-INIT_MMX mmxext
-PRED16x16_HORIZONTAL
-INIT_XMM sse2
-PRED16x16_HORIZONTAL
-
-;-----------------------------------------------------------------------------
-; void ff_pred16x16_dc_10(pixel *src, ptrdiff_t stride)
-;-----------------------------------------------------------------------------
-%macro PRED16x16_DC 0
-cglobal pred16x16_dc_10, 2, 6
-    mov        r5, r0
-    sub        r0, r1
-    mova       m0, [r0+0]
-    paddw      m0, [r0+mmsize]
-%if mmsize==8
-    paddw      m0, [r0+16]
-    paddw      m0, [r0+24]
-%endif
-    HADDW      m0, m2
-
-    lea        r0, [r0+r1-2]
-    movzx     r3d, word [r0]
-    movzx     r4d, word [r0+r1]
-%rep 7
-    lea        r0, [r0+r1*2]
-    movzx     r2d, word [r0]
-    add       r3d, r2d
-    movzx     r2d, word [r0+r1]
-    add       r4d, r2d
-%endrep
-    lea       r3d, [r3+r4+16]
-
-    movd       m1, r3d
-    paddw      m0, m1
-    psrlw      m0, 5
-    SPLATW     m0, m0
-    mov       r3d, 8
-.loop:
-    MOV16 r5+r1*0, m0, m0, m0, m0
-    MOV16 r5+r1*1, m0, m0, m0, m0
-    lea        r5, [r5+r1*2]
-    dec       r3d
-    jg .loop
-    REP_RET
-%endmacro
-
-INIT_MMX mmxext
-PRED16x16_DC
-INIT_XMM sse2
-PRED16x16_DC
-
-;-----------------------------------------------------------------------------
-; void ff_pred16x16_top_dc_10(pixel *src, ptrdiff_t stride)
-;-----------------------------------------------------------------------------
-%macro PRED16x16_TOP_DC 0
-cglobal pred16x16_top_dc_10, 2, 3
-    sub        r0, r1
-    mova       m0, [r0+0]
-    paddw      m0, [r0+mmsize]
-%if mmsize==8
-    paddw      m0, [r0+16]
-    paddw      m0, [r0+24]
-%endif
-    HADDW      m0, m2
-
-    SPLATW     m0, m0
-    paddw      m0, [pw_8]
-    psrlw      m0, 4
-    mov       r2d, 8
-.loop:
-    MOV16 r0+r1*1, m0, m0, m0, m0
-    MOV16 r0+r1*2, m0, m0, m0, m0
-    lea        r0, [r0+r1*2]
-    dec       r2d
-    jg .loop
-    REP_RET
-%endmacro
-
-INIT_MMX mmxext
-PRED16x16_TOP_DC
-INIT_XMM sse2
-PRED16x16_TOP_DC
-
-;-----------------------------------------------------------------------------
-; void ff_pred16x16_left_dc_10(pixel *src, ptrdiff_t stride)
-;-----------------------------------------------------------------------------
-%macro PRED16x16_LEFT_DC 0
-cglobal pred16x16_left_dc_10, 2, 6
-    mov        r5, r0
-
-    sub        r0, 2
-    movzx     r3d, word [r0]
-    movzx     r4d, word [r0+r1]
-%rep 7
-    lea        r0, [r0+r1*2]
-    movzx     r2d, word [r0]
-    add       r3d, r2d
-    movzx     r2d, word [r0+r1]
-    add       r4d, r2d
-%endrep
-    lea       r3d, [r3+r4+8]
-    shr       r3d, 4
-
-    movd       m0, r3d
-    SPLATW     m0, m0
-    mov       r3d, 8
-.loop:
-    MOV16 r5+r1*0, m0, m0, m0, m0
-    MOV16 r5+r1*1, m0, m0, m0, m0
-    lea        r5, [r5+r1*2]
-    dec       r3d
-    jg .loop
-    REP_RET
-%endmacro
-
-INIT_MMX mmxext
-PRED16x16_LEFT_DC
-INIT_XMM sse2
-PRED16x16_LEFT_DC
-
-;-----------------------------------------------------------------------------
-; void ff_pred16x16_128_dc_10(pixel *src, ptrdiff_t stride)
-;-----------------------------------------------------------------------------
-%macro PRED16x16_128_DC 0
-cglobal pred16x16_128_dc_10, 2,3
-    mova       m0, [pw_512]
-    mov       r2d, 8
-.loop:
-    MOV16 r0+r1*0, m0, m0, m0, m0
-    MOV16 r0+r1*1, m0, m0, m0, m0
-    lea        r0, [r0+r1*2]
-    dec       r2d
-    jg .loop
-    REP_RET
-%endmacro
-
-INIT_MMX mmxext
-PRED16x16_128_DC
-INIT_XMM sse2
-PRED16x16_128_DC
diff -uparN ffmpeg-4.1/libavcodec/x86/h264_intrapred.asm ffmpeg-y/libavcodec/x86/h264_intrapred.asm
--- ffmpeg-4.1/libavcodec/x86/h264_intrapred.asm	2018-11-02 02:34:25.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/h264_intrapred.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,2757 +0,0 @@
-;******************************************************************************
-;* H.264 intra prediction asm optimizations
-;* Copyright (c) 2010 Fiona Glaser
-;* Copyright (c) 2010 Holger Lubitz
-;* Copyright (c) 2010 Loren Merritt
-;* Copyright (c) 2010 Ronald S. Bultje
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-
-tm_shuf: times 8 db 0x03, 0x80
-pw_ff00: times 8 dw 0xff00
-plane_shuf:  db -8, -7, -6, -5, -4, -3, -2, -1
-             db  1,  2,  3,  4,  5,  6,  7,  8
-plane8_shuf: db -4, -3, -2, -1,  0,  0,  0,  0
-             db  1,  2,  3,  4,  0,  0,  0,  0
-pw_0to7:     dw  0,  1,  2,  3,  4,  5,  6,  7
-pw_1to8:     dw  1,  2,  3,  4,  5,  6,  7,  8
-pw_m8tom1:   dw -8, -7, -6, -5, -4, -3, -2, -1
-pw_m4to4:    dw -4, -3, -2, -1,  1,  2,  3,  4
-
-SECTION .text
-
-cextern pb_1
-cextern pb_3
-cextern pw_4
-cextern pw_5
-cextern pw_8
-cextern pw_16
-cextern pw_17
-cextern pw_32
-
-;-----------------------------------------------------------------------------
-; void ff_pred16x16_vertical_8(uint8_t *src, ptrdiff_t stride)
-;-----------------------------------------------------------------------------
-
-INIT_MMX mmx
-cglobal pred16x16_vertical_8, 2,3
-    sub   r0, r1
-    mov   r2, 8
-    movq mm0, [r0+0]
-    movq mm1, [r0+8]
-.loop:
-    movq [r0+r1*1+0], mm0
-    movq [r0+r1*1+8], mm1
-    movq [r0+r1*2+0], mm0
-    movq [r0+r1*2+8], mm1
-    lea   r0, [r0+r1*2]
-    dec   r2
-    jg .loop
-    REP_RET
-
-INIT_XMM sse
-cglobal pred16x16_vertical_8, 2,3
-    sub   r0, r1
-    mov   r2, 4
-    movaps xmm0, [r0]
-.loop:
-    movaps [r0+r1*1], xmm0
-    movaps [r0+r1*2], xmm0
-    lea   r0, [r0+r1*2]
-    movaps [r0+r1*1], xmm0
-    movaps [r0+r1*2], xmm0
-    lea   r0, [r0+r1*2]
-    dec   r2
-    jg .loop
-    REP_RET
-
-;-----------------------------------------------------------------------------
-; void ff_pred16x16_horizontal_8(uint8_t *src, ptrdiff_t stride)
-;-----------------------------------------------------------------------------
-
-%macro PRED16x16_H 0
-cglobal pred16x16_horizontal_8, 2,3
-    mov       r2, 8
-%if cpuflag(ssse3)
-    mova      m2, [pb_3]
-%endif
-.loop:
-    movd      m0, [r0+r1*0-4]
-    movd      m1, [r0+r1*1-4]
-
-%if cpuflag(ssse3)
-    pshufb    m0, m2
-    pshufb    m1, m2
-%else
-    punpcklbw m0, m0
-    punpcklbw m1, m1
-    SPLATW    m0, m0, 3
-    SPLATW    m1, m1, 3
-    mova [r0+r1*0+8], m0
-    mova [r0+r1*1+8], m1
-%endif
-
-    mova [r0+r1*0], m0
-    mova [r0+r1*1], m1
-    lea       r0, [r0+r1*2]
-    dec       r2
-    jg .loop
-    REP_RET
-%endmacro
-
-INIT_MMX mmx
-PRED16x16_H
-INIT_MMX mmxext
-PRED16x16_H
-INIT_XMM ssse3
-PRED16x16_H
-
-;-----------------------------------------------------------------------------
-; void ff_pred16x16_dc_8(uint8_t *src, ptrdiff_t stride)
-;-----------------------------------------------------------------------------
-
-%macro PRED16x16_DC 0
-cglobal pred16x16_dc_8, 2,7
-    mov       r4, r0
-    sub       r0, r1
-    pxor      mm0, mm0
-    pxor      mm1, mm1
-    psadbw    mm0, [r0+0]
-    psadbw    mm1, [r0+8]
-    dec        r0
-    movzx     r5d, byte [r0+r1*1]
-    paddw     mm0, mm1
-    movd      r6d, mm0
-    lea        r0, [r0+r1*2]
-%rep 7
-    movzx     r2d, byte [r0+r1*0]
-    movzx     r3d, byte [r0+r1*1]
-    add       r5d, r2d
-    add       r6d, r3d
-    lea        r0, [r0+r1*2]
-%endrep
-    movzx     r2d, byte [r0+r1*0]
-    add       r5d, r6d
-    lea       r2d, [r2+r5+16]
-    shr       r2d, 5
-%if cpuflag(ssse3)
-    pxor       m1, m1
-%endif
-    SPLATB_REG m0, r2, m1
-
-%if mmsize==8
-    mov       r3d, 8
-.loop:
-    mova [r4+r1*0+0], m0
-    mova [r4+r1*0+8], m0
-    mova [r4+r1*1+0], m0
-    mova [r4+r1*1+8], m0
-%else
-    mov       r3d, 4
-.loop:
-    mova [r4+r1*0], m0
-    mova [r4+r1*1], m0
-    lea   r4, [r4+r1*2]
-    mova [r4+r1*0], m0
-    mova [r4+r1*1], m0
-%endif
-    lea   r4, [r4+r1*2]
-    dec   r3d
-    jg .loop
-    REP_RET
-%endmacro
-
-INIT_MMX mmxext
-PRED16x16_DC
-INIT_XMM sse2
-PRED16x16_DC
-INIT_XMM ssse3
-PRED16x16_DC
-
-;-----------------------------------------------------------------------------
-; void ff_pred16x16_tm_vp8_8(uint8_t *src, ptrdiff_t stride)
-;-----------------------------------------------------------------------------
-
-%macro PRED16x16_TM 0
-cglobal pred16x16_tm_vp8_8, 2,5
-    sub        r0, r1
-    pxor      mm7, mm7
-    movq      mm0, [r0+0]
-    movq      mm2, [r0+8]
-    movq      mm1, mm0
-    movq      mm3, mm2
-    punpcklbw mm0, mm7
-    punpckhbw mm1, mm7
-    punpcklbw mm2, mm7
-    punpckhbw mm3, mm7
-    movzx     r3d, byte [r0-1]
-    mov       r4d, 16
-.loop:
-    movzx     r2d, byte [r0+r1-1]
-    sub       r2d, r3d
-    movd      mm4, r2d
-    SPLATW    mm4, mm4, 0
-    movq      mm5, mm4
-    movq      mm6, mm4
-    movq      mm7, mm4
-    paddw     mm4, mm0
-    paddw     mm5, mm1
-    paddw     mm6, mm2
-    paddw     mm7, mm3
-    packuswb  mm4, mm5
-    packuswb  mm6, mm7
-    movq [r0+r1+0], mm4
-    movq [r0+r1+8], mm6
-    add        r0, r1
-    dec       r4d
-    jg .loop
-    REP_RET
-%endmacro
-
-INIT_MMX mmx
-PRED16x16_TM
-INIT_MMX mmxext
-PRED16x16_TM
-
-INIT_XMM sse2
-cglobal pred16x16_tm_vp8_8, 2,6,6
-    sub          r0, r1
-    pxor       xmm2, xmm2
-    movdqa     xmm0, [r0]
-    movdqa     xmm1, xmm0
-    punpcklbw  xmm0, xmm2
-    punpckhbw  xmm1, xmm2
-    movzx       r4d, byte [r0-1]
-    mov         r5d, 8
-.loop:
-    movzx       r2d, byte [r0+r1*1-1]
-    movzx       r3d, byte [r0+r1*2-1]
-    sub         r2d, r4d
-    sub         r3d, r4d
-    movd       xmm2, r2d
-    movd       xmm4, r3d
-    pshuflw    xmm2, xmm2, 0
-    pshuflw    xmm4, xmm4, 0
-    punpcklqdq xmm2, xmm2
-    punpcklqdq xmm4, xmm4
-    movdqa     xmm3, xmm2
-    movdqa     xmm5, xmm4
-    paddw      xmm2, xmm0
-    paddw      xmm3, xmm1
-    paddw      xmm4, xmm0
-    paddw      xmm5, xmm1
-    packuswb   xmm2, xmm3
-    packuswb   xmm4, xmm5
-    movdqa [r0+r1*1], xmm2
-    movdqa [r0+r1*2], xmm4
-    lea          r0, [r0+r1*2]
-    dec         r5d
-    jg .loop
-    REP_RET
-
-%if HAVE_AVX2_EXTERNAL
-INIT_YMM avx2
-cglobal pred16x16_tm_vp8_8, 2, 4, 5, dst, stride, stride3, iteration
-    sub                       dstq, strideq
-    pmovzxbw                    m0, [dstq]
-    vpbroadcastb               xm1, [r0-1]
-    pmovzxbw                    m1, xm1
-    psubw                       m0, m1
-    mov                 iterationd, 4
-    lea                   stride3q, [strideq*3]
-.loop:
-    vpbroadcastb               xm1, [dstq+strideq*1-1]
-    vpbroadcastb               xm2, [dstq+strideq*2-1]
-    vpbroadcastb               xm3, [dstq+stride3q-1]
-    vpbroadcastb               xm4, [dstq+strideq*4-1]
-    pmovzxbw                    m1, xm1
-    pmovzxbw                    m2, xm2
-    pmovzxbw                    m3, xm3
-    pmovzxbw                    m4, xm4
-    paddw                       m1, m0
-    paddw                       m2, m0
-    paddw                       m3, m0
-    paddw                       m4, m0
-    vpackuswb                   m1, m1, m2
-    vpackuswb                   m3, m3, m4
-    vpermq                      m1, m1, q3120
-    vpermq                      m3, m3, q3120
-    movdqa        [dstq+strideq*1], xm1
-    vextracti128  [dstq+strideq*2], m1, 1
-    movdqa       [dstq+stride3q*1], xm3
-    vextracti128  [dstq+strideq*4], m3, 1
-    lea                       dstq, [dstq+strideq*4]
-    dec                 iterationd
-    jg .loop
-    REP_RET
-%endif
-
-;-----------------------------------------------------------------------------
-; void ff_pred16x16_plane_*_8(uint8_t *src, ptrdiff_t stride)
-;-----------------------------------------------------------------------------
-
-%macro H264_PRED16x16_PLANE 1
-cglobal pred16x16_plane_%1_8, 2,9,7
-    mov          r2, r1           ; +stride
-    neg          r1               ; -stride
-
-    movh         m0, [r0+r1  -1]
-%if mmsize == 8
-    pxor         m4, m4
-    movh         m1, [r0+r1  +3 ]
-    movh         m2, [r0+r1  +8 ]
-    movh         m3, [r0+r1  +12]
-    punpcklbw    m0, m4
-    punpcklbw    m1, m4
-    punpcklbw    m2, m4
-    punpcklbw    m3, m4
-    pmullw       m0, [pw_m8tom1  ]
-    pmullw       m1, [pw_m8tom1+8]
-    pmullw       m2, [pw_1to8    ]
-    pmullw       m3, [pw_1to8  +8]
-    paddw        m0, m2
-    paddw        m1, m3
-%else ; mmsize == 16
-%if cpuflag(ssse3)
-    movhps       m0, [r0+r1  +8]
-    pmaddubsw    m0, [plane_shuf] ; H coefficients
-%else ; sse2
-    pxor         m2, m2
-    movh         m1, [r0+r1  +8]
-    punpcklbw    m0, m2
-    punpcklbw    m1, m2
-    pmullw       m0, [pw_m8tom1]
-    pmullw       m1, [pw_1to8]
-    paddw        m0, m1
-%endif
-    movhlps      m1, m0
-%endif
-    paddw        m0, m1
-%if cpuflag(mmxext)
-    PSHUFLW      m1, m0, 0xE
-%elif cpuflag(mmx)
-    mova         m1, m0
-    psrlq        m1, 32
-%endif
-    paddw        m0, m1
-%if cpuflag(mmxext)
-    PSHUFLW      m1, m0, 0x1
-%elif cpuflag(mmx)
-    mova         m1, m0
-    psrlq        m1, 16
-%endif
-    paddw        m0, m1           ; sum of H coefficients
-
-    lea          r4, [r0+r2*8-1]
-    lea          r3, [r0+r2*4-1]
-    add          r4, r2
-
-%if ARCH_X86_64
-%define e_reg r8
-%else
-%define e_reg r0
-%endif
-
-    movzx     e_reg, byte [r3+r2*2   ]
-    movzx        r5, byte [r4+r1     ]
-    sub          r5, e_reg
-
-    movzx     e_reg, byte [r3+r2     ]
-    movzx        r6, byte [r4        ]
-    sub          r6, e_reg
-    lea          r5, [r5+r6*2]
-
-    movzx     e_reg, byte [r3+r1     ]
-    movzx        r6, byte [r4+r2*2   ]
-    sub          r6, e_reg
-    lea          r5, [r5+r6*4]
-
-    movzx     e_reg, byte [r3        ]
-%if ARCH_X86_64
-    movzx        r7, byte [r4+r2     ]
-    sub          r7, e_reg
-%else
-    movzx        r6, byte [r4+r2     ]
-    sub          r6, e_reg
-    lea          r5, [r5+r6*4]
-    sub          r5, r6
-%endif
-
-    lea       e_reg, [r3+r1*4]
-    lea          r3, [r4+r2*4]
-
-    movzx        r4, byte [e_reg+r2  ]
-    movzx        r6, byte [r3        ]
-    sub          r6, r4
-%if ARCH_X86_64
-    lea          r6, [r7+r6*2]
-    lea          r5, [r5+r6*2]
-    add          r5, r6
-%else
-    lea          r5, [r5+r6*4]
-    lea          r5, [r5+r6*2]
-%endif
-
-    movzx        r4, byte [e_reg     ]
-%if ARCH_X86_64
-    movzx        r7, byte [r3   +r2  ]
-    sub          r7, r4
-    sub          r5, r7
-%else
-    movzx        r6, byte [r3   +r2  ]
-    sub          r6, r4
-    lea          r5, [r5+r6*8]
-    sub          r5, r6
-%endif
-
-    movzx        r4, byte [e_reg+r1  ]
-    movzx        r6, byte [r3   +r2*2]
-    sub          r6, r4
-%if ARCH_X86_64
-    add          r6, r7
-%endif
-    lea          r5, [r5+r6*8]
-
-    movzx        r4, byte [e_reg+r2*2]
-    movzx        r6, byte [r3   +r1  ]
-    sub          r6, r4
-    lea          r5, [r5+r6*4]
-    add          r5, r6           ; sum of V coefficients
-
-%if ARCH_X86_64 == 0
-    mov          r0, r0m
-%endif
-
-%ifidn %1, h264
-    lea          r5, [r5*5+32]
-    sar          r5, 6
-%elifidn %1, rv40
-    lea          r5, [r5*5]
-    sar          r5, 6
-%elifidn %1, svq3
-    test         r5, r5
-    lea          r6, [r5+3]
-    cmovs        r5, r6
-    sar          r5, 2            ; V/4
-    lea          r5, [r5*5]       ; 5*(V/4)
-    test         r5, r5
-    lea          r6, [r5+15]
-    cmovs        r5, r6
-    sar          r5, 4            ; (5*(V/4))/16
-%endif
-
-    movzx        r4, byte [r0+r1  +15]
-    movzx        r3, byte [r3+r2*2   ]
-    lea          r3, [r3+r4+1]
-    shl          r3, 4
-
-    movd        r1d, m0
-    movsx       r1d, r1w
-%ifnidn %1, svq3
-%ifidn %1, h264
-    lea         r1d, [r1d*5+32]
-%else ; rv40
-    lea         r1d, [r1d*5]
-%endif
-    sar         r1d, 6
-%else ; svq3
-    test        r1d, r1d
-    lea         r4d, [r1d+3]
-    cmovs       r1d, r4d
-    sar         r1d, 2           ; H/4
-    lea         r1d, [r1d*5]     ; 5*(H/4)
-    test        r1d, r1d
-    lea         r4d, [r1d+15]
-    cmovs       r1d, r4d
-    sar         r1d, 4           ; (5*(H/4))/16
-%endif
-    movd         m0, r1d
-
-    add         r1d, r5d
-    add         r3d, r1d
-    shl         r1d, 3
-    sub         r3d, r1d          ; a
-
-    movd         m1, r5d
-    movd         m3, r3d
-    SPLATW       m0, m0, 0        ; H
-    SPLATW       m1, m1, 0        ; V
-    SPLATW       m3, m3, 0        ; a
-%ifidn %1, svq3
-    SWAP          0, 1
-%endif
-    mova         m2, m0
-%if mmsize == 8
-    mova         m5, m0
-%endif
-    pmullw       m0, [pw_0to7]    ; 0*H, 1*H, ..., 7*H  (words)
-%if mmsize == 16
-    psllw        m2, 3
-%else
-    psllw        m5, 3
-    psllw        m2, 2
-    mova         m6, m5
-    paddw        m6, m2
-%endif
-    paddw        m0, m3           ; a + {0,1,2,3,4,5,6,7}*H
-    paddw        m2, m0           ; a + {8,9,10,11,12,13,14,15}*H
-%if mmsize == 8
-    paddw        m5, m0           ; a + {8,9,10,11}*H
-    paddw        m6, m0           ; a + {12,13,14,15}*H
-%endif
-
-    mov          r4, 8
-.loop:
-    mova         m3, m0           ; b[0..7]
-    mova         m4, m2           ; b[8..15]
-    psraw        m3, 5
-    psraw        m4, 5
-    packuswb     m3, m4
-    mova       [r0], m3
-%if mmsize == 8
-    mova         m3, m5           ; b[8..11]
-    mova         m4, m6           ; b[12..15]
-    psraw        m3, 5
-    psraw        m4, 5
-    packuswb     m3, m4
-    mova     [r0+8], m3
-%endif
-    paddw        m0, m1
-    paddw        m2, m1
-%if mmsize == 8
-    paddw        m5, m1
-    paddw        m6, m1
-%endif
-
-    mova         m3, m0           ; b[0..7]
-    mova         m4, m2           ; b[8..15]
-    psraw        m3, 5
-    psraw        m4, 5
-    packuswb     m3, m4
-    mova    [r0+r2], m3
-%if mmsize == 8
-    mova         m3, m5           ; b[8..11]
-    mova         m4, m6           ; b[12..15]
-    psraw        m3, 5
-    psraw        m4, 5
-    packuswb     m3, m4
-    mova  [r0+r2+8], m3
-%endif
-    paddw        m0, m1
-    paddw        m2, m1
-%if mmsize == 8
-    paddw        m5, m1
-    paddw        m6, m1
-%endif
-
-    lea          r0, [r0+r2*2]
-    dec          r4
-    jg .loop
-    REP_RET
-%endmacro
-
-INIT_MMX mmx
-H264_PRED16x16_PLANE h264
-H264_PRED16x16_PLANE rv40
-H264_PRED16x16_PLANE svq3
-INIT_MMX mmxext
-H264_PRED16x16_PLANE h264
-H264_PRED16x16_PLANE rv40
-H264_PRED16x16_PLANE svq3
-INIT_XMM sse2
-H264_PRED16x16_PLANE h264
-H264_PRED16x16_PLANE rv40
-H264_PRED16x16_PLANE svq3
-INIT_XMM ssse3
-H264_PRED16x16_PLANE h264
-H264_PRED16x16_PLANE rv40
-H264_PRED16x16_PLANE svq3
-
-;-----------------------------------------------------------------------------
-; void ff_pred8x8_plane_8(uint8_t *src, ptrdiff_t stride)
-;-----------------------------------------------------------------------------
-
-%macro H264_PRED8x8_PLANE 0
-cglobal pred8x8_plane_8, 2,9,7
-    mov          r2, r1           ; +stride
-    neg          r1               ; -stride
-
-    movd         m0, [r0+r1  -1]
-%if mmsize == 8
-    pxor         m2, m2
-    movh         m1, [r0+r1  +4 ]
-    punpcklbw    m0, m2
-    punpcklbw    m1, m2
-    pmullw       m0, [pw_m4to4]
-    pmullw       m1, [pw_m4to4+8]
-%else ; mmsize == 16
-%if cpuflag(ssse3)
-    movhps       m0, [r0+r1  +4]   ; this reads 4 bytes more than necessary
-    pmaddubsw    m0, [plane8_shuf] ; H coefficients
-%else ; sse2
-    pxor         m2, m2
-    movd         m1, [r0+r1  +4]
-    punpckldq    m0, m1
-    punpcklbw    m0, m2
-    pmullw       m0, [pw_m4to4]
-%endif
-    movhlps      m1, m0
-%endif
-    paddw        m0, m1
-
-%if notcpuflag(ssse3)
-%if cpuflag(mmxext)
-    PSHUFLW      m1, m0, 0xE
-%elif cpuflag(mmx)
-    mova         m1, m0
-    psrlq        m1, 32
-%endif
-    paddw        m0, m1
-%endif ; !ssse3
-
-%if cpuflag(mmxext)
-    PSHUFLW      m1, m0, 0x1
-%elif cpuflag(mmx)
-    mova         m1, m0
-    psrlq        m1, 16
-%endif
-    paddw        m0, m1           ; sum of H coefficients
-
-    lea          r4, [r0+r2*4-1]
-    lea          r3, [r0     -1]
-    add          r4, r2
-
-%if ARCH_X86_64
-%define e_reg r8
-%else
-%define e_reg r0
-%endif
-
-    movzx     e_reg, byte [r3+r2*2   ]
-    movzx        r5, byte [r4+r1     ]
-    sub          r5, e_reg
-
-    movzx     e_reg, byte [r3        ]
-%if ARCH_X86_64
-    movzx        r7, byte [r4+r2     ]
-    sub          r7, e_reg
-    sub          r5, r7
-%else
-    movzx        r6, byte [r4+r2     ]
-    sub          r6, e_reg
-    lea          r5, [r5+r6*4]
-    sub          r5, r6
-%endif
-
-    movzx     e_reg, byte [r3+r1     ]
-    movzx        r6, byte [r4+r2*2   ]
-    sub          r6, e_reg
-%if ARCH_X86_64
-    add          r6, r7
-%endif
-    lea          r5, [r5+r6*4]
-
-    movzx     e_reg, byte [r3+r2     ]
-    movzx        r6, byte [r4        ]
-    sub          r6, e_reg
-    lea          r6, [r5+r6*2]
-
-    lea          r5, [r6*9+16]
-    lea          r5, [r5+r6*8]
-    sar          r5, 5
-
-%if ARCH_X86_64 == 0
-    mov          r0, r0m
-%endif
-
-    movzx        r3, byte [r4+r2*2  ]
-    movzx        r4, byte [r0+r1  +7]
-    lea          r3, [r3+r4+1]
-    shl          r3, 4
-    movd        r1d, m0
-    movsx       r1d, r1w
-    imul        r1d, 17
-    add         r1d, 16
-    sar         r1d, 5
-    movd         m0, r1d
-    add         r1d, r5d
-    sub         r3d, r1d
-    add         r1d, r1d
-    sub         r3d, r1d          ; a
-
-    movd         m1, r5d
-    movd         m3, r3d
-    SPLATW       m0, m0, 0        ; H
-    SPLATW       m1, m1, 0        ; V
-    SPLATW       m3, m3, 0        ; a
-%if mmsize == 8
-    mova         m2, m0
-%endif
-    pmullw       m0, [pw_0to7]    ; 0*H, 1*H, ..., 7*H  (words)
-    paddw        m0, m3           ; a + {0,1,2,3,4,5,6,7}*H
-%if mmsize == 8
-    psllw        m2, 2
-    paddw        m2, m0           ; a + {4,5,6,7}*H
-%endif
-
-    mov          r4, 4
-ALIGN 16
-.loop:
-%if mmsize == 16
-    mova         m3, m0           ; b[0..7]
-    paddw        m0, m1
-    psraw        m3, 5
-    mova         m4, m0           ; V+b[0..7]
-    paddw        m0, m1
-    psraw        m4, 5
-    packuswb     m3, m4
-    movh       [r0], m3
-    movhps  [r0+r2], m3
-%else ; mmsize == 8
-    mova         m3, m0           ; b[0..3]
-    mova         m4, m2           ; b[4..7]
-    paddw        m0, m1
-    paddw        m2, m1
-    psraw        m3, 5
-    psraw        m4, 5
-    mova         m5, m0           ; V+b[0..3]
-    mova         m6, m2           ; V+b[4..7]
-    paddw        m0, m1
-    paddw        m2, m1
-    psraw        m5, 5
-    psraw        m6, 5
-    packuswb     m3, m4
-    packuswb     m5, m6
-    mova       [r0], m3
-    mova    [r0+r2], m5
-%endif
-
-    lea          r0, [r0+r2*2]
-    dec          r4
-    jg .loop
-    REP_RET
-%endmacro
-
-INIT_MMX mmx
-H264_PRED8x8_PLANE
-INIT_MMX mmxext
-H264_PRED8x8_PLANE
-INIT_XMM sse2
-H264_PRED8x8_PLANE
-INIT_XMM ssse3
-H264_PRED8x8_PLANE
-
-;-----------------------------------------------------------------------------
-; void ff_pred8x8_vertical_8(uint8_t *src, ptrdiff_t stride)
-;-----------------------------------------------------------------------------
-
-INIT_MMX mmx
-cglobal pred8x8_vertical_8, 2,2
-    sub    r0, r1
-    movq  mm0, [r0]
-%rep 3
-    movq [r0+r1*1], mm0
-    movq [r0+r1*2], mm0
-    lea    r0, [r0+r1*2]
-%endrep
-    movq [r0+r1*1], mm0
-    movq [r0+r1*2], mm0
-    RET
-
-;-----------------------------------------------------------------------------
-; void ff_pred8x8_horizontal_8(uint8_t *src, ptrdiff_t stride)
-;-----------------------------------------------------------------------------
-
-%macro PRED8x8_H 0
-cglobal pred8x8_horizontal_8, 2,3
-    mov       r2, 4
-%if cpuflag(ssse3)
-    mova      m2, [pb_3]
-%endif
-.loop:
-    SPLATB_LOAD m0, r0+r1*0-1, m2
-    SPLATB_LOAD m1, r0+r1*1-1, m2
-    mova [r0+r1*0], m0
-    mova [r0+r1*1], m1
-    lea       r0, [r0+r1*2]
-    dec       r2
-    jg .loop
-    REP_RET
-%endmacro
-
-INIT_MMX mmx
-PRED8x8_H
-INIT_MMX mmxext
-PRED8x8_H
-INIT_MMX ssse3
-PRED8x8_H
-
-;-----------------------------------------------------------------------------
-; void ff_pred8x8_top_dc_8_mmxext(uint8_t *src, ptrdiff_t stride)
-;-----------------------------------------------------------------------------
-INIT_MMX mmxext
-cglobal pred8x8_top_dc_8, 2,5
-    sub         r0, r1
-    movq       mm0, [r0]
-    pxor       mm1, mm1
-    pxor       mm2, mm2
-    lea         r2, [r0+r1*2]
-    punpckhbw  mm1, mm0
-    punpcklbw  mm0, mm2
-    psadbw     mm1, mm2        ; s1
-    lea         r3, [r2+r1*2]
-    psadbw     mm0, mm2        ; s0
-    psrlw      mm1, 1
-    psrlw      mm0, 1
-    pavgw      mm1, mm2
-    lea         r4, [r3+r1*2]
-    pavgw      mm0, mm2
-    pshufw     mm1, mm1, 0
-    pshufw     mm0, mm0, 0     ; dc0 (w)
-    packuswb   mm0, mm1        ; dc0,dc1 (b)
-    movq [r0+r1*1], mm0
-    movq [r0+r1*2], mm0
-    lea         r0, [r3+r1*2]
-    movq [r2+r1*1], mm0
-    movq [r2+r1*2], mm0
-    movq [r3+r1*1], mm0
-    movq [r3+r1*2], mm0
-    movq [r0+r1*1], mm0
-    movq [r0+r1*2], mm0
-    RET
-
-;-----------------------------------------------------------------------------
-; void ff_pred8x8_dc_8_mmxext(uint8_t *src, ptrdiff_t stride)
-;-----------------------------------------------------------------------------
-
-INIT_MMX mmxext
-cglobal pred8x8_dc_8, 2,5
-    sub       r0, r1
-    pxor      m7, m7
-    movd      m0, [r0+0]
-    movd      m1, [r0+4]
-    psadbw    m0, m7            ; s0
-    mov       r4, r0
-    psadbw    m1, m7            ; s1
-
-    movzx    r2d, byte [r0+r1*1-1]
-    movzx    r3d, byte [r0+r1*2-1]
-    lea       r0, [r0+r1*2]
-    add      r2d, r3d
-    movzx    r3d, byte [r0+r1*1-1]
-    add      r2d, r3d
-    movzx    r3d, byte [r0+r1*2-1]
-    add      r2d, r3d
-    lea       r0, [r0+r1*2]
-    movd      m2, r2d            ; s2
-    movzx    r2d, byte [r0+r1*1-1]
-    movzx    r3d, byte [r0+r1*2-1]
-    lea       r0, [r0+r1*2]
-    add      r2d, r3d
-    movzx    r3d, byte [r0+r1*1-1]
-    add      r2d, r3d
-    movzx    r3d, byte [r0+r1*2-1]
-    add      r2d, r3d
-    movd      m3, r2d            ; s3
-
-    punpcklwd m0, m1
-    mov       r0, r4
-    punpcklwd m2, m3
-    punpckldq m0, m2            ; s0, s1, s2, s3
-    pshufw    m3, m0, 11110110b ; s2, s1, s3, s3
-    lea       r2, [r0+r1*2]
-    pshufw    m0, m0, 01110100b ; s0, s1, s3, s1
-    paddw     m0, m3
-    lea       r3, [r2+r1*2]
-    psrlw     m0, 2
-    pavgw     m0, m7            ; s0+s2, s1, s3, s1+s3
-    lea       r4, [r3+r1*2]
-    packuswb  m0, m0
-    punpcklbw m0, m0
-    movq      m1, m0
-    punpcklbw m0, m0
-    punpckhbw m1, m1
-    movq [r0+r1*1], m0
-    movq [r0+r1*2], m0
-    movq [r2+r1*1], m0
-    movq [r2+r1*2], m0
-    movq [r3+r1*1], m1
-    movq [r3+r1*2], m1
-    movq [r4+r1*1], m1
-    movq [r4+r1*2], m1
-    RET
-
-;-----------------------------------------------------------------------------
-; void ff_pred8x8_dc_rv40_8(uint8_t *src, ptrdiff_t stride)
-;-----------------------------------------------------------------------------
-
-INIT_MMX mmxext
-cglobal pred8x8_dc_rv40_8, 2,7
-    mov       r4, r0
-    sub       r0, r1
-    pxor      mm0, mm0
-    psadbw    mm0, [r0]
-    dec        r0
-    movzx     r5d, byte [r0+r1*1]
-    movd      r6d, mm0
-    lea        r0, [r0+r1*2]
-%rep 3
-    movzx     r2d, byte [r0+r1*0]
-    movzx     r3d, byte [r0+r1*1]
-    add       r5d, r2d
-    add       r6d, r3d
-    lea        r0, [r0+r1*2]
-%endrep
-    movzx     r2d, byte [r0+r1*0]
-    add       r5d, r6d
-    lea       r2d, [r2+r5+8]
-    shr       r2d, 4
-    movd      mm0, r2d
-    punpcklbw mm0, mm0
-    pshufw    mm0, mm0, 0
-    mov       r3d, 4
-.loop:
-    movq [r4+r1*0], mm0
-    movq [r4+r1*1], mm0
-    lea   r4, [r4+r1*2]
-    dec   r3d
-    jg .loop
-    REP_RET
-
-;-----------------------------------------------------------------------------
-; void ff_pred8x8_tm_vp8_8(uint8_t *src, ptrdiff_t stride)
-;-----------------------------------------------------------------------------
-
-%macro PRED8x8_TM 0
-cglobal pred8x8_tm_vp8_8, 2,6
-    sub        r0, r1
-    pxor      mm7, mm7
-    movq      mm0, [r0]
-    movq      mm1, mm0
-    punpcklbw mm0, mm7
-    punpckhbw mm1, mm7
-    movzx     r4d, byte [r0-1]
-    mov       r5d, 4
-.loop:
-    movzx     r2d, byte [r0+r1*1-1]
-    movzx     r3d, byte [r0+r1*2-1]
-    sub       r2d, r4d
-    sub       r3d, r4d
-    movd      mm2, r2d
-    movd      mm4, r3d
-    SPLATW    mm2, mm2, 0
-    SPLATW    mm4, mm4, 0
-    movq      mm3, mm2
-    movq      mm5, mm4
-    paddw     mm2, mm0
-    paddw     mm3, mm1
-    paddw     mm4, mm0
-    paddw     mm5, mm1
-    packuswb  mm2, mm3
-    packuswb  mm4, mm5
-    movq [r0+r1*1], mm2
-    movq [r0+r1*2], mm4
-    lea        r0, [r0+r1*2]
-    dec       r5d
-    jg .loop
-    REP_RET
-%endmacro
-
-INIT_MMX mmx
-PRED8x8_TM
-INIT_MMX mmxext
-PRED8x8_TM
-
-INIT_XMM sse2
-cglobal pred8x8_tm_vp8_8, 2,6,4
-    sub          r0, r1
-    pxor       xmm1, xmm1
-    movq       xmm0, [r0]
-    punpcklbw  xmm0, xmm1
-    movzx       r4d, byte [r0-1]
-    mov         r5d, 4
-.loop:
-    movzx       r2d, byte [r0+r1*1-1]
-    movzx       r3d, byte [r0+r1*2-1]
-    sub         r2d, r4d
-    sub         r3d, r4d
-    movd       xmm2, r2d
-    movd       xmm3, r3d
-    pshuflw    xmm2, xmm2, 0
-    pshuflw    xmm3, xmm3, 0
-    punpcklqdq xmm2, xmm2
-    punpcklqdq xmm3, xmm3
-    paddw      xmm2, xmm0
-    paddw      xmm3, xmm0
-    packuswb   xmm2, xmm3
-    movq   [r0+r1*1], xmm2
-    movhps [r0+r1*2], xmm2
-    lea          r0, [r0+r1*2]
-    dec         r5d
-    jg .loop
-    REP_RET
-
-INIT_XMM ssse3
-cglobal pred8x8_tm_vp8_8, 2,3,6
-    sub          r0, r1
-    movdqa     xmm4, [tm_shuf]
-    pxor       xmm1, xmm1
-    movq       xmm0, [r0]
-    punpcklbw  xmm0, xmm1
-    movd       xmm5, [r0-4]
-    pshufb     xmm5, xmm4
-    mov         r2d, 4
-.loop:
-    movd       xmm2, [r0+r1*1-4]
-    movd       xmm3, [r0+r1*2-4]
-    pshufb     xmm2, xmm4
-    pshufb     xmm3, xmm4
-    psubw      xmm2, xmm5
-    psubw      xmm3, xmm5
-    paddw      xmm2, xmm0
-    paddw      xmm3, xmm0
-    packuswb   xmm2, xmm3
-    movq   [r0+r1*1], xmm2
-    movhps [r0+r1*2], xmm2
-    lea          r0, [r0+r1*2]
-    dec         r2d
-    jg .loop
-    REP_RET
-
-; dest, left, right, src, tmp
-; output: %1 = (t[n-1] + t[n]*2 + t[n+1] + 2) >> 2
-%macro PRED4x4_LOWPASS 5
-    mova    %5, %2
-    pavgb   %2, %3
-    pxor    %3, %5
-    mova    %1, %4
-    pand    %3, [pb_1]
-    psubusb %2, %3
-    pavgb   %1, %2
-%endmacro
-
-;-----------------------------------------------------------------------------
-; void ff_pred8x8l_top_dc_8(uint8_t *src, int has_topleft, int has_topright,
-;                           ptrdiff_t stride)
-;-----------------------------------------------------------------------------
-%macro PRED8x8L_TOP_DC 0
-cglobal pred8x8l_top_dc_8, 4,4
-    sub          r0, r3
-    pxor        mm7, mm7
-    movq        mm0, [r0-8]
-    movq        mm3, [r0]
-    movq        mm1, [r0+8]
-    movq        mm2, mm3
-    movq        mm4, mm3
-    PALIGNR     mm2, mm0, 7, mm0
-    PALIGNR     mm1, mm4, 1, mm4
-    test        r1d, r1d ; top_left
-    jz .fix_lt_2
-    test        r2d, r2d ; top_right
-    jz .fix_tr_1
-    jmp .body
-.fix_lt_2:
-    movq        mm5, mm3
-    pxor        mm5, mm2
-    psllq       mm5, 56
-    psrlq       mm5, 56
-    pxor        mm2, mm5
-    test        r2d, r2d ; top_right
-    jnz .body
-.fix_tr_1:
-    movq        mm5, mm3
-    pxor        mm5, mm1
-    psrlq       mm5, 56
-    psllq       mm5, 56
-    pxor        mm1, mm5
-.body:
-    PRED4x4_LOWPASS mm0, mm2, mm1, mm3, mm5
-    psadbw   mm7, mm0
-    paddw    mm7, [pw_4]
-    psrlw    mm7, 3
-    pshufw   mm7, mm7, 0
-    packuswb mm7, mm7
-%rep 3
-    movq [r0+r3*1], mm7
-    movq [r0+r3*2], mm7
-    lea    r0, [r0+r3*2]
-%endrep
-    movq [r0+r3*1], mm7
-    movq [r0+r3*2], mm7
-    RET
-%endmacro
-
-INIT_MMX mmxext
-PRED8x8L_TOP_DC
-INIT_MMX ssse3
-PRED8x8L_TOP_DC
-
-;-----------------------------------------------------------------------------
-; void ff_pred8x8l_dc_8(uint8_t *src, int has_topleft, int has_topright,
-;                       ptrdiff_t stride)
-;-----------------------------------------------------------------------------
-
-%macro PRED8x8L_DC 0
-cglobal pred8x8l_dc_8, 4,5
-    sub          r0, r3
-    lea          r4, [r0+r3*2]
-    movq        mm0, [r0+r3*1-8]
-    punpckhbw   mm0, [r0+r3*0-8]
-    movq        mm1, [r4+r3*1-8]
-    punpckhbw   mm1, [r0+r3*2-8]
-    mov          r4, r0
-    punpckhwd   mm1, mm0
-    lea          r0, [r0+r3*4]
-    movq        mm2, [r0+r3*1-8]
-    punpckhbw   mm2, [r0+r3*0-8]
-    lea          r0, [r0+r3*2]
-    movq        mm3, [r0+r3*1-8]
-    punpckhbw   mm3, [r0+r3*0-8]
-    punpckhwd   mm3, mm2
-    punpckhdq   mm3, mm1
-    lea          r0, [r0+r3*2]
-    movq        mm0, [r0+r3*0-8]
-    movq        mm1, [r4]
-    mov          r0, r4
-    movq        mm4, mm3
-    movq        mm2, mm3
-    PALIGNR     mm4, mm0, 7, mm0
-    PALIGNR     mm1, mm2, 1, mm2
-    test        r1d, r1d
-    jnz .do_left
-.fix_lt_1:
-    movq        mm5, mm3
-    pxor        mm5, mm4
-    psrlq       mm5, 56
-    psllq       mm5, 48
-    pxor        mm1, mm5
-    jmp .do_left
-.fix_lt_2:
-    movq        mm5, mm3
-    pxor        mm5, mm2
-    psllq       mm5, 56
-    psrlq       mm5, 56
-    pxor        mm2, mm5
-    test        r2d, r2d
-    jnz .body
-.fix_tr_1:
-    movq        mm5, mm3
-    pxor        mm5, mm1
-    psrlq       mm5, 56
-    psllq       mm5, 56
-    pxor        mm1, mm5
-    jmp .body
-.do_left:
-    movq        mm0, mm4
-    PRED4x4_LOWPASS mm2, mm1, mm4, mm3, mm5
-    movq        mm4, mm0
-    movq        mm7, mm2
-    PRED4x4_LOWPASS mm1, mm3, mm0, mm4, mm5
-    psllq       mm1, 56
-    PALIGNR     mm7, mm1, 7, mm3
-    movq        mm0, [r0-8]
-    movq        mm3, [r0]
-    movq        mm1, [r0+8]
-    movq        mm2, mm3
-    movq        mm4, mm3
-    PALIGNR     mm2, mm0, 7, mm0
-    PALIGNR     mm1, mm4, 1, mm4
-    test        r1d, r1d
-    jz .fix_lt_2
-    test        r2d, r2d
-    jz .fix_tr_1
-.body:
-    lea          r1, [r0+r3*2]
-    PRED4x4_LOWPASS mm6, mm2, mm1, mm3, mm5
-    pxor        mm0, mm0
-    pxor        mm1, mm1
-    lea          r2, [r1+r3*2]
-    psadbw      mm0, mm7
-    psadbw      mm1, mm6
-    paddw       mm0, [pw_8]
-    paddw       mm0, mm1
-    lea          r4, [r2+r3*2]
-    psrlw       mm0, 4
-    pshufw      mm0, mm0, 0
-    packuswb    mm0, mm0
-    movq [r0+r3*1], mm0
-    movq [r0+r3*2], mm0
-    movq [r1+r3*1], mm0
-    movq [r1+r3*2], mm0
-    movq [r2+r3*1], mm0
-    movq [r2+r3*2], mm0
-    movq [r4+r3*1], mm0
-    movq [r4+r3*2], mm0
-    RET
-%endmacro
-
-INIT_MMX mmxext
-PRED8x8L_DC
-INIT_MMX ssse3
-PRED8x8L_DC
-
-;-----------------------------------------------------------------------------
-; void ff_pred8x8l_horizontal_8(uint8_t *src, int has_topleft,
-;                               int has_topright, ptrdiff_t stride)
-;-----------------------------------------------------------------------------
-
-%macro PRED8x8L_HORIZONTAL 0
-cglobal pred8x8l_horizontal_8, 4,4
-    sub          r0, r3
-    lea          r2, [r0+r3*2]
-    movq        mm0, [r0+r3*1-8]
-    test        r1d, r1d
-    lea          r1, [r0+r3]
-    cmovnz       r1, r0
-    punpckhbw   mm0, [r1+r3*0-8]
-    movq        mm1, [r2+r3*1-8]
-    punpckhbw   mm1, [r0+r3*2-8]
-    mov          r2, r0
-    punpckhwd   mm1, mm0
-    lea          r0, [r0+r3*4]
-    movq        mm2, [r0+r3*1-8]
-    punpckhbw   mm2, [r0+r3*0-8]
-    lea          r0, [r0+r3*2]
-    movq        mm3, [r0+r3*1-8]
-    punpckhbw   mm3, [r0+r3*0-8]
-    punpckhwd   mm3, mm2
-    punpckhdq   mm3, mm1
-    lea          r0, [r0+r3*2]
-    movq        mm0, [r0+r3*0-8]
-    movq        mm1, [r1+r3*0-8]
-    mov          r0, r2
-    movq        mm4, mm3
-    movq        mm2, mm3
-    PALIGNR     mm4, mm0, 7, mm0
-    PALIGNR     mm1, mm2, 1, mm2
-    movq        mm0, mm4
-    PRED4x4_LOWPASS mm2, mm1, mm4, mm3, mm5
-    movq        mm4, mm0
-    movq        mm7, mm2
-    PRED4x4_LOWPASS mm1, mm3, mm0, mm4, mm5
-    psllq       mm1, 56
-    PALIGNR     mm7, mm1, 7, mm3
-    movq        mm3, mm7
-    lea         r1, [r0+r3*2]
-    movq       mm7, mm3
-    punpckhbw  mm3, mm3
-    punpcklbw  mm7, mm7
-    pshufw     mm0, mm3, 0xff
-    pshufw     mm1, mm3, 0xaa
-    lea         r2, [r1+r3*2]
-    pshufw     mm2, mm3, 0x55
-    pshufw     mm3, mm3, 0x00
-    pshufw     mm4, mm7, 0xff
-    pshufw     mm5, mm7, 0xaa
-    pshufw     mm6, mm7, 0x55
-    pshufw     mm7, mm7, 0x00
-    movq [r0+r3*1], mm0
-    movq [r0+r3*2], mm1
-    movq [r1+r3*1], mm2
-    movq [r1+r3*2], mm3
-    movq [r2+r3*1], mm4
-    movq [r2+r3*2], mm5
-    lea         r0, [r2+r3*2]
-    movq [r0+r3*1], mm6
-    movq [r0+r3*2], mm7
-    RET
-%endmacro
-
-INIT_MMX mmxext
-PRED8x8L_HORIZONTAL
-INIT_MMX ssse3
-PRED8x8L_HORIZONTAL
-
-;-----------------------------------------------------------------------------
-; void ff_pred8x8l_vertical_8(uint8_t *src, int has_topleft, int has_topright,
-;                             ptrdiff_t stride)
-;-----------------------------------------------------------------------------
-
-%macro PRED8x8L_VERTICAL 0
-cglobal pred8x8l_vertical_8, 4,4
-    sub          r0, r3
-    movq        mm0, [r0-8]
-    movq        mm3, [r0]
-    movq        mm1, [r0+8]
-    movq        mm2, mm3
-    movq        mm4, mm3
-    PALIGNR     mm2, mm0, 7, mm0
-    PALIGNR     mm1, mm4, 1, mm4
-    test        r1d, r1d ; top_left
-    jz .fix_lt_2
-    test        r2d, r2d ; top_right
-    jz .fix_tr_1
-    jmp .body
-.fix_lt_2:
-    movq        mm5, mm3
-    pxor        mm5, mm2
-    psllq       mm5, 56
-    psrlq       mm5, 56
-    pxor        mm2, mm5
-    test        r2d, r2d ; top_right
-    jnz .body
-.fix_tr_1:
-    movq        mm5, mm3
-    pxor        mm5, mm1
-    psrlq       mm5, 56
-    psllq       mm5, 56
-    pxor        mm1, mm5
-.body:
-    PRED4x4_LOWPASS mm0, mm2, mm1, mm3, mm5
-%rep 3
-    movq [r0+r3*1], mm0
-    movq [r0+r3*2], mm0
-    lea    r0, [r0+r3*2]
-%endrep
-    movq [r0+r3*1], mm0
-    movq [r0+r3*2], mm0
-    RET
-%endmacro
-
-INIT_MMX mmxext
-PRED8x8L_VERTICAL
-INIT_MMX ssse3
-PRED8x8L_VERTICAL
-
-;-----------------------------------------------------------------------------
-; void ff_pred8x8l_down_left_8(uint8_t *src, int has_topleft,
-;                              int has_topright, ptrdiff_t stride)
-;-----------------------------------------------------------------------------
-
-INIT_MMX mmxext
-cglobal pred8x8l_down_left_8, 4,5
-    sub          r0, r3
-    movq        mm0, [r0-8]
-    movq        mm3, [r0]
-    movq        mm1, [r0+8]
-    movq        mm2, mm3
-    movq        mm4, mm3
-    PALIGNR     mm2, mm0, 7, mm0
-    PALIGNR     mm1, mm4, 1, mm4
-    test        r1d, r1d
-    jz .fix_lt_2
-    test        r2d, r2d
-    jz .fix_tr_1
-    jmp .do_top
-.fix_lt_2:
-    movq        mm5, mm3
-    pxor        mm5, mm2
-    psllq       mm5, 56
-    psrlq       mm5, 56
-    pxor        mm2, mm5
-    test        r2d, r2d
-    jnz .do_top
-.fix_tr_1:
-    movq        mm5, mm3
-    pxor        mm5, mm1
-    psrlq       mm5, 56
-    psllq       mm5, 56
-    pxor        mm1, mm5
-    jmp .do_top
-.fix_tr_2:
-    punpckhbw   mm3, mm3
-    pshufw      mm1, mm3, 0xFF
-    jmp .do_topright
-.do_top:
-    PRED4x4_LOWPASS mm4, mm2, mm1, mm3, mm5
-    movq        mm7, mm4
-    test        r2d, r2d
-    jz .fix_tr_2
-    movq        mm0, [r0+8]
-    movq        mm5, mm0
-    movq        mm2, mm0
-    movq        mm4, mm0
-    psrlq       mm5, 56
-    PALIGNR     mm2, mm3, 7, mm3
-    PALIGNR     mm5, mm4, 1, mm4
-    PRED4x4_LOWPASS mm1, mm2, mm5, mm0, mm4
-.do_topright:
-    lea          r1, [r0+r3*2]
-    movq        mm6, mm1
-    psrlq       mm1, 56
-    movq        mm4, mm1
-    lea          r2, [r1+r3*2]
-    movq        mm2, mm6
-    PALIGNR     mm2, mm7, 1, mm0
-    movq        mm3, mm6
-    PALIGNR     mm3, mm7, 7, mm0
-    PALIGNR     mm4, mm6, 1, mm0
-    movq        mm5, mm7
-    movq        mm1, mm7
-    movq        mm7, mm6
-    lea          r4, [r2+r3*2]
-    psllq       mm1, 8
-    PRED4x4_LOWPASS mm0, mm1, mm2, mm5, mm6
-    PRED4x4_LOWPASS mm1, mm3, mm4, mm7, mm6
-    movq  [r4+r3*2], mm1
-    movq        mm2, mm0
-    psllq       mm1, 8
-    psrlq       mm2, 56
-    psllq       mm0, 8
-    por         mm1, mm2
-    movq  [r4+r3*1], mm1
-    movq        mm2, mm0
-    psllq       mm1, 8
-    psrlq       mm2, 56
-    psllq       mm0, 8
-    por         mm1, mm2
-    movq  [r2+r3*2], mm1
-    movq        mm2, mm0
-    psllq       mm1, 8
-    psrlq       mm2, 56
-    psllq       mm0, 8
-    por         mm1, mm2
-    movq  [r2+r3*1], mm1
-    movq        mm2, mm0
-    psllq       mm1, 8
-    psrlq       mm2, 56
-    psllq       mm0, 8
-    por         mm1, mm2
-    movq  [r1+r3*2], mm1
-    movq        mm2, mm0
-    psllq       mm1, 8
-    psrlq       mm2, 56
-    psllq       mm0, 8
-    por         mm1, mm2
-    movq  [r1+r3*1], mm1
-    movq        mm2, mm0
-    psllq       mm1, 8
-    psrlq       mm2, 56
-    psllq       mm0, 8
-    por         mm1, mm2
-    movq  [r0+r3*2], mm1
-    psllq       mm1, 8
-    psrlq       mm0, 56
-    por         mm1, mm0
-    movq  [r0+r3*1], mm1
-    RET
-
-%macro PRED8x8L_DOWN_LEFT 0
-cglobal pred8x8l_down_left_8, 4,4
-    sub          r0, r3
-    movq        mm0, [r0-8]
-    movq        mm3, [r0]
-    movq        mm1, [r0+8]
-    movq        mm2, mm3
-    movq        mm4, mm3
-    PALIGNR     mm2, mm0, 7, mm0
-    PALIGNR     mm1, mm4, 1, mm4
-    test        r1d, r1d ; top_left
-    jz .fix_lt_2
-    test        r2d, r2d ; top_right
-    jz .fix_tr_1
-    jmp .do_top
-.fix_lt_2:
-    movq        mm5, mm3
-    pxor        mm5, mm2
-    psllq       mm5, 56
-    psrlq       mm5, 56
-    pxor        mm2, mm5
-    test        r2d, r2d ; top_right
-    jnz .do_top
-.fix_tr_1:
-    movq        mm5, mm3
-    pxor        mm5, mm1
-    psrlq       mm5, 56
-    psllq       mm5, 56
-    pxor        mm1, mm5
-    jmp .do_top
-.fix_tr_2:
-    punpckhbw   mm3, mm3
-    pshufw      mm1, mm3, 0xFF
-    jmp .do_topright
-.do_top:
-    PRED4x4_LOWPASS mm4, mm2, mm1, mm3, mm5
-    movq2dq    xmm3, mm4
-    test        r2d, r2d ; top_right
-    jz .fix_tr_2
-    movq        mm0, [r0+8]
-    movq        mm5, mm0
-    movq        mm2, mm0
-    movq        mm4, mm0
-    psrlq       mm5, 56
-    PALIGNR     mm2, mm3, 7, mm3
-    PALIGNR     mm5, mm4, 1, mm4
-    PRED4x4_LOWPASS mm1, mm2, mm5, mm0, mm4
-.do_topright:
-    movq2dq    xmm4, mm1
-    psrlq       mm1, 56
-    movq2dq    xmm5, mm1
-    lea         r1, [r0+r3*2]
-    pslldq    xmm4, 8
-    por       xmm3, xmm4
-    movdqa    xmm2, xmm3
-    psrldq    xmm2, 1
-    pslldq    xmm5, 15
-    por       xmm2, xmm5
-    lea         r2, [r1+r3*2]
-    movdqa    xmm1, xmm3
-    pslldq    xmm1, 1
-INIT_XMM cpuname
-    PRED4x4_LOWPASS xmm0, xmm1, xmm2, xmm3, xmm4
-    psrldq    xmm0, 1
-    movq [r0+r3*1], xmm0
-    psrldq    xmm0, 1
-    movq [r0+r3*2], xmm0
-    psrldq    xmm0, 1
-    lea         r0, [r2+r3*2]
-    movq [r1+r3*1], xmm0
-    psrldq    xmm0, 1
-    movq [r1+r3*2], xmm0
-    psrldq    xmm0, 1
-    movq [r2+r3*1], xmm0
-    psrldq    xmm0, 1
-    movq [r2+r3*2], xmm0
-    psrldq    xmm0, 1
-    movq [r0+r3*1], xmm0
-    psrldq    xmm0, 1
-    movq [r0+r3*2], xmm0
-    RET
-%endmacro
-
-INIT_MMX sse2
-PRED8x8L_DOWN_LEFT
-INIT_MMX ssse3
-PRED8x8L_DOWN_LEFT
-
-;-----------------------------------------------------------------------------
-; void ff_pred8x8l_down_right_8_mmxext(uint8_t *src, int has_topleft,
-;                                      int has_topright, ptrdiff_t stride)
-;-----------------------------------------------------------------------------
-
-INIT_MMX mmxext
-cglobal pred8x8l_down_right_8, 4,5
-    sub          r0, r3
-    lea          r4, [r0+r3*2]
-    movq        mm0, [r0+r3*1-8]
-    punpckhbw   mm0, [r0+r3*0-8]
-    movq        mm1, [r4+r3*1-8]
-    punpckhbw   mm1, [r0+r3*2-8]
-    mov          r4, r0
-    punpckhwd   mm1, mm0
-    lea          r0, [r0+r3*4]
-    movq        mm2, [r0+r3*1-8]
-    punpckhbw   mm2, [r0+r3*0-8]
-    lea          r0, [r0+r3*2]
-    movq        mm3, [r0+r3*1-8]
-    punpckhbw   mm3, [r0+r3*0-8]
-    punpckhwd   mm3, mm2
-    punpckhdq   mm3, mm1
-    lea          r0, [r0+r3*2]
-    movq        mm0, [r0+r3*0-8]
-    movq        mm1, [r4]
-    mov          r0, r4
-    movq        mm4, mm3
-    movq        mm2, mm3
-    PALIGNR     mm4, mm0, 7, mm0
-    PALIGNR     mm1, mm2, 1, mm2
-    test        r1d, r1d ; top_left
-    jz .fix_lt_1
-.do_left:
-    movq        mm0, mm4
-    PRED4x4_LOWPASS mm2, mm1, mm4, mm3, mm5
-    movq        mm4, mm0
-    movq        mm7, mm2
-    movq        mm6, mm2
-    PRED4x4_LOWPASS mm1, mm3, mm0, mm4, mm5
-    psllq       mm1, 56
-    PALIGNR     mm7, mm1, 7, mm3
-    movq        mm0, [r0-8]
-    movq        mm3, [r0]
-    movq        mm1, [r0+8]
-    movq        mm2, mm3
-    movq        mm4, mm3
-    PALIGNR     mm2, mm0, 7, mm0
-    PALIGNR     mm1, mm4, 1, mm4
-    test        r1d, r1d ; top_left
-    jz .fix_lt_2
-    test        r2d, r2d ; top_right
-    jz .fix_tr_1
-.do_top:
-    PRED4x4_LOWPASS mm4, mm2, mm1, mm3, mm5
-    movq        mm5, mm4
-    jmp .body
-.fix_lt_1:
-    movq        mm5, mm3
-    pxor        mm5, mm4
-    psrlq       mm5, 56
-    psllq       mm5, 48
-    pxor        mm1, mm5
-    jmp .do_left
-.fix_lt_2:
-    movq        mm5, mm3
-    pxor        mm5, mm2
-    psllq       mm5, 56
-    psrlq       mm5, 56
-    pxor        mm2, mm5
-    test        r2d, r2d ; top_right
-    jnz .do_top
-.fix_tr_1:
-    movq        mm5, mm3
-    pxor        mm5, mm1
-    psrlq       mm5, 56
-    psllq       mm5, 56
-    pxor        mm1, mm5
-    jmp .do_top
-.body:
-    lea         r1, [r0+r3*2]
-    movq       mm1, mm7
-    movq       mm7, mm5
-    movq       mm5, mm6
-    movq       mm2, mm7
-    lea         r2, [r1+r3*2]
-    PALIGNR    mm2, mm6, 1, mm0
-    movq       mm3, mm7
-    PALIGNR    mm3, mm6, 7, mm0
-    movq       mm4, mm7
-    lea         r4, [r2+r3*2]
-    psrlq      mm4, 8
-    PRED4x4_LOWPASS mm0, mm1, mm2, mm5, mm6
-    PRED4x4_LOWPASS mm1, mm3, mm4, mm7, mm6
-    movq [r4+r3*2], mm0
-    movq       mm2, mm1
-    psrlq      mm0, 8
-    psllq      mm2, 56
-    psrlq      mm1, 8
-    por        mm0, mm2
-    movq [r4+r3*1], mm0
-    movq       mm2, mm1
-    psrlq      mm0, 8
-    psllq      mm2, 56
-    psrlq      mm1, 8
-    por        mm0, mm2
-    movq [r2+r3*2], mm0
-    movq       mm2, mm1
-    psrlq      mm0, 8
-    psllq      mm2, 56
-    psrlq      mm1, 8
-    por        mm0, mm2
-    movq [r2+r3*1], mm0
-    movq       mm2, mm1
-    psrlq      mm0, 8
-    psllq      mm2, 56
-    psrlq      mm1, 8
-    por        mm0, mm2
-    movq [r1+r3*2], mm0
-    movq       mm2, mm1
-    psrlq      mm0, 8
-    psllq      mm2, 56
-    psrlq      mm1, 8
-    por        mm0, mm2
-    movq [r1+r3*1], mm0
-    movq       mm2, mm1
-    psrlq      mm0, 8
-    psllq      mm2, 56
-    psrlq      mm1, 8
-    por        mm0, mm2
-    movq [r0+r3*2], mm0
-    psrlq      mm0, 8
-    psllq      mm1, 56
-    por        mm0, mm1
-    movq [r0+r3*1], mm0
-    RET
-
-%macro PRED8x8L_DOWN_RIGHT 0
-cglobal pred8x8l_down_right_8, 4,5
-    sub          r0, r3
-    lea          r4, [r0+r3*2]
-    movq        mm0, [r0+r3*1-8]
-    punpckhbw   mm0, [r0+r3*0-8]
-    movq        mm1, [r4+r3*1-8]
-    punpckhbw   mm1, [r0+r3*2-8]
-    mov          r4, r0
-    punpckhwd   mm1, mm0
-    lea          r0, [r0+r3*4]
-    movq        mm2, [r0+r3*1-8]
-    punpckhbw   mm2, [r0+r3*0-8]
-    lea          r0, [r0+r3*2]
-    movq        mm3, [r0+r3*1-8]
-    punpckhbw   mm3, [r0+r3*0-8]
-    punpckhwd   mm3, mm2
-    punpckhdq   mm3, mm1
-    lea          r0, [r0+r3*2]
-    movq        mm0, [r0+r3*0-8]
-    movq        mm1, [r4]
-    mov          r0, r4
-    movq        mm4, mm3
-    movq        mm2, mm3
-    PALIGNR     mm4, mm0, 7, mm0
-    PALIGNR     mm1, mm2, 1, mm2
-    test        r1d, r1d
-    jz .fix_lt_1
-    jmp .do_left
-.fix_lt_1:
-    movq        mm5, mm3
-    pxor        mm5, mm4
-    psrlq       mm5, 56
-    psllq       mm5, 48
-    pxor        mm1, mm5
-    jmp .do_left
-.fix_lt_2:
-    movq        mm5, mm3
-    pxor        mm5, mm2
-    psllq       mm5, 56
-    psrlq       mm5, 56
-    pxor        mm2, mm5
-    test        r2d, r2d
-    jnz .do_top
-.fix_tr_1:
-    movq        mm5, mm3
-    pxor        mm5, mm1
-    psrlq       mm5, 56
-    psllq       mm5, 56
-    pxor        mm1, mm5
-    jmp .do_top
-.do_left:
-    movq        mm0, mm4
-    PRED4x4_LOWPASS mm2, mm1, mm4, mm3, mm5
-    movq        mm4, mm0
-    movq        mm7, mm2
-    movq2dq    xmm3, mm2
-    PRED4x4_LOWPASS mm1, mm3, mm0, mm4, mm5
-    psllq       mm1, 56
-    PALIGNR     mm7, mm1, 7, mm3
-    movq2dq    xmm1, mm7
-    movq        mm0, [r0-8]
-    movq        mm3, [r0]
-    movq        mm1, [r0+8]
-    movq        mm2, mm3
-    movq        mm4, mm3
-    PALIGNR     mm2, mm0, 7, mm0
-    PALIGNR     mm1, mm4, 1, mm4
-    test        r1d, r1d
-    jz .fix_lt_2
-    test        r2d, r2d
-    jz .fix_tr_1
-.do_top:
-    PRED4x4_LOWPASS mm4, mm2, mm1, mm3, mm5
-    movq2dq   xmm4, mm4
-    lea         r1, [r0+r3*2]
-    movdqa    xmm0, xmm3
-    pslldq    xmm4, 8
-    por       xmm3, xmm4
-    lea         r2, [r1+r3*2]
-    pslldq    xmm4, 1
-    por       xmm1, xmm4
-    psrldq    xmm0, 7
-    pslldq    xmm0, 15
-    psrldq    xmm0, 7
-    por       xmm1, xmm0
-    lea         r0, [r2+r3*2]
-    movdqa    xmm2, xmm3
-    psrldq    xmm2, 1
-INIT_XMM cpuname
-    PRED4x4_LOWPASS xmm0, xmm1, xmm2, xmm3, xmm4
-    movdqa    xmm1, xmm0
-    psrldq    xmm1, 1
-    movq [r0+r3*2], xmm0
-    movq [r0+r3*1], xmm1
-    psrldq    xmm0, 2
-    psrldq    xmm1, 2
-    movq [r2+r3*2], xmm0
-    movq [r2+r3*1], xmm1
-    psrldq    xmm0, 2
-    psrldq    xmm1, 2
-    movq [r1+r3*2], xmm0
-    movq [r1+r3*1], xmm1
-    psrldq    xmm0, 2
-    psrldq    xmm1, 2
-    movq [r4+r3*2], xmm0
-    movq [r4+r3*1], xmm1
-    RET
-%endmacro
-
-INIT_MMX sse2
-PRED8x8L_DOWN_RIGHT
-INIT_MMX ssse3
-PRED8x8L_DOWN_RIGHT
-
-;-----------------------------------------------------------------------------
-; void ff_pred8x8l_vertical_right_8(uint8_t *src, int has_topleft,
-;                                   int has_topright, ptrdiff_t stride)
-;-----------------------------------------------------------------------------
-
-INIT_MMX mmxext
-cglobal pred8x8l_vertical_right_8, 4,5
-    sub          r0, r3
-    lea          r4, [r0+r3*2]
-    movq        mm0, [r0+r3*1-8]
-    punpckhbw   mm0, [r0+r3*0-8]
-    movq        mm1, [r4+r3*1-8]
-    punpckhbw   mm1, [r0+r3*2-8]
-    mov          r4, r0
-    punpckhwd   mm1, mm0
-    lea          r0, [r0+r3*4]
-    movq        mm2, [r0+r3*1-8]
-    punpckhbw   mm2, [r0+r3*0-8]
-    lea          r0, [r0+r3*2]
-    movq        mm3, [r0+r3*1-8]
-    punpckhbw   mm3, [r0+r3*0-8]
-    punpckhwd   mm3, mm2
-    punpckhdq   mm3, mm1
-    lea          r0, [r0+r3*2]
-    movq        mm0, [r0+r3*0-8]
-    movq        mm1, [r4]
-    mov          r0, r4
-    movq        mm4, mm3
-    movq        mm2, mm3
-    PALIGNR     mm4, mm0, 7, mm0
-    PALIGNR     mm1, mm2, 1, mm2
-    test        r1d, r1d
-    jz .fix_lt_1
-    jmp .do_left
-.fix_lt_1:
-    movq        mm5, mm3
-    pxor        mm5, mm4
-    psrlq       mm5, 56
-    psllq       mm5, 48
-    pxor        mm1, mm5
-    jmp .do_left
-.fix_lt_2:
-    movq        mm5, mm3
-    pxor        mm5, mm2
-    psllq       mm5, 56
-    psrlq       mm5, 56
-    pxor        mm2, mm5
-    test        r2d, r2d
-    jnz .do_top
-.fix_tr_1:
-    movq        mm5, mm3
-    pxor        mm5, mm1
-    psrlq       mm5, 56
-    psllq       mm5, 56
-    pxor        mm1, mm5
-    jmp .do_top
-.do_left:
-    movq        mm0, mm4
-    PRED4x4_LOWPASS mm2, mm1, mm4, mm3, mm5
-    movq        mm7, mm2
-    movq        mm0, [r0-8]
-    movq        mm3, [r0]
-    movq        mm1, [r0+8]
-    movq        mm2, mm3
-    movq        mm4, mm3
-    PALIGNR     mm2, mm0, 7, mm0
-    PALIGNR     mm1, mm4, 1, mm4
-    test        r1d, r1d
-    jz .fix_lt_2
-    test        r2d, r2d
-    jz .fix_tr_1
-.do_top:
-    PRED4x4_LOWPASS mm6, mm2, mm1, mm3, mm5
-    lea         r1, [r0+r3*2]
-    movq       mm2, mm6
-    movq       mm3, mm6
-    PALIGNR    mm3, mm7, 7, mm0
-    PALIGNR    mm6, mm7, 6, mm1
-    movq       mm4, mm3
-    pavgb      mm3, mm2
-    lea         r2, [r1+r3*2]
-    PRED4x4_LOWPASS mm0, mm6, mm2, mm4, mm5
-    movq [r0+r3*1], mm3
-    movq [r0+r3*2], mm0
-    movq       mm5, mm0
-    movq       mm6, mm3
-    movq       mm1, mm7
-    movq       mm2, mm1
-    psllq      mm2, 8
-    movq       mm3, mm1
-    psllq      mm3, 16
-    lea         r4, [r2+r3*2]
-    PRED4x4_LOWPASS mm0, mm1, mm3, mm2, mm4
-    PALIGNR    mm6, mm0, 7, mm2
-    movq [r1+r3*1], mm6
-    psllq      mm0, 8
-    PALIGNR    mm5, mm0, 7, mm1
-    movq [r1+r3*2], mm5
-    psllq      mm0, 8
-    PALIGNR    mm6, mm0, 7, mm2
-    movq [r2+r3*1], mm6
-    psllq      mm0, 8
-    PALIGNR    mm5, mm0, 7, mm1
-    movq [r2+r3*2], mm5
-    psllq      mm0, 8
-    PALIGNR    mm6, mm0, 7, mm2
-    movq [r4+r3*1], mm6
-    psllq      mm0, 8
-    PALIGNR    mm5, mm0, 7, mm1
-    movq [r4+r3*2], mm5
-    RET
-
-%macro PRED8x8L_VERTICAL_RIGHT 0
-cglobal pred8x8l_vertical_right_8, 4,5,7
-    ; manually spill XMM registers for Win64 because
-    ; the code here is initialized with INIT_MMX
-    WIN64_SPILL_XMM 7
-    sub          r0, r3
-    lea          r4, [r0+r3*2]
-    movq        mm0, [r0+r3*1-8]
-    punpckhbw   mm0, [r0+r3*0-8]
-    movq        mm1, [r4+r3*1-8]
-    punpckhbw   mm1, [r0+r3*2-8]
-    mov          r4, r0
-    punpckhwd   mm1, mm0
-    lea          r0, [r0+r3*4]
-    movq        mm2, [r0+r3*1-8]
-    punpckhbw   mm2, [r0+r3*0-8]
-    lea          r0, [r0+r3*2]
-    movq        mm3, [r0+r3*1-8]
-    punpckhbw   mm3, [r0+r3*0-8]
-    punpckhwd   mm3, mm2
-    punpckhdq   mm3, mm1
-    lea          r0, [r0+r3*2]
-    movq        mm0, [r0+r3*0-8]
-    movq        mm1, [r4]
-    mov          r0, r4
-    movq        mm4, mm3
-    movq        mm2, mm3
-    PALIGNR     mm4, mm0, 7, mm0
-    PALIGNR     mm1, mm2, 1, mm2
-    test        r1d, r1d
-    jnz .do_left
-.fix_lt_1:
-    movq        mm5, mm3
-    pxor        mm5, mm4
-    psrlq       mm5, 56
-    psllq       mm5, 48
-    pxor        mm1, mm5
-    jmp .do_left
-.fix_lt_2:
-    movq        mm5, mm3
-    pxor        mm5, mm2
-    psllq       mm5, 56
-    psrlq       mm5, 56
-    pxor        mm2, mm5
-    test        r2d, r2d
-    jnz .do_top
-.fix_tr_1:
-    movq        mm5, mm3
-    pxor        mm5, mm1
-    psrlq       mm5, 56
-    psllq       mm5, 56
-    pxor        mm1, mm5
-    jmp .do_top
-.do_left:
-    movq        mm0, mm4
-    PRED4x4_LOWPASS mm2, mm1, mm4, mm3, mm5
-    movq2dq    xmm0, mm2
-    movq        mm0, [r0-8]
-    movq        mm3, [r0]
-    movq        mm1, [r0+8]
-    movq        mm2, mm3
-    movq        mm4, mm3
-    PALIGNR     mm2, mm0, 7, mm0
-    PALIGNR     mm1, mm4, 1, mm4
-    test        r1d, r1d
-    jz .fix_lt_2
-    test        r2d, r2d
-    jz .fix_tr_1
-.do_top:
-    PRED4x4_LOWPASS mm6, mm2, mm1, mm3, mm5
-    lea           r1, [r0+r3*2]
-    movq2dq     xmm4, mm6
-    pslldq      xmm4, 8
-    por         xmm0, xmm4
-    movdqa      xmm6, [pw_ff00]
-    movdqa      xmm1, xmm0
-    lea           r2, [r1+r3*2]
-    movdqa      xmm2, xmm0
-    movdqa      xmm3, xmm0
-    pslldq      xmm0, 1
-    pslldq      xmm1, 2
-    pavgb       xmm2, xmm0
-INIT_XMM cpuname
-    PRED4x4_LOWPASS xmm4, xmm3, xmm1, xmm0, xmm5
-    pandn       xmm6, xmm4
-    movdqa      xmm5, xmm4
-    psrlw       xmm4, 8
-    packuswb    xmm6, xmm4
-    movhlps     xmm4, xmm6
-    movhps [r0+r3*2], xmm5
-    movhps [r0+r3*1], xmm2
-    psrldq      xmm5, 4
-    movss       xmm5, xmm6
-    psrldq      xmm2, 4
-    movss       xmm2, xmm4
-    lea           r0, [r2+r3*2]
-    psrldq      xmm5, 1
-    psrldq      xmm2, 1
-    movq        [r0+r3*2], xmm5
-    movq        [r0+r3*1], xmm2
-    psrldq      xmm5, 1
-    psrldq      xmm2, 1
-    movq        [r2+r3*2], xmm5
-    movq        [r2+r3*1], xmm2
-    psrldq      xmm5, 1
-    psrldq      xmm2, 1
-    movq        [r1+r3*2], xmm5
-    movq        [r1+r3*1], xmm2
-    RET
-%endmacro
-
-INIT_MMX sse2
-PRED8x8L_VERTICAL_RIGHT
-INIT_MMX ssse3
-PRED8x8L_VERTICAL_RIGHT
-
-;-----------------------------------------------------------------------------
-; void ff_pred8x8l_vertical_left_8(uint8_t *src, int has_topleft,
-;                                  int has_topright, ptrdiff_t stride)
-;-----------------------------------------------------------------------------
-
-%macro PRED8x8L_VERTICAL_LEFT 0
-cglobal pred8x8l_vertical_left_8, 4,4
-    sub          r0, r3
-    movq        mm0, [r0-8]
-    movq        mm3, [r0]
-    movq        mm1, [r0+8]
-    movq        mm2, mm3
-    movq        mm4, mm3
-    PALIGNR     mm2, mm0, 7, mm0
-    PALIGNR     mm1, mm4, 1, mm4
-    test        r1d, r1d
-    jz .fix_lt_2
-    test        r2d, r2d
-    jz .fix_tr_1
-    jmp .do_top
-.fix_lt_2:
-    movq        mm5, mm3
-    pxor        mm5, mm2
-    psllq       mm5, 56
-    psrlq       mm5, 56
-    pxor        mm2, mm5
-    test        r2d, r2d
-    jnz .do_top
-.fix_tr_1:
-    movq        mm5, mm3
-    pxor        mm5, mm1
-    psrlq       mm5, 56
-    psllq       mm5, 56
-    pxor        mm1, mm5
-    jmp .do_top
-.fix_tr_2:
-    punpckhbw   mm3, mm3
-    pshufw      mm1, mm3, 0xFF
-    jmp .do_topright
-.do_top:
-    PRED4x4_LOWPASS mm4, mm2, mm1, mm3, mm5
-    movq2dq    xmm4, mm4
-    test        r2d, r2d
-    jz .fix_tr_2
-    movq        mm0, [r0+8]
-    movq        mm5, mm0
-    movq        mm2, mm0
-    movq        mm4, mm0
-    psrlq       mm5, 56
-    PALIGNR     mm2, mm3, 7, mm3
-    PALIGNR     mm5, mm4, 1, mm4
-    PRED4x4_LOWPASS mm1, mm2, mm5, mm0, mm4
-.do_topright:
-    movq2dq   xmm3, mm1
-    lea         r1, [r0+r3*2]
-    pslldq    xmm3, 8
-    por       xmm4, xmm3
-    movdqa    xmm2, xmm4
-    movdqa    xmm1, xmm4
-    movdqa    xmm3, xmm4
-    psrldq    xmm2, 1
-    pslldq    xmm1, 1
-    pavgb     xmm3, xmm2
-    lea         r2, [r1+r3*2]
-INIT_XMM cpuname
-    PRED4x4_LOWPASS xmm0, xmm1, xmm2, xmm4, xmm5
-    psrldq    xmm0, 1
-    movq [r0+r3*1], xmm3
-    movq [r0+r3*2], xmm0
-    lea         r0, [r2+r3*2]
-    psrldq    xmm3, 1
-    psrldq    xmm0, 1
-    movq [r1+r3*1], xmm3
-    movq [r1+r3*2], xmm0
-    psrldq    xmm3, 1
-    psrldq    xmm0, 1
-    movq [r2+r3*1], xmm3
-    movq [r2+r3*2], xmm0
-    psrldq    xmm3, 1
-    psrldq    xmm0, 1
-    movq [r0+r3*1], xmm3
-    movq [r0+r3*2], xmm0
-    RET
-%endmacro
-
-INIT_MMX sse2
-PRED8x8L_VERTICAL_LEFT
-INIT_MMX ssse3
-PRED8x8L_VERTICAL_LEFT
-
-;-----------------------------------------------------------------------------
-; void ff_pred8x8l_horizontal_up_8(uint8_t *src, int has_topleft,
-;                                  int has_topright, ptrdiff_t stride)
-;-----------------------------------------------------------------------------
-
-%macro PRED8x8L_HORIZONTAL_UP 0
-cglobal pred8x8l_horizontal_up_8, 4,4
-    sub          r0, r3
-    lea          r2, [r0+r3*2]
-    movq        mm0, [r0+r3*1-8]
-    test        r1d, r1d
-    lea          r1, [r0+r3]
-    cmovnz       r1, r0
-    punpckhbw   mm0, [r1+r3*0-8]
-    movq        mm1, [r2+r3*1-8]
-    punpckhbw   mm1, [r0+r3*2-8]
-    mov          r2, r0
-    punpckhwd   mm1, mm0
-    lea          r0, [r0+r3*4]
-    movq        mm2, [r0+r3*1-8]
-    punpckhbw   mm2, [r0+r3*0-8]
-    lea          r0, [r0+r3*2]
-    movq        mm3, [r0+r3*1-8]
-    punpckhbw   mm3, [r0+r3*0-8]
-    punpckhwd   mm3, mm2
-    punpckhdq   mm3, mm1
-    lea          r0, [r0+r3*2]
-    movq        mm0, [r0+r3*0-8]
-    movq        mm1, [r1+r3*0-8]
-    mov          r0, r2
-    movq        mm4, mm3
-    movq        mm2, mm3
-    PALIGNR     mm4, mm0, 7, mm0
-    PALIGNR     mm1, mm2, 1, mm2
-    movq       mm0, mm4
-    PRED4x4_LOWPASS mm2, mm1, mm4, mm3, mm5
-    movq       mm4, mm0
-    movq       mm7, mm2
-    PRED4x4_LOWPASS mm1, mm3, mm0, mm4, mm5
-    psllq      mm1, 56
-    PALIGNR    mm7, mm1, 7, mm3
-    lea         r1, [r0+r3*2]
-    pshufw     mm0, mm7, 00011011b ; l6 l7 l4 l5 l2 l3 l0 l1
-    psllq      mm7, 56             ; l7 .. .. .. .. .. .. ..
-    movq       mm2, mm0
-    psllw      mm0, 8
-    psrlw      mm2, 8
-    por        mm2, mm0            ; l7 l6 l5 l4 l3 l2 l1 l0
-    movq       mm3, mm2
-    movq       mm4, mm2
-    movq       mm5, mm2
-    psrlq      mm2, 8
-    psrlq      mm3, 16
-    lea         r2, [r1+r3*2]
-    por        mm2, mm7            ; l7 l7 l6 l5 l4 l3 l2 l1
-    punpckhbw  mm7, mm7
-    por        mm3, mm7            ; l7 l7 l7 l6 l5 l4 l3 l2
-    pavgb      mm4, mm2
-    PRED4x4_LOWPASS mm1, mm3, mm5, mm2, mm6
-    movq       mm5, mm4
-    punpcklbw  mm4, mm1            ; p4 p3 p2 p1
-    punpckhbw  mm5, mm1            ; p8 p7 p6 p5
-    movq       mm6, mm5
-    movq       mm7, mm5
-    movq       mm0, mm5
-    PALIGNR    mm5, mm4, 2, mm1
-    pshufw     mm1, mm6, 11111001b
-    PALIGNR    mm6, mm4, 4, mm2
-    pshufw     mm2, mm7, 11111110b
-    PALIGNR    mm7, mm4, 6, mm3
-    pshufw     mm3, mm0, 11111111b
-    movq [r0+r3*1], mm4
-    movq [r0+r3*2], mm5
-    lea         r0, [r2+r3*2]
-    movq [r1+r3*1], mm6
-    movq [r1+r3*2], mm7
-    movq [r2+r3*1], mm0
-    movq [r2+r3*2], mm1
-    movq [r0+r3*1], mm2
-    movq [r0+r3*2], mm3
-    RET
-%endmacro
-
-INIT_MMX mmxext
-PRED8x8L_HORIZONTAL_UP
-INIT_MMX ssse3
-PRED8x8L_HORIZONTAL_UP
-
-;-----------------------------------------------------------------------------
-; void ff_pred8x8l_horizontal_down_8(uint8_t *src, int has_topleft,
-;                                    int has_topright, ptrdiff_t stride)
-;-----------------------------------------------------------------------------
-
-INIT_MMX mmxext
-cglobal pred8x8l_horizontal_down_8, 4,5
-    sub          r0, r3
-    lea          r4, [r0+r3*2]
-    movq        mm0, [r0+r3*1-8]
-    punpckhbw   mm0, [r0+r3*0-8]
-    movq        mm1, [r4+r3*1-8]
-    punpckhbw   mm1, [r0+r3*2-8]
-    mov          r4, r0
-    punpckhwd   mm1, mm0
-    lea          r0, [r0+r3*4]
-    movq        mm2, [r0+r3*1-8]
-    punpckhbw   mm2, [r0+r3*0-8]
-    lea          r0, [r0+r3*2]
-    movq        mm3, [r0+r3*1-8]
-    punpckhbw   mm3, [r0+r3*0-8]
-    punpckhwd   mm3, mm2
-    punpckhdq   mm3, mm1
-    lea          r0, [r0+r3*2]
-    movq        mm0, [r0+r3*0-8]
-    movq        mm1, [r4]
-    mov          r0, r4
-    movq        mm4, mm3
-    movq        mm2, mm3
-    PALIGNR     mm4, mm0, 7, mm0
-    PALIGNR     mm1, mm2, 1, mm2
-    test        r1d, r1d
-    jnz .do_left
-.fix_lt_1:
-    movq        mm5, mm3
-    pxor        mm5, mm4
-    psrlq       mm5, 56
-    psllq       mm5, 48
-    pxor        mm1, mm5
-    jmp .do_left
-.fix_lt_2:
-    movq        mm5, mm3
-    pxor        mm5, mm2
-    psllq       mm5, 56
-    psrlq       mm5, 56
-    pxor        mm2, mm5
-    test        r2d, r2d
-    jnz .do_top
-.fix_tr_1:
-    movq        mm5, mm3
-    pxor        mm5, mm1
-    psrlq       mm5, 56
-    psllq       mm5, 56
-    pxor        mm1, mm5
-    jmp .do_top
-.do_left:
-    movq        mm0, mm4
-    PRED4x4_LOWPASS mm2, mm1, mm4, mm3, mm5
-    movq        mm4, mm0
-    movq        mm7, mm2
-    movq        mm6, mm2
-    PRED4x4_LOWPASS mm1, mm3, mm0, mm4, mm5
-    psllq       mm1, 56
-    PALIGNR     mm7, mm1, 7, mm3
-    movq        mm0, [r0-8]
-    movq        mm3, [r0]
-    movq        mm1, [r0+8]
-    movq        mm2, mm3
-    movq        mm4, mm3
-    PALIGNR     mm2, mm0, 7, mm0
-    PALIGNR     mm1, mm4, 1, mm4
-    test        r1d, r1d
-    jz .fix_lt_2
-    test        r2d, r2d
-    jz .fix_tr_1
-.do_top:
-    PRED4x4_LOWPASS mm4, mm2, mm1, mm3, mm5
-    movq       mm5, mm4
-    lea         r1, [r0+r3*2]
-    psllq      mm7, 56
-    movq       mm2, mm5
-    movq       mm3, mm6
-    movq       mm4, mm2
-    PALIGNR    mm2, mm6, 7, mm5
-    PALIGNR    mm6, mm7, 7, mm0
-    lea         r2, [r1+r3*2]
-    PALIGNR    mm4, mm3, 1, mm7
-    movq       mm5, mm3
-    pavgb      mm3, mm6
-    PRED4x4_LOWPASS mm0, mm4, mm6, mm5, mm7
-    movq       mm4, mm2
-    movq       mm1, mm2
-    lea         r4, [r2+r3*2]
-    psrlq      mm4, 16
-    psrlq      mm1, 8
-    PRED4x4_LOWPASS mm6, mm4, mm2, mm1, mm5
-    movq       mm7, mm3
-    punpcklbw  mm3, mm0
-    punpckhbw  mm7, mm0
-    movq       mm1, mm7
-    movq       mm0, mm7
-    movq       mm4, mm7
-    movq [r4+r3*2], mm3
-    PALIGNR    mm7, mm3, 2, mm5
-    movq [r4+r3*1], mm7
-    PALIGNR    mm1, mm3, 4, mm5
-    movq [r2+r3*2], mm1
-    PALIGNR    mm0, mm3, 6, mm3
-    movq [r2+r3*1], mm0
-    movq       mm2, mm6
-    movq       mm3, mm6
-    movq [r1+r3*2], mm4
-    PALIGNR    mm6, mm4, 2, mm5
-    movq [r1+r3*1], mm6
-    PALIGNR    mm2, mm4, 4, mm5
-    movq [r0+r3*2], mm2
-    PALIGNR    mm3, mm4, 6, mm4
-    movq [r0+r3*1], mm3
-    RET
-
-%macro PRED8x8L_HORIZONTAL_DOWN 0
-cglobal pred8x8l_horizontal_down_8, 4,5
-    sub          r0, r3
-    lea          r4, [r0+r3*2]
-    movq        mm0, [r0+r3*1-8]
-    punpckhbw   mm0, [r0+r3*0-8]
-    movq        mm1, [r4+r3*1-8]
-    punpckhbw   mm1, [r0+r3*2-8]
-    mov          r4, r0
-    punpckhwd   mm1, mm0
-    lea          r0, [r0+r3*4]
-    movq        mm2, [r0+r3*1-8]
-    punpckhbw   mm2, [r0+r3*0-8]
-    lea          r0, [r0+r3*2]
-    movq        mm3, [r0+r3*1-8]
-    punpckhbw   mm3, [r0+r3*0-8]
-    punpckhwd   mm3, mm2
-    punpckhdq   mm3, mm1
-    lea          r0, [r0+r3*2]
-    movq        mm0, [r0+r3*0-8]
-    movq        mm1, [r4]
-    mov          r0, r4
-    movq        mm4, mm3
-    movq        mm2, mm3
-    PALIGNR     mm4, mm0, 7, mm0
-    PALIGNR     mm1, mm2, 1, mm2
-    test        r1d, r1d
-    jnz .do_left
-.fix_lt_1:
-    movq        mm5, mm3
-    pxor        mm5, mm4
-    psrlq       mm5, 56
-    psllq       mm5, 48
-    pxor        mm1, mm5
-    jmp .do_left
-.fix_lt_2:
-    movq        mm5, mm3
-    pxor        mm5, mm2
-    psllq       mm5, 56
-    psrlq       mm5, 56
-    pxor        mm2, mm5
-    test        r2d, r2d
-    jnz .do_top
-.fix_tr_1:
-    movq        mm5, mm3
-    pxor        mm5, mm1
-    psrlq       mm5, 56
-    psllq       mm5, 56
-    pxor        mm1, mm5
-    jmp .do_top
-.fix_tr_2:
-    punpckhbw   mm3, mm3
-    pshufw      mm1, mm3, 0xFF
-    jmp .do_topright
-.do_left:
-    movq        mm0, mm4
-    PRED4x4_LOWPASS mm2, mm1, mm4, mm3, mm5
-    movq2dq    xmm0, mm2
-    pslldq     xmm0, 8
-    movq        mm4, mm0
-    PRED4x4_LOWPASS mm1, mm3, mm0, mm4, mm5
-    movq2dq    xmm2, mm1
-    pslldq     xmm2, 15
-    psrldq     xmm2, 8
-    por        xmm0, xmm2
-    movq        mm0, [r0-8]
-    movq        mm3, [r0]
-    movq        mm1, [r0+8]
-    movq        mm2, mm3
-    movq        mm4, mm3
-    PALIGNR     mm2, mm0, 7, mm0
-    PALIGNR     mm1, mm4, 1, mm4
-    test        r1d, r1d
-    jz .fix_lt_2
-    test        r2d, r2d
-    jz .fix_tr_1
-.do_top:
-    PRED4x4_LOWPASS mm4, mm2, mm1, mm3, mm5
-    movq2dq    xmm1, mm4
-    test        r2d, r2d
-    jz .fix_tr_2
-    movq        mm0, [r0+8]
-    movq        mm5, mm0
-    movq        mm2, mm0
-    movq        mm4, mm0
-    psrlq       mm5, 56
-    PALIGNR     mm2, mm3, 7, mm3
-    PALIGNR     mm5, mm4, 1, mm4
-    PRED4x4_LOWPASS mm1, mm2, mm5, mm0, mm4
-.do_topright:
-    movq2dq    xmm5, mm1
-    pslldq     xmm5, 8
-    por        xmm1, xmm5
-INIT_XMM cpuname
-    lea         r2, [r4+r3*2]
-    movdqa    xmm2, xmm1
-    movdqa    xmm3, xmm1
-    PALIGNR   xmm1, xmm0, 7, xmm4
-    PALIGNR   xmm2, xmm0, 9, xmm5
-    lea         r1, [r2+r3*2]
-    PALIGNR   xmm3, xmm0, 8, xmm0
-    movdqa    xmm4, xmm1
-    pavgb     xmm4, xmm3
-    lea         r0, [r1+r3*2]
-    PRED4x4_LOWPASS xmm0, xmm1, xmm2, xmm3, xmm5
-    punpcklbw xmm4, xmm0
-    movhlps   xmm0, xmm4
-    movq   [r0+r3*2], xmm4
-    movq   [r2+r3*2], xmm0
-    psrldq xmm4, 2
-    psrldq xmm0, 2
-    movq   [r0+r3*1], xmm4
-    movq   [r2+r3*1], xmm0
-    psrldq xmm4, 2
-    psrldq xmm0, 2
-    movq   [r1+r3*2], xmm4
-    movq   [r4+r3*2], xmm0
-    psrldq xmm4, 2
-    psrldq xmm0, 2
-    movq   [r1+r3*1], xmm4
-    movq   [r4+r3*1], xmm0
-    RET
-%endmacro
-
-INIT_MMX sse2
-PRED8x8L_HORIZONTAL_DOWN
-INIT_MMX ssse3
-PRED8x8L_HORIZONTAL_DOWN
-
-;-------------------------------------------------------------------------------
-; void ff_pred4x4_dc_8_mmxext(uint8_t *src, const uint8_t *topright,
-;                             ptrdiff_t stride)
-;-------------------------------------------------------------------------------
-
-INIT_MMX mmxext
-cglobal pred4x4_dc_8, 3,5
-    pxor   mm7, mm7
-    mov     r4, r0
-    sub     r0, r2
-    movd   mm0, [r0]
-    psadbw mm0, mm7
-    movzx  r1d, byte [r0+r2*1-1]
-    movd   r3d, mm0
-    add    r3d, r1d
-    movzx  r1d, byte [r0+r2*2-1]
-    lea     r0, [r0+r2*2]
-    add    r3d, r1d
-    movzx  r1d, byte [r0+r2*1-1]
-    add    r3d, r1d
-    movzx  r1d, byte [r0+r2*2-1]
-    add    r3d, r1d
-    add    r3d, 4
-    shr    r3d, 3
-    imul   r3d, 0x01010101
-    mov   [r4+r2*0], r3d
-    mov   [r0+r2*0], r3d
-    mov   [r0+r2*1], r3d
-    mov   [r0+r2*2], r3d
-    RET
-
-;-----------------------------------------------------------------------------
-; void ff_pred4x4_tm_vp8_8_mmxext(uint8_t *src, const uint8_t *topright,
-;                                 ptrdiff_t stride)
-;-----------------------------------------------------------------------------
-
-%macro PRED4x4_TM 0
-cglobal pred4x4_tm_vp8_8, 3,6
-    sub        r0, r2
-    pxor      mm7, mm7
-    movd      mm0, [r0]
-    punpcklbw mm0, mm7
-    movzx     r4d, byte [r0-1]
-    mov       r5d, 2
-.loop:
-    movzx     r1d, byte [r0+r2*1-1]
-    movzx     r3d, byte [r0+r2*2-1]
-    sub       r1d, r4d
-    sub       r3d, r4d
-    movd      mm2, r1d
-    movd      mm4, r3d
-%if cpuflag(mmxext)
-    pshufw    mm2, mm2, 0
-    pshufw    mm4, mm4, 0
-%else
-    punpcklwd mm2, mm2
-    punpcklwd mm4, mm4
-    punpckldq mm2, mm2
-    punpckldq mm4, mm4
-%endif
-    paddw     mm2, mm0
-    paddw     mm4, mm0
-    packuswb  mm2, mm2
-    packuswb  mm4, mm4
-    movd [r0+r2*1], mm2
-    movd [r0+r2*2], mm4
-    lea        r0, [r0+r2*2]
-    dec       r5d
-    jg .loop
-    REP_RET
-%endmacro
-
-INIT_MMX mmx
-PRED4x4_TM
-INIT_MMX mmxext
-PRED4x4_TM
-
-INIT_XMM ssse3
-cglobal pred4x4_tm_vp8_8, 3,3
-    sub         r0, r2
-    movq       mm6, [tm_shuf]
-    pxor       mm1, mm1
-    movd       mm0, [r0]
-    punpcklbw  mm0, mm1
-    movd       mm7, [r0-4]
-    pshufb     mm7, mm6
-    lea         r1, [r0+r2*2]
-    movd       mm2, [r0+r2*1-4]
-    movd       mm3, [r0+r2*2-4]
-    movd       mm4, [r1+r2*1-4]
-    movd       mm5, [r1+r2*2-4]
-    pshufb     mm2, mm6
-    pshufb     mm3, mm6
-    pshufb     mm4, mm6
-    pshufb     mm5, mm6
-    psubw      mm0, mm7
-    paddw      mm2, mm0
-    paddw      mm3, mm0
-    paddw      mm4, mm0
-    paddw      mm5, mm0
-    packuswb   mm2, mm2
-    packuswb   mm3, mm3
-    packuswb   mm4, mm4
-    packuswb   mm5, mm5
-    movd [r0+r2*1], mm2
-    movd [r0+r2*2], mm3
-    movd [r1+r2*1], mm4
-    movd [r1+r2*2], mm5
-    RET
-
-;-----------------------------------------------------------------------------
-; void ff_pred4x4_vertical_vp8_8_mmxext(uint8_t *src, const uint8_t *topright,
-;                                       ptrdiff_t stride)
-;-----------------------------------------------------------------------------
-
-INIT_MMX mmxext
-cglobal pred4x4_vertical_vp8_8, 3,3
-    sub       r0, r2
-    movd      m1, [r0-1]
-    movd      m0, [r0]
-    mova      m2, m0   ;t0 t1 t2 t3
-    punpckldq m0, [r1] ;t0 t1 t2 t3 t4 t5 t6 t7
-    lea       r1, [r0+r2*2]
-    psrlq     m0, 8    ;t1 t2 t3 t4
-    PRED4x4_LOWPASS m3, m1, m0, m2, m4
-    movd [r0+r2*1], m3
-    movd [r0+r2*2], m3
-    movd [r1+r2*1], m3
-    movd [r1+r2*2], m3
-    RET
-
-;-----------------------------------------------------------------------------
-; void ff_pred4x4_down_left_8_mmxext(uint8_t *src, const uint8_t *topright,
-;                                    ptrdiff_t stride)
-;-----------------------------------------------------------------------------
-INIT_MMX mmxext
-cglobal pred4x4_down_left_8, 3,3
-    sub       r0, r2
-    movq      m1, [r0]
-    punpckldq m1, [r1]
-    movq      m2, m1
-    movq      m3, m1
-    psllq     m1, 8
-    pxor      m2, m1
-    psrlq     m2, 8
-    pxor      m2, m3
-    PRED4x4_LOWPASS m0, m1, m2, m3, m4
-    lea       r1, [r0+r2*2]
-    psrlq     m0, 8
-    movd      [r0+r2*1], m0
-    psrlq     m0, 8
-    movd      [r0+r2*2], m0
-    psrlq     m0, 8
-    movd      [r1+r2*1], m0
-    psrlq     m0, 8
-    movd      [r1+r2*2], m0
-    RET
-
-;------------------------------------------------------------------------------
-; void ff_pred4x4_vertical_left_8_mmxext(uint8_t *src, const uint8_t *topright,
-;                                        ptrdiff_t stride)
-;------------------------------------------------------------------------------
-
-INIT_MMX mmxext
-cglobal pred4x4_vertical_left_8, 3,3
-    sub       r0, r2
-    movq      m1, [r0]
-    punpckldq m1, [r1]
-    movq      m3, m1
-    movq      m2, m1
-    psrlq     m3, 8
-    psrlq     m2, 16
-    movq      m4, m3
-    pavgb     m4, m1
-    PRED4x4_LOWPASS m0, m1, m2, m3, m5
-    lea       r1, [r0+r2*2]
-    movh      [r0+r2*1], m4
-    movh      [r0+r2*2], m0
-    psrlq     m4, 8
-    psrlq     m0, 8
-    movh      [r1+r2*1], m4
-    movh      [r1+r2*2], m0
-    RET
-
-;------------------------------------------------------------------------------
-; void ff_pred4x4_horizontal_up_8_mmxext(uint8_t *src, const uint8_t *topright,
-;                                        ptrdiff_t stride)
-;------------------------------------------------------------------------------
-
-INIT_MMX mmxext
-cglobal pred4x4_horizontal_up_8, 3,3
-    sub       r0, r2
-    lea       r1, [r0+r2*2]
-    movd      m0, [r0+r2*1-4]
-    punpcklbw m0, [r0+r2*2-4]
-    movd      m1, [r1+r2*1-4]
-    punpcklbw m1, [r1+r2*2-4]
-    punpckhwd m0, m1
-    movq      m1, m0
-    punpckhbw m1, m1
-    pshufw    m1, m1, 0xFF
-    punpckhdq m0, m1
-    movq      m2, m0
-    movq      m3, m0
-    movq      m7, m0
-    psrlq     m2, 16
-    psrlq     m3, 8
-    pavgb     m7, m3
-    PRED4x4_LOWPASS m4, m0, m2, m3, m5
-    punpcklbw m7, m4
-    movd    [r0+r2*1], m7
-    psrlq    m7, 16
-    movd    [r0+r2*2], m7
-    psrlq    m7, 16
-    movd    [r1+r2*1], m7
-    movd    [r1+r2*2], m1
-    RET
-
-;------------------------------------------------------------------------------
-; void ff_pred4x4_horizontal_down_8_mmxext(uint8_t *src,
-;                                          const uint8_t *topright,
-;                                          ptrdiff_t stride)
-;------------------------------------------------------------------------------
-
-INIT_MMX mmxext
-cglobal pred4x4_horizontal_down_8, 3,3
-    sub       r0, r2
-    lea       r1, [r0+r2*2]
-    movh      m0, [r0-4]      ; lt ..
-    punpckldq m0, [r0]        ; t3 t2 t1 t0 lt .. .. ..
-    psllq     m0, 8           ; t2 t1 t0 lt .. .. .. ..
-    movd      m1, [r1+r2*2-4] ; l3
-    punpcklbw m1, [r1+r2*1-4] ; l2 l3
-    movd      m2, [r0+r2*2-4] ; l1
-    punpcklbw m2, [r0+r2*1-4] ; l0 l1
-    punpckhwd m1, m2          ; l0 l1 l2 l3
-    punpckhdq m1, m0          ; t2 t1 t0 lt l0 l1 l2 l3
-    movq      m0, m1
-    movq      m2, m1
-    movq      m5, m1
-    psrlq     m0, 16          ; .. .. t2 t1 t0 lt l0 l1
-    psrlq     m2, 8           ; .. t2 t1 t0 lt l0 l1 l2
-    pavgb     m5, m2
-    PRED4x4_LOWPASS m3, m1, m0, m2, m4
-    punpcklbw m5, m3
-    psrlq     m3, 32
-    PALIGNR   m3, m5, 6, m4
-    movh      [r1+r2*2], m5
-    psrlq     m5, 16
-    movh      [r1+r2*1], m5
-    psrlq     m5, 16
-    movh      [r0+r2*2], m5
-    movh      [r0+r2*1], m3
-    RET
-
-;-----------------------------------------------------------------------------
-; void ff_pred4x4_vertical_right_8_mmxext(uint8_t *src,
-;                                         const uint8_t *topright,
-;                                         ptrdiff_t stride)
-;-----------------------------------------------------------------------------
-
-INIT_MMX mmxext
-cglobal pred4x4_vertical_right_8, 3,3
-    sub     r0, r2
-    lea     r1, [r0+r2*2]
-    movh    m0, [r0]                    ; ........t3t2t1t0
-    movq    m5, m0
-    PALIGNR m0, [r0-8], 7, m1           ; ......t3t2t1t0lt
-    pavgb   m5, m0
-    PALIGNR m0, [r0+r2*1-8], 7, m1      ; ....t3t2t1t0ltl0
-    movq    m1, m0
-    PALIGNR m0, [r0+r2*2-8], 7, m2      ; ..t3t2t1t0ltl0l1
-    movq    m2, m0
-    PALIGNR m0, [r1+r2*1-8], 7, m3      ; t3t2t1t0ltl0l1l2
-    PRED4x4_LOWPASS m3, m1, m0, m2, m4
-    movq    m1, m3
-    psrlq   m3, 16
-    psllq   m1, 48
-    movh    [r0+r2*1], m5
-    movh    [r0+r2*2], m3
-    PALIGNR m5, m1, 7, m2
-    psllq   m1, 8
-    movh    [r1+r2*1], m5
-    PALIGNR m3, m1, 7, m1
-    movh    [r1+r2*2], m3
-    RET
-
-;-----------------------------------------------------------------------------
-; void ff_pred4x4_down_right_8_mmxext(uint8_t *src, const uint8_t *topright,
-;                                     ptrdiff_t stride)
-;-----------------------------------------------------------------------------
-
-INIT_MMX mmxext
-cglobal pred4x4_down_right_8, 3,3
-    sub       r0, r2
-    lea       r1, [r0+r2*2]
-    movq      m1, [r1-8]
-    movq      m2, [r0+r2*1-8]
-    punpckhbw m2, [r0-8]
-    movh      m3, [r0]
-    punpckhwd m1, m2
-    PALIGNR   m3, m1, 5, m1
-    movq      m1, m3
-    PALIGNR   m3, [r1+r2*1-8], 7, m4
-    movq      m2, m3
-    PALIGNR   m3, [r1+r2*2-8], 7, m4
-    PRED4x4_LOWPASS m0, m3, m1, m2, m4
-    movh      [r1+r2*2], m0
-    psrlq     m0, 8
-    movh      [r1+r2*1], m0
-    psrlq     m0, 8
-    movh      [r0+r2*2], m0
-    psrlq     m0, 8
-    movh      [r0+r2*1], m0
-    RET
diff -uparN ffmpeg-4.1/libavcodec/x86/h264_qpel_10bit.asm ffmpeg-y/libavcodec/x86/h264_qpel_10bit.asm
--- ffmpeg-4.1/libavcodec/x86/h264_qpel_10bit.asm	2018-07-17 17:27:41.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/h264_qpel_10bit.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,884 +0,0 @@
-;*****************************************************************************
-;* MMX/SSE2/AVX-optimized 10-bit H.264 qpel code
-;*****************************************************************************
-;* Copyright (C) 2011 x264 project
-;*
-;* Authors: Daniel Kang <daniel.d.kang@gmail.com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA 32
-
-cextern pd_65535
-cextern pw_1023
-%define pw_pixel_max pw_1023
-cextern pw_16
-cextern pw_1
-cextern pb_0
-
-pad10: times 8 dw 10*1023
-pad20: times 8 dw 20*1023
-pad30: times 8 dw 30*1023
-depad: times 4 dd 32*20*1023 + 512
-depad2: times 8 dw 20*1023 + 16*1022 + 16
-unpad: times 8 dw 16*1022/32 ; needs to be mod 16
-
-tap1: times 4 dw  1, -5
-tap2: times 4 dw 20, 20
-tap3: times 4 dw -5,  1
-
-SECTION .text
-
-
-%macro AVG_MOV 2
-    pavgw %2, %1
-    mova  %1, %2
-%endmacro
-
-%macro ADDW 3
-%if mmsize == 8
-    paddw %1, %2
-%else
-    movu  %3, %2
-    paddw %1, %3
-%endif
-%endmacro
-
-%macro FILT_H 4
-    paddw  %1, %4
-    psubw  %1, %2  ; a-b
-    psraw  %1, 2   ; (a-b)/4
-    psubw  %1, %2  ; (a-b)/4-b
-    paddw  %1, %3  ; (a-b)/4-b+c
-    psraw  %1, 2   ; ((a-b)/4-b+c)/4
-    paddw  %1, %3  ; ((a-b)/4-b+c)/4+c = (a-5*b+20*c)/16
-%endmacro
-
-%macro PRELOAD_V 0
-    lea      r3, [r2*3]
-    sub      r1, r3
-    movu     m0, [r1+r2]
-    movu     m1, [r1+r2*2]
-    add      r1, r3
-    movu     m2, [r1]
-    movu     m3, [r1+r2]
-    movu     m4, [r1+r2*2]
-    add      r1, r3
-%endmacro
-
-%macro FILT_V 8
-    movu     %6, [r1]
-    paddw    %1, %6
-    mova     %7, %2
-    paddw    %7, %5
-    mova     %8, %3
-    paddw    %8, %4
-    FILT_H   %1, %7, %8, [pw_16]
-    psraw    %1, 1
-    CLIPW    %1, [pb_0], [pw_pixel_max]
-%endmacro
-
-%macro MC 1
-%define OP_MOV mova
-INIT_MMX mmxext
-%1 put, 4
-INIT_XMM sse2
-%1 put, 8
-
-%define OP_MOV AVG_MOV
-INIT_MMX mmxext
-%1 avg, 4
-INIT_XMM sse2
-%1 avg, 8
-%endmacro
-
-%macro MCAxA_OP 7
-%if ARCH_X86_32
-cglobal %1_h264_qpel%4_%2_10, %5,%6,%7
-    call stub_%1_h264_qpel%3_%2_10 %+ SUFFIX
-    mov  r0, r0m
-    mov  r1, r1m
-    add  r0, %3*2
-    add  r1, %3*2
-    call stub_%1_h264_qpel%3_%2_10 %+ SUFFIX
-    mov  r0, r0m
-    mov  r1, r1m
-    lea  r0, [r0+r2*%3]
-    lea  r1, [r1+r2*%3]
-    call stub_%1_h264_qpel%3_%2_10 %+ SUFFIX
-    mov  r0, r0m
-    mov  r1, r1m
-    lea  r0, [r0+r2*%3+%3*2]
-    lea  r1, [r1+r2*%3+%3*2]
-    call stub_%1_h264_qpel%3_%2_10 %+ SUFFIX
-    RET
-%else ; ARCH_X86_64
-cglobal %1_h264_qpel%4_%2_10, %5,%6 + 2,%7
-    mov r%6, r0
-%assign p1 %6+1
-    mov r %+ p1, r1
-    call stub_%1_h264_qpel%3_%2_10 %+ SUFFIX
-    lea  r0, [r%6+%3*2]
-    lea  r1, [r %+ p1+%3*2]
-    call stub_%1_h264_qpel%3_%2_10 %+ SUFFIX
-    lea  r0, [r%6+r2*%3]
-    lea  r1, [r %+ p1+r2*%3]
-    call stub_%1_h264_qpel%3_%2_10 %+ SUFFIX
-    lea  r0, [r%6+r2*%3+%3*2]
-    lea  r1, [r %+ p1+r2*%3+%3*2]
-%if UNIX64 == 0 ; fall through to function
-    call stub_%1_h264_qpel%3_%2_10 %+ SUFFIX
-    RET
-%endif
-%endif
-%endmacro
-
-;cpu, put/avg, mc, 4/8, ...
-%macro cglobal_mc 6
-%assign i %3*2
-%if ARCH_X86_32 || cpuflag(sse2)
-MCAxA_OP %1, %2, %3, i, %4,%5,%6
-%endif
-
-cglobal %1_h264_qpel%3_%2_10, %4,%5,%6
-%if UNIX64 == 0 ; no prologue or epilogue for UNIX64
-    call stub_%1_h264_qpel%3_%2_10 %+ SUFFIX
-    RET
-%endif
-
-stub_%1_h264_qpel%3_%2_10 %+ SUFFIX:
-%endmacro
-
-;-----------------------------------------------------------------------------
-; void ff_h264_qpel_mc00(uint8_t *dst, uint8_t *src, int stride)
-;-----------------------------------------------------------------------------
-%macro COPY4 0
-    movu          m0, [r1     ]
-    OP_MOV [r0     ], m0
-    movu          m0, [r1+r2  ]
-    OP_MOV [r0+r2  ], m0
-    movu          m0, [r1+r2*2]
-    OP_MOV [r0+r2*2], m0
-    movu          m0, [r1+r3  ]
-    OP_MOV [r0+r3  ], m0
-%endmacro
-
-%macro MC00 1
-INIT_MMX mmxext
-cglobal_mc %1, mc00, 4, 3,4,0
-    lea           r3, [r2*3]
-    COPY4
-    ret
-
-INIT_XMM sse2
-cglobal %1_h264_qpel8_mc00_10, 3,4
-    lea  r3, [r2*3]
-    COPY4
-    lea  r0, [r0+r2*4]
-    lea  r1, [r1+r2*4]
-    COPY4
-    RET
-
-cglobal %1_h264_qpel16_mc00_10, 3,4
-    mov r3d, 8
-.loop:
-    movu           m0, [r1      ]
-    movu           m1, [r1   +16]
-    OP_MOV [r0      ], m0
-    OP_MOV [r0   +16], m1
-    movu           m0, [r1+r2   ]
-    movu           m1, [r1+r2+16]
-    OP_MOV [r0+r2   ], m0
-    OP_MOV [r0+r2+16], m1
-    lea            r0, [r0+r2*2]
-    lea            r1, [r1+r2*2]
-    dec r3d
-    jg .loop
-    REP_RET
-%endmacro
-
-%define OP_MOV mova
-MC00 put
-
-%define OP_MOV AVG_MOV
-MC00 avg
-
-;-----------------------------------------------------------------------------
-; void ff_h264_qpel_mc20(uint8_t *dst, uint8_t *src, int stride)
-;-----------------------------------------------------------------------------
-%macro MC_CACHE 1
-%define OP_MOV mova
-INIT_MMX mmxext
-%1 put, 4
-INIT_XMM sse2, cache64
-%1 put, 8
-INIT_XMM ssse3, cache64
-%1 put, 8
-INIT_XMM sse2
-%1 put, 8
-
-%define OP_MOV AVG_MOV
-INIT_MMX mmxext
-%1 avg, 4
-INIT_XMM sse2, cache64
-%1 avg, 8
-INIT_XMM ssse3, cache64
-%1 avg, 8
-INIT_XMM sse2
-%1 avg, 8
-%endmacro
-
-%macro MC20 2
-cglobal_mc %1, mc20, %2, 3,4,9
-    mov     r3d, %2
-    mova     m1, [pw_pixel_max]
-%if num_mmregs > 8
-    mova     m8, [pw_16]
-    %define p16 m8
-%else
-    %define p16 [pw_16]
-%endif
-.nextrow:
-%if %0 == 4
-    movu     m2, [r1-4]
-    movu     m3, [r1-2]
-    movu     m4, [r1+0]
-    ADDW     m2, [r1+6], m5
-    ADDW     m3, [r1+4], m5
-    ADDW     m4, [r1+2], m5
-%else ; movu is slow on these processors
-%if mmsize==16
-    movu     m2, [r1-4]
-    movu     m0, [r1+6]
-    mova     m6, m0
-    psrldq   m0, 6
-
-    paddw    m6, m2
-    PALIGNR  m3, m0, m2, 2, m5
-    PALIGNR  m7, m0, m2, 8, m5
-    paddw    m3, m7
-    PALIGNR  m4, m0, m2, 4, m5
-    PALIGNR  m7, m0, m2, 6, m5
-    paddw    m4, m7
-    SWAP      2, 6
-%else
-    movu     m2, [r1-4]
-    movu     m6, [r1+4]
-    PALIGNR  m3, m6, m2, 2, m5
-    paddw    m3, m6
-    PALIGNR  m4, m6, m2, 4, m5
-    PALIGNR  m7, m6, m2, 6, m5
-    paddw    m4, m7
-    paddw    m2, [r1+6]
-%endif
-%endif
-
-    FILT_H   m2, m3, m4, p16
-    psraw    m2, 1
-    pxor     m0, m0
-    CLIPW    m2, m0, m1
-    OP_MOV [r0], m2
-    add      r0, r2
-    add      r1, r2
-    dec     r3d
-    jg .nextrow
-    rep ret
-%endmacro
-
-MC_CACHE MC20
-
-;-----------------------------------------------------------------------------
-; void ff_h264_qpel_mc30(uint8_t *dst, uint8_t *src, int stride)
-;-----------------------------------------------------------------------------
-%macro MC30 2
-cglobal_mc %1, mc30, %2, 3,5,9
-    lea r4, [r1+2]
-    jmp stub_%1_h264_qpel%2_mc10_10 %+ SUFFIX %+ .body
-%endmacro
-
-MC_CACHE MC30
-
-;-----------------------------------------------------------------------------
-; void ff_h264_qpel_mc10(uint8_t *dst, uint8_t *src, int stride)
-;-----------------------------------------------------------------------------
-%macro MC10 2
-cglobal_mc %1, mc10, %2, 3,5,9
-    mov      r4, r1
-.body:
-    mov     r3d, %2
-    mova     m1, [pw_pixel_max]
-%if num_mmregs > 8
-    mova     m8, [pw_16]
-    %define p16 m8
-%else
-    %define p16 [pw_16]
-%endif
-.nextrow:
-%if %0 == 4
-    movu     m2, [r1-4]
-    movu     m3, [r1-2]
-    movu     m4, [r1+0]
-    ADDW     m2, [r1+6], m5
-    ADDW     m3, [r1+4], m5
-    ADDW     m4, [r1+2], m5
-%else ; movu is slow on these processors
-%if mmsize==16
-    movu     m2, [r1-4]
-    movu     m0, [r1+6]
-    mova     m6, m0
-    psrldq   m0, 6
-
-    paddw    m6, m2
-    PALIGNR  m3, m0, m2, 2, m5
-    PALIGNR  m7, m0, m2, 8, m5
-    paddw    m3, m7
-    PALIGNR  m4, m0, m2, 4, m5
-    PALIGNR  m7, m0, m2, 6, m5
-    paddw    m4, m7
-    SWAP      2, 6
-%else
-    movu     m2, [r1-4]
-    movu     m6, [r1+4]
-    PALIGNR  m3, m6, m2, 2, m5
-    paddw    m3, m6
-    PALIGNR  m4, m6, m2, 4, m5
-    PALIGNR  m7, m6, m2, 6, m5
-    paddw    m4, m7
-    paddw    m2, [r1+6]
-%endif
-%endif
-
-    FILT_H   m2, m3, m4, p16
-    psraw    m2, 1
-    pxor     m0, m0
-    CLIPW    m2, m0, m1
-    movu     m3, [r4]
-    pavgw    m2, m3
-    OP_MOV [r0], m2
-    add      r0, r2
-    add      r1, r2
-    add      r4, r2
-    dec     r3d
-    jg .nextrow
-    rep ret
-%endmacro
-
-MC_CACHE MC10
-
-;-----------------------------------------------------------------------------
-; void ff_h264_qpel_mc02(uint8_t *dst, uint8_t *src, int stride)
-;-----------------------------------------------------------------------------
-%macro V_FILT 10
-v_filt%9_%10_10:
-    add    r4, r2
-.no_addr4:
-    FILT_V m0, m1, m2, m3, m4, m5, m6, m7
-    add    r1, r2
-    add    r0, r2
-    ret
-%endmacro
-
-INIT_MMX mmxext
-RESET_MM_PERMUTATION
-%assign i 0
-%rep 4
-V_FILT m0, m1, m2, m3, m4, m5, m6, m7, 4, i
-SWAP 0,1,2,3,4,5
-%assign i i+1
-%endrep
-
-INIT_XMM sse2
-RESET_MM_PERMUTATION
-%assign i 0
-%rep 6
-V_FILT m0, m1, m2, m3, m4, m5, m6, m7, 8, i
-SWAP 0,1,2,3,4,5
-%assign i i+1
-%endrep
-
-%macro MC02 2
-cglobal_mc %1, mc02, %2, 3,4,8
-    PRELOAD_V
-
-    sub      r0, r2
-%assign j 0
-%rep %2
-    %assign i (j % 6)
-    call v_filt%2_ %+ i %+ _10.no_addr4
-    OP_MOV [r0], m0
-    SWAP 0,1,2,3,4,5
-    %assign j j+1
-%endrep
-    ret
-%endmacro
-
-MC MC02
-
-;-----------------------------------------------------------------------------
-; void ff_h264_qpel_mc01(uint8_t *dst, uint8_t *src, int stride)
-;-----------------------------------------------------------------------------
-%macro MC01 2
-cglobal_mc %1, mc01, %2, 3,5,8
-    mov      r4, r1
-.body:
-    PRELOAD_V
-
-    sub      r4, r2
-    sub      r0, r2
-%assign j 0
-%rep %2
-    %assign i (j % 6)
-    call v_filt%2_ %+ i %+ _10
-    movu     m7, [r4]
-    pavgw    m0, m7
-    OP_MOV [r0], m0
-    SWAP 0,1,2,3,4,5
-    %assign j j+1
-%endrep
-    ret
-%endmacro
-
-MC MC01
-
-;-----------------------------------------------------------------------------
-; void ff_h264_qpel_mc03(uint8_t *dst, uint8_t *src, int stride)
-;-----------------------------------------------------------------------------
-%macro MC03 2
-cglobal_mc %1, mc03, %2, 3,5,8
-    lea r4, [r1+r2]
-    jmp stub_%1_h264_qpel%2_mc01_10 %+ SUFFIX %+ .body
-%endmacro
-
-MC MC03
-
-;-----------------------------------------------------------------------------
-; void ff_h264_qpel_mc11(uint8_t *dst, uint8_t *src, int stride)
-;-----------------------------------------------------------------------------
-%macro H_FILT_AVG 2-3
-h_filt%1_%2_10:
-;FILT_H with fewer registers and averaged with the FILT_V result
-;m6,m7 are tmp registers, m0 is the FILT_V result, the rest are to be used next in the next iteration
-;unfortunately I need three registers, so m5 will have to be re-read from memory
-    movu     m5, [r4-4]
-    ADDW     m5, [r4+6], m7
-    movu     m6, [r4-2]
-    ADDW     m6, [r4+4], m7
-    paddw    m5, [pw_16]
-    psubw    m5, m6  ; a-b
-    psraw    m5, 2   ; (a-b)/4
-    psubw    m5, m6  ; (a-b)/4-b
-    movu     m6, [r4+0]
-    ADDW     m6, [r4+2], m7
-    paddw    m5, m6  ; (a-b)/4-b+c
-    psraw    m5, 2   ; ((a-b)/4-b+c)/4
-    paddw    m5, m6  ; ((a-b)/4-b+c)/4+c = (a-5*b+20*c)/16
-    psraw    m5, 1
-    CLIPW    m5, [pb_0], [pw_pixel_max]
-;avg FILT_V, FILT_H
-    pavgw    m0, m5
-%if %0!=4
-    movu     m5, [r1+r5]
-%endif
-    ret
-%endmacro
-
-INIT_MMX mmxext
-RESET_MM_PERMUTATION
-%assign i 0
-%rep 3
-H_FILT_AVG 4, i
-SWAP 0,1,2,3,4,5
-%assign i i+1
-%endrep
-H_FILT_AVG 4, i, 0
-
-INIT_XMM sse2
-RESET_MM_PERMUTATION
-%assign i 0
-%rep 6
-%if i==1
-H_FILT_AVG 8, i, 0
-%else
-H_FILT_AVG 8, i
-%endif
-SWAP 0,1,2,3,4,5
-%assign i i+1
-%endrep
-
-%macro MC11 2
-; this REALLY needs x86_64
-cglobal_mc %1, mc11, %2, 3,6,8
-    mov      r4, r1
-.body:
-    PRELOAD_V
-
-    sub      r0, r2
-    sub      r4, r2
-    mov      r5, r2
-    neg      r5
-%assign j 0
-%rep %2
-    %assign i (j % 6)
-    call v_filt%2_ %+ i %+ _10
-    call h_filt%2_ %+ i %+ _10
-%if %2==8 && i==1
-    movu     m5, [r1+r5]
-%endif
-    OP_MOV [r0], m0
-    SWAP 0,1,2,3,4,5
-    %assign j j+1
-%endrep
-    ret
-%endmacro
-
-MC MC11
-
-;-----------------------------------------------------------------------------
-; void ff_h264_qpel_mc31(uint8_t *dst, uint8_t *src, int stride)
-;-----------------------------------------------------------------------------
-%macro MC31 2
-cglobal_mc %1, mc31, %2, 3,6,8
-    mov r4, r1
-    add r1, 2
-    jmp stub_%1_h264_qpel%2_mc11_10 %+ SUFFIX %+ .body
-%endmacro
-
-MC MC31
-
-;-----------------------------------------------------------------------------
-; void ff_h264_qpel_mc13(uint8_t *dst, uint8_t *src, int stride)
-;-----------------------------------------------------------------------------
-%macro MC13 2
-cglobal_mc %1, mc13, %2, 3,7,12
-    lea r4, [r1+r2]
-    jmp stub_%1_h264_qpel%2_mc11_10 %+ SUFFIX %+ .body
-%endmacro
-
-MC MC13
-
-;-----------------------------------------------------------------------------
-; void ff_h264_qpel_mc33(uint8_t *dst, uint8_t *src, int stride)
-;-----------------------------------------------------------------------------
-%macro MC33 2
-cglobal_mc %1, mc33, %2, 3,6,8
-    lea r4, [r1+r2]
-    add r1, 2
-    jmp stub_%1_h264_qpel%2_mc11_10 %+ SUFFIX %+ .body
-%endmacro
-
-MC MC33
-
-;-----------------------------------------------------------------------------
-; void ff_h264_qpel_mc22(uint8_t *dst, uint8_t *src, int stride)
-;-----------------------------------------------------------------------------
-%macro FILT_H2 3
-    psubw  %1, %2  ; a-b
-    psubw  %2, %3  ; b-c
-    psllw  %2, 2
-    psubw  %1, %2  ; a-5*b+4*c
-    psllw  %3, 4
-    paddw  %1, %3  ; a-5*b+20*c
-%endmacro
-
-%macro FILT_VNRD 8
-    movu     %6, [r1]
-    paddw    %1, %6
-    mova     %7, %2
-    paddw    %7, %5
-    mova     %8, %3
-    paddw    %8, %4
-    FILT_H2  %1, %7, %8
-%endmacro
-
-%macro HV 1
-%if mmsize==16
-%define PAD 12
-%define COUNT 2
-%else
-%define PAD 4
-%define COUNT 3
-%endif
-put_hv%1_10:
-    neg      r2           ; This actually saves instructions
-    lea      r1, [r1+r2*2-mmsize+PAD]
-    lea      r4, [rsp+PAD+gprsize]
-    mov     r3d, COUNT
-.v_loop:
-    movu     m0, [r1]
-    sub      r1, r2
-    movu     m1, [r1]
-    sub      r1, r2
-    movu     m2, [r1]
-    sub      r1, r2
-    movu     m3, [r1]
-    sub      r1, r2
-    movu     m4, [r1]
-    sub      r1, r2
-%assign i 0
-%rep %1-1
-    FILT_VNRD m0, m1, m2, m3, m4, m5, m6, m7
-    psubw    m0, [pad20]
-    movu     [r4+i*mmsize*3], m0
-    sub      r1, r2
-    SWAP 0,1,2,3,4,5
-%assign i i+1
-%endrep
-    FILT_VNRD m0, m1, m2, m3, m4, m5, m6, m7
-    psubw    m0, [pad20]
-    movu     [r4+i*mmsize*3], m0
-    add      r4, mmsize
-    lea      r1, [r1+r2*8+mmsize]
-%if %1==8
-    lea      r1, [r1+r2*4]
-%endif
-    dec      r3d
-    jg .v_loop
-    neg      r2
-    ret
-%endmacro
-
-INIT_MMX mmxext
-HV 4
-INIT_XMM sse2
-HV 8
-
-%macro H_LOOP 1
-%if num_mmregs > 8
-    %define s1 m8
-    %define s2 m9
-    %define s3 m10
-    %define d1 m11
-%else
-    %define s1 [tap1]
-    %define s2 [tap2]
-    %define s3 [tap3]
-    %define d1 [depad]
-%endif
-h%1_loop_op:
-    movu       m1, [r1+mmsize-4]
-    movu       m2, [r1+mmsize-2]
-    mova       m3, [r1+mmsize+0]
-    movu       m4, [r1+mmsize+2]
-    movu       m5, [r1+mmsize+4]
-    movu       m6, [r1+mmsize+6]
-%if num_mmregs > 8
-    pmaddwd    m1, s1
-    pmaddwd    m2, s1
-    pmaddwd    m3, s2
-    pmaddwd    m4, s2
-    pmaddwd    m5, s3
-    pmaddwd    m6, s3
-    paddd      m1, d1
-    paddd      m2, d1
-%else
-    mova       m0, s1
-    pmaddwd    m1, m0
-    pmaddwd    m2, m0
-    mova       m0, s2
-    pmaddwd    m3, m0
-    pmaddwd    m4, m0
-    mova       m0, s3
-    pmaddwd    m5, m0
-    pmaddwd    m6, m0
-    mova       m0, d1
-    paddd      m1, m0
-    paddd      m2, m0
-%endif
-    paddd      m3, m5
-    paddd      m4, m6
-    paddd      m1, m3
-    paddd      m2, m4
-    psrad      m1, 10
-    psrad      m2, 10
-    pslld      m2, 16
-    pand       m1, [pd_65535]
-    por        m1, m2
-%if num_mmregs <= 8
-    pxor       m0, m0
-%endif
-    CLIPW      m1, m0, m7
-    add        r1, mmsize*3
-    ret
-%endmacro
-
-INIT_MMX mmxext
-H_LOOP 4
-INIT_XMM sse2
-H_LOOP 8
-
-%macro MC22 2
-cglobal_mc %1, mc22, %2, 3,7,12
-%define PAD mmsize*8*4*2      ; SIZE*16*4*sizeof(pixel)
-    mov      r6, rsp          ; backup stack pointer
-    and     rsp, ~(mmsize-1)  ; align stack
-    sub     rsp, PAD
-
-    call put_hv%2_10
-
-    mov       r3d, %2
-    mova       m7, [pw_pixel_max]
-%if num_mmregs > 8
-    pxor       m0, m0
-    mova       m8, [tap1]
-    mova       m9, [tap2]
-    mova      m10, [tap3]
-    mova      m11, [depad]
-%endif
-    mov        r1, rsp
-.h_loop:
-    call h%2_loop_op
-
-    OP_MOV   [r0], m1
-    add        r0, r2
-    dec       r3d
-    jg .h_loop
-
-    mov     rsp, r6          ; restore stack pointer
-    ret
-%endmacro
-
-MC MC22
-
-;-----------------------------------------------------------------------------
-; void ff_h264_qpel_mc12(uint8_t *dst, uint8_t *src, int stride)
-;-----------------------------------------------------------------------------
-%macro MC12 2
-cglobal_mc %1, mc12, %2, 3,7,12
-%define PAD mmsize*8*4*2        ; SIZE*16*4*sizeof(pixel)
-    mov        r6, rsp          ; backup stack pointer
-    and       rsp, ~(mmsize-1)  ; align stack
-    sub       rsp, PAD
-
-    call put_hv%2_10
-
-    xor       r4d, r4d
-.body:
-    mov       r3d, %2
-    pxor       m0, m0
-    mova       m7, [pw_pixel_max]
-%if num_mmregs > 8
-    mova       m8, [tap1]
-    mova       m9, [tap2]
-    mova      m10, [tap3]
-    mova      m11, [depad]
-%endif
-    mov        r1, rsp
-.h_loop:
-    call h%2_loop_op
-
-    movu       m3, [r1+r4-2*mmsize] ; movu needed for mc32, etc
-    paddw      m3, [depad2]
-    psrlw      m3, 5
-    psubw      m3, [unpad]
-    CLIPW      m3, m0, m7
-    pavgw      m1, m3
-
-    OP_MOV   [r0], m1
-    add        r0, r2
-    dec       r3d
-    jg .h_loop
-
-    mov     rsp, r6          ; restore stack pointer
-    ret
-%endmacro
-
-MC MC12
-
-;-----------------------------------------------------------------------------
-; void ff_h264_qpel_mc32(uint8_t *dst, uint8_t *src, int stride)
-;-----------------------------------------------------------------------------
-%macro MC32 2
-cglobal_mc %1, mc32, %2, 3,7,12
-%define PAD mmsize*8*3*2  ; SIZE*16*4*sizeof(pixel)
-    mov  r6, rsp          ; backup stack pointer
-    and rsp, ~(mmsize-1)  ; align stack
-    sub rsp, PAD
-
-    call put_hv%2_10
-
-    mov r4d, 2            ; sizeof(pixel)
-    jmp stub_%1_h264_qpel%2_mc12_10 %+ SUFFIX %+ .body
-%endmacro
-
-MC MC32
-
-;-----------------------------------------------------------------------------
-; void ff_h264_qpel_mc21(uint8_t *dst, uint8_t *src, int stride)
-;-----------------------------------------------------------------------------
-%macro H_NRD 1
-put_h%1_10:
-    add       rsp, gprsize
-    mov       r3d, %1
-    xor       r4d, r4d
-    mova       m6, [pad20]
-.nextrow:
-    movu       m2, [r5-4]
-    movu       m3, [r5-2]
-    movu       m4, [r5+0]
-    ADDW       m2, [r5+6], m5
-    ADDW       m3, [r5+4], m5
-    ADDW       m4, [r5+2], m5
-
-    FILT_H2    m2, m3, m4
-    psubw      m2, m6
-    mova [rsp+r4], m2
-    add       r4d, mmsize*3
-    add        r5, r2
-    dec       r3d
-    jg .nextrow
-    sub       rsp, gprsize
-    ret
-%endmacro
-
-INIT_MMX mmxext
-H_NRD 4
-INIT_XMM sse2
-H_NRD 8
-
-%macro MC21 2
-cglobal_mc %1, mc21, %2, 3,7,12
-    mov   r5, r1
-.body:
-%define PAD mmsize*8*3*2   ; SIZE*16*4*sizeof(pixel)
-    mov   r6, rsp          ; backup stack pointer
-    and  rsp, ~(mmsize-1)  ; align stack
-
-    sub  rsp, PAD
-    call put_h%2_10
-
-    sub  rsp, PAD
-    call put_hv%2_10
-
-    mov r4d, PAD-mmsize    ; H buffer
-    jmp stub_%1_h264_qpel%2_mc12_10 %+ SUFFIX %+ .body
-%endmacro
-
-MC MC21
-
-;-----------------------------------------------------------------------------
-; void ff_h264_qpel_mc23(uint8_t *dst, uint8_t *src, int stride)
-;-----------------------------------------------------------------------------
-%macro MC23 2
-cglobal_mc %1, mc23, %2, 3,7,12
-    lea   r5, [r1+r2]
-    jmp stub_%1_h264_qpel%2_mc21_10 %+ SUFFIX %+ .body
-%endmacro
-
-MC MC23
diff -uparN ffmpeg-4.1/libavcodec/x86/h264_qpel_8bit.asm ffmpeg-y/libavcodec/x86/h264_qpel_8bit.asm
--- ffmpeg-4.1/libavcodec/x86/h264_qpel_8bit.asm	2016-03-29 10:25:25.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/h264_qpel_8bit.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,862 +0,0 @@
-;*****************************************************************************
-;* MMX/SSE2/SSSE3-optimized H.264 QPEL code
-;*****************************************************************************
-;* Copyright (c) 2004-2005 Michael Niedermayer, Loren Merritt
-;* Copyright (C) 2012 Daniel Kang
-;*
-;* Authors: Daniel Kang <daniel.d.kang@gmail.com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA 32
-
-cextern pw_16
-cextern pw_5
-cextern pb_0
-
-SECTION .text
-
-
-%macro op_avgh 3
-    movh   %3, %2
-    pavgb  %1, %3
-    movh   %2, %1
-%endmacro
-
-%macro op_avg 2-3
-    pavgb  %1, %2
-    mova   %2, %1
-%endmacro
-
-%macro op_puth 2-3
-    movh   %2, %1
-%endmacro
-
-%macro op_put 2-3
-    mova   %2, %1
-%endmacro
-
-%macro QPEL4_H_LOWPASS_OP 1
-cglobal %1_h264_qpel4_h_lowpass, 4,5 ; dst, src, dstStride, srcStride
-    movsxdifnidn  r2, r2d
-    movsxdifnidn  r3, r3d
-    pxor          m7, m7
-    mova          m4, [pw_5]
-    mova          m5, [pw_16]
-    mov          r4d, 4
-.loop:
-    movh          m1, [r1-1]
-    movh          m2, [r1+0]
-    movh          m3, [r1+1]
-    movh          m0, [r1+2]
-    punpcklbw     m1, m7
-    punpcklbw     m2, m7
-    punpcklbw     m3, m7
-    punpcklbw     m0, m7
-    paddw         m1, m0
-    paddw         m2, m3
-    movh          m0, [r1-2]
-    movh          m3, [r1+3]
-    punpcklbw     m0, m7
-    punpcklbw     m3, m7
-    paddw         m0, m3
-    psllw         m2, 2
-    psubw         m2, m1
-    pmullw        m2, m4
-    paddw         m0, m5
-    paddw         m0, m2
-    psraw         m0, 5
-    packuswb      m0, m0
-    op_%1h        m0, [r0], m6
-    add           r0, r2
-    add           r1, r3
-    dec          r4d
-    jg         .loop
-    REP_RET
-%endmacro
-
-INIT_MMX mmxext
-QPEL4_H_LOWPASS_OP put
-QPEL4_H_LOWPASS_OP avg
-
-%macro QPEL8_H_LOWPASS_OP 1
-cglobal %1_h264_qpel8_h_lowpass, 4,5 ; dst, src, dstStride, srcStride
-    movsxdifnidn  r2, r2d
-    movsxdifnidn  r3, r3d
-    mov          r4d, 8
-    pxor          m7, m7
-    mova          m6, [pw_5]
-.loop:
-    mova          m0, [r1]
-    mova          m2, [r1+1]
-    mova          m1, m0
-    mova          m3, m2
-    punpcklbw     m0, m7
-    punpckhbw     m1, m7
-    punpcklbw     m2, m7
-    punpckhbw     m3, m7
-    paddw         m0, m2
-    paddw         m1, m3
-    psllw         m0, 2
-    psllw         m1, 2
-    mova          m2, [r1-1]
-    mova          m4, [r1+2]
-    mova          m3, m2
-    mova          m5, m4
-    punpcklbw     m2, m7
-    punpckhbw     m3, m7
-    punpcklbw     m4, m7
-    punpckhbw     m5, m7
-    paddw         m2, m4
-    paddw         m5, m3
-    psubw         m0, m2
-    psubw         m1, m5
-    pmullw        m0, m6
-    pmullw        m1, m6
-    movd          m2, [r1-2]
-    movd          m5, [r1+7]
-    punpcklbw     m2, m7
-    punpcklbw     m5, m7
-    paddw         m2, m3
-    paddw         m4, m5
-    mova          m5, [pw_16]
-    paddw         m2, m5
-    paddw         m4, m5
-    paddw         m0, m2
-    paddw         m1, m4
-    psraw         m0, 5
-    psraw         m1, 5
-    packuswb      m0, m1
-    op_%1         m0, [r0], m4
-    add           r0, r2
-    add           r1, r3
-    dec          r4d
-    jg         .loop
-    REP_RET
-%endmacro
-
-INIT_MMX mmxext
-QPEL8_H_LOWPASS_OP put
-QPEL8_H_LOWPASS_OP avg
-
-%macro QPEL8_H_LOWPASS_OP_XMM 1
-cglobal %1_h264_qpel8_h_lowpass, 4,5,8 ; dst, src, dstStride, srcStride
-    movsxdifnidn  r2, r2d
-    movsxdifnidn  r3, r3d
-    mov          r4d, 8
-    pxor          m7, m7
-    mova          m6, [pw_5]
-.loop:
-    movu          m1, [r1-2]
-    mova          m0, m1
-    punpckhbw     m1, m7
-    punpcklbw     m0, m7
-    mova          m2, m1
-    mova          m3, m1
-    mova          m4, m1
-    mova          m5, m1
-    palignr       m4, m0, 2
-    palignr       m3, m0, 4
-    palignr       m2, m0, 6
-    palignr       m1, m0, 8
-    palignr       m5, m0, 10
-    paddw         m0, m5
-    paddw         m2, m3
-    paddw         m1, m4
-    psllw         m2, 2
-    psubw         m2, m1
-    paddw         m0, [pw_16]
-    pmullw        m2, m6
-    paddw         m2, m0
-    psraw         m2, 5
-    packuswb      m2, m2
-    op_%1h        m2, [r0], m4
-    add           r1, r3
-    add           r0, r2
-    dec          r4d
-    jne        .loop
-    REP_RET
-%endmacro
-
-INIT_XMM ssse3
-QPEL8_H_LOWPASS_OP_XMM put
-QPEL8_H_LOWPASS_OP_XMM avg
-
-
-%macro QPEL4_H_LOWPASS_L2_OP 1
-cglobal %1_h264_qpel4_h_lowpass_l2, 5,6 ; dst, src, src2, dstStride, srcStride
-    movsxdifnidn  r3, r3d
-    movsxdifnidn  r4, r4d
-    pxor          m7, m7
-    mova          m4, [pw_5]
-    mova          m5, [pw_16]
-    mov          r5d, 4
-.loop:
-    movh          m1, [r1-1]
-    movh          m2, [r1+0]
-    movh          m3, [r1+1]
-    movh          m0, [r1+2]
-    punpcklbw     m1, m7
-    punpcklbw     m2, m7
-    punpcklbw     m3, m7
-    punpcklbw     m0, m7
-    paddw         m1, m0
-    paddw         m2, m3
-    movh          m0, [r1-2]
-    movh          m3, [r1+3]
-    punpcklbw     m0, m7
-    punpcklbw     m3, m7
-    paddw         m0, m3
-    psllw         m2, 2
-    psubw         m2, m1
-    pmullw        m2, m4
-    paddw         m0, m5
-    paddw         m0, m2
-    movh          m3, [r2]
-    psraw         m0, 5
-    packuswb      m0, m0
-    pavgb         m0, m3
-    op_%1h        m0, [r0], m6
-    add           r0, r3
-    add           r1, r3
-    add           r2, r4
-    dec          r5d
-    jg         .loop
-    REP_RET
-%endmacro
-
-INIT_MMX mmxext
-QPEL4_H_LOWPASS_L2_OP put
-QPEL4_H_LOWPASS_L2_OP avg
-
-
-%macro QPEL8_H_LOWPASS_L2_OP 1
-cglobal %1_h264_qpel8_h_lowpass_l2, 5,6 ; dst, src, src2, dstStride, srcStride
-    movsxdifnidn  r3, r3d
-    movsxdifnidn  r4, r4d
-    mov          r5d, 8
-    pxor          m7, m7
-    mova          m6, [pw_5]
-.loop:
-    mova          m0, [r1]
-    mova          m2, [r1+1]
-    mova          m1, m0
-    mova          m3, m2
-    punpcklbw     m0, m7
-    punpckhbw     m1, m7
-    punpcklbw     m2, m7
-    punpckhbw     m3, m7
-    paddw         m0, m2
-    paddw         m1, m3
-    psllw         m0, 2
-    psllw         m1, 2
-    mova          m2, [r1-1]
-    mova          m4, [r1+2]
-    mova          m3, m2
-    mova          m5, m4
-    punpcklbw     m2, m7
-    punpckhbw     m3, m7
-    punpcklbw     m4, m7
-    punpckhbw     m5, m7
-    paddw         m2, m4
-    paddw         m5, m3
-    psubw         m0, m2
-    psubw         m1, m5
-    pmullw        m0, m6
-    pmullw        m1, m6
-    movd          m2, [r1-2]
-    movd          m5, [r1+7]
-    punpcklbw     m2, m7
-    punpcklbw     m5, m7
-    paddw         m2, m3
-    paddw         m4, m5
-    mova          m5, [pw_16]
-    paddw         m2, m5
-    paddw         m4, m5
-    paddw         m0, m2
-    paddw         m1, m4
-    psraw         m0, 5
-    psraw         m1, 5
-    mova          m4, [r2]
-    packuswb      m0, m1
-    pavgb         m0, m4
-    op_%1         m0, [r0], m4
-    add           r0, r3
-    add           r1, r3
-    add           r2, r4
-    dec          r5d
-    jg         .loop
-    REP_RET
-%endmacro
-
-INIT_MMX mmxext
-QPEL8_H_LOWPASS_L2_OP put
-QPEL8_H_LOWPASS_L2_OP avg
-
-
-%macro QPEL8_H_LOWPASS_L2_OP_XMM 1
-cglobal %1_h264_qpel8_h_lowpass_l2, 5,6,8 ; dst, src, src2, dstStride, src2Stride
-    movsxdifnidn  r3, r3d
-    movsxdifnidn  r4, r4d
-    mov          r5d, 8
-    pxor          m7, m7
-    mova          m6, [pw_5]
-.loop:
-    lddqu         m1, [r1-2]
-    mova          m0, m1
-    punpckhbw     m1, m7
-    punpcklbw     m0, m7
-    mova          m2, m1
-    mova          m3, m1
-    mova          m4, m1
-    mova          m5, m1
-    palignr       m4, m0, 2
-    palignr       m3, m0, 4
-    palignr       m2, m0, 6
-    palignr       m1, m0, 8
-    palignr       m5, m0, 10
-    paddw         m0, m5
-    paddw         m2, m3
-    paddw         m1, m4
-    psllw         m2, 2
-    movh          m3, [r2]
-    psubw         m2, m1
-    paddw         m0, [pw_16]
-    pmullw        m2, m6
-    paddw         m2, m0
-    psraw         m2, 5
-    packuswb      m2, m2
-    pavgb         m2, m3
-    op_%1h        m2, [r0], m4
-    add           r1, r3
-    add           r0, r3
-    add           r2, r4
-    dec          r5d
-    jg         .loop
-    REP_RET
-%endmacro
-
-INIT_XMM ssse3
-QPEL8_H_LOWPASS_L2_OP_XMM put
-QPEL8_H_LOWPASS_L2_OP_XMM avg
-
-
-; All functions that call this are required to have function arguments of
-; dst, src, dstStride, srcStride
-%macro FILT_V 1
-    mova      m6, m2
-    movh      m5, [r1]
-    paddw     m6, m3
-    psllw     m6, 2
-    psubw     m6, m1
-    psubw     m6, m4
-    punpcklbw m5, m7
-    pmullw    m6, [pw_5]
-    paddw     m0, [pw_16]
-    add       r1, r3
-    paddw     m0, m5
-    paddw     m6, m0
-    psraw     m6, 5
-    packuswb  m6, m6
-    op_%1h    m6, [r0], m0 ; 1
-    add       r0, r2
-    SWAP       0, 1, 2, 3, 4, 5
-%endmacro
-
-%macro QPEL4_V_LOWPASS_OP 1
-cglobal %1_h264_qpel4_v_lowpass, 4,4 ; dst, src, dstStride, srcStride
-    movsxdifnidn  r2, r2d
-    movsxdifnidn  r3, r3d
-    sub           r1, r3
-    sub           r1, r3
-    pxor          m7, m7
-    movh          m0, [r1]
-    movh          m1, [r1+r3]
-    lea           r1, [r1+2*r3]
-    movh          m2, [r1]
-    movh          m3, [r1+r3]
-    lea           r1, [r1+2*r3]
-    movh          m4, [r1]
-    add           r1, r3
-    punpcklbw     m0, m7
-    punpcklbw     m1, m7
-    punpcklbw     m2, m7
-    punpcklbw     m3, m7
-    punpcklbw     m4, m7
-    FILT_V        %1
-    FILT_V        %1
-    FILT_V        %1
-    FILT_V        %1
-    RET
-%endmacro
-
-INIT_MMX mmxext
-QPEL4_V_LOWPASS_OP put
-QPEL4_V_LOWPASS_OP avg
-
-
-
-%macro QPEL8OR16_V_LOWPASS_OP 1
-%if cpuflag(sse2)
-cglobal %1_h264_qpel8or16_v_lowpass, 5,5,8 ; dst, src, dstStride, srcStride, h
-    movsxdifnidn  r2, r2d
-    movsxdifnidn  r3, r3d
-    sub           r1, r3
-    sub           r1, r3
-%else
-cglobal %1_h264_qpel8or16_v_lowpass_op, 5,5,8 ; dst, src, dstStride, srcStride, h
-    movsxdifnidn  r2, r2d
-    movsxdifnidn  r3, r3d
-%endif
-    pxor          m7, m7
-    movh          m0, [r1]
-    movh          m1, [r1+r3]
-    lea           r1, [r1+2*r3]
-    movh          m2, [r1]
-    movh          m3, [r1+r3]
-    lea           r1, [r1+2*r3]
-    movh          m4, [r1]
-    add           r1, r3
-    punpcklbw     m0, m7
-    punpcklbw     m1, m7
-    punpcklbw     m2, m7
-    punpcklbw     m3, m7
-    punpcklbw     m4, m7
-    FILT_V        %1
-    FILT_V        %1
-    FILT_V        %1
-    FILT_V        %1
-    FILT_V        %1
-    FILT_V        %1
-    FILT_V        %1
-    FILT_V        %1
-    cmp          r4d, 16
-    jne         .end
-    FILT_V        %1
-    FILT_V        %1
-    FILT_V        %1
-    FILT_V        %1
-    FILT_V        %1
-    FILT_V        %1
-    FILT_V        %1
-    FILT_V        %1
-.end:
-    REP_RET
-%endmacro
-
-INIT_MMX mmxext
-QPEL8OR16_V_LOWPASS_OP put
-QPEL8OR16_V_LOWPASS_OP avg
-
-INIT_XMM sse2
-QPEL8OR16_V_LOWPASS_OP put
-QPEL8OR16_V_LOWPASS_OP avg
-
-
-; All functions that use this are required to have args:
-; src, tmp, srcSize
-%macro FILT_HV 1 ; offset
-    mova           m6, m2
-    movh           m5, [r0]
-    paddw          m6, m3
-    psllw          m6, 2
-    paddw          m0, [pw_16]
-    psubw          m6, m1
-    psubw          m6, m4
-    punpcklbw      m5, m7
-    pmullw         m6, [pw_5]
-    paddw          m0, m5
-    add            r0, r2
-    paddw          m6, m0
-    mova      [r1+%1], m6
-    SWAP            0, 1, 2, 3, 4, 5
-%endmacro
-
-%macro QPEL4_HV1_LOWPASS_OP 1
-cglobal %1_h264_qpel4_hv_lowpass_v, 3,3 ; src, tmp, srcStride
-    movsxdifnidn  r2, r2d
-    pxor          m7, m7
-    movh          m0, [r0]
-    movh          m1, [r0+r2]
-    lea           r0, [r0+2*r2]
-    movh          m2, [r0]
-    movh          m3, [r0+r2]
-    lea           r0, [r0+2*r2]
-    movh          m4, [r0]
-    add           r0, r2
-    punpcklbw     m0, m7
-    punpcklbw     m1, m7
-    punpcklbw     m2, m7
-    punpcklbw     m3, m7
-    punpcklbw     m4, m7
-    FILT_HV       0*24
-    FILT_HV       1*24
-    FILT_HV       2*24
-    FILT_HV       3*24
-    RET
-
-cglobal %1_h264_qpel4_hv_lowpass_h, 3,4 ; tmp, dst, dstStride
-    movsxdifnidn  r2, r2d
-    mov          r3d, 4
-.loop:
-    mova          m0, [r0]
-    paddw         m0, [r0+10]
-    mova          m1, [r0+2]
-    paddw         m1, [r0+8]
-    mova          m2, [r0+4]
-    paddw         m2, [r0+6]
-    psubw         m0, m1
-    psraw         m0, 2
-    psubw         m0, m1
-    paddsw        m0, m2
-    psraw         m0, 2
-    paddw         m0, m2
-    psraw         m0, 6
-    packuswb      m0, m0
-    op_%1h        m0, [r1], m7
-    add           r0, 24
-    add           r1, r2
-    dec          r3d
-    jnz        .loop
-    REP_RET
-%endmacro
-
-INIT_MMX mmxext
-QPEL4_HV1_LOWPASS_OP put
-QPEL4_HV1_LOWPASS_OP avg
-
-%macro QPEL8OR16_HV1_LOWPASS_OP 1
-cglobal %1_h264_qpel8or16_hv1_lowpass_op, 4,4,8 ; src, tmp, srcStride, size
-    movsxdifnidn  r2, r2d
-    pxor          m7, m7
-    movh          m0, [r0]
-    movh          m1, [r0+r2]
-    lea           r0, [r0+2*r2]
-    movh          m2, [r0]
-    movh          m3, [r0+r2]
-    lea           r0, [r0+2*r2]
-    movh          m4, [r0]
-    add           r0, r2
-    punpcklbw     m0, m7
-    punpcklbw     m1, m7
-    punpcklbw     m2, m7
-    punpcklbw     m3, m7
-    punpcklbw     m4, m7
-    FILT_HV     0*48
-    FILT_HV     1*48
-    FILT_HV     2*48
-    FILT_HV     3*48
-    FILT_HV     4*48
-    FILT_HV     5*48
-    FILT_HV     6*48
-    FILT_HV     7*48
-    cmp          r3d, 16
-    jne         .end
-    FILT_HV     8*48
-    FILT_HV     9*48
-    FILT_HV    10*48
-    FILT_HV    11*48
-    FILT_HV    12*48
-    FILT_HV    13*48
-    FILT_HV    14*48
-    FILT_HV    15*48
-.end:
-    REP_RET
-%endmacro
-
-INIT_MMX mmxext
-QPEL8OR16_HV1_LOWPASS_OP put
-QPEL8OR16_HV1_LOWPASS_OP avg
-
-INIT_XMM sse2
-QPEL8OR16_HV1_LOWPASS_OP put
-
-
-
-%macro QPEL8OR16_HV2_LOWPASS_OP 1
-; unused is to match ssse3 and mmxext args
-cglobal %1_h264_qpel8or16_hv2_lowpass_op, 5,5 ; dst, tmp, dstStride, unused, h
-    movsxdifnidn  r2, r2d
-.loop:
-    mova          m0, [r1]
-    mova          m3, [r1+8]
-    mova          m1, [r1+2]
-    mova          m4, [r1+10]
-    paddw         m0, m4
-    paddw         m1, m3
-    paddw         m3, [r1+18]
-    paddw         m4, [r1+16]
-    mova          m2, [r1+4]
-    mova          m5, [r1+12]
-    paddw         m2, [r1+6]
-    paddw         m5, [r1+14]
-    psubw         m0, m1
-    psubw         m3, m4
-    psraw         m0, 2
-    psraw         m3, 2
-    psubw         m0, m1
-    psubw         m3, m4
-    paddsw        m0, m2
-    paddsw        m3, m5
-    psraw         m0, 2
-    psraw         m3, 2
-    paddw         m0, m2
-    paddw         m3, m5
-    psraw         m0, 6
-    psraw         m3, 6
-    packuswb      m0, m3
-    op_%1         m0, [r0], m7
-    add           r1, 48
-    add           r0, r2
-    dec          r4d
-    jne        .loop
-    REP_RET
-%endmacro
-
-INIT_MMX mmxext
-QPEL8OR16_HV2_LOWPASS_OP put
-QPEL8OR16_HV2_LOWPASS_OP avg
-
-%macro QPEL8OR16_HV2_LOWPASS_OP_XMM 1
-cglobal %1_h264_qpel8or16_hv2_lowpass, 5,5,8 ; dst, tmp, dstStride, tmpStride, size
-    movsxdifnidn  r2, r2d
-    movsxdifnidn  r3, r3d
-    cmp          r4d, 16
-    je         .op16
-.loop8:
-    mova          m1, [r1+16]
-    mova          m0, [r1]
-    mova          m2, m1
-    mova          m3, m1
-    mova          m4, m1
-    mova          m5, m1
-    palignr       m5, m0, 10
-    palignr       m4, m0, 8
-    palignr       m3, m0, 6
-    palignr       m2, m0, 4
-    palignr       m1, m0, 2
-    paddw         m0, m5
-    paddw         m1, m4
-    paddw         m2, m3
-    psubw         m0, m1
-    psraw         m0, 2
-    psubw         m0, m1
-    paddw         m0, m2
-    psraw         m0, 2
-    paddw         m0, m2
-    psraw         m0, 6
-    packuswb      m0, m0
-    op_%1h        m0, [r0], m7
-    add           r1, 48
-    add           r0, r2
-    dec          r4d
-    jne       .loop8
-    jmp        .done
-.op16:
-    mova          m4, [r1+32]
-    mova          m5, [r1+16]
-    mova          m7, [r1]
-    mova          m3, m4
-    mova          m2, m4
-    mova          m1, m4
-    mova          m0, m4
-    palignr       m0, m5, 10
-    palignr       m1, m5, 8
-    palignr       m2, m5, 6
-    palignr       m3, m5, 4
-    palignr       m4, m5, 2
-    paddw         m0, m5
-    paddw         m1, m4
-    paddw         m2, m3
-    mova          m6, m5
-    mova          m4, m5
-    mova          m3, m5
-    palignr       m4, m7, 8
-    palignr       m6, m7, 2
-    palignr       m3, m7, 10
-    paddw         m4, m6
-    mova          m6, m5
-    palignr       m5, m7, 6
-    palignr       m6, m7, 4
-    paddw         m3, m7
-    paddw         m5, m6
-    psubw         m0, m1
-    psubw         m3, m4
-    psraw         m0, 2
-    psraw         m3, 2
-    psubw         m0, m1
-    psubw         m3, m4
-    paddw         m0, m2
-    paddw         m3, m5
-    psraw         m0, 2
-    psraw         m3, 2
-    paddw         m0, m2
-    paddw         m3, m5
-    psraw         m0, 6
-    psraw         m3, 6
-    packuswb      m3, m0
-    op_%1         m3, [r0], m7
-    add           r1, 48
-    add           r0, r2
-    dec          r4d
-    jne        .op16
-.done:
-    REP_RET
-%endmacro
-
-INIT_XMM ssse3
-QPEL8OR16_HV2_LOWPASS_OP_XMM put
-QPEL8OR16_HV2_LOWPASS_OP_XMM avg
-
-
-%macro PIXELS4_L2_SHIFT5 1
-cglobal %1_pixels4_l2_shift5,6,6 ; dst, src16, src8, dstStride, src8Stride, h
-    movsxdifnidn  r3, r3d
-    movsxdifnidn  r4, r4d
-    mova          m0, [r1]
-    mova          m1, [r1+24]
-    psraw         m0, 5
-    psraw         m1, 5
-    packuswb      m0, m0
-    packuswb      m1, m1
-    pavgb         m0, [r2]
-    pavgb         m1, [r2+r4]
-    op_%1h        m0, [r0], m4
-    op_%1h        m1, [r0+r3], m5
-    lea           r2, [r2+r4*2]
-    lea           r0, [r0+r3*2]
-    mova          m0, [r1+48]
-    mova          m1, [r1+72]
-    psraw         m0, 5
-    psraw         m1, 5
-    packuswb      m0, m0
-    packuswb      m1, m1
-    pavgb         m0, [r2]
-    pavgb         m1, [r2+r4]
-    op_%1h        m0, [r0], m4
-    op_%1h        m1, [r0+r3], m5
-    RET
-%endmacro
-
-INIT_MMX mmxext
-PIXELS4_L2_SHIFT5 put
-PIXELS4_L2_SHIFT5 avg
-
-
-%macro PIXELS8_L2_SHIFT5 1
-cglobal %1_pixels8_l2_shift5, 6, 6 ; dst, src16, src8, dstStride, src8Stride, h
-    movsxdifnidn  r3, r3d
-    movsxdifnidn  r4, r4d
-.loop:
-    mova          m0, [r1]
-    mova          m1, [r1+8]
-    mova          m2, [r1+48]
-    mova          m3, [r1+48+8]
-    psraw         m0, 5
-    psraw         m1, 5
-    psraw         m2, 5
-    psraw         m3, 5
-    packuswb      m0, m1
-    packuswb      m2, m3
-    pavgb         m0, [r2]
-    pavgb         m2, [r2+r4]
-    op_%1         m0, [r0], m4
-    op_%1         m2, [r0+r3], m5
-    lea           r2, [r2+2*r4]
-    add           r1, 48*2
-    lea           r0, [r0+2*r3]
-    sub          r5d, 2
-    jne        .loop
-    REP_RET
-%endmacro
-
-INIT_MMX mmxext
-PIXELS8_L2_SHIFT5 put
-PIXELS8_L2_SHIFT5 avg
-
-
-%if ARCH_X86_64
-%macro QPEL16_H_LOWPASS_L2_OP 1
-cglobal %1_h264_qpel16_h_lowpass_l2, 5, 6, 16 ; dst, src, src2, dstStride, src2Stride
-    movsxdifnidn  r3, r3d
-    movsxdifnidn  r4, r4d
-    mov          r5d, 16
-    pxor         m15, m15
-    mova         m14, [pw_5]
-    mova         m13, [pw_16]
-.loop:
-    lddqu         m1, [r1+6]
-    lddqu         m7, [r1-2]
-    mova          m0, m1
-    punpckhbw     m1, m15
-    punpcklbw     m0, m15
-    punpcklbw     m7, m15
-    mova          m2, m1
-    mova          m6, m0
-    mova          m3, m1
-    mova          m8, m0
-    mova          m4, m1
-    mova          m9, m0
-    mova         m12, m0
-    mova         m11, m1
-    palignr      m11, m0, 10
-    palignr      m12, m7, 10
-    palignr       m4, m0, 2
-    palignr       m9, m7, 2
-    palignr       m3, m0, 4
-    palignr       m8, m7, 4
-    palignr       m2, m0, 6
-    palignr       m6, m7, 6
-    paddw        m11, m0
-    palignr       m1, m0, 8
-    palignr       m0, m7, 8
-    paddw         m7, m12
-    paddw         m2, m3
-    paddw         m6, m8
-    paddw         m1, m4
-    paddw         m0, m9
-    psllw         m2, 2
-    psllw         m6, 2
-    psubw         m2, m1
-    psubw         m6, m0
-    paddw        m11, m13
-    paddw         m7, m13
-    pmullw        m2, m14
-    pmullw        m6, m14
-    lddqu         m3, [r2]
-    paddw         m2, m11
-    paddw         m6, m7
-    psraw         m2, 5
-    psraw         m6, 5
-    packuswb      m6, m2
-    pavgb         m6, m3
-    op_%1         m6, [r0], m11
-    add           r1, r3
-    add           r0, r3
-    add           r2, r4
-    dec          r5d
-    jg         .loop
-    REP_RET
-%endmacro
-
-INIT_XMM ssse3
-QPEL16_H_LOWPASS_L2_OP put
-QPEL16_H_LOWPASS_L2_OP avg
-%endif
diff -uparN ffmpeg-4.1/libavcodec/x86/h264_weight_10bit.asm ffmpeg-y/libavcodec/x86/h264_weight_10bit.asm
--- ffmpeg-4.1/libavcodec/x86/h264_weight_10bit.asm	2018-01-01 06:35:48.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/h264_weight_10bit.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,284 +0,0 @@
-;*****************************************************************************
-;* MMX/SSE2/AVX-optimized 10-bit H.264 weighted prediction code
-;*****************************************************************************
-;* Copyright (C) 2005-2011 x264 project
-;*
-;* Authors: Daniel Kang <daniel.d.kang@gmail.com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA 32
-
-sq_1: dq 1
-      dq 0
-
-cextern pw_1
-cextern pw_1023
-%define pw_pixel_max pw_1023
-
-SECTION .text
-
-;-----------------------------------------------------------------------------
-; void ff_h264_weight_16_10(uint8_t *dst, int stride, int height,
-;                           int log2_denom, int weight, int offset);
-;-----------------------------------------------------------------------------
-%macro WEIGHT_PROLOGUE 0
-.prologue:
-    PROLOGUE 0,6,8
-    movifnidn  r0, r0mp
-    movifnidn r1d, r1m
-    movifnidn r2d, r2m
-    movifnidn r4d, r4m
-    movifnidn r5d, r5m
-%endmacro
-
-%macro WEIGHT_SETUP 0
-    mova       m0, [pw_1]
-    movd       m2, r3m
-    pslld      m0, m2       ; 1<<log2_denom
-    SPLATW     m0, m0
-    shl        r5, 19       ; *8, move to upper half of dword
-    lea        r5, [r5+r4*2+0x10000]
-    movd       m3, r5d      ; weight<<1 | 1+(offset<<(3))
-    pshufd     m3, m3, 0
-    mova       m4, [pw_pixel_max]
-    paddw      m2, [sq_1]   ; log2_denom+1
-%if notcpuflag(sse4)
-    pxor       m7, m7
-%endif
-%endmacro
-
-%macro WEIGHT_OP 1-2
-%if %0==1
-    mova        m5, [r0+%1]
-    punpckhwd   m6, m5, m0
-    punpcklwd   m5, m0
-%else
-    movq        m5, [r0+%1]
-    movq        m6, [r0+%2]
-    punpcklwd   m5, m0
-    punpcklwd   m6, m0
-%endif
-    pmaddwd     m5, m3
-    pmaddwd     m6, m3
-    psrad       m5, m2
-    psrad       m6, m2
-%if cpuflag(sse4)
-    packusdw    m5, m6
-    pminsw      m5, m4
-%else
-    packssdw    m5, m6
-    CLIPW       m5, m7, m4
-%endif
-%endmacro
-
-%macro WEIGHT_FUNC_DBL 0
-cglobal h264_weight_16_10
-    WEIGHT_PROLOGUE
-    WEIGHT_SETUP
-.nextrow:
-    WEIGHT_OP  0
-    mova [r0   ], m5
-    WEIGHT_OP 16
-    mova [r0+16], m5
-    add       r0, r1
-    dec       r2d
-    jnz .nextrow
-    REP_RET
-%endmacro
-
-INIT_XMM sse2
-WEIGHT_FUNC_DBL
-INIT_XMM sse4
-WEIGHT_FUNC_DBL
-
-
-%macro WEIGHT_FUNC_MM 0
-cglobal h264_weight_8_10
-    WEIGHT_PROLOGUE
-    WEIGHT_SETUP
-.nextrow:
-    WEIGHT_OP   0
-    mova     [r0], m5
-    add        r0, r1
-    dec        r2d
-    jnz .nextrow
-    REP_RET
-%endmacro
-
-INIT_XMM sse2
-WEIGHT_FUNC_MM
-INIT_XMM sse4
-WEIGHT_FUNC_MM
-
-
-%macro WEIGHT_FUNC_HALF_MM 0
-cglobal h264_weight_4_10
-    WEIGHT_PROLOGUE
-    sar         r2d, 1
-    WEIGHT_SETUP
-    lea         r3, [r1*2]
-.nextrow:
-    WEIGHT_OP    0, r1
-    movh      [r0], m5
-    movhps [r0+r1], m5
-    add         r0, r3
-    dec         r2d
-    jnz .nextrow
-    REP_RET
-%endmacro
-
-INIT_XMM sse2
-WEIGHT_FUNC_HALF_MM
-INIT_XMM sse4
-WEIGHT_FUNC_HALF_MM
-
-
-;-----------------------------------------------------------------------------
-; void ff_h264_biweight_16_10(uint8_t *dst, uint8_t *src, int stride,
-;                             int height, int log2_denom, int weightd,
-;                             int weights, int offset);
-;-----------------------------------------------------------------------------
-%if ARCH_X86_32
-DECLARE_REG_TMP 3
-%else
-DECLARE_REG_TMP 7
-%endif
-
-%macro BIWEIGHT_PROLOGUE 0
-.prologue:
-    PROLOGUE 0,8,8
-    movifnidn  r0, r0mp
-    movifnidn  r1, r1mp
-    movifnidn r2d, r2m
-    movifnidn r5d, r5m
-    movifnidn r6d, r6m
-    movifnidn t0d, r7m
-%endmacro
-
-%macro BIWEIGHT_SETUP 0
-    lea        t0, [t0*4+1] ; (offset<<2)+1
-    or         t0, 1
-    shl        r6, 16
-    or         r5, r6
-    movd       m4, r5d      ; weightd | weights
-    movd       m5, t0d      ; (offset+1)|1
-    movd       m6, r4m      ; log2_denom
-    pslld      m5, m6       ; (((offset<<2)+1)|1)<<log2_denom
-    paddd      m6, [sq_1]
-    pshufd     m4, m4, 0
-    pshufd     m5, m5, 0
-    mova       m3, [pw_pixel_max]
-    movifnidn r3d, r3m
-%if notcpuflag(sse4)
-    pxor       m7, m7
-%endif
-%endmacro
-
-%macro BIWEIGHT 1-2
-%if %0==1
-    mova       m0, [r0+%1]
-    mova       m1, [r1+%1]
-    punpckhwd  m2, m0, m1
-    punpcklwd  m0, m1
-%else
-    movq       m0, [r0+%1]
-    movq       m1, [r1+%1]
-    punpcklwd  m0, m1
-    movq       m2, [r0+%2]
-    movq       m1, [r1+%2]
-    punpcklwd  m2, m1
-%endif
-    pmaddwd    m0, m4
-    pmaddwd    m2, m4
-    paddd      m0, m5
-    paddd      m2, m5
-    psrad      m0, m6
-    psrad      m2, m6
-%if cpuflag(sse4)
-    packusdw   m0, m2
-    pminsw     m0, m3
-%else
-    packssdw   m0, m2
-    CLIPW      m0, m7, m3
-%endif
-%endmacro
-
-%macro BIWEIGHT_FUNC_DBL 0
-cglobal h264_biweight_16_10
-    BIWEIGHT_PROLOGUE
-    BIWEIGHT_SETUP
-.nextrow:
-    BIWEIGHT   0
-    mova [r0   ], m0
-    BIWEIGHT  16
-    mova [r0+16], m0
-    add       r0, r2
-    add       r1, r2
-    dec       r3d
-    jnz .nextrow
-    REP_RET
-%endmacro
-
-INIT_XMM sse2
-BIWEIGHT_FUNC_DBL
-INIT_XMM sse4
-BIWEIGHT_FUNC_DBL
-
-%macro BIWEIGHT_FUNC 0
-cglobal h264_biweight_8_10
-    BIWEIGHT_PROLOGUE
-    BIWEIGHT_SETUP
-.nextrow:
-    BIWEIGHT  0
-    mova   [r0], m0
-    add      r0, r2
-    add      r1, r2
-    dec      r3d
-    jnz .nextrow
-    REP_RET
-%endmacro
-
-INIT_XMM sse2
-BIWEIGHT_FUNC
-INIT_XMM sse4
-BIWEIGHT_FUNC
-
-%macro BIWEIGHT_FUNC_HALF 0
-cglobal h264_biweight_4_10
-    BIWEIGHT_PROLOGUE
-    BIWEIGHT_SETUP
-    sar        r3d, 1
-    lea        r4, [r2*2]
-.nextrow:
-    BIWEIGHT     0, r2
-    movh   [r0   ], m0
-    movhps [r0+r2], m0
-    add         r0, r4
-    add         r1, r4
-    dec         r3d
-    jnz .nextrow
-    REP_RET
-%endmacro
-
-INIT_XMM sse2
-BIWEIGHT_FUNC_HALF
-INIT_XMM sse4
-BIWEIGHT_FUNC_HALF
diff -uparN ffmpeg-4.1/libavcodec/x86/h264_weight.asm ffmpeg-y/libavcodec/x86/h264_weight.asm
--- ffmpeg-4.1/libavcodec/x86/h264_weight.asm	2018-11-02 02:34:25.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/h264_weight.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,320 +0,0 @@
-;*****************************************************************************
-;* SSE2-optimized weighted prediction code
-;*****************************************************************************
-;* Copyright (c) 2004-2005 Michael Niedermayer, Loren Merritt
-;* Copyright (C) 2010 Eli Friedman <eli.friedman@gmail.com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION .text
-
-;-----------------------------------------------------------------------------
-; biweight pred:
-;
-; void ff_h264_biweight_16_sse2(uint8_t *dst, uint8_t *src, int stride,
-;                               int height, int log2_denom, int weightd,
-;                               int weights, int offset);
-; and
-; void ff_h264_weight_16_sse2(uint8_t *dst, int stride, int height,
-;                             int log2_denom, int weight, int offset);
-;-----------------------------------------------------------------------------
-
-%macro WEIGHT_SETUP 0
-    add        r5, r5
-    inc        r5
-    movd       m3, r4d
-    movd       m5, r5d
-    movd       m6, r3d
-    pslld      m5, m6
-    psrld      m5, 1
-%if mmsize == 16
-    pshuflw    m3, m3, 0
-    pshuflw    m5, m5, 0
-    punpcklqdq m3, m3
-    punpcklqdq m5, m5
-%else
-    pshufw     m3, m3, 0
-    pshufw     m5, m5, 0
-%endif
-    pxor       m7, m7
-%endmacro
-
-%macro WEIGHT_OP 2
-    movh          m0, [r0+%1]
-    movh          m1, [r0+%2]
-    punpcklbw     m0, m7
-    punpcklbw     m1, m7
-    pmullw        m0, m3
-    pmullw        m1, m3
-    paddsw        m0, m5
-    paddsw        m1, m5
-    psraw         m0, m6
-    psraw         m1, m6
-    packuswb      m0, m1
-%endmacro
-
-INIT_MMX mmxext
-cglobal h264_weight_16, 6, 6, 0
-    WEIGHT_SETUP
-.nextrow:
-    WEIGHT_OP 0,  4
-    mova     [r0  ], m0
-    WEIGHT_OP 8, 12
-    mova     [r0+8], m0
-    add        r0, r1
-    dec        r2d
-    jnz .nextrow
-    REP_RET
-
-%macro WEIGHT_FUNC_MM 2
-cglobal h264_weight_%1, 6, 6, %2
-    WEIGHT_SETUP
-.nextrow:
-    WEIGHT_OP 0, mmsize/2
-    mova     [r0], m0
-    add        r0, r1
-    dec        r2d
-    jnz .nextrow
-    REP_RET
-%endmacro
-
-INIT_MMX mmxext
-WEIGHT_FUNC_MM  8, 0
-INIT_XMM sse2
-WEIGHT_FUNC_MM 16, 8
-
-%macro WEIGHT_FUNC_HALF_MM 2
-cglobal h264_weight_%1, 6, 6, %2
-    WEIGHT_SETUP
-    sar       r2d, 1
-    lea        r3, [r1*2]
-.nextrow:
-    WEIGHT_OP 0, r1
-    movh     [r0], m0
-%if mmsize == 16
-    movhps   [r0+r1], m0
-%else
-    psrlq      m0, 32
-    movh     [r0+r1], m0
-%endif
-    add        r0, r3
-    dec        r2d
-    jnz .nextrow
-    REP_RET
-%endmacro
-
-INIT_MMX mmxext
-WEIGHT_FUNC_HALF_MM 4, 0
-INIT_XMM sse2
-WEIGHT_FUNC_HALF_MM 8, 8
-
-%macro BIWEIGHT_SETUP 0
-%if ARCH_X86_64
-%define off_regd r7d
-%else
-%define off_regd r3d
-%endif
-    mov  off_regd, r7m
-    add  off_regd, 1
-    or   off_regd, 1
-    add       r4d, 1
-    cmp       r6d, 128
-    je .nonnormal
-    cmp       r5d, 128
-    jne .normal
-.nonnormal:
-    sar       r5d, 1
-    sar       r6d, 1
-    sar  off_regd, 1
-    sub       r4d, 1
-.normal:
-%if cpuflag(ssse3)
-    movd       m4, r5d
-    movd       m0, r6d
-%else
-    movd       m3, r5d
-    movd       m4, r6d
-%endif
-    movd       m5, off_regd
-    movd       m6, r4d
-    pslld      m5, m6
-    psrld      m5, 1
-%if cpuflag(ssse3)
-    punpcklbw  m4, m0
-    pshuflw    m4, m4, 0
-    pshuflw    m5, m5, 0
-    punpcklqdq m4, m4
-    punpcklqdq m5, m5
-
-%else
-%if mmsize == 16
-    pshuflw    m3, m3, 0
-    pshuflw    m4, m4, 0
-    pshuflw    m5, m5, 0
-    punpcklqdq m3, m3
-    punpcklqdq m4, m4
-    punpcklqdq m5, m5
-%else
-    pshufw     m3, m3, 0
-    pshufw     m4, m4, 0
-    pshufw     m5, m5, 0
-%endif
-    pxor       m7, m7
-%endif
-%endmacro
-
-%macro BIWEIGHT_STEPA 3
-    movh       m%1, [r0+%3]
-    movh       m%2, [r1+%3]
-    punpcklbw  m%1, m7
-    punpcklbw  m%2, m7
-    pmullw     m%1, m3
-    pmullw     m%2, m4
-    paddsw     m%1, m%2
-%endmacro
-
-%macro BIWEIGHT_STEPB 0
-    paddsw     m0, m5
-    paddsw     m1, m5
-    psraw      m0, m6
-    psraw      m1, m6
-    packuswb   m0, m1
-%endmacro
-
-INIT_MMX mmxext
-cglobal h264_biweight_16, 7, 8, 0
-    BIWEIGHT_SETUP
-    movifnidn r3d, r3m
-.nextrow:
-    BIWEIGHT_STEPA 0, 1, 0
-    BIWEIGHT_STEPA 1, 2, 4
-    BIWEIGHT_STEPB
-    mova       [r0], m0
-    BIWEIGHT_STEPA 0, 1, 8
-    BIWEIGHT_STEPA 1, 2, 12
-    BIWEIGHT_STEPB
-    mova     [r0+8], m0
-    add        r0, r2
-    add        r1, r2
-    dec        r3d
-    jnz .nextrow
-    REP_RET
-
-%macro BIWEIGHT_FUNC_MM 2
-cglobal h264_biweight_%1, 7, 8, %2
-    BIWEIGHT_SETUP
-    movifnidn r3d, r3m
-.nextrow:
-    BIWEIGHT_STEPA 0, 1, 0
-    BIWEIGHT_STEPA 1, 2, mmsize/2
-    BIWEIGHT_STEPB
-    mova       [r0], m0
-    add        r0, r2
-    add        r1, r2
-    dec        r3d
-    jnz .nextrow
-    REP_RET
-%endmacro
-
-INIT_MMX mmxext
-BIWEIGHT_FUNC_MM  8, 0
-INIT_XMM sse2
-BIWEIGHT_FUNC_MM 16, 8
-
-%macro BIWEIGHT_FUNC_HALF_MM 2
-cglobal h264_biweight_%1, 7, 8, %2
-    BIWEIGHT_SETUP
-    movifnidn r3d, r3m
-    sar        r3, 1
-    lea        r4, [r2*2]
-.nextrow:
-    BIWEIGHT_STEPA 0, 1, 0
-    BIWEIGHT_STEPA 1, 2, r2
-    BIWEIGHT_STEPB
-    movh       [r0], m0
-%if mmsize == 16
-    movhps     [r0+r2], m0
-%else
-    psrlq      m0, 32
-    movh       [r0+r2], m0
-%endif
-    add        r0, r4
-    add        r1, r4
-    dec        r3d
-    jnz .nextrow
-    REP_RET
-%endmacro
-
-INIT_MMX mmxext
-BIWEIGHT_FUNC_HALF_MM 4, 0
-INIT_XMM sse2
-BIWEIGHT_FUNC_HALF_MM 8, 8
-
-%macro BIWEIGHT_SSSE3_OP 0
-    pmaddubsw  m0, m4
-    pmaddubsw  m2, m4
-    paddsw     m0, m5
-    paddsw     m2, m5
-    psraw      m0, m6
-    psraw      m2, m6
-    packuswb   m0, m2
-%endmacro
-
-INIT_XMM ssse3
-cglobal h264_biweight_16, 7, 8, 8
-    BIWEIGHT_SETUP
-    movifnidn r3d, r3m
-
-.nextrow:
-    movh       m0, [r0]
-    movh       m2, [r0+8]
-    movh       m3, [r1+8]
-    punpcklbw  m0, [r1]
-    punpcklbw  m2, m3
-    BIWEIGHT_SSSE3_OP
-    mova       [r0], m0
-    add        r0, r2
-    add        r1, r2
-    dec        r3d
-    jnz .nextrow
-    REP_RET
-
-INIT_XMM ssse3
-cglobal h264_biweight_8, 7, 8, 8
-    BIWEIGHT_SETUP
-    movifnidn r3d, r3m
-    sar        r3, 1
-    lea        r4, [r2*2]
-
-.nextrow:
-    movh       m0, [r0]
-    movh       m1, [r1]
-    movh       m2, [r0+r2]
-    movh       m3, [r1+r2]
-    punpcklbw  m0, m1
-    punpcklbw  m2, m3
-    BIWEIGHT_SSSE3_OP
-    movh       [r0], m0
-    movhps     [r0+r2], m0
-    add        r0, r4
-    add        r1, r4
-    dec        r3d
-    jnz .nextrow
-    REP_RET
diff -uparN ffmpeg-4.1/libavcodec/x86/hevc_add_res.asm ffmpeg-y/libavcodec/x86/hevc_add_res.asm
--- ffmpeg-4.1/libavcodec/x86/hevc_add_res.asm	2018-11-02 02:34:25.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/hevc_add_res.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,369 +0,0 @@
-; *****************************************************************************
-; * Provide SIMD optimizations for add_residual functions for HEVC decoding
-; * Copyright (c) 2014 Pierre-Edouard LEPERE
-; *
-; * This file is part of FFmpeg.
-; *
-; * FFmpeg is free software; you can redistribute it and/or
-; * modify it under the terms of the GNU Lesser General Public
-; * License as published by the Free Software Foundation; either
-; * version 2.1 of the License, or (at your option) any later version.
-; *
-; * FFmpeg is distributed in the hope that it will be useful,
-; * but WITHOUT ANY WARRANTY; without even the implied warranty of
-; * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-; * Lesser General Public License for more details.
-; *
-; * You should have received a copy of the GNU Lesser General Public
-; * License along with FFmpeg; if not, write to the Free Software
-; * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-; ******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION .text
-
-cextern pw_1023
-%define max_pixels_10 pw_1023
-
-; the add_res macros and functions were largely inspired by h264_idct.asm from the x264 project
-%macro ADD_RES_MMX_4_8 0
-    mova              m0, [r1]
-    mova              m2, [r1+8]
-    pxor              m1, m1
-    pxor              m3, m3
-    psubw             m1, m0
-    psubw             m3, m2
-    packuswb          m0, m2
-    packuswb          m1, m3
-
-    movd              m2, [r0]
-    movd              m3, [r0+r2]
-    punpckldq         m2, m3
-    paddusb           m0, m2
-    psubusb           m0, m1
-    movd            [r0], m0
-    psrlq             m0, 32
-    movd         [r0+r2], m0
-%endmacro
-
-
-INIT_MMX mmxext
-; void ff_hevc_add_residual_4_8_mmxext(uint8_t *dst, int16_t *res, ptrdiff_t stride)
-cglobal hevc_add_residual_4_8, 3, 3, 6
-    ADD_RES_MMX_4_8
-    add               r1, 16
-    lea               r0, [r0+r2*2]
-    ADD_RES_MMX_4_8
-    RET
-
-%macro ADD_RES_SSE_8_8 0
-    pxor              m3, m3
-    mova              m4, [r1]
-    mova              m6, [r1+16]
-    mova              m0, [r1+32]
-    mova              m2, [r1+48]
-    psubw             m5, m3, m4
-    psubw             m7, m3, m6
-    psubw             m1, m3, m0
-    packuswb          m4, m0
-    packuswb          m5, m1
-    psubw             m3, m2
-    packuswb          m6, m2
-    packuswb          m7, m3
-
-    movq              m0, [r0]
-    movq              m1, [r0+r2]
-    movhps            m0, [r0+r2*2]
-    movhps            m1, [r0+r3]
-    paddusb           m0, m4
-    paddusb           m1, m6
-    psubusb           m0, m5
-    psubusb           m1, m7
-    movq            [r0], m0
-    movq         [r0+r2], m1
-    movhps     [r0+2*r2], m0
-    movhps       [r0+r3], m1
-%endmacro
-
-%macro ADD_RES_SSE_16_32_8 3
-    mova             xm2, [r1+%1]
-    mova             xm6, [r1+%1+16]
-%if cpuflag(avx2)
-    vinserti128       m2, m2, [r1+%1+32], 1
-    vinserti128       m6, m6, [r1+%1+48], 1
-%endif
-    psubw             m1, m0, m2
-    psubw             m5, m0, m6
-    packuswb          m2, m6
-    packuswb          m1, m5
-
-    mova             xm4, [r1+%1+mmsize*2]
-    mova             xm6, [r1+%1+mmsize*2+16]
-%if cpuflag(avx2)
-    vinserti128       m4, m4, [r1+%1+96 ], 1
-    vinserti128       m6, m6, [r1+%1+112], 1
-%endif
-    psubw             m3, m0, m4
-    psubw             m5, m0, m6
-    packuswb          m4, m6
-    packuswb          m3, m5
-
-    paddusb           m2, [%2]
-    paddusb           m4, [%3]
-    psubusb           m2, m1
-    psubusb           m4, m3
-    mova            [%2], m2
-    mova            [%3], m4
-%endmacro
-
-
-%macro TRANSFORM_ADD_8 0
-; void ff_hevc_add_residual_8_8_<opt>(uint8_t *dst, int16_t *res, ptrdiff_t stride)
-cglobal hevc_add_residual_8_8, 3, 4, 8
-    lea               r3, [r2*3]
-    ADD_RES_SSE_8_8
-    add               r1, 64
-    lea               r0, [r0+r2*4]
-    ADD_RES_SSE_8_8
-    RET
-
-; void ff_hevc_add_residual_16_8_<opt>(uint8_t *dst, int16_t *res, ptrdiff_t stride)
-cglobal hevc_add_residual_16_8, 3, 5, 7
-    pxor                m0, m0
-    lea                 r3, [r2*3]
-    mov                r4d, 4
-.loop:
-    ADD_RES_SSE_16_32_8  0, r0,      r0+r2
-    ADD_RES_SSE_16_32_8 64, r0+r2*2, r0+r3
-    add                 r1, 128
-    lea                 r0, [r0+r2*4]
-    dec                r4d
-    jg .loop
-    RET
-
-; void ff_hevc_add_residual_32_8_<opt>(uint8_t *dst, int16_t *res, ptrdiff_t stride)
-cglobal hevc_add_residual_32_8, 3, 5, 7
-    pxor                m0, m0
-    mov                r4d, 16
-.loop:
-    ADD_RES_SSE_16_32_8  0, r0,    r0+16
-    ADD_RES_SSE_16_32_8 64, r0+r2, r0+r2+16
-    add                 r1, 128
-    lea                 r0, [r0+r2*2]
-    dec                r4d
-    jg .loop
-    RET
-%endmacro
-
-INIT_XMM sse2
-TRANSFORM_ADD_8
-INIT_XMM avx
-TRANSFORM_ADD_8
-
-%if HAVE_AVX2_EXTERNAL
-INIT_YMM avx2
-; void ff_hevc_add_residual_32_8_avx2(uint8_t *dst, int16_t *res, ptrdiff_t stride)
-cglobal hevc_add_residual_32_8, 3, 5, 7
-    pxor                 m0, m0
-    lea                  r3, [r2*3]
-    mov                 r4d, 8
-.loop:
-    ADD_RES_SSE_16_32_8   0, r0,      r0+r2
-    ADD_RES_SSE_16_32_8 128, r0+r2*2, r0+r3
-    add                  r1, 256
-    lea                  r0, [r0+r2*4]
-    dec                 r4d
-    jg .loop
-    RET
-%endif ;HAVE_AVX2_EXTERNAL
-
-%macro ADD_RES_SSE_8_10 4
-    mova              m0, [%4]
-    mova              m1, [%4+16]
-    mova              m2, [%4+32]
-    mova              m3, [%4+48]
-    paddw             m0, [%1+0]
-    paddw             m1, [%1+%2]
-    paddw             m2, [%1+%2*2]
-    paddw             m3, [%1+%3]
-    CLIPW             m0, m4, m5
-    CLIPW             m1, m4, m5
-    CLIPW             m2, m4, m5
-    CLIPW             m3, m4, m5
-    mova          [%1+0], m0
-    mova         [%1+%2], m1
-    mova       [%1+%2*2], m2
-    mova         [%1+%3], m3
-%endmacro
-
-%macro ADD_RES_MMX_4_10 3
-    mova              m0, [%1+0]
-    mova              m1, [%1+%2]
-    paddw             m0, [%3]
-    paddw             m1, [%3+8]
-    CLIPW             m0, m2, m3
-    CLIPW             m1, m2, m3
-    mova          [%1+0], m0
-    mova         [%1+%2], m1
-%endmacro
-
-%macro ADD_RES_SSE_16_10 3
-    mova              m0, [%3]
-    mova              m1, [%3+16]
-    mova              m2, [%3+32]
-    mova              m3, [%3+48]
-    paddw             m0, [%1]
-    paddw             m1, [%1+16]
-    paddw             m2, [%1+%2]
-    paddw             m3, [%1+%2+16]
-    CLIPW             m0, m4, m5
-    CLIPW             m1, m4, m5
-    CLIPW             m2, m4, m5
-    CLIPW             m3, m4, m5
-    mova            [%1], m0
-    mova         [%1+16], m1
-    mova         [%1+%2], m2
-    mova      [%1+%2+16], m3
-%endmacro
-
-%macro ADD_RES_SSE_32_10 2
-    mova              m0, [%2]
-    mova              m1, [%2+16]
-    mova              m2, [%2+32]
-    mova              m3, [%2+48]
-
-    paddw             m0, [%1]
-    paddw             m1, [%1+16]
-    paddw             m2, [%1+32]
-    paddw             m3, [%1+48]
-    CLIPW             m0, m4, m5
-    CLIPW             m1, m4, m5
-    CLIPW             m2, m4, m5
-    CLIPW             m3, m4, m5
-    mova            [%1], m0
-    mova         [%1+16], m1
-    mova         [%1+32], m2
-    mova         [%1+48], m3
-%endmacro
-
-%macro ADD_RES_AVX2_16_10 4
-    mova              m0, [%4]
-    mova              m1, [%4+32]
-    mova              m2, [%4+64]
-    mova              m3, [%4+96]
-
-    paddw             m0, [%1+0]
-    paddw             m1, [%1+%2]
-    paddw             m2, [%1+%2*2]
-    paddw             m3, [%1+%3]
-
-    CLIPW             m0, m4, m5
-    CLIPW             m1, m4, m5
-    CLIPW             m2, m4, m5
-    CLIPW             m3, m4, m5
-    mova          [%1+0], m0
-    mova         [%1+%2], m1
-    mova       [%1+%2*2], m2
-    mova         [%1+%3], m3
-%endmacro
-
-%macro ADD_RES_AVX2_32_10 3
-    mova              m0, [%3]
-    mova              m1, [%3+32]
-    mova              m2, [%3+64]
-    mova              m3, [%3+96]
-
-    paddw             m0, [%1]
-    paddw             m1, [%1+32]
-    paddw             m2, [%1+%2]
-    paddw             m3, [%1+%2+32]
-
-    CLIPW             m0, m4, m5
-    CLIPW             m1, m4, m5
-    CLIPW             m2, m4, m5
-    CLIPW             m3, m4, m5
-    mova            [%1], m0
-    mova         [%1+32], m1
-    mova         [%1+%2], m2
-    mova      [%1+%2+32], m3
-%endmacro
-
-; void ff_hevc_add_residual_<4|8|16|32>_10(pixel *dst, int16_t *block, ptrdiff_t stride)
-INIT_MMX mmxext
-cglobal hevc_add_residual_4_10, 3, 3, 6
-    pxor              m2, m2
-    mova              m3, [max_pixels_10]
-    ADD_RES_MMX_4_10  r0, r2, r1
-    add               r1, 16
-    lea               r0, [r0+2*r2]
-    ADD_RES_MMX_4_10  r0, r2, r1
-    RET
-
-INIT_XMM sse2
-cglobal hevc_add_residual_8_10, 3, 4, 6
-    pxor              m4, m4
-    mova              m5, [max_pixels_10]
-    lea               r3, [r2*3]
-
-    ADD_RES_SSE_8_10  r0, r2, r3, r1
-    lea               r0, [r0+r2*4]
-    add               r1, 64
-    ADD_RES_SSE_8_10  r0, r2, r3, r1
-    RET
-
-cglobal hevc_add_residual_16_10, 3, 5, 6
-    pxor              m4, m4
-    mova              m5, [max_pixels_10]
-
-    mov              r4d, 8
-.loop:
-    ADD_RES_SSE_16_10 r0, r2, r1
-    lea               r0, [r0+r2*2]
-    add               r1, 64
-    dec              r4d
-    jg .loop
-    RET
-
-cglobal hevc_add_residual_32_10, 3, 5, 6
-    pxor              m4, m4
-    mova              m5, [max_pixels_10]
-
-    mov              r4d, 32
-.loop:
-    ADD_RES_SSE_32_10 r0, r1
-    lea               r0, [r0+r2]
-    add               r1, 64
-    dec              r4d
-    jg .loop
-    RET
-
-%if HAVE_AVX2_EXTERNAL
-INIT_YMM avx2
-cglobal hevc_add_residual_16_10, 3, 5, 6
-    pxor               m4, m4
-    mova               m5, [max_pixels_10]
-    lea                r3, [r2*3]
-
-    mov               r4d, 4
-.loop:
-    ADD_RES_AVX2_16_10 r0, r2, r3, r1
-    lea                r0, [r0+r2*4]
-    add                r1, 128
-    dec               r4d
-    jg .loop
-    RET
-
-cglobal hevc_add_residual_32_10, 3, 5, 6
-    pxor               m4, m4
-    mova               m5, [max_pixels_10]
-
-    mov               r4d, 16
-.loop:
-    ADD_RES_AVX2_32_10 r0, r2, r1
-    lea                r0, [r0+r2*2]
-    add                r1, 128
-    dec               r4d
-    jg .loop
-    RET
-%endif ;HAVE_AVX2_EXTERNAL
diff -uparN ffmpeg-4.1/libavcodec/x86/hevc_deblock.asm ffmpeg-y/libavcodec/x86/hevc_deblock.asm
--- ffmpeg-4.1/libavcodec/x86/hevc_deblock.asm	2018-11-02 02:34:25.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/hevc_deblock.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,871 +0,0 @@
-;*****************************************************************************
-;* SSE2-optimized HEVC deblocking code
-;*****************************************************************************
-;* Copyright (C) 2013 VTT
-;*
-;* Authors: Seppo Tomperi <seppo.tomperi@vtt.fi>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-
-cextern pw_1023
-%define pw_pixel_max_10 pw_1023
-pw_pixel_max_12: times 8 dw ((1 << 12)-1)
-pw_m2:           times 8 dw -2
-pd_1 :           times 4 dd  1
-
-cextern pw_4
-cextern pw_8
-cextern pw_m1
-
-SECTION .text
-INIT_XMM sse2
-
-; in: 8 rows of 4 bytes in %4..%11
-; out: 4 rows of 8 words in m0..m3
-%macro TRANSPOSE4x8B_LOAD 8
-    movd             m0, %1
-    movd             m2, %2
-    movd             m1, %3
-    movd             m3, %4
-
-    punpcklbw        m0, m2
-    punpcklbw        m1, m3
-    punpcklwd        m0, m1
-
-    movd             m4, %5
-    movd             m6, %6
-    movd             m5, %7
-    movd             m3, %8
-
-    punpcklbw        m4, m6
-    punpcklbw        m5, m3
-    punpcklwd        m4, m5
-
-    punpckhdq        m2, m0, m4
-    punpckldq        m0, m4
-
-    pxor             m5, m5
-    punpckhbw        m1, m0, m5
-    punpcklbw        m0, m5
-    punpckhbw        m3, m2, m5
-    punpcklbw        m2, m5
-%endmacro
-
-; in: 4 rows of 8 words in m0..m3
-; out: 8 rows of 4 bytes in %1..%8
-%macro TRANSPOSE8x4B_STORE 8
-    packuswb         m0, m2
-    packuswb         m1, m3
-    SBUTTERFLY bw, 0, 1, 2
-    SBUTTERFLY wd, 0, 1, 2
-
-    movd             %1, m0
-    pshufd           m0, m0, 0x39
-    movd             %2, m0
-    pshufd           m0, m0, 0x39
-    movd             %3, m0
-    pshufd           m0, m0, 0x39
-    movd             %4, m0
-
-    movd             %5, m1
-    pshufd           m1, m1, 0x39
-    movd             %6, m1
-    pshufd           m1, m1, 0x39
-    movd             %7, m1
-    pshufd           m1, m1, 0x39
-    movd             %8, m1
-%endmacro
-
-; in: 8 rows of 4 words in %4..%11
-; out: 4 rows of 8 words in m0..m3
-%macro TRANSPOSE4x8W_LOAD 8
-    movq             m0, %1
-    movq             m2, %2
-    movq             m1, %3
-    movq             m3, %4
-
-    punpcklwd        m0, m2
-    punpcklwd        m1, m3
-    punpckhdq        m2, m0, m1
-    punpckldq        m0, m1
-
-    movq             m4, %5
-    movq             m6, %6
-    movq             m5, %7
-    movq             m3, %8
-
-    punpcklwd        m4, m6
-    punpcklwd        m5, m3
-    punpckhdq        m6, m4, m5
-    punpckldq        m4, m5
-
-    punpckhqdq       m1, m0, m4
-    punpcklqdq       m0, m4
-    punpckhqdq       m3, m2, m6
-    punpcklqdq       m2, m6
-
-%endmacro
-
-; in: 4 rows of 8 words in m0..m3
-; out: 8 rows of 4 words in %1..%8
-%macro TRANSPOSE8x4W_STORE 9
-    TRANSPOSE4x4W     0, 1, 2, 3, 4
-
-    pxor             m5, m5; zeros reg
-    CLIPW            m0, m5, %9
-    CLIPW            m1, m5, %9
-    CLIPW            m2, m5, %9
-    CLIPW            m3, m5, %9
-
-    movq             %1, m0
-    movhps           %2, m0
-    movq             %3, m1
-    movhps           %4, m1
-    movq             %5, m2
-    movhps           %6, m2
-    movq             %7, m3
-    movhps           %8, m3
-%endmacro
-
-; in: 8 rows of 8 bytes in %1..%8
-; out: 8 rows of 8 words in m0..m7
-%macro TRANSPOSE8x8B_LOAD 8
-    movq             m7, %1
-    movq             m2, %2
-    movq             m1, %3
-    movq             m3, %4
-
-    punpcklbw        m7, m2
-    punpcklbw        m1, m3
-    punpcklwd        m3, m7, m1
-    punpckhwd        m7, m1
-
-    movq             m4, %5
-    movq             m6, %6
-    movq             m5, %7
-    movq            m15, %8
-
-    punpcklbw        m4, m6
-    punpcklbw        m5, m15
-    punpcklwd        m9, m4, m5
-    punpckhwd        m4, m5
-
-    punpckldq        m1, m3, m9;  0, 1
-    punpckhdq        m3, m9;  2, 3
-
-    punpckldq        m5, m7, m4;  4, 5
-    punpckhdq        m7, m4;  6, 7
-
-    pxor            m13, m13
-
-    punpcklbw        m0, m1, m13; 0 in 16 bit
-    punpckhbw        m1, m13; 1 in 16 bit
-
-    punpcklbw        m2, m3, m13; 2
-    punpckhbw        m3, m13; 3
-
-    punpcklbw        m4, m5, m13; 4
-    punpckhbw        m5, m13; 5
-
-    punpcklbw        m6, m7, m13; 6
-    punpckhbw        m7, m13; 7
-%endmacro
-
-
-; in: 8 rows of 8 words in m0..m8
-; out: 8 rows of 8 bytes in %1..%8
-%macro TRANSPOSE8x8B_STORE 8
-    packuswb         m0, m4
-    packuswb         m1, m5
-    packuswb         m2, m6
-    packuswb         m3, m7
-    TRANSPOSE2x4x4B   0, 1, 2, 3, 4
-
-    movq             %1, m0
-    movhps           %2, m0
-    movq             %3, m1
-    movhps           %4, m1
-    movq             %5, m2
-    movhps           %6, m2
-    movq             %7, m3
-    movhps           %8, m3
-%endmacro
-
-; in: 8 rows of 8 words in %1..%8
-; out: 8 rows of 8 words in m0..m7
-%macro TRANSPOSE8x8W_LOAD 8
-    movdqu           m0, %1
-    movdqu           m1, %2
-    movdqu           m2, %3
-    movdqu           m3, %4
-    movdqu           m4, %5
-    movdqu           m5, %6
-    movdqu           m6, %7
-    movdqu           m7, %8
-    TRANSPOSE8x8W     0, 1, 2, 3, 4, 5, 6, 7, 8
-%endmacro
-
-; in: 8 rows of 8 words in m0..m8
-; out: 8 rows of 8 words in %1..%8
-%macro TRANSPOSE8x8W_STORE 9
-    TRANSPOSE8x8W     0, 1, 2, 3, 4, 5, 6, 7, 8
-
-    pxor             m8, m8
-    CLIPW            m0, m8, %9
-    CLIPW            m1, m8, %9
-    CLIPW            m2, m8, %9
-    CLIPW            m3, m8, %9
-    CLIPW            m4, m8, %9
-    CLIPW            m5, m8, %9
-    CLIPW            m6, m8, %9
-    CLIPW            m7, m8, %9
-
-    movdqu           %1, m0
-    movdqu           %2, m1
-    movdqu           %3, m2
-    movdqu           %4, m3
-    movdqu           %5, m4
-    movdqu           %6, m5
-    movdqu           %7, m6
-    movdqu           %8, m7
-%endmacro
-
-
-; in: %2 clobbered
-; out: %1
-; mask in m11
-; clobbers m10
-%macro MASKED_COPY 2
-    pand             %2, m11 ; and mask
-    pandn           m10, m11, %1; and -mask
-    por              %2, m10
-    mova             %1, %2
-%endmacro
-
-; in: %2 clobbered
-; out: %1
-; mask in %3, will be clobbered
-%macro MASKED_COPY2 3
-    pand             %2, %3 ; and mask
-    pandn            %3, %1; and -mask
-    por              %2, %3
-    mova             %1, %2
-%endmacro
-
-ALIGN 16
-; input in m0 ... m3 and tcs in r2. Output in m1 and m2
-%macro CHROMA_DEBLOCK_BODY 1
-    psubw            m4, m2, m1; q0 - p0
-    psubw            m5, m0, m3; p1 - q1
-    psllw            m4, 2; << 2
-    paddw            m5, m4;
-
-    ;tc calculations
-    movq             m6, [tcq]; tc0
-    punpcklwd        m6, m6
-    pshufd           m6, m6, 0xA0; tc0, tc1
-%if cpuflag(ssse3)
-    psignw           m4, m6, [pw_m1]; -tc0, -tc1
-%else
-    pmullw           m4, m6, [pw_m1]; -tc0, -tc1
-%endif
-    ;end tc calculations
-
-    paddw            m5, [pw_4]; +4
-    psraw            m5, 3; >> 3
-
-%if %1 > 8
-    psllw            m4, %1-8; << (BIT_DEPTH - 8)
-    psllw            m6, %1-8; << (BIT_DEPTH - 8)
-%endif
-    pmaxsw           m5, m4
-    pminsw           m5, m6
-    paddw            m1, m5; p0 + delta0
-    psubw            m2, m5; q0 - delta0
-%endmacro
-
-; input in m0 ... m7, beta in r2 tcs in r3. Output in m1...m6
-%macro LUMA_DEBLOCK_BODY 2
-    psllw            m9, m2, 1; *2
-    psubw           m10, m1, m9
-    paddw           m10, m3
-    ABS1            m10, m11 ; 0dp0, 0dp3 , 1dp0, 1dp3
-
-    psllw            m9, m5, 1; *2
-    psubw           m11, m6, m9
-    paddw           m11, m4
-    ABS1            m11, m13 ; 0dq0, 0dq3 , 1dq0, 1dq3
-
-    ;beta calculations
-%if %1 > 8
-    shl             betaq, %1 - 8
-%endif
-    movd            m13, betad
-    SPLATW          m13, m13, 0
-    ;end beta calculations
-
-    paddw            m9, m10, m11;   0d0, 0d3  ,  1d0, 1d3
-
-    pshufhw         m14, m9, 0x0f ;0b00001111;  0d3 0d3 0d0 0d0 in high
-    pshuflw         m14, m14, 0x0f ;0b00001111;  1d3 1d3 1d0 1d0 in low
-
-    pshufhw          m9, m9, 0xf0 ;0b11110000; 0d0 0d0 0d3 0d3
-    pshuflw          m9, m9, 0xf0 ;0b11110000; 1d0 1d0 1d3 1d3
-
-    paddw           m14, m9; 0d0+0d3, 1d0+1d3
-
-    ;compare
-    pcmpgtw         m15, m13, m14
-    movmskps        r13, m15 ;filtering mask 0d0 + 0d3 < beta0 (bit 2 or 3) , 1d0 + 1d3 < beta1 (bit 0 or 1)
-    test            r13, r13
-    je              .bypassluma
-
-    ;weak / strong decision compare to beta_2
-    psraw           m15, m13, 2;   beta >> 2
-    psllw            m8, m9, 1;
-    pcmpgtw         m15, m8; (d0 << 1) < beta_2, (d3 << 1) < beta_2
-    movmskps        r6, m15;
-    ;end weak / strong decision
-
-    ; weak filter nd_p/q calculation
-    pshufd           m8, m10, 0x31
-    psrld            m8, 16
-    paddw            m8, m10
-    movd            r7d, m8
-    pshufd           m8, m8, 0x4E
-    movd            r8d, m8
-
-    pshufd           m8, m11, 0x31
-    psrld            m8, 16
-    paddw            m8, m11
-    movd            r9d, m8
-    pshufd           m8, m8, 0x4E
-    movd           r10d, m8
-    ; end calc for weak filter
-
-    ; filtering mask
-    mov             r11, r13
-    shr             r11, 3
-    movd            m15, r11d
-    and             r13, 1
-    movd            m11, r13d
-    shufps          m11, m15, 0
-    shl             r11, 1
-    or              r13, r11
-
-    pcmpeqd         m11, [pd_1]; filtering mask
-
-    ;decide between strong and weak filtering
-    ;tc25 calculations
-    mov            r11d, [tcq];
-%if %1 > 8
-    shl             r11, %1 - 8
-%endif
-    movd             m8, r11d; tc0
-    mov             r3d, [tcq+4];
-%if %1 > 8
-    shl              r3, %1 - 8
-%endif
-    add            r11d, r3d; tc0 + tc1
-    jz             .bypassluma
-    movd             m9, r3d; tc1
-    punpcklwd        m8, m8
-    punpcklwd        m9, m9
-    shufps           m8, m9, 0; tc0, tc1
-    mova             m9, m8
-    psllw            m8, 2; tc << 2
-    pavgw            m8, m9; tc25 = ((tc * 5 + 1) >> 1)
-    ;end tc25 calculations
-
-    ;----beta_3 comparison-----
-    psubw           m12, m0, m3;      p3 - p0
-    ABS1            m12, m14; abs(p3 - p0)
-
-    psubw           m15, m7, m4;      q3 - q0
-    ABS1            m15, m14; abs(q3 - q0)
-
-    paddw           m12, m15; abs(p3 - p0) + abs(q3 - q0)
-
-    pshufhw         m12, m12, 0xf0 ;0b11110000;
-    pshuflw         m12, m12, 0xf0 ;0b11110000;
-
-    psraw           m13, 3; beta >> 3
-    pcmpgtw         m13, m12;
-    movmskps        r11, m13;
-    and             r6, r11; strong mask , beta_2 and beta_3 comparisons
-    ;----beta_3 comparison end-----
-    ;----tc25 comparison---
-    psubw           m12, m3, m4;      p0 - q0
-    ABS1            m12, m14; abs(p0 - q0)
-
-    pshufhw         m12, m12, 0xf0 ;0b11110000;
-    pshuflw         m12, m12, 0xf0 ;0b11110000;
-
-    pcmpgtw          m8, m12; tc25 comparisons
-    movmskps        r11, m8;
-    and             r6, r11; strong mask, beta_2, beta_3 and tc25 comparisons
-    ;----tc25 comparison end---
-    mov             r11, r6;
-    shr             r11, 1;
-    and             r6, r11; strong mask, bits 2 and 0
-
-    pmullw          m14, m9, [pw_m2]; -tc * 2
-    paddw            m9, m9
-
-    and             r6, 5; 0b101
-    mov             r11, r6; strong mask
-    shr             r6, 2;
-    movd            m12, r6d; store to xmm for mask generation
-    shl             r6, 1
-    and             r11, 1
-    movd            m10, r11d; store to xmm for mask generation
-    or              r6, r11; final strong mask, bits 1 and 0
-    jz      .weakfilter
-
-    shufps          m10, m12, 0
-    pcmpeqd         m10, [pd_1]; strong mask
-
-    mova            m13, [pw_4]; 4 in every cell
-    pand            m11, m10; combine filtering mask and strong mask
-    paddw           m12, m2, m3;          p1 +   p0
-    paddw           m12, m4;          p1 +   p0 +   q0
-    mova            m10, m12; copy
-    paddw           m12, m12;       2*p1 + 2*p0 + 2*q0
-    paddw           m12, m1;   p2 + 2*p1 + 2*p0 + 2*q0
-    paddw           m12, m5;   p2 + 2*p1 + 2*p0 + 2*q0 + q1
-    paddw           m12, m13;  p2 + 2*p1 + 2*p0 + 2*q0 + q1 + 4
-    psraw           m12, 3;  ((p2 + 2*p1 + 2*p0 + 2*q0 + q1 + 4) >> 3)
-    psubw           m12, m3; ((p2 + 2*p1 + 2*p0 + 2*q0 + q1 + 4) >> 3) - p0
-    pmaxsw          m12, m14
-    pminsw          m12, m9; av_clip( , -2 * tc, 2 * tc)
-    paddw           m12, m3; p0'
-
-    paddw           m15, m1, m10; p2 + p1 + p0 + q0
-    psrlw           m13, 1; 2 in every cell
-    paddw           m15, m13; p2 + p1 + p0 + q0 + 2
-    psraw           m15, 2;  (p2 + p1 + p0 + q0 + 2) >> 2
-    psubw           m15, m2;((p2 + p1 + p0 + q0 + 2) >> 2) - p1
-    pmaxsw          m15, m14
-    pminsw          m15, m9; av_clip( , -2 * tc, 2 * tc)
-    paddw           m15, m2; p1'
-
-    paddw            m8, m1, m0;     p3 +   p2
-    paddw            m8, m8;   2*p3 + 2*p2
-    paddw            m8, m1;   2*p3 + 3*p2
-    paddw            m8, m10;  2*p3 + 3*p2 + p1 + p0 + q0
-    paddw           m13, m13
-    paddw            m8, m13;  2*p3 + 3*p2 + p1 + p0 + q0 + 4
-    psraw            m8, 3;   (2*p3 + 3*p2 + p1 + p0 + q0 + 4) >> 3
-    psubw            m8, m1; ((2*p3 + 3*p2 + p1 + p0 + q0 + 4) >> 3) - p2
-    pmaxsw           m8, m14
-    pminsw           m8, m9; av_clip( , -2 * tc, 2 * tc)
-    paddw            m8, m1; p2'
-    MASKED_COPY      m1, m8
-
-    paddw            m8, m3, m4;         p0 +   q0
-    paddw            m8, m5;         p0 +   q0 +   q1
-    paddw            m8, m8;       2*p0 + 2*q0 + 2*q1
-    paddw            m8, m2;  p1 + 2*p0 + 2*q0 + 2*q1
-    paddw            m8, m6;  p1 + 2*p0 + 2*q0 + 2*q1 + q2
-    paddw            m8, m13; p1 + 2*p0 + 2*q0 + 2*q1 + q2 + 4
-    psraw            m8, 3;  (p1 + 2*p0 + 2*q0 + 2*q1 + q2 + 4) >>3
-    psubw            m8, m4;
-    pmaxsw           m8, m14
-    pminsw           m8, m9; av_clip( , -2 * tc, 2 * tc)
-    paddw            m8, m4; q0'
-    MASKED_COPY      m2, m15
-
-    paddw           m15, m3, m4;   p0 + q0
-    paddw           m15, m5;   p0 + q0 + q1
-    mova            m10, m15;
-    paddw           m15, m6;   p0 + q0 + q1 + q2
-    psrlw           m13, 1; 2 in every cell
-    paddw           m15, m13;  p0 + q0 + q1 + q2 + 2
-    psraw           m15, 2;   (p0 + q0 + q1 + q2 + 2) >> 2
-    psubw           m15, m5; ((p0 + q0 + q1 + q2 + 2) >> 2) - q1
-    pmaxsw          m15, m14
-    pminsw          m15, m9; av_clip( , -2 * tc, 2 * tc)
-    paddw           m15, m5; q1'
-
-    paddw           m13, m7;      q3 + 2
-    paddw           m13, m6;      q3 +  q2 + 2
-    paddw           m13, m13;   2*q3 + 2*q2 + 4
-    paddw           m13, m6;    2*q3 + 3*q2 + 4
-    paddw           m13, m10;   2*q3 + 3*q2 + q1 + q0 + p0 + 4
-    psraw           m13, 3;    (2*q3 + 3*q2 + q1 + q0 + p0 + 4) >> 3
-    psubw           m13, m6;  ((2*q3 + 3*q2 + q1 + q0 + p0 + 4) >> 3) - q2
-    pmaxsw          m13, m14
-    pminsw          m13, m9; av_clip( , -2 * tc, 2 * tc)
-    paddw           m13, m6; q2'
-
-    MASKED_COPY      m6, m13
-    MASKED_COPY      m5, m15
-    MASKED_COPY      m4, m8
-    MASKED_COPY      m3, m12
-
-.weakfilter:
-    not             r6; strong mask -> weak mask
-    and             r6, r13; final weak filtering mask, bits 0 and 1
-    jz             .store
-
-    ; weak filtering mask
-    mov             r11, r6
-    shr             r11, 1
-    movd            m12, r11d
-    and             r6, 1
-    movd            m11, r6d
-    shufps          m11, m12, 0
-    pcmpeqd         m11, [pd_1]; filtering mask
-
-    mov             r13, betaq
-    shr             r13, 1;
-    add             betaq, r13
-    shr             betaq, 3; ((beta + (beta >> 1)) >> 3))
-
-    mova            m13, [pw_8]
-    psubw           m12, m4, m3 ; q0 - p0
-    psllw           m10, m12, 3; 8 * (q0 - p0)
-    paddw           m12, m10 ; 9 * (q0 - p0)
-
-    psubw           m10, m5, m2 ; q1 - p1
-    psllw            m8, m10, 1; 2 * ( q1 - p1 )
-    paddw           m10, m8; 3 * ( q1 - p1 )
-    psubw           m12, m10; 9 * (q0 - p0) - 3 * ( q1 - p1 )
-    paddw           m12, m13; + 8
-    psraw           m12, 4; >> 4 , delta0
-    PABSW           m13, m12; abs(delta0)
-
-
-    psllw           m10, m9, 2; 8 * tc
-    paddw           m10, m9; 10 * tc
-    pcmpgtw         m10, m13
-    pand            m11, m10
-
-    psraw            m9, 1;   tc * 2 -> tc
-    psraw           m14, 1; -tc * 2 -> -tc
-
-    pmaxsw          m12, m14
-    pminsw          m12, m9;  av_clip(delta0, -tc, tc)
-
-    psraw            m9, 1;   tc -> tc / 2
-%if cpuflag(ssse3)
-    psignw          m14, m9, [pw_m1]; -tc / 2
-%else
-    pmullw          m14, m9, [pw_m1]; -tc / 2
-%endif
-
-    pavgw           m15, m1, m3;   (p2 + p0 + 1) >> 1
-    psubw           m15, m2;  ((p2 + p0 + 1) >> 1) - p1
-    paddw           m15, m12; ((p2 + p0 + 1) >> 1) - p1 + delta0
-    psraw           m15, 1;   (((p2 + p0 + 1) >> 1) - p1 + delta0) >> 1
-    pmaxsw          m15, m14
-    pminsw          m15, m9; av_clip(deltap1, -tc/2, tc/2)
-    paddw           m15, m2; p1'
-
-    ;beta calculations
-    movd            m10, betad
-    SPLATW          m10, m10, 0
-
-    movd            m13, r7d; 1dp0 + 1dp3
-    movd             m8, r8d; 0dp0 + 0dp3
-    punpcklwd        m8, m8
-    punpcklwd       m13, m13
-    shufps          m13, m8, 0;
-    pcmpgtw          m8, m10, m13
-    pand             m8, m11
-    ;end beta calculations
-    MASKED_COPY2     m2, m15, m8; write p1'
-
-    pavgw            m8, m6, m4;   (q2 + q0 + 1) >> 1
-    psubw            m8, m5;  ((q2 + q0 + 1) >> 1) - q1
-    psubw            m8, m12; ((q2 + q0 + 1) >> 1) - q1 - delta0)
-    psraw            m8, 1;   ((q2 + q0 + 1) >> 1) - q1 - delta0) >> 1
-    pmaxsw           m8, m14
-    pminsw           m8, m9; av_clip(deltaq1, -tc/2, tc/2)
-    paddw            m8, m5; q1'
-
-    movd            m13, r9d;
-    movd            m15, r10d;
-    punpcklwd       m15, m15
-    punpcklwd       m13, m13
-    shufps          m13, m15, 0; dq0 + dq3
-
-    pcmpgtw         m10, m13; compare to ((beta+(beta>>1))>>3)
-    pand            m10, m11
-    MASKED_COPY2     m5, m8, m10; write q1'
-
-    paddw           m15, m3, m12 ; p0 + delta0
-    MASKED_COPY      m3, m15
-
-    psubw            m8, m4, m12 ; q0 - delta0
-    MASKED_COPY      m4, m8
-%endmacro
-
-;-----------------------------------------------------------------------------
-; void ff_hevc_v_loop_filter_chroma(uint8_t *_pix, ptrdiff_t _stride, int32_t *tc,
-;                                   uint8_t *_no_p, uint8_t *_no_q);
-;-----------------------------------------------------------------------------
-%macro LOOP_FILTER_CHROMA 0
-cglobal hevc_v_loop_filter_chroma_8, 3, 5, 7, pix, stride, tc, pix0, r3stride
-    sub            pixq, 2
-    lea       r3strideq, [3*strideq]
-    mov           pix0q, pixq
-    add            pixq, r3strideq
-    TRANSPOSE4x8B_LOAD  PASS8ROWS(pix0q, pixq, strideq, r3strideq)
-    CHROMA_DEBLOCK_BODY 8
-    TRANSPOSE8x4B_STORE PASS8ROWS(pix0q, pixq, strideq, r3strideq)
-    RET
-
-cglobal hevc_v_loop_filter_chroma_10, 3, 5, 7, pix, stride, tc, pix0, r3stride
-    sub            pixq, 4
-    lea       r3strideq, [3*strideq]
-    mov           pix0q, pixq
-    add            pixq, r3strideq
-    TRANSPOSE4x8W_LOAD  PASS8ROWS(pix0q, pixq, strideq, r3strideq)
-    CHROMA_DEBLOCK_BODY 10
-    TRANSPOSE8x4W_STORE PASS8ROWS(pix0q, pixq, strideq, r3strideq), [pw_pixel_max_10]
-    RET
-
-cglobal hevc_v_loop_filter_chroma_12, 3, 5, 7, pix, stride, tc, pix0, r3stride
-    sub            pixq, 4
-    lea       r3strideq, [3*strideq]
-    mov           pix0q, pixq
-    add            pixq, r3strideq
-    TRANSPOSE4x8W_LOAD  PASS8ROWS(pix0q, pixq, strideq, r3strideq)
-    CHROMA_DEBLOCK_BODY 12
-    TRANSPOSE8x4W_STORE PASS8ROWS(pix0q, pixq, strideq, r3strideq), [pw_pixel_max_12]
-    RET
-
-;-----------------------------------------------------------------------------
-; void ff_hevc_h_loop_filter_chroma(uint8_t *_pix, ptrdiff_t _stride, int32_t *tc,
-;                                   uint8_t *_no_p, uint8_t *_no_q);
-;-----------------------------------------------------------------------------
-cglobal hevc_h_loop_filter_chroma_8, 3, 4, 7, pix, stride, tc, pix0
-    mov           pix0q, pixq
-    sub           pix0q, strideq
-    sub           pix0q, strideq
-    movq             m0, [pix0q];    p1
-    movq             m1, [pix0q+strideq]; p0
-    movq             m2, [pixq];    q0
-    movq             m3, [pixq+strideq]; q1
-    pxor             m5, m5; zeros reg
-    punpcklbw        m0, m5
-    punpcklbw        m1, m5
-    punpcklbw        m2, m5
-    punpcklbw        m3, m5
-    CHROMA_DEBLOCK_BODY  8
-    packuswb         m1, m2
-    movh[pix0q+strideq], m1
-    movhps       [pixq], m1
-    RET
-
-cglobal hevc_h_loop_filter_chroma_10, 3, 4, 7, pix, stride, tc, pix0
-    mov          pix0q, pixq
-    sub          pix0q, strideq
-    sub          pix0q, strideq
-    movu            m0, [pix0q];    p1
-    movu            m1, [pix0q+strideq]; p0
-    movu            m2, [pixq];    q0
-    movu            m3, [pixq+strideq]; q1
-    CHROMA_DEBLOCK_BODY 10
-    pxor            m5, m5; zeros reg
-    CLIPW           m1, m5, [pw_pixel_max_10]
-    CLIPW           m2, m5, [pw_pixel_max_10]
-    movu [pix0q+strideq], m1
-    movu        [pixq], m2
-    RET
-
-cglobal hevc_h_loop_filter_chroma_12, 3, 4, 7, pix, stride, tc, pix0
-    mov          pix0q, pixq
-    sub          pix0q, strideq
-    sub          pix0q, strideq
-    movu            m0, [pix0q];    p1
-    movu            m1, [pix0q+strideq]; p0
-    movu            m2, [pixq];    q0
-    movu            m3, [pixq+strideq]; q1
-    CHROMA_DEBLOCK_BODY 12
-    pxor            m5, m5; zeros reg
-    CLIPW           m1, m5, [pw_pixel_max_12]
-    CLIPW           m2, m5, [pw_pixel_max_12]
-    movu [pix0q+strideq], m1
-    movu        [pixq], m2
-    RET
-%endmacro
-
-INIT_XMM sse2
-LOOP_FILTER_CHROMA
-INIT_XMM avx
-LOOP_FILTER_CHROMA
-
-%if ARCH_X86_64
-%macro LOOP_FILTER_LUMA 0
-;-----------------------------------------------------------------------------
-; void ff_hevc_v_loop_filter_luma(uint8_t *_pix, ptrdiff_t _stride, int beta,
-;                                 int32_t *tc, uint8_t *_no_p, uint8_t *_no_q);
-;-----------------------------------------------------------------------------
-cglobal hevc_v_loop_filter_luma_8, 4, 14, 16, pix, stride, beta, tc, pix0, src3stride
-    sub            pixq, 4
-    lea           pix0q, [3 * r1]
-    mov     src3strideq, pixq
-    add            pixq, pix0q
-    TRANSPOSE8x8B_LOAD  PASS8ROWS(src3strideq, pixq, r1, pix0q)
-    LUMA_DEBLOCK_BODY 8, v
-.store:
-    TRANSPOSE8x8B_STORE PASS8ROWS(src3strideq, pixq, r1, pix0q)
-.bypassluma:
-    RET
-
-cglobal hevc_v_loop_filter_luma_10, 4, 14, 16, pix, stride, beta, tc, pix0, src3stride
-    sub            pixq, 8
-    lea           pix0q, [3 * strideq]
-    mov     src3strideq, pixq
-    add            pixq, pix0q
-    TRANSPOSE8x8W_LOAD  PASS8ROWS(src3strideq, pixq, strideq, pix0q)
-    LUMA_DEBLOCK_BODY 10, v
-.store:
-    TRANSPOSE8x8W_STORE PASS8ROWS(src3strideq, pixq, r1, pix0q), [pw_pixel_max_10]
-.bypassluma:
-    RET
-
-cglobal hevc_v_loop_filter_luma_12, 4, 14, 16, pix, stride, beta, tc, pix0, src3stride
-    sub            pixq, 8
-    lea           pix0q, [3 * strideq]
-    mov     src3strideq, pixq
-    add            pixq, pix0q
-    TRANSPOSE8x8W_LOAD  PASS8ROWS(src3strideq, pixq, strideq, pix0q)
-    LUMA_DEBLOCK_BODY 12, v
-.store:
-    TRANSPOSE8x8W_STORE PASS8ROWS(src3strideq, pixq, r1, pix0q), [pw_pixel_max_12]
-.bypassluma:
-    RET
-
-;-----------------------------------------------------------------------------
-; void ff_hevc_h_loop_filter_luma(uint8_t *_pix, ptrdiff_t _stride, int beta,
-;                                 int32_t *tc, uint8_t *_no_p, uint8_t *_no_q);
-;-----------------------------------------------------------------------------
-cglobal hevc_h_loop_filter_luma_8, 4, 14, 16, pix, stride, beta, tc, pix0, src3stride
-    lea     src3strideq, [3 * strideq]
-    mov           pix0q, pixq
-    sub           pix0q, src3strideq
-    sub           pix0q, strideq
-    movq             m0, [pix0q];               p3
-    movq             m1, [pix0q +     strideq]; p2
-    movq             m2, [pix0q + 2 * strideq]; p1
-    movq             m3, [pix0q + src3strideq]; p0
-    movq             m4, [pixq];                q0
-    movq             m5, [pixq +     strideq];  q1
-    movq             m6, [pixq + 2 * strideq];  q2
-    movq             m7, [pixq + src3strideq];  q3
-    pxor             m8, m8
-    punpcklbw        m0, m8
-    punpcklbw        m1, m8
-    punpcklbw        m2, m8
-    punpcklbw        m3, m8
-    punpcklbw        m4, m8
-    punpcklbw        m5, m8
-    punpcklbw        m6, m8
-    punpcklbw        m7, m8
-    LUMA_DEBLOCK_BODY 8, h
-.store:
-    packuswb          m1, m2
-    packuswb          m3, m4
-    packuswb          m5, m6
-    movh   [pix0q +     strideq], m1
-    movhps [pix0q + 2 * strideq], m1
-    movh   [pix0q + src3strideq], m3
-    movhps [pixq               ], m3
-    movh   [pixq  +     strideq], m5
-    movhps [pixq  + 2 * strideq], m5
-.bypassluma:
-    RET
-
-cglobal hevc_h_loop_filter_luma_10, 4, 14, 16, pix, stride, beta, tc, pix0, src3stride
-    lea                  src3strideq, [3 * strideq]
-    mov                        pix0q, pixq
-    sub                        pix0q, src3strideq
-    sub                        pix0q, strideq
-    movdqu                        m0, [pix0q];               p3
-    movdqu                        m1, [pix0q +     strideq]; p2
-    movdqu                        m2, [pix0q + 2 * strideq]; p1
-    movdqu                        m3, [pix0q + src3strideq]; p0
-    movdqu                        m4, [pixq];                q0
-    movdqu                        m5, [pixq  +     strideq]; q1
-    movdqu                        m6, [pixq  + 2 * strideq]; q2
-    movdqu                        m7, [pixq  + src3strideq]; q3
-    LUMA_DEBLOCK_BODY             10, h
-.store:
-    pxor                          m8, m8; zeros reg
-    CLIPW                         m1, m8, [pw_pixel_max_10]
-    CLIPW                         m2, m8, [pw_pixel_max_10]
-    CLIPW                         m3, m8, [pw_pixel_max_10]
-    CLIPW                         m4, m8, [pw_pixel_max_10]
-    CLIPW                         m5, m8, [pw_pixel_max_10]
-    CLIPW                         m6, m8, [pw_pixel_max_10]
-    movdqu     [pix0q +     strideq], m1;  p2
-    movdqu     [pix0q + 2 * strideq], m2;  p1
-    movdqu     [pix0q + src3strideq], m3;  p0
-    movdqu     [pixq               ], m4;  q0
-    movdqu     [pixq  +     strideq], m5;  q1
-    movdqu     [pixq  + 2 * strideq], m6;  q2
-.bypassluma:
-    RET
-
-cglobal hevc_h_loop_filter_luma_12, 4, 14, 16, pix, stride, beta, tc, pix0, src3stride
-    lea                  src3strideq, [3 * strideq]
-    mov                        pix0q, pixq
-    sub                        pix0q, src3strideq
-    sub                        pix0q, strideq
-    movdqu                        m0, [pix0q];               p3
-    movdqu                        m1, [pix0q +     strideq]; p2
-    movdqu                        m2, [pix0q + 2 * strideq]; p1
-    movdqu                        m3, [pix0q + src3strideq]; p0
-    movdqu                        m4, [pixq];                q0
-    movdqu                        m5, [pixq  +     strideq]; q1
-    movdqu                        m6, [pixq  + 2 * strideq]; q2
-    movdqu                        m7, [pixq  + src3strideq]; q3
-    LUMA_DEBLOCK_BODY             12, h
-.store:
-    pxor                          m8, m8; zeros reg
-    CLIPW                         m1, m8, [pw_pixel_max_12]
-    CLIPW                         m2, m8, [pw_pixel_max_12]
-    CLIPW                         m3, m8, [pw_pixel_max_12]
-    CLIPW                         m4, m8, [pw_pixel_max_12]
-    CLIPW                         m5, m8, [pw_pixel_max_12]
-    CLIPW                         m6, m8, [pw_pixel_max_12]
-    movdqu     [pix0q +     strideq], m1;  p2
-    movdqu     [pix0q + 2 * strideq], m2;  p1
-    movdqu     [pix0q + src3strideq], m3;  p0
-    movdqu     [pixq               ], m4;  q0
-    movdqu     [pixq  +     strideq], m5;  q1
-    movdqu     [pixq  + 2 * strideq], m6;  q2
-.bypassluma:
-    RET
-
-%endmacro
-
-INIT_XMM sse2
-LOOP_FILTER_LUMA
-INIT_XMM ssse3
-LOOP_FILTER_LUMA
-INIT_XMM avx
-LOOP_FILTER_LUMA
-%endif
diff -uparN ffmpeg-4.1/libavcodec/x86/hevc_idct.asm ffmpeg-y/libavcodec/x86/hevc_idct.asm
--- ffmpeg-4.1/libavcodec/x86/hevc_idct.asm	2018-11-02 02:34:25.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/hevc_idct.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,853 +0,0 @@
-;*******************************************************************************
-;* SIMD-optimized IDCT functions for HEVC decoding
-;* Copyright (c) 2014 Pierre-Edouard LEPERE
-;* Copyright (c) 2014 James Almer
-;* Copyright (c) 2016 Alexandra Hájková
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-
-pd_64: times 4 dd 64
-pd_2048: times 4 dd 2048
-pd_512: times 4 dd 512
-
-; 4x4 transform coeffs
-cextern pw_64
-pw_64_m64: times 4 dw 64, -64
-pw_83_36: times 4 dw 83, 36
-pw_36_m83: times 4 dw 36, -83
-
-; 8x8 transform coeffs
-pw_89_75: times 4 dw 89, 75
-pw_50_18: times 4 dw 50, 18
-
-pw_75_m18: times 4 dw 75, -18
-pw_m89_m50: times 4 dw -89, -50
-
-pw_50_m89: times 4 dw 50, -89
-pw_18_75: times 4 dw 18, 75
-
-pw_18_m50: times 4 dw 18, -50
-pw_75_m89: times 4 dw 75, -89
-
-; 16x16 transformation coeffs
-trans_coeffs16: times 4 dw 90, 87
-times 4 dw 80, 70
-times 4 dw 57, 43
-times 4 dw 25, 9
-
-times 4 dw 87, 57
-times 4 dw 9, -43
-times 4 dw -80, -90
-times 4 dw -70, -25
-
-times 4 dw 80, 9
-times 4 dw -70, -87
-times 4 dw -25, 57
-times 4 dw 90, 43
-
-times 4 dw 70, -43
-times 4 dw -87, 9
-times 4 dw 90, 25
-times 4 dw -80, -57
-
-times 4 dw 57, -80
-times 4 dw -25, 90
-times 4 dw -9, -87
-times 4 dw 43, 70
-
-times 4 dw 43, -90
-times 4 dw 57, 25
-times 4 dw -87, 70
-times 4 dw 9, -80
-
-times 4 dw 25, -70
-times 4 dw 90, -80
-times 4 dw 43, 9
-times 4 dw -57, 87
-
-times 4 dw 9, -25
-times 4 dw 43, -57
-times 4 dw 70, -80
-times 4 dw 87, -90
-
-; 32x32 transform coeffs
-trans_coeff32: times 8 dw 90
-times 4 dw 88, 85
-times 4 dw 82, 78
-times 4 dw 73, 67
-times 4 dw 61, 54
-times 4 dw 46, 38
-times 4 dw 31, 22
-times 4 dw 13, 4
-
-times 4 dw 90, 82
-times 4 dw 67, 46
-times 4 dw 22, -4
-times 4 dw -31, -54
-times 4 dw -73, -85
-times 4 dw -90, -88
-times 4 dw -78, -61
-times 4 dw -38, -13
-
-times 4 dw 88, 67
-times 4 dw 31, -13
-times 4 dw -54, -82
-times 4 dw -90, -78
-times 4 dw -46, -4
-times 4 dw 38, 73
-times 4 dw 90, 85
-times 4 dw 61, 22
-
-times 4 dw 85, 46
-times 4 dw -13, -67
-times 4 dw -90, -73
-times 4 dw -22, 38
-times 4 dw 82, 88
-times 4 dw 54, -4
-times 4 dw -61, -90
-times 4 dw -78, -31
-
-times 4 dw 82, 22
-times 4 dw -54, -90
-times 4 dw -61, 13
-times 4 dw 78, 85
-times 4 dw 31, -46
-times 4 dw -90, -67
-times 4 dw 4, 73
-times 4 dw 88, 38
-
-times 4 dw 78, -4
-times 4 dw -82, -73
-times 4 dw 13, 85
-times 4 dw 67, -22
-times 4 dw -88, -61
-times 4 dw 31, 90
-times 4 dw 54, -38
-times 4 dw -90, -46
-
-times 4 dw 73, -31
-times 4 dw -90, -22
-times 4 dw 78, 67
-times 4 dw -38, -90
-times 4 dw -13, 82
-times 4 dw 61, -46
-times 4 dw -88, -4
-times 4 dw 85, 54
-
-times 4 dw 67, -54
-times 4 dw -78, 38
-times 4 dw 85, -22
-times 4 dw -90, 4
-times 4 dw 90, 13
-times 4 dw -88, -31
-times 4 dw 82, 46
-times 4 dw -73, -61
-
-times 4 dw 61, -73
-times 4 dw -46, 82
-times 4 dw 31, -88
-times 4 dw -13, 90
-times 4 dw -4, -90
-times 4 dw 22, 85
-times 4 dw -38, -78
-times 4 dw 54, 67
-
-times 4 dw 54, -85
-times 4 dw -4, 88
-times 4 dw -46, -61
-times 4 dw 82, 13
-times 4 dw -90, 38
-times 4 dw 67, -78
-times 4 dw -22, 90
-times 4 dw -31, -73
-
-times 4 dw 46, -90
-times 4 dw 38, 54
-times 4 dw -90, 31
-times 4 dw 61, -88
-times 4 dw 22, 67
-times 4 dw -85, 13
-times 4 dw 73, -82
-times 4 dw 4, 78
-
-times 4 dw 38, -88
-times 4 dw 73, -4
-times 4 dw -67, 90
-times 4 dw -46, -31
-times 4 dw 85, -78
-times 4 dw 13, 61
-times 4 dw -90, 54
-times 4 dw 22, -82
-
-times 4 dw 31, -78
-times 4 dw 90, -61
-times 4 dw 4, 54
-times 4 dw -88, 82
-times 4 dw -38, -22
-times 4 dw 73, -90
-times 4 dw 67, -13
-times 4 dw -46, 85
-
-times 4 dw 22, -61
-times 4 dw 85, -90
-times 4 dw 73, -38
-times 4 dw -4, 46
-times 4 dw -78, 90
-times 4 dw -82, 54
-times 4 dw -13, -31
-times 4 dw 67, -88
-
-times 4 dw 13, -38
-times 4 dw 61, -78
-times 4 dw 88, -90
-times 4 dw 85, -73
-times 4 dw 54, -31
-times 4 dw 4, 22
-times 4 dw -46, 67
-times 4 dw -82, 90
-
-times 4 dw 4, -13
-times 4 dw 22, -31
-times 4 dw 38, -46
-times 4 dw 54, -61
-times 4 dw 67, -73
-times 4 dw 78, -82
-times 4 dw 85, -88
-times 4 dw 90, -90
-
-SECTION .text
-
-; void ff_hevc_idct_HxW_dc_{8,10}_<opt>(int16_t *coeffs)
-; %1 = HxW
-; %2 = number of loops
-; %3 = bitdepth
-%macro IDCT_DC 3
-cglobal hevc_idct_%1x%1_dc_%3, 1, 2, 1, coeff, tmp
-    movsx             tmpd, word [coeffq]
-    add               tmpd, (1 << (14 - %3)) + 1
-    sar               tmpd, (15 - %3)
-    movd               xm0, tmpd
-    SPLATW              m0, xm0
-    DEFINE_ARGS coeff, cnt
-    mov               cntd, %2
-.loop:
-    mova [coeffq+mmsize*0], m0
-    mova [coeffq+mmsize*1], m0
-    mova [coeffq+mmsize*2], m0
-    mova [coeffq+mmsize*3], m0
-    add  coeffq, mmsize*8
-    mova [coeffq+mmsize*-4], m0
-    mova [coeffq+mmsize*-3], m0
-    mova [coeffq+mmsize*-2], m0
-    mova [coeffq+mmsize*-1], m0
-    dec  cntd
-    jg  .loop
-    RET
-%endmacro
-
-; %1 = HxW
-; %2 = bitdepth
-%macro IDCT_DC_NL 2 ; No loop
-cglobal hevc_idct_%1x%1_dc_%2, 1, 2, 1, coeff, tmp
-    movsx             tmpd, word [coeffq]
-    add               tmpd, (1 << (14 - %2)) + 1
-    sar               tmpd, (15 - %2)
-    movd                m0, tmpd
-    SPLATW              m0, xm0
-    mova [coeffq+mmsize*0], m0
-    mova [coeffq+mmsize*1], m0
-    mova [coeffq+mmsize*2], m0
-    mova [coeffq+mmsize*3], m0
-%if mmsize == 16
-    mova [coeffq+mmsize*4], m0
-    mova [coeffq+mmsize*5], m0
-    mova [coeffq+mmsize*6], m0
-    mova [coeffq+mmsize*7], m0
-%endif
-    RET
-%endmacro
-
-; IDCT 4x4, expects input in m0, m1
-; %1 - shift
-; %2 - 1/0 - SCALE and Transpose or not
-; %3 - 1/0 add constant or not
-%macro TR_4x4 3
-    ; interleaves src0 with src2 to m0
-    ;         and src1 with scr3 to m2
-    ; src0: 00 01 02 03     m0: 00 20 01 21 02 22 03 23
-    ; src1: 10 11 12 13 -->
-    ; src2: 20 21 22 23     m1: 10 30 11 31 12 32 13 33
-    ; src3: 30 31 32 33
-
-    SBUTTERFLY wd, 0, 1, 2
-
-    pmaddwd m2, m0, [pw_64]    ; e0
-    pmaddwd m3, m1, [pw_83_36] ; o0
-    pmaddwd m0, [pw_64_m64]    ; e1
-    pmaddwd m1, [pw_36_m83]    ; o1
-
-%if %3 == 1
-    %assign %%add 1 << (%1 - 1)
-    mova  m4, [pd_ %+ %%add]
-    paddd m2, m4
-    paddd m0, m4
-%endif
-
-    SUMSUB_BADC d, 3, 2, 1, 0, 4
-
-%if %2 == 1
-    psrad m3, %1 ; e0 + o0
-    psrad m1, %1 ; e1 + o1
-    psrad m2, %1 ; e0 - o0
-    psrad m0, %1 ; e1 - o1
-    ;clip16
-    packssdw m3, m1
-    packssdw m0, m2
-    ; Transpose
-    SBUTTERFLY wd, 3, 0, 1
-    SBUTTERFLY wd, 3, 0, 1
-    SWAP 3, 1, 0
-%else
-    SWAP 3, 2, 0
-%endif
-%endmacro
-
-%macro DEFINE_BIAS 1
-    %assign shift (20 - %1)
-    %assign c_add (1 << (shift - 1))
-    %define arr_add pd_ %+ c_add
-%endmacro
-
-; %1 - bit_depth
-; %2 - register add constant
-; is loaded to
-; shift = 20 - bit_depth
-%macro LOAD_BIAS 2
-    DEFINE_BIAS %1
-    mova %2, [arr_add]
-%endmacro
-
-; %1, %2 - registers to load packed 16 bit values to
-; %3, %4, %5, %6 - vertical offsets
-; %7 - horizontal offset
-%macro LOAD_BLOCK 7
-    movq   %1, [r0 + %3 + %7]
-    movhps %1, [r0 + %5 + %7]
-    movq   %2, [r0 + %4 + %7]
-    movhps %2, [r0 + %6 + %7]
-%endmacro
-
-; void ff_hevc_idct_4x4__{8,10}_<opt>(int16_t *coeffs, int col_limit)
-; %1 = bitdepth
-%macro IDCT_4x4 1
-cglobal hevc_idct_4x4_%1, 1, 1, 5, coeffs
-    mova m0, [coeffsq]
-    mova m1, [coeffsq + 16]
-
-    TR_4x4 7, 1, 1
-    TR_4x4 20 - %1, 1, 1
-
-    mova [coeffsq],      m0
-    mova [coeffsq + 16], m1
-    RET
-%endmacro
-
-; scale, pack (clip16) and store the residuals     0 e8[0] + o8[0] --> + %1
-; 4 at one time (4 columns)                        1 e8[1] + o8[1]
-; from %5: e8/16 + o8/16, with %1 offset                  ...
-; and  %3: e8/16 - o8/16, with %2 offset           6 e8[1] - o8[1]
-; %4 - shift                                       7 e8[0] - o8[0] --> + %2
-%macro STORE_8 7
-    psrad    %5, %4
-    psrad    %3, %4
-    packssdw %5, %3
-    movq     [coeffsq + %1], %5
-    movhps   [coeffsq + %2], %5
-%endmacro
-
-; %1 - horizontal offset
-; %2 - shift
-; %3, %4 - transform coeffs
-; %5 - vertical offset for e8 + o8
-; %6 - vertical offset for e8 - o8
-; %7 - register with e8 inside
-; %8 - block_size
-; %9 - register to store e8 +o8
-; %10 - register to store e8 - o8
-%macro E8_O8 10
-    pmaddwd m6, m4, %3
-    pmaddwd m7, m5, %4
-
-    paddd m6, m7
-    paddd m7, m6, %7 ; o8 + e8
-    psubd %7, m6     ; e8 - o8
-%if %8 == 8
-    STORE_8 %5 + %1, %6 + %1, %7, %2, m7, 0, 0
-%else
-    SWAP m7, %9
-    SWAP %7, %10
-%endif
-%endmacro
-
-; 8x4 residuals are processed and stored
-; %1 - horizontal offset
-; %2 - shift
-; %3 - offset of the even row
-; %4 - step: 1 for 8x8, 2 for 16x16, 4 for 32x32
-; %5 - offset of the odd row
-; %6 - block size
-; %7 - 1/0 add a constant in TR_4x4 or not
-; I want to add a constant for 8x8 transform but not for 16x16 and 32x32
-%macro TR_8x4 7
-    ; load 4 columns of even rows
-    LOAD_BLOCK  m0, m1, 0, 2 * %4 * %3, %4 * %3, 3 * %4 * %3, %1
-
-    TR_4x4 %2, 0, %7 ; e8: m0, m1, m2, m3, for 4 columns only
-
-    ; load 4 columns of odd rows
-    LOAD_BLOCK m4, m5, %4 * %5, 3 * %4 * %5, 5 * %4 * %5, 7 * %4 * %5, %1
-
-    ; 00 01 02 03
-    ; 10 11 12 13      m4: 10 30 11 31 12 32 13 33
-
-    ; ...        -- >
-    ;                  m5: 50 70 51 71 52 72 53 73
-    ; 70 71 72 73
-    SBUTTERFLY wd, 4, 5, 6
-
-    E8_O8 %1, %2, [pw_89_75],  [pw_50_18],   0,      %5 * 7, m0, %6, m8, m15
-    E8_O8 %1, %2, [pw_75_m18], [pw_m89_m50], %5,     %5 * 6, m1, %6, m9, m14
-    E8_O8 %1, %2, [pw_50_m89], [pw_18_75],   %5 * 2, %5 * 5, m2, %6, m10, m13
-    E8_O8 %1, %2, [pw_18_m50], [pw_75_m89],  %5 * 3, %5 * 4, m3, %6, m11, m12
-%endmacro
-
-%macro STORE_PACKED 7
-    movq   [r0 + %3 + %7], %1
-    movhps [r0 + %4 + %7], %1
-    movq   [r0 + %5 + %7], %2
-    movhps [r0 + %6 + %7], %2
-%endmacro
-
-; transpose 4x4 block packed
-; in %1 and %2 registers
-; %3 - temporary register
-%macro TRANSPOSE_4x4 3
-    SBUTTERFLY wd, %1, %2, %3
-    SBUTTERFLY dq, %1, %2, %3
-%endmacro
-
-; %1 - horizontal offset of the block i
-; %2 - vertical offset of the block i
-; %3 - width in bytes
-; %4 - vertical offset for the block j
-; %5 - horizontal offset for the block j
-%macro SWAP_BLOCKS 5
-    ; M_j
-    LOAD_BLOCK m4, m5, %4, %4 + %3, %4 + 2 * %3, %4 + 3 * %3, %5
-    TRANSPOSE_4x4 4, 5, 6
-
-    ; M_i
-    LOAD_BLOCK m6, m7, %2, %2 + %3, %2 + 2 * %3, %2 + 3 * %3, %1
-
-    STORE_PACKED m4, m5, %2, %2 + %3, %2 + 2 * %3, %2 + 3 * %3, %1
-
-    ; transpose and store M_i
-    SWAP m6, m4
-    SWAP m7, m5
-    TRANSPOSE_4x4 4, 5, 6
-    STORE_PACKED m4, m5, %4, %4 + %3, %4 + 2 * %3, %4 + 3 * %3, %5
-%endmacro
-
-; %1 - horizontal offset
-; %2 - vertical offset of the block
-; %3 - width in bytes
-%macro TRANSPOSE_BLOCK 3
-    LOAD_BLOCK m4, m5, %2, %2 + %3, %2 + 2 * %3, %2 + 3 * %3, %1
-    TRANSPOSE_4x4 4, 5, 6
-    STORE_PACKED m4, m5, %2, %2 + %3, %2 + 2 * %3, %2 + 3 * %3, %1
-%endmacro
-
-%macro TRANSPOSE_8x8 0
-cglobal hevc_idct_transpose_8x8, 0, 0, 0
-    ; M1 M2 ^T = M1^t M3^t
-    ; M3 M4      M2^t M4^t
-
-    ; M1 4x4 block
-    TRANSPOSE_BLOCK 0, 0, 16
-
-    ; M2 and M3
-    SWAP_BLOCKS 0, 64, 16, 0, 8
-
-    ; M4
-    TRANSPOSE_BLOCK 8, 64, 16
-
-    ret
-%endmacro
-
-; void ff_hevc_idct_8x8_{8,10}_<opt>(int16_t *coeffs, int col_limit)
-; %1 = bitdepth
-%macro IDCT_8x8 1
-cglobal hevc_idct_8x8_%1, 1, 1, 8, coeffs
-    TR_8x4 0, 7, 32, 1, 16, 8, 1
-    TR_8x4 8, 7, 32, 1, 16, 8, 1
-
-    call hevc_idct_transpose_8x8_ %+ cpuname
-
-    DEFINE_BIAS %1
-    TR_8x4 0, shift, 32, 1, 16, 8, 1
-    TR_8x4 8, shift, 32, 1, 16, 8, 1
-
-    TAIL_CALL hevc_idct_transpose_8x8_ %+ cpuname, 1
-%endmacro
-
-; store intermedite e32 coeffs on stack
-; as 16x4 matrix
-; from m10: e8 + o8, with %6 offset
-; and  %3:  e8 - o8, with %7 offset
-; %4 - shift, unused here
-%macro STORE_16 7
-    mova [rsp + %6], %5
-    mova [rsp + %7], %3
-%endmacro
-
-; %1, %2 - transform constants
-; %3, %4 - regs with interleaved coeffs
-; %5 - 1/0 SWAP or add
-; %6, %7 - registers for intermidiate sums
-; %8 - accumulator register
-%macro ADD_ROWS 8
-    pmaddwd %6, %3, %1
-    pmaddwd %7, %4, %2
-    paddd   %6, %7
-%if %5 == 1
-    SWAP %6, %8
-%else
-    paddd %8, %6
-%endif
-%endmacro
-
-; %1 - transform coeffs
-; %2, %3 offsets for storing e+o/e-o back to coeffsq
-; %4 - shift
-; %5 - add
-; %6 - block_size
-; %7 - register with e16
-; %8, %9 - stack offsets for storing e+o/e-o
-%macro E16_O16 9
-    ADD_ROWS [%1],          [%1 +     16], m0, m1, 1, m5, m6, m7
-    ADD_ROWS [%1 + 2 * 16], [%1 + 3 * 16], m2, m3, 0, m5, m6, m7
-
-%if %6 == 8
-    paddd %7, %5
-%endif
-
-    paddd m4, m7, %7 ; o16 + e16
-    psubd %7, m7     ; e16 - o16
-    STORE_%6 %2, %3, %7, %4, m4, %8, %9
-%endmacro
-
-%macro TR_16x4 10
-    ; produce 8x4 matrix of e16 coeffs
-    ; for 4 first rows and store it on stack (128 bytes)
-    TR_8x4 %1, 7, %4, %5, %6, %8, 0
-
-    ; load 8 even rows
-    LOAD_BLOCK m0, m1, %9 * %6, %9 * 3 * %6, %9 * 5 * %6, %9 * 7 * %6, %1
-    LOAD_BLOCK m2, m3, %9 * 9 * %6, %9 * 11 * %6, %9 * 13 * %6, %9 * 15 * %6, %1
-
-    SBUTTERFLY wd, 0, 1, 4
-    SBUTTERFLY wd, 2, 3, 4
-
-    E16_O16 trans_coeffs16,               0 + %1, 15 * %6 + %1, %2, %3, %7, m8,       0, 15 * 16
-    mova m8, %3
-    E16_O16 trans_coeffs16 +     64,     %6 + %1, 14 * %6 + %1, %2, m8, %7, m9,      16, 14 * 16
-    E16_O16 trans_coeffs16 + 2 * 64, 2 * %6 + %1, 13 * %6 + %1, %2, m8, %7, m10, 2 * 16, 13 * 16
-    E16_O16 trans_coeffs16 + 3 * 64, 3 * %6 + %1, 12 * %6 + %1, %2, m8, %7, m11, 3 * 16, 12 * 16
-    E16_O16 trans_coeffs16 + 4 * 64, 4 * %6 + %1, 11 * %6 + %1, %2, m8, %7, m12, 4 * 16, 11 * 16
-    E16_O16 trans_coeffs16 + 5 * 64, 5 * %6 + %1, 10 * %6 + %1, %2, m8, %7, m13, 5 * 16, 10 * 16
-    E16_O16 trans_coeffs16 + 6 * 64, 6 * %6 + %1,  9 * %6 + %1, %2, m8, %7, m14, 6 * 16,  9 * 16
-    E16_O16 trans_coeffs16 + 7 * 64, 7 * %6 + %1,  8 * %6 + %1, %2, m8, %7, m15, 7 * 16,  8 * 16
-%endmacro
-
-%macro TRANSPOSE_16x16 0
-cglobal hevc_idct_transpose_16x16, 0, 0, 0
-; M1  M2  M3  M4 ^T      m1 m5 m9  m13   M_i^T = m_i
-; M5  M6  M7  M8    -->  m2 m6 m10 m14
-; M9  M10 M11 M12        m3 m7 m11 m15
-; M13 M14 M15 M16        m4 m8 m12 m16
-
-    ; M1 4x4 block
-    TRANSPOSE_BLOCK 0, 0, 32
-
-    ; M5, M2
-    SWAP_BLOCKS 0, 128, 32, 0, 8
-    ; M9, M3
-    SWAP_BLOCKS 0, 256, 32, 0, 16
-    ; M13, M4
-    SWAP_BLOCKS 0, 384, 32, 0, 24
-
-    ;M6
-    TRANSPOSE_BLOCK 8, 128, 32
-
-    ; M10, M7
-    SWAP_BLOCKS 8, 256, 32, 128, 16
-    ; M14, M8
-    SWAP_BLOCKS 8, 384, 32, 128, 24
-
-    ;M11
-    TRANSPOSE_BLOCK 16, 256, 32
-
-    ; M15, M12
-    SWAP_BLOCKS 16, 384, 32, 256, 24
-
-    ;M16
-    TRANSPOSE_BLOCK 24, 384, 32
-
-    ret
-%endmacro
-
-; void ff_hevc_idct_16x16_{8,10}_<opt>(int16_t *coeffs, int col_limit)
-; %1 = bitdepth
-%macro IDCT_16x16 1
-cglobal hevc_idct_16x16_%1, 1, 2, 16, coeffs
-    mov r1d, 3
-.loop16:
-    TR_16x4 8 * r1, 7, [pd_64], 64, 2, 32, 8, 16, 1, 0
-    dec r1d
-    jge .loop16
-
-    call hevc_idct_transpose_16x16_ %+ cpuname
-
-    DEFINE_BIAS %1
-    mov r1d, 3
-.loop16_2:
-    TR_16x4 8 * r1, shift, [arr_add], 64, 2, 32, 8, 16, 1, 1
-    dec r1d
-    jge .loop16_2
-
-    TAIL_CALL hevc_idct_transpose_16x16_ %+ cpuname, 1
-%endmacro
-
-; scale, pack (clip16) and store the residuals     0 e32[0] + o32[0] --> %1
-; 4 at one time (4 columns)                        1 e32[1] + o32[1]
-; %1 - address to store e32 + o32
-; %2 - address to store e32 - e32
-; %5 - reg with e32 + o32                                  ...
-; %3 - reg with e32 - o32                          30 e32[1] - o32[1]
-; %4 - shift                                       31 e32[0] - o32[0] --> %2
-%macro STORE_32 5
-    psrad    %5, %4
-    psrad    %3, %4
-    packssdw %5, %3
-    movq     [%1], %5
-    movhps   [%2], %5
-%endmacro
-
-; %1 - transform coeffs
-; %2 - stack offset for e32
-; %2, %3 offsets for storing e+o/e-o back to coeffsq
-; %4 - shift
-; %5 - stack offset of e32
-%macro E32_O32 5
-    ADD_ROWS [%1],          [%1 +     16], m0, m1, 1, m8, m9, m10
-    ADD_ROWS [%1 + 2 * 16], [%1 + 3 * 16], m2, m3, 0, m8, m9, m10
-    ADD_ROWS [%1 + 4 * 16], [%1 + 5 * 16], m4, m5, 0, m8, m9, m10
-    ADD_ROWS [%1 + 6 * 16], [%1 + 7 * 16], m6, m7, 0, m8, m9, m10
-
-    paddd m11, m14, [rsp + %5]
-    paddd m12, m10, m11 ; o32 + e32
-    psubd m11, m10      ; e32 - o32
-    STORE_32 %2, %3, m11, %4, m12
-%endmacro
-
-; %1 - horizontal offset
-; %2 - bitdepth
-%macro TR_32x4 3
-    TR_16x4 %1, 7, [pd_64], 128, 4, 64, 16, 16, 2, 0
-
-    LOAD_BLOCK m0, m1,      64,  3 * 64,  5 * 64,  7 * 64, %1
-    LOAD_BLOCK m2, m3,  9 * 64, 11 * 64, 13 * 64, 15 * 64, %1
-    LOAD_BLOCK m4, m5, 17 * 64, 19 * 64, 21 * 64, 23 * 64, %1
-    LOAD_BLOCK m6, m7, 25 * 64, 27 * 64, 29 * 64, 31 * 64, %1
-
-    SBUTTERFLY wd, 0, 1, 8
-    SBUTTERFLY wd, 2, 3, 8
-    SBUTTERFLY wd, 4, 5, 8
-    SBUTTERFLY wd, 6, 7, 8
-
-%if %3 == 1
-    %assign shift 7
-    mova m14, [pd_64]
-%else
-    LOAD_BIAS %2, m14
-%endif
-
-    lea r2, [trans_coeff32 + 15 * 128]
-    lea r3, [coeffsq + %1]
-    lea r4, [r3 + 16 * 64]
-    mov r5d, 15 * 16
-%%loop:
-    E32_O32 r2, r3 + r5 * 4, r4, shift, r5
-    sub r2, 128
-    add r4, 64
-    sub r5d, 16
-    jge %%loop
-%endmacro
-
-%macro TRANSPOSE_32x32 0
-cglobal hevc_idct_transpose_32x32, 0, 0, 0
-    ; M0  M1 ... M7
-    ; M8         M15
-    ;
-    ; ...
-    ;
-    ; M56        M63
-
-    TRANSPOSE_BLOCK 0, 0, 64 ; M1
-    mov r1d, 7
-    mov r2d, 7 * 256
-.loop_transpose:
-    SWAP_BLOCKS 0, r2, 64, 0, r1 * 8
-    sub r2d, 256
-    dec r1d
-    jg .loop_transpose
-
-    TRANSPOSE_BLOCK 8, 256, 64 ; M9
-    mov r1d, 6
-    mov r2d, 512
-    mov r3d, 16
-.loop_transpose2:
-    SWAP_BLOCKS 8, r2, 64, 256, r3
-    add r3d, 8
-    add r2d, 256
-    dec r1d
-    jg .loop_transpose2
-
-    TRANSPOSE_BLOCK 2 * 8, 2 * 256, 64 ; M9
-    mov r1d, 5
-    mov r2d, 768
-    mov r3d, 24
-.loop_transpose3:
-    SWAP_BLOCKS 2 * 8, r2, 64, 2 * 256, r3
-    add r3d, 8
-    add r2d, 256
-    dec r1d
-    jg .loop_transpose3
-
-    TRANSPOSE_BLOCK 3 * 8, 3 * 256, 64 ; M27
-    mov r1d, 4
-    mov r2d, 1024
-    mov r3d, 32
-.loop_transpose4:
-    SWAP_BLOCKS 3 * 8, r2, 64, 3 * 256, r3
-    add r3d, 8
-    add r2d, 256
-    dec r1d
-    jg .loop_transpose4
-
-    TRANSPOSE_BLOCK 4 * 8, 4 * 256, 64 ; M36
-    mov r1d, 3
-    mov r2d, 1280
-    mov r3d, 40
-.loop_transpose5:
-    SWAP_BLOCKS 4 * 8, r2, 64, 4 * 256, r3
-    add r3d, 8
-    add r2d, 256
-    dec r1d
-    jg .loop_transpose5
-
-    TRANSPOSE_BLOCK 5 * 8, 5 * 256, 64 ; M45
-    SWAP_BLOCKS 5 * 8, 6 * 256, 64, 5 * 256, 6 * 8
-    SWAP_BLOCKS 5 * 8, 7 * 256, 64, 5 * 256, 7 * 8
-
-    TRANSPOSE_BLOCK 6 * 8, 6 * 256, 64 ; M54
-    SWAP_BLOCKS 6 * 8, 7 * 256, 64, 6 * 256, 7 * 8
-
-    TRANSPOSE_BLOCK 7 * 8, 7 * 256, 64 ; M63
-
-    ret
-%endmacro
-
-; void ff_hevc_idct_32x32_{8,10}_<opt>(int16_t *coeffs, int col_limit)
-; %1 = bitdepth
-%macro IDCT_32x32 1
-cglobal hevc_idct_32x32_%1, 1, 6, 16, 256, coeffs
-    mov r1d, 7
-.loop32:
-    TR_32x4 8 * r1, %1, 1
-    dec r1d
-    jge .loop32
-
-    call hevc_idct_transpose_32x32_ %+ cpuname
-
-    mov r1d, 7
-.loop32_2:
-    TR_32x4 8 * r1, %1, 0
-    dec r1d
-    jge .loop32_2
-
-    TAIL_CALL hevc_idct_transpose_32x32_ %+ cpuname, 1
-%endmacro
-
-%macro INIT_IDCT_DC 1
-INIT_MMX mmxext
-IDCT_DC_NL  4,      %1
-IDCT_DC     8,  2,  %1
-
-INIT_XMM sse2
-IDCT_DC_NL  8,      %1
-IDCT_DC    16,  4,  %1
-IDCT_DC    32, 16,  %1
-
-%if HAVE_AVX2_EXTERNAL
-    INIT_YMM avx2
-    IDCT_DC    16,  2,  %1
-    IDCT_DC    32,  8,  %1
-%endif ;HAVE_AVX2_EXTERNAL
-%endmacro
-
-%macro INIT_IDCT 2
-INIT_XMM %2
-%if %1 == 8
-    TRANSPOSE_8x8
-    %if ARCH_X86_64
-        TRANSPOSE_16x16
-        TRANSPOSE_32x32
-    %endif
-%endif
-%if ARCH_X86_64
-    IDCT_32x32 %1
-    IDCT_16x16 %1
-%endif
-IDCT_8x8 %1
-IDCT_4x4 %1
-%endmacro
-
-INIT_IDCT_DC 8
-INIT_IDCT_DC 10
-INIT_IDCT_DC 12
-INIT_IDCT 8, sse2
-INIT_IDCT 8, avx
-INIT_IDCT 10, sse2
-INIT_IDCT 10, avx
-;INIT_IDCT 12, sse2
-;INIT_IDCT 12, avx
diff -uparN ffmpeg-4.1/libavcodec/x86/hevc_mc.asm ffmpeg-y/libavcodec/x86/hevc_mc.asm
--- ffmpeg-4.1/libavcodec/x86/hevc_mc.asm	2018-01-01 06:35:48.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/hevc_mc.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,1672 +0,0 @@
-; /*
-; * Provide SSE luma and chroma mc functions for HEVC decoding
-; * Copyright (c) 2013 Pierre-Edouard LEPERE
-; *
-; * This file is part of FFmpeg.
-; *
-; * FFmpeg is free software; you can redistribute it and/or
-; * modify it under the terms of the GNU Lesser General Public
-; * License as published by the Free Software Foundation; either
-; * version 2.1 of the License, or (at your option) any later version.
-; *
-; * FFmpeg is distributed in the hope that it will be useful,
-; * but WITHOUT ANY WARRANTY; without even the implied warranty of
-; * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-; * Lesser General Public License for more details.
-; *
-; * You should have received a copy of the GNU Lesser General Public
-; * License along with FFmpeg; if not, write to the Free Software
-; * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-; */
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA 32
-cextern pw_255
-cextern pw_512
-cextern pw_2048
-cextern pw_8192
-cextern pw_1023
-cextern pw_1024
-cextern pw_4096
-%define pw_8 pw_512
-%define pw_10 pw_2048
-%define pw_12 pw_8192
-%define pw_bi_10 pw_1024
-%define pw_bi_12 pw_4096
-%define max_pixels_8 pw_255
-%define max_pixels_10 pw_1023
-pw_bi_8:                times 16 dw  (1 <<  8)
-max_pixels_12:          times 16 dw ((1 << 12)-1)
-cextern pd_1
-cextern pb_0
-
-%macro EPEL_TABLE 4
-hevc_epel_filters_%4_%1 times %2 d%3 -2, 58
-                        times %2 d%3 10, -2
-                        times %2 d%3 -4, 54
-                        times %2 d%3 16, -2
-                        times %2 d%3 -6, 46
-                        times %2 d%3 28, -4
-                        times %2 d%3 -4, 36
-                        times %2 d%3 36, -4
-                        times %2 d%3 -4, 28
-                        times %2 d%3 46, -6
-                        times %2 d%3 -2, 16
-                        times %2 d%3 54, -4
-                        times %2 d%3 -2, 10
-                        times %2 d%3 58, -2
-%endmacro
-
-
-EPEL_TABLE  8,16, b, avx2
-EPEL_TABLE 10, 8, w, avx2
-
-EPEL_TABLE  8, 8, b, sse4
-EPEL_TABLE 10, 4, w, sse4
-EPEL_TABLE 12, 4, w, sse4
-
-%macro QPEL_TABLE 4
-hevc_qpel_filters_%4_%1 times %2 d%3  -1,  4
-                        times %2 d%3 -10, 58
-                        times %2 d%3  17, -5
-                        times %2 d%3   1,  0
-                        times %2 d%3  -1,  4
-                        times %2 d%3 -11, 40
-                        times %2 d%3  40,-11
-                        times %2 d%3   4, -1
-                        times %2 d%3   0,  1
-                        times %2 d%3  -5, 17
-                        times %2 d%3  58,-10
-                        times %2 d%3   4, -1
-%endmacro
-
-QPEL_TABLE  8, 8, b, sse4
-QPEL_TABLE 10, 4, w, sse4
-QPEL_TABLE 12, 4, w, sse4
-
-QPEL_TABLE  8,16, b, avx2
-QPEL_TABLE 10, 8, w, avx2
-
-SECTION .text
-
-%define MAX_PB_SIZE  64
-
-%define hevc_qpel_filters_sse4_14 hevc_qpel_filters_sse4_10
-
-%define hevc_qpel_filters_avx2_14 hevc_qpel_filters_avx2_10
-
-%if ARCH_X86_64
-
-%macro SIMPLE_BILOAD 4   ;width, tab, r1, r2
-%if %1 <= 4
-    movq              %3, [%2]                                              ; load data from source2
-%elif %1 <= 8
-    movdqa            %3, [%2]                                              ; load data from source2
-%elif %1 <= 12
-%if cpuflag(avx2)
-    mova              %3, [%2]
-%else
-    movdqa            %3, [%2]                                              ; load data from source2
-    movq              %4, [%2+16]                                           ; load data from source2
-%endif ;avx
-%elif %1 <= 16
-%if cpuflag(avx2)
-    mova              %3, [%2]
-%else
-    movdqa            %3, [%2]                                              ; load data from source2
-    movdqa            %4, [%2+16]                                           ; load data from source2
-%endif ; avx
-%else ; %1 = 32
-    mova              %3, [%2]
-    mova              %4, [%2+32]
-%endif
-%endmacro
-
-%macro SIMPLE_LOAD 4    ;width, bitd, tab, r1
-%if %1 == 2 || (%2 == 8 && %1 <= 4)
-    movd              %4, [%3]                                               ; load data from source
-%elif %1 == 4 || (%2 == 8 && %1 <= 8)
-    movq              %4, [%3]                                               ; load data from source
-%elif notcpuflag(avx)
-    movu              %4, [%3]                                               ; load data from source
-%elif %1 <= 8 || (%2 == 8 && %1 <= 16)
-    movdqu           %4, [%3]
-%else
-    movu              %4, [%3]
-%endif
-%endmacro
-
-
-%macro EPEL_FILTER 5 ; bit depth, filter index, xmma, xmmb, gprtmp
-%if cpuflag(avx2)
-%assign %%offset 32
-%ifdef PIC
-    lea              %5q, [hevc_epel_filters_avx2_%1]
-    %define FILTER %5q
-%else
-    %define FILTER hevc_epel_filters_avx2_%1
-%endif
-%else
-%assign %%offset 16
-%ifdef PIC
-    lea              %5q, [hevc_epel_filters_sse4_%1]
-    %define FILTER %5q
-%else
-    %define FILTER hevc_epel_filters_sse4_%1
-%endif
-%endif ;cpuflag(avx2)
-    sub              %2q, 1
-%if cpuflag(avx2)
-    shl              %2q, 6                      ; multiply by 64
-  %else
-    shl              %2q, 5                      ; multiply by 32
-%endif
-    mova           %3, [FILTER + %2q]        ; get 2 first values of filters
-    mova           %4, [FILTER + %2q+%%offset]     ; get 2 last values of filters
-%endmacro
-
-%macro EPEL_HV_FILTER 1
-%if cpuflag(avx2)
-%assign %%offset 32
-%assign %%shift  6
-%define %%table  hevc_epel_filters_avx2_%1
-%else
-%assign %%offset 16
-%assign %%shift  5
-%define %%table  hevc_epel_filters_sse4_%1
-%endif
-
-%ifdef PIC
-    lea           r3srcq, [%%table]
-    %define FILTER r3srcq
-%else
-    %define FILTER %%table
-%endif
-    sub              mxq, 1
-    sub              myq, 1
-    shl              mxq, %%shift                ; multiply by 32
-    shl              myq, %%shift                ; multiply by 32
-    mova             m14, [FILTER + mxq]        ; get 2 first values of filters
-    mova             m15, [FILTER + mxq+%%offset]     ; get 2 last values of filters
-
-%if cpuflag(avx2)
-%define %%table  hevc_epel_filters_avx2_10
-%else
-%define %%table  hevc_epel_filters_sse4_10
-%endif
-%ifdef PIC
-    lea           r3srcq, [%%table]
-    %define FILTER r3srcq
-%else
-    %define FILTER %%table
-%endif
-    mova             m12, [FILTER + myq]        ; get 2 first values of filters
-    mova             m13, [FILTER + myq+%%offset]     ; get 2 last values of filters
-    lea           r3srcq, [srcstrideq*3]
-%endmacro
-
-%macro QPEL_FILTER 2
-
-%if cpuflag(avx2)
-%assign %%offset 32
-%assign %%shift  7
-%define %%table  hevc_qpel_filters_avx2_%1
-%else
-%assign %%offset 16
-%assign %%shift  6
-%define %%table  hevc_qpel_filters_sse4_%1
-%endif
-
-%ifdef PIC
-    lea         rfilterq, [%%table]
-%else
-    %define rfilterq %%table
-%endif
-    sub              %2q, 1
-    shl              %2q, %%shift                        ; multiply by 32
-    mova             m12, [rfilterq + %2q]               ; get 4 first values of filters
-    mova             m13, [rfilterq + %2q +   %%offset]  ; get 4 first values of filters
-    mova             m14, [rfilterq + %2q + 2*%%offset]  ; get 4 first values of filters
-    mova             m15, [rfilterq + %2q + 3*%%offset]  ; get 4 first values of filters
-%endmacro
-
-%macro EPEL_LOAD 4
-%if (%1 == 8 && %4 <= 4)
-%define %%load movd
-%elif (%1 == 8 && %4 <= 8) || (%1 > 8 && %4 <= 4)
-%define %%load movq
-%else
-%define %%load movdqu
-%endif
-
-    %%load            m0, [%2q ]
-%ifnum %3
-    %%load            m1, [%2q+  %3]
-    %%load            m2, [%2q+2*%3]
-    %%load            m3, [%2q+3*%3]
-%else
-    %%load            m1, [%2q+  %3q]
-    %%load            m2, [%2q+2*%3q]
-    %%load            m3, [%2q+r3srcq]
-%endif
-%if %1 == 8
-%if %4 > 8
-    SBUTTERFLY        bw, 0, 1, 7
-    SBUTTERFLY        bw, 2, 3, 7
-%else
-    punpcklbw         m0, m1
-    punpcklbw         m2, m3
-%endif
-%else
-%if %4 > 4
-    SBUTTERFLY        wd, 0, 1, 7
-    SBUTTERFLY        wd, 2, 3, 7
-%else
-    punpcklwd         m0, m1
-    punpcklwd         m2, m3
-%endif
-%endif
-%endmacro
-
-
-%macro QPEL_H_LOAD 4
-%assign %%stride (%1+7)/8
-%if %1 == 8
-%if %3 <= 4
-%define %%load movd
-%elif %3 == 8
-%define %%load movq
-%else
-%define %%load movu
-%endif
-%else
-%if %3 == 2
-%define %%load movd
-%elif %3 == 4
-%define %%load movq
-%else
-%define %%load movu
-%endif
-%endif
-    %%load            m0, [%2-3*%%stride]        ;load data from source
-    %%load            m1, [%2-2*%%stride]
-    %%load            m2, [%2-%%stride  ]
-    %%load            m3, [%2           ]
-    %%load            m4, [%2+%%stride  ]
-    %%load            m5, [%2+2*%%stride]
-    %%load            m6, [%2+3*%%stride]
-    %%load            m7, [%2+4*%%stride]
-
-%if %1 == 8
-%if %3 > 8
-    SBUTTERFLY        wd, 0, 1, %4
-    SBUTTERFLY        wd, 2, 3, %4
-    SBUTTERFLY        wd, 4, 5, %4
-    SBUTTERFLY        wd, 6, 7, %4
-%else
-    punpcklbw         m0, m1
-    punpcklbw         m2, m3
-    punpcklbw         m4, m5
-    punpcklbw         m6, m7
-%endif
-%else
-%if %3 > 4
-    SBUTTERFLY        dq, 0, 1, %4
-    SBUTTERFLY        dq, 2, 3, %4
-    SBUTTERFLY        dq, 4, 5, %4
-    SBUTTERFLY        dq, 6, 7, %4
-%else
-    punpcklwd         m0, m1
-    punpcklwd         m2, m3
-    punpcklwd         m4, m5
-    punpcklwd         m6, m7
-%endif
-%endif
-%endmacro
-
-%macro QPEL_V_LOAD 5
-    lea              %5q, [%2]
-    sub              %5q, r3srcq
-    movu              m0, [%5q            ]      ;load x- 3*srcstride
-    movu              m1, [%5q+   %3q     ]      ;load x- 2*srcstride
-    movu              m2, [%5q+ 2*%3q     ]      ;load x-srcstride
-    movu              m3, [%2       ]      ;load x
-    movu              m4, [%2+   %3q]      ;load x+stride
-    movu              m5, [%2+ 2*%3q]      ;load x+2*stride
-    movu              m6, [%2+r3srcq]      ;load x+3*stride
-    movu              m7, [%2+ 4*%3q]      ;load x+4*stride
-%if %1 == 8
-%if %4 > 8
-    SBUTTERFLY        bw, 0, 1, 8
-    SBUTTERFLY        bw, 2, 3, 8
-    SBUTTERFLY        bw, 4, 5, 8
-    SBUTTERFLY        bw, 6, 7, 8
-%else
-    punpcklbw         m0, m1
-    punpcklbw         m2, m3
-    punpcklbw         m4, m5
-    punpcklbw         m6, m7
-%endif
-%else
-%if %4 > 4
-    SBUTTERFLY        wd, 0, 1, 8
-    SBUTTERFLY        wd, 2, 3, 8
-    SBUTTERFLY        wd, 4, 5, 8
-    SBUTTERFLY        wd, 6, 7, 8
-%else
-    punpcklwd         m0, m1
-    punpcklwd         m2, m3
-    punpcklwd         m4, m5
-    punpcklwd         m6, m7
-%endif
-%endif
-%endmacro
-
-%macro PEL_12STORE2 3
-    movd           [%1], %2
-%endmacro
-%macro PEL_12STORE4 3
-    movq           [%1], %2
-%endmacro
-%macro PEL_12STORE6 3
-    movq           [%1], %2
-    psrldq            %2, 8
-    movd         [%1+8], %2
-%endmacro
-%macro PEL_12STORE8 3
-    movdqa         [%1], %2
-%endmacro
-%macro PEL_12STORE12 3
-    movdqa         [%1], %2
-    movq        [%1+16], %3
-%endmacro
-%macro PEL_12STORE16 3
-    PEL_12STORE8      %1, %2, %3
-    movdqa       [%1+16], %3
-%endmacro
-
-%macro PEL_10STORE2 3
-    movd           [%1], %2
-%endmacro
-%macro PEL_10STORE4 3
-    movq           [%1], %2
-%endmacro
-%macro PEL_10STORE6 3
-    movq           [%1], %2
-    psrldq            %2, 8
-    movd         [%1+8], %2
-%endmacro
-%macro PEL_10STORE8 3
-    movdqa         [%1], %2
-%endmacro
-%macro PEL_10STORE12 3
-    movdqa         [%1], %2
-    movq        [%1+16], %3
-%endmacro
-%macro PEL_10STORE16 3
-%if cpuflag(avx2)
-    movu            [%1], %2
-%else
-    PEL_10STORE8      %1, %2, %3
-    movdqa       [%1+16], %3
-%endif
-%endmacro
-
-%macro PEL_10STORE32 3
-    PEL_10STORE16     %1, %2, %3
-    movu         [%1+32], %3
-%endmacro
-
-%macro PEL_8STORE2 3
-    pextrw          [%1], %2, 0
-%endmacro
-%macro PEL_8STORE4 3
-    movd            [%1], %2
-%endmacro
-%macro PEL_8STORE6 3
-    movd            [%1], %2
-    pextrw        [%1+4], %2, 2
-%endmacro
-%macro PEL_8STORE8 3
-    movq           [%1], %2
-%endmacro
-%macro PEL_8STORE12 3
-    movq            [%1], %2
-    psrldq            %2, 8
-    movd          [%1+8], %2
-%endmacro
-%macro PEL_8STORE16 3
-%if cpuflag(avx2)
-    movdqu        [%1], %2
-%else
-    mova          [%1], %2
-%endif ; avx
-%endmacro
-%macro PEL_8STORE32 3
-    movu          [%1], %2
-%endmacro
-
-%macro LOOP_END 3
-    add              %1q, 2*MAX_PB_SIZE          ; dst += dststride
-    add              %2q, %3q                    ; src += srcstride
-    dec          heightd                         ; cmp height
-    jnz               .loop                      ; height loop
-%endmacro
-
-
-%macro MC_PIXEL_COMPUTE 2-3 ;width, bitdepth
-%if %2 == 8
-%if cpuflag(avx2) && %0 ==3
-%if %1 > 16
-    vextracti128 xm1, m0, 1
-    pmovzxbw      m1, xm1
-    psllw         m1, 14-%2
-%endif
-    pmovzxbw      m0, xm0
-%else ; not avx
-%if %1 > 8
-    punpckhbw     m1, m0, m2
-    psllw         m1, 14-%2
-%endif
-    punpcklbw     m0, m2
-%endif
-%endif ;avx
-    psllw         m0, 14-%2
-%endmacro
-
-%macro EPEL_COMPUTE 4-8 ; bitdepth, width, filter1, filter2, HV/m0, m2, m1, m3
-%if %0 == 8
-%define %%reg0 %5
-%define %%reg2 %6
-%define %%reg1 %7
-%define %%reg3 %8
-%else
-%define %%reg0 m0
-%define %%reg2 m2
-%define %%reg1 m1
-%define %%reg3 m3
-%endif
-%if %1 == 8
-%if cpuflag(avx2) && (%0 == 5)
-%if %2 > 16
-    vperm2i128    m10, m0, m1, q0301
-%endif
-    vinserti128    m0, m0, xm1, 1
-    mova           m1, m10
-%if %2 > 16
-    vperm2i128    m10, m2, m3, q0301
-%endif
-    vinserti128    m2, m2, xm3, 1
-    mova           m3, m10
-%endif
-    pmaddubsw      %%reg0, %3   ;x1*c1+x2*c2
-    pmaddubsw      %%reg2, %4   ;x3*c3+x4*c4
-    paddw          %%reg0, %%reg2
-%if %2 > 8
-    pmaddubsw      %%reg1, %3
-    pmaddubsw      %%reg3, %4
-    paddw          %%reg1, %%reg3
-%endif
-%else
-    pmaddwd        %%reg0, %3
-    pmaddwd        %%reg2, %4
-    paddd          %%reg0, %%reg2
-%if %2 > 4
-    pmaddwd        %%reg1, %3
-    pmaddwd        %%reg3, %4
-    paddd          %%reg1, %%reg3
-%if %1 != 8
-    psrad          %%reg1, %1-8
-%endif
-%endif
-%if %1 != 8
-    psrad          %%reg0, %1-8
-%endif
-    packssdw       %%reg0, %%reg1
-%endif
-%endmacro
-
-%macro QPEL_HV_COMPUTE 4     ; width, bitdepth, filter idx
-
-%if cpuflag(avx2)
-%assign %%offset 32
-%define %%table  hevc_qpel_filters_avx2_%2
-%else
-%assign %%offset 16
-%define %%table  hevc_qpel_filters_sse4_%2
-%endif
-
-%ifdef PIC
-    lea         rfilterq, [%%table]
-%else
-    %define rfilterq %%table
-%endif
-
-%if %2 == 8
-    pmaddubsw         m0, [rfilterq + %3q*8   ]   ;x1*c1+x2*c2
-    pmaddubsw         m2, [rfilterq + %3q*8+%%offset]   ;x3*c3+x4*c4
-    pmaddubsw         m4, [rfilterq + %3q*8+2*%%offset]   ;x5*c5+x6*c6
-    pmaddubsw         m6, [rfilterq + %3q*8+3*%%offset]   ;x7*c7+x8*c8
-    paddw             m0, m2
-    paddw             m4, m6
-    paddw             m0, m4
-%else
-    pmaddwd           m0, [rfilterq + %3q*8   ]
-    pmaddwd           m2, [rfilterq + %3q*8+%%offset]
-    pmaddwd           m4, [rfilterq + %3q*8+2*%%offset]
-    pmaddwd           m6, [rfilterq + %3q*8+3*%%offset]
-    paddd             m0, m2
-    paddd             m4, m6
-    paddd             m0, m4
-%if %2 != 8
-    psrad             m0, %2-8
-%endif
-%if %1 > 4
-    pmaddwd           m1, [rfilterq + %3q*8   ]
-    pmaddwd           m3, [rfilterq + %3q*8+%%offset]
-    pmaddwd           m5, [rfilterq + %3q*8+2*%%offset]
-    pmaddwd           m7, [rfilterq + %3q*8+3*%%offset]
-    paddd             m1, m3
-    paddd             m5, m7
-    paddd             m1, m5
-%if %2 != 8
-    psrad             m1, %2-8
-%endif
-%endif
-    p%4               m0, m1
-%endif
-%endmacro
-
-%macro QPEL_COMPUTE 2-3     ; width, bitdepth
-%if %2 == 8
-%if cpuflag(avx2) && (%0 == 3)
-
-    vperm2i128 m10, m0,  m1, q0301
-    vinserti128 m0, m0, xm1, 1
-    SWAP 1, 10
-
-    vperm2i128 m10, m2,  m3, q0301
-    vinserti128 m2, m2, xm3, 1
-    SWAP 3, 10
-
-
-    vperm2i128 m10, m4,  m5, q0301
-    vinserti128 m4, m4, xm5, 1
-    SWAP 5, 10
-
-    vperm2i128 m10, m6,  m7, q0301
-    vinserti128 m6, m6, xm7, 1
-    SWAP 7, 10
-%endif
-
-    pmaddubsw         m0, m12   ;x1*c1+x2*c2
-    pmaddubsw         m2, m13   ;x3*c3+x4*c4
-    pmaddubsw         m4, m14   ;x5*c5+x6*c6
-    pmaddubsw         m6, m15   ;x7*c7+x8*c8
-    paddw             m0, m2
-    paddw             m4, m6
-    paddw             m0, m4
-%if %1 > 8
-    pmaddubsw         m1, m12
-    pmaddubsw         m3, m13
-    pmaddubsw         m5, m14
-    pmaddubsw         m7, m15
-    paddw             m1, m3
-    paddw             m5, m7
-    paddw             m1, m5
-%endif
-%else
-    pmaddwd           m0, m12
-    pmaddwd           m2, m13
-    pmaddwd           m4, m14
-    pmaddwd           m6, m15
-    paddd             m0, m2
-    paddd             m4, m6
-    paddd             m0, m4
-%if %2 != 8
-    psrad             m0, %2-8
-%endif
-%if %1 > 4
-    pmaddwd           m1, m12
-    pmaddwd           m3, m13
-    pmaddwd           m5, m14
-    pmaddwd           m7, m15
-    paddd             m1, m3
-    paddd             m5, m7
-    paddd             m1, m5
-%if %2 != 8
-    psrad             m1, %2-8
-%endif
-%endif
-%endif
-%endmacro
-
-%macro BI_COMPUTE 7-8     ; width, bitd, src1l, src1h, scr2l, scr2h, pw
-    paddsw            %3, %5
-%if %1 > 8
-    paddsw            %4, %6
-%endif
-    UNI_COMPUTE       %1, %2, %3, %4, %7
-%if %0 == 8 && cpuflag(avx2) && (%2 == 8)
-    vpermq            %3, %3, 216
-    vpermq            %4, %4, 216
-%endif
-%endmacro
-
-%macro UNI_COMPUTE 5
-    pmulhrsw          %3, %5
-%if %1 > 8 || (%2 > 8 && %1 > 4)
-    pmulhrsw          %4, %5
-%endif
-%if %2 == 8
-    packuswb          %3, %4
-%else
-    CLIPW             %3, [pb_0], [max_pixels_%2]
-%if (%1 > 8 && notcpuflag(avx)) || %1 > 16
-    CLIPW             %4, [pb_0], [max_pixels_%2]
-%endif
-%endif
-%endmacro
-
-
-; ******************************
-; void put_hevc_mc_pixels(int16_t *dst, ptrdiff_t dststride,
-;                         uint8_t *_src, ptrdiff_t _srcstride,
-;                         int height, int mx, int my)
-; ******************************
-
-%macro HEVC_PUT_HEVC_PEL_PIXELS 2
-HEVC_PEL_PIXELS     %1, %2
-HEVC_UNI_PEL_PIXELS %1, %2
-HEVC_BI_PEL_PIXELS  %1, %2
-%endmacro
-
-%macro HEVC_PEL_PIXELS 2
-cglobal hevc_put_hevc_pel_pixels%1_%2, 4, 4, 3, dst, src, srcstride,height
-    pxor               m2, m2
-.loop:
-    SIMPLE_LOAD       %1, %2, srcq, m0
-    MC_PIXEL_COMPUTE  %1, %2, 1
-    PEL_10STORE%1     dstq, m0, m1
-    LOOP_END         dst, src, srcstride
-    RET
- %endmacro
-
-%macro HEVC_UNI_PEL_PIXELS 2
-cglobal hevc_put_hevc_uni_pel_pixels%1_%2, 5, 5, 2, dst, dststride, src, srcstride,height
-.loop:
-    SIMPLE_LOAD       %1, %2, srcq, m0
-    PEL_%2STORE%1   dstq, m0, m1
-    add             dstq, dststrideq             ; dst += dststride
-    add             srcq, srcstrideq             ; src += srcstride
-    dec          heightd                         ; cmp height
-    jnz               .loop                      ; height loop
-    RET
-%endmacro
-
-%macro HEVC_BI_PEL_PIXELS 2
-cglobal hevc_put_hevc_bi_pel_pixels%1_%2, 6, 6, 6, dst, dststride, src, srcstride, src2, height
-    pxor              m2, m2
-    movdqa            m5, [pw_bi_%2]
-.loop:
-    SIMPLE_LOAD       %1, %2, srcq, m0
-    SIMPLE_BILOAD     %1, src2q, m3, m4
-    MC_PIXEL_COMPUTE  %1, %2, 1
-    BI_COMPUTE        %1, %2, m0, m1, m3, m4, m5, 1
-    PEL_%2STORE%1   dstq, m0, m1
-    add             dstq, dststrideq             ; dst += dststride
-    add             srcq, srcstrideq             ; src += srcstride
-    add            src2q, 2*MAX_PB_SIZE          ; src += srcstride
-    dec          heightd                         ; cmp height
-    jnz               .loop                      ; height loop
-    RET
-%endmacro
-
-
-; ******************************
-; void put_hevc_epel_hX(int16_t *dst, ptrdiff_t dststride,
-;                       uint8_t *_src, ptrdiff_t _srcstride,
-;                       int height, int mx, int my, int width);
-; ******************************
-
-
-%macro HEVC_PUT_HEVC_EPEL 2
-%if cpuflag(avx2)
-%define XMM_REGS  11
-%else
-%define XMM_REGS  8
-%endif
-
-cglobal hevc_put_hevc_epel_h%1_%2, 5, 6, XMM_REGS, dst, src, srcstride, height, mx, rfilter
-%assign %%stride ((%2 + 7)/8)
-    EPEL_FILTER       %2, mx, m4, m5, rfilter
-.loop:
-    EPEL_LOAD         %2, srcq-%%stride, %%stride, %1
-    EPEL_COMPUTE      %2, %1, m4, m5, 1
-    PEL_10STORE%1      dstq, m0, m1
-    LOOP_END         dst, src, srcstride
-    RET
-
-cglobal hevc_put_hevc_uni_epel_h%1_%2, 6, 7, XMM_REGS, dst, dststride, src, srcstride, height, mx, rfilter
-%assign %%stride ((%2 + 7)/8)
-    movdqa            m6, [pw_%2]
-    EPEL_FILTER       %2, mx, m4, m5, rfilter
-.loop:
-    EPEL_LOAD         %2, srcq-%%stride, %%stride, %1
-    EPEL_COMPUTE      %2, %1, m4, m5
-    UNI_COMPUTE       %1, %2, m0, m1, m6
-    PEL_%2STORE%1   dstq, m0, m1
-    add             dstq, dststrideq             ; dst += dststride
-    add             srcq, srcstrideq             ; src += srcstride
-    dec          heightd                         ; cmp height
-    jnz               .loop                      ; height loop
-    RET
-
-cglobal hevc_put_hevc_bi_epel_h%1_%2, 7, 8, XMM_REGS, dst, dststride, src, srcstride, src2, height, mx, rfilter
-    movdqa            m6, [pw_bi_%2]
-    EPEL_FILTER       %2, mx, m4, m5, rfilter
-.loop:
-    EPEL_LOAD         %2, srcq-%%stride, %%stride, %1
-    EPEL_COMPUTE      %2, %1, m4, m5, 1
-    SIMPLE_BILOAD     %1, src2q, m2, m3
-    BI_COMPUTE        %1, %2, m0, m1, m2, m3, m6, 1
-    PEL_%2STORE%1   dstq, m0, m1
-    add             dstq, dststrideq             ; dst += dststride
-    add             srcq, srcstrideq             ; src += srcstride
-    add            src2q, 2*MAX_PB_SIZE          ; src += srcstride
-    dec          heightd                         ; cmp height
-    jnz               .loop                      ; height loop
-    RET
-
-; ******************************
-; void put_hevc_epel_v(int16_t *dst, ptrdiff_t dststride,
-;                      uint8_t *_src, ptrdiff_t _srcstride,
-;                      int height, int mx, int my, int width)
-; ******************************
-
-cglobal hevc_put_hevc_epel_v%1_%2, 4, 6, XMM_REGS, dst, src, srcstride, height, r3src, my
-    movifnidn        myd, mym
-    sub             srcq, srcstrideq
-    EPEL_FILTER       %2, my, m4, m5, r3src
-    lea           r3srcq, [srcstrideq*3]
-.loop:
-    EPEL_LOAD         %2, srcq, srcstride, %1
-    EPEL_COMPUTE      %2, %1, m4, m5, 1
-    PEL_10STORE%1     dstq, m0, m1
-    LOOP_END          dst, src, srcstride
-    RET
-
-cglobal hevc_put_hevc_uni_epel_v%1_%2, 5, 7, XMM_REGS, dst, dststride, src, srcstride, height, r3src, my
-    movifnidn        myd, mym
-    movdqa            m6, [pw_%2]
-    sub             srcq, srcstrideq
-    EPEL_FILTER       %2, my, m4, m5, r3src
-    lea           r3srcq, [srcstrideq*3]
-.loop:
-    EPEL_LOAD         %2, srcq, srcstride, %1
-    EPEL_COMPUTE      %2, %1, m4, m5
-    UNI_COMPUTE       %1, %2, m0, m1, m6
-    PEL_%2STORE%1   dstq, m0, m1
-    add             dstq, dststrideq             ; dst += dststride
-    add             srcq, srcstrideq             ; src += srcstride
-    dec          heightd                         ; cmp height
-    jnz               .loop                      ; height loop
-    RET
-
-
-cglobal hevc_put_hevc_bi_epel_v%1_%2, 6, 8, XMM_REGS, dst, dststride, src, srcstride, src2, height, r3src, my
-    movifnidn        myd, mym
-    movdqa            m6, [pw_bi_%2]
-    sub             srcq, srcstrideq
-    EPEL_FILTER       %2, my, m4, m5, r3src
-    lea           r3srcq, [srcstrideq*3]
-.loop:
-    EPEL_LOAD         %2, srcq, srcstride, %1
-    EPEL_COMPUTE      %2, %1, m4, m5, 1
-    SIMPLE_BILOAD     %1, src2q, m2, m3
-    BI_COMPUTE        %1, %2, m0, m1, m2, m3, m6, 1
-    PEL_%2STORE%1   dstq, m0, m1
-    add             dstq, dststrideq             ; dst += dststride
-    add             srcq, srcstrideq             ; src += srcstride
-    add            src2q, 2*MAX_PB_SIZE          ; src += srcstride
-    dec          heightd                         ; cmp height
-    jnz               .loop                      ; height loop
-    RET
-%endmacro
-
-
-; ******************************
-; void put_hevc_epel_hv(int16_t *dst, ptrdiff_t dststride,
-;                       uint8_t *_src, ptrdiff_t _srcstride,
-;                       int height, int mx, int my, int width)
-; ******************************
-
-%macro HEVC_PUT_HEVC_EPEL_HV 2
-cglobal hevc_put_hevc_epel_hv%1_%2, 6, 7, 16 , dst, src, srcstride, height, mx, my, r3src
-%assign %%stride ((%2 + 7)/8)
-    sub             srcq, srcstrideq
-    EPEL_HV_FILTER    %2
-    EPEL_LOAD         %2, srcq-%%stride, %%stride, %1
-    EPEL_COMPUTE      %2, %1, m14, m15
-%if (%1 > 8 && (%2 == 8))
-    SWAP              m8, m1
-%endif
-    SWAP              m4, m0
-    add             srcq, srcstrideq
-    EPEL_LOAD         %2, srcq-%%stride, %%stride, %1
-    EPEL_COMPUTE      %2, %1, m14, m15
-%if (%1 > 8 && (%2 == 8))
-    SWAP              m9, m1
-%endif
-    SWAP              m5, m0
-    add             srcq, srcstrideq
-    EPEL_LOAD         %2, srcq-%%stride, %%stride, %1
-    EPEL_COMPUTE      %2, %1, m14, m15
-%if (%1 > 8 && (%2 == 8))
-    SWAP             m10, m1
-%endif
-    SWAP              m6, m0
-    add             srcq, srcstrideq
-.loop:
-    EPEL_LOAD         %2, srcq-%%stride, %%stride, %1
-    EPEL_COMPUTE      %2, %1, m14, m15
-%if (%1 > 8 && (%2 == 8))
-    SWAP             m11, m1
-%endif
-    SWAP              m7, m0
-    punpcklwd         m0, m4, m5
-    punpcklwd         m2, m6, m7
-%if %1 > 4
-    punpckhwd         m1, m4, m5
-    punpckhwd         m3, m6, m7
-%endif
-    EPEL_COMPUTE      14, %1, m12, m13
-%if (%1 > 8 && (%2 == 8))
-    punpcklwd         m4, m8, m9
-    punpcklwd         m2, m10, m11
-    punpckhwd         m8, m8, m9
-    punpckhwd         m3, m10, m11
-    EPEL_COMPUTE      14, %1, m12, m13, m4, m2, m8, m3
-%if cpuflag(avx2)
-    vinserti128       m2, m0, xm4, 1
-    vperm2i128        m3, m0, m4, q0301
-    PEL_10STORE%1     dstq, m2, m3
-%else
-    PEL_10STORE%1     dstq, m0, m4
-%endif
-%else
-    PEL_10STORE%1     dstq, m0, m1
-%endif
-    movdqa            m4, m5
-    movdqa            m5, m6
-    movdqa            m6, m7
-%if (%1 > 8 && (%2 == 8))
-    mova              m8, m9
-    mova              m9, m10
-    mova             m10, m11
-%endif
-    LOOP_END         dst, src, srcstride
-    RET
-
-cglobal hevc_put_hevc_uni_epel_hv%1_%2, 7, 8, 16 , dst, dststride, src, srcstride, height, mx, my, r3src
-%assign %%stride ((%2 + 7)/8)
-    sub             srcq, srcstrideq
-    EPEL_HV_FILTER    %2
-    EPEL_LOAD         %2, srcq-%%stride, %%stride, %1
-    EPEL_COMPUTE      %2, %1, m14, m15
-%if (%1 > 8 && (%2 == 8))
-    SWAP              m8, m1
-%endif
-    SWAP              m4, m0
-    add             srcq, srcstrideq
-    EPEL_LOAD         %2, srcq-%%stride, %%stride, %1
-    EPEL_COMPUTE      %2, %1, m14, m15
-%if (%1 > 8 && (%2 == 8))
-    SWAP              m9, m1
-%endif
-    SWAP              m5, m0
-    add             srcq, srcstrideq
-    EPEL_LOAD         %2, srcq-%%stride, %%stride, %1
-    EPEL_COMPUTE      %2, %1, m14, m15
-%if (%1 > 8 && (%2 == 8))
-    SWAP             m10, m1
-%endif
-    SWAP              m6, m0
-    add             srcq, srcstrideq
-.loop:
-    EPEL_LOAD         %2, srcq-%%stride, %%stride, %1
-    EPEL_COMPUTE      %2, %1, m14, m15
-%if (%1 > 8 && (%2 == 8))
-    SWAP             m11, m1
-%endif
-    mova              m7, m0
-    punpcklwd         m0, m4, m5
-    punpcklwd         m2, m6, m7
-%if %1 > 4
-    punpckhwd         m1, m4, m5
-    punpckhwd         m3, m6, m7
-%endif
-    EPEL_COMPUTE      14, %1, m12, m13
-%if (%1 > 8 && (%2 == 8))
-    punpcklwd         m4, m8, m9
-    punpcklwd         m2, m10, m11
-    punpckhwd         m8, m8, m9
-    punpckhwd         m3, m10, m11
-    EPEL_COMPUTE      14, %1, m12, m13, m4, m2, m8, m3
-    UNI_COMPUTE       %1, %2, m0, m4, [pw_%2]
-%else
-    UNI_COMPUTE       %1, %2, m0, m1, [pw_%2]
-%endif
-    PEL_%2STORE%1   dstq, m0, m1
-    mova              m4, m5
-    mova              m5, m6
-    mova              m6, m7
-%if (%1 > 8 && (%2 == 8))
-    mova              m8, m9
-    mova              m9, m10
-    mova             m10, m11
-%endif
-    add             dstq, dststrideq             ; dst += dststride
-    add             srcq, srcstrideq             ; src += srcstride
-    dec          heightd                         ; cmp height
-    jnz               .loop                      ; height loop
-    RET
-
-cglobal hevc_put_hevc_bi_epel_hv%1_%2, 8, 9, 16, dst, dststride, src, srcstride, src2, height, mx, my, r3src
-%assign %%stride ((%2 + 7)/8)
-    sub             srcq, srcstrideq
-    EPEL_HV_FILTER    %2
-    EPEL_LOAD         %2, srcq-%%stride, %%stride, %1
-    EPEL_COMPUTE      %2, %1, m14, m15
-%if (%1 > 8 && (%2 == 8))
-    SWAP              m8, m1
-%endif
-    SWAP              m4, m0
-    add             srcq, srcstrideq
-    EPEL_LOAD         %2, srcq-%%stride, %%stride, %1
-    EPEL_COMPUTE      %2, %1, m14, m15
-%if (%1 > 8 && (%2 == 8))
-    SWAP              m9, m1
-%endif
-    SWAP              m5, m0
-    add             srcq, srcstrideq
-    EPEL_LOAD         %2, srcq-%%stride, %%stride, %1
-    EPEL_COMPUTE      %2, %1, m14, m15
-%if (%1 > 8 && (%2 == 8))
-    SWAP             m10, m1
-%endif
-    SWAP              m6, m0
-    add             srcq, srcstrideq
-.loop:
-    EPEL_LOAD         %2, srcq-%%stride, %%stride, %1
-    EPEL_COMPUTE      %2, %1, m14, m15
-%if (%1 > 8 && (%2 == 8))
-    SWAP             m11, m1
-%endif
-    SWAP              m7, m0
-    punpcklwd         m0, m4, m5
-    punpcklwd         m2, m6, m7
-%if %1 > 4
-    punpckhwd         m1, m4, m5
-    punpckhwd         m3, m6, m7
-%endif
-    EPEL_COMPUTE      14, %1, m12, m13
-%if (%1 > 8 && (%2 == 8))
-    punpcklwd         m4, m8, m9
-    punpcklwd         m2, m10, m11
-    punpckhwd         m8, m8, m9
-    punpckhwd         m3, m10, m11
-    EPEL_COMPUTE      14, %1, m12, m13, m4, m2, m8, m3
-    SIMPLE_BILOAD     %1, src2q, m8, m3
-%if cpuflag(avx2)
-    vinserti128       m1, m8, xm3, 1
-    vperm2i128        m2, m8, m3, q0301
-    BI_COMPUTE        %1, %2, m0, m4, m1, m2, [pw_bi_%2]
-%else
-    BI_COMPUTE        %1, %2, m0, m4, m8, m3, [pw_bi_%2]
-%endif
-%else
-    SIMPLE_BILOAD     %1, src2q, m8, m9
-    BI_COMPUTE        %1, %2, m0, m1, m8, m9, [pw_bi_%2]
-%endif
-    PEL_%2STORE%1   dstq, m0, m4
-    mova              m4, m5
-    mova              m5, m6
-    mova              m6, m7
-%if (%1 > 8 && (%2 == 8))
-    mova              m8, m9
-    mova              m9, m10
-    mova             m10, m11
-%endif
-    add             dstq, dststrideq             ; dst += dststride
-    add             srcq, srcstrideq             ; src += srcstride
-    add            src2q, 2*MAX_PB_SIZE          ; src += srcstride
-    dec          heightd                         ; cmp height
-    jnz               .loop                      ; height loop
-    RET
-%endmacro
-
-; ******************************
-; void put_hevc_qpel_hX_X_X(int16_t *dst, ptrdiff_t dststride,
-;                       uint8_t *_src, ptrdiff_t _srcstride,
-;                       int height, int mx, int my, int width)
-; ******************************
-
-%macro HEVC_PUT_HEVC_QPEL 2
-cglobal hevc_put_hevc_qpel_h%1_%2, 5, 6, 16, dst, src, srcstride, height, mx, rfilter
-    QPEL_FILTER       %2, mx
-.loop:
-    QPEL_H_LOAD       %2, srcq, %1, 10
-    QPEL_COMPUTE      %1, %2, 1
-%if %2 > 8
-    packssdw          m0, m1
-%endif
-    PEL_10STORE%1     dstq, m0, m1
-    LOOP_END          dst, src, srcstride
-    RET
-
-cglobal hevc_put_hevc_uni_qpel_h%1_%2, 6, 7, 16 , dst, dststride, src, srcstride, height, mx, rfilter
-    mova              m9, [pw_%2]
-    QPEL_FILTER       %2, mx
-.loop:
-    QPEL_H_LOAD       %2, srcq, %1, 10
-    QPEL_COMPUTE      %1, %2
-%if %2 > 8
-    packssdw          m0, m1
-%endif
-    UNI_COMPUTE       %1, %2, m0, m1, m9
-    PEL_%2STORE%1   dstq, m0, m1
-    add             dstq, dststrideq             ; dst += dststride
-    add             srcq, srcstrideq             ; src += srcstride
-    dec          heightd                         ; cmp height
-    jnz               .loop                      ; height loop
-    RET
-
-cglobal hevc_put_hevc_bi_qpel_h%1_%2, 7, 8, 16 , dst, dststride, src, srcstride, src2, height, mx, rfilter
-    movdqa            m9, [pw_bi_%2]
-    QPEL_FILTER       %2, mx
-.loop:
-    QPEL_H_LOAD       %2, srcq, %1, 10
-    QPEL_COMPUTE      %1, %2, 1
-%if %2 > 8
-    packssdw          m0, m1
-%endif
-    SIMPLE_BILOAD     %1, src2q, m10, m11
-    BI_COMPUTE        %1, %2, m0, m1, m10, m11, m9, 1
-    PEL_%2STORE%1   dstq, m0, m1
-    add             dstq, dststrideq             ; dst += dststride
-    add             srcq, srcstrideq             ; src += srcstride
-    add            src2q, 2*MAX_PB_SIZE          ; src += srcstride
-    dec          heightd                         ; cmp height
-    jnz               .loop                      ; height loop
-    RET
-
-
-; ******************************
-; void put_hevc_qpel_vX_X_X(int16_t *dst, ptrdiff_t dststride,
-;                       uint8_t *_src, ptrdiff_t _srcstride,
-;                       int height, int mx, int my, int width)
-; ******************************
-
-cglobal hevc_put_hevc_qpel_v%1_%2, 4, 8, 16, dst, src, srcstride, height, r3src, my, rfilter
-    movifnidn        myd, mym
-    lea           r3srcq, [srcstrideq*3]
-    QPEL_FILTER       %2, my
-.loop:
-    QPEL_V_LOAD       %2, srcq, srcstride, %1, r7
-    QPEL_COMPUTE      %1, %2, 1
-%if %2 > 8
-    packssdw          m0, m1
-%endif
-    PEL_10STORE%1     dstq, m0, m1
-    LOOP_END         dst, src, srcstride
-    RET
-
-cglobal hevc_put_hevc_uni_qpel_v%1_%2, 5, 9, 16, dst, dststride, src, srcstride, height, r3src, my, rfilter
-    movifnidn        myd, mym
-    movdqa            m9, [pw_%2]
-    lea           r3srcq, [srcstrideq*3]
-    QPEL_FILTER       %2, my
-.loop:
-    QPEL_V_LOAD       %2, srcq, srcstride, %1, r8
-    QPEL_COMPUTE      %1, %2
-%if %2 > 8
-    packssdw          m0, m1
-%endif
-    UNI_COMPUTE       %1, %2, m0, m1, m9
-    PEL_%2STORE%1   dstq, m0, m1
-    add             dstq, dststrideq             ; dst += dststride
-    add             srcq, srcstrideq             ; src += srcstride
-    dec          heightd                         ; cmp height
-    jnz               .loop                      ; height loop
-    RET
-
-cglobal hevc_put_hevc_bi_qpel_v%1_%2, 6, 10, 16, dst, dststride, src, srcstride, src2, height, r3src, my, rfilter
-    movifnidn        myd, mym
-    movdqa            m9, [pw_bi_%2]
-    lea           r3srcq, [srcstrideq*3]
-    QPEL_FILTER       %2, my
-.loop:
-    QPEL_V_LOAD       %2, srcq, srcstride, %1, r9
-    QPEL_COMPUTE      %1, %2, 1
-%if %2 > 8
-    packssdw          m0, m1
-%endif
-    SIMPLE_BILOAD     %1, src2q, m10, m11
-    BI_COMPUTE        %1, %2, m0, m1, m10, m11, m9, 1
-    PEL_%2STORE%1   dstq, m0, m1
-    add             dstq, dststrideq             ; dst += dststride
-    add             srcq, srcstrideq             ; src += srcstride
-    add            src2q, 2*MAX_PB_SIZE          ; src += srcstride
-    dec          heightd                         ; cmp height
-    jnz               .loop                      ; height loop
-    RET
-%endmacro
-
-
-; ******************************
-; void put_hevc_qpel_hvX_X(int16_t *dst, ptrdiff_t dststride,
-;                       uint8_t *_src, ptrdiff_t _srcstride,
-;                       int height, int mx, int my)
-; ******************************
-%macro HEVC_PUT_HEVC_QPEL_HV 2
-cglobal hevc_put_hevc_qpel_hv%1_%2, 6, 8, 16, dst, src, srcstride, height, mx, my, r3src, rfilter
-%if cpuflag(avx2)
-%assign %%shift  4
-%else
-%assign %%shift  3
-%endif
-    sub              mxq, 1
-    sub              myq, 1
-    shl              mxq, %%shift                ; multiply by 32
-    shl              myq, %%shift                ; multiply by 32
-    lea           r3srcq, [srcstrideq*3]
-    sub             srcq, r3srcq
-    QPEL_H_LOAD       %2, srcq, %1, 15
-    QPEL_HV_COMPUTE   %1, %2, mx, ackssdw
-    SWAP              m8, m0
-    add             srcq, srcstrideq
-    QPEL_H_LOAD       %2, srcq, %1, 15
-    QPEL_HV_COMPUTE   %1, %2, mx, ackssdw
-    SWAP              m9, m0
-    add             srcq, srcstrideq
-    QPEL_H_LOAD       %2, srcq, %1, 15
-    QPEL_HV_COMPUTE   %1, %2, mx, ackssdw
-    SWAP             m10, m0
-    add             srcq, srcstrideq
-    QPEL_H_LOAD       %2, srcq, %1, 15
-    QPEL_HV_COMPUTE   %1, %2, mx, ackssdw
-    SWAP             m11, m0
-    add             srcq, srcstrideq
-    QPEL_H_LOAD       %2, srcq, %1, 15
-    QPEL_HV_COMPUTE   %1, %2, mx, ackssdw
-    SWAP             m12, m0
-    add             srcq, srcstrideq
-    QPEL_H_LOAD       %2, srcq, %1, 15
-    QPEL_HV_COMPUTE   %1, %2, mx, ackssdw
-    SWAP             m13, m0
-    add             srcq, srcstrideq
-    QPEL_H_LOAD       %2, srcq, %1, 15
-    QPEL_HV_COMPUTE   %1, %2, mx, ackssdw
-    SWAP             m14, m0
-    add             srcq, srcstrideq
-.loop:
-    QPEL_H_LOAD       %2, srcq, %1, 15
-    QPEL_HV_COMPUTE   %1, %2, mx, ackssdw
-    SWAP             m15, m0
-    punpcklwd         m0, m8, m9
-    punpcklwd         m2, m10, m11
-    punpcklwd         m4, m12, m13
-    punpcklwd         m6, m14, m15
-%if %1 > 4
-    punpckhwd         m1, m8, m9
-    punpckhwd         m3, m10, m11
-    punpckhwd         m5, m12, m13
-    punpckhwd         m7, m14, m15
-%endif
-    QPEL_HV_COMPUTE   %1, 14, my, ackssdw
-    PEL_10STORE%1     dstq, m0, m1
-%if %1 <= 4
-    movq              m8, m9
-    movq              m9, m10
-    movq             m10, m11
-    movq             m11, m12
-    movq             m12, m13
-    movq             m13, m14
-    movq             m14, m15
-%else
-    movdqa            m8, m9
-    movdqa            m9, m10
-    movdqa           m10, m11
-    movdqa           m11, m12
-    movdqa           m12, m13
-    movdqa           m13, m14
-    movdqa           m14, m15
-%endif
-    LOOP_END         dst, src, srcstride
-    RET
-
-cglobal hevc_put_hevc_uni_qpel_hv%1_%2, 7, 9, 16 , dst, dststride, src, srcstride, height, mx, my, r3src, rfilter
-%if cpuflag(avx2)
-%assign %%shift  4
-%else
-%assign %%shift  3
-%endif
-    sub              mxq, 1
-    sub              myq, 1
-    shl              mxq, %%shift                ; multiply by 32
-    shl              myq, %%shift                ; multiply by 32
-    lea           r3srcq, [srcstrideq*3]
-    sub             srcq, r3srcq
-    QPEL_H_LOAD       %2, srcq, %1, 15
-    QPEL_HV_COMPUTE   %1, %2, mx, ackssdw
-    SWAP              m8, m0
-    add             srcq, srcstrideq
-    QPEL_H_LOAD       %2, srcq, %1, 15
-    QPEL_HV_COMPUTE   %1, %2, mx, ackssdw
-    SWAP              m9, m0
-    add             srcq, srcstrideq
-    QPEL_H_LOAD       %2, srcq, %1, 15
-    QPEL_HV_COMPUTE   %1, %2, mx, ackssdw
-    SWAP             m10, m0
-    add             srcq, srcstrideq
-    QPEL_H_LOAD       %2, srcq, %1, 15
-    QPEL_HV_COMPUTE   %1, %2, mx, ackssdw
-    SWAP             m11, m0
-    add             srcq, srcstrideq
-    QPEL_H_LOAD       %2, srcq, %1, 15
-    QPEL_HV_COMPUTE   %1, %2, mx, ackssdw
-    SWAP             m12, m0
-    add             srcq, srcstrideq
-    QPEL_H_LOAD       %2, srcq, %1, 15
-    QPEL_HV_COMPUTE   %1, %2, mx, ackssdw
-    SWAP             m13, m0
-    add             srcq, srcstrideq
-    QPEL_H_LOAD       %2, srcq, %1, 15
-    QPEL_HV_COMPUTE   %1, %2, mx, ackssdw
-    SWAP             m14, m0
-    add             srcq, srcstrideq
-.loop:
-    QPEL_H_LOAD       %2, srcq, %1, 15
-    QPEL_HV_COMPUTE   %1, %2, mx, ackssdw
-    SWAP             m15, m0
-    punpcklwd         m0, m8, m9
-    punpcklwd         m2, m10, m11
-    punpcklwd         m4, m12, m13
-    punpcklwd         m6, m14, m15
-%if %1 > 4
-    punpckhwd         m1, m8, m9
-    punpckhwd         m3, m10, m11
-    punpckhwd         m5, m12, m13
-    punpckhwd         m7, m14, m15
-%endif
-    QPEL_HV_COMPUTE   %1, 14, my, ackusdw
-    UNI_COMPUTE       %1, %2, m0, m1, [pw_%2]
-    PEL_%2STORE%1   dstq, m0, m1
-
-%if %1 <= 4
-    movq              m8, m9
-    movq              m9, m10
-    movq             m10, m11
-    movq             m11, m12
-    movq             m12, m13
-    movq             m13, m14
-    movq             m14, m15
-%else
-    mova            m8, m9
-    mova            m9, m10
-    mova           m10, m11
-    mova           m11, m12
-    mova           m12, m13
-    mova           m13, m14
-    mova           m14, m15
-%endif
-    add             dstq, dststrideq             ; dst += dststride
-    add             srcq, srcstrideq             ; src += srcstride
-    dec          heightd                         ; cmp height
-    jnz               .loop                      ; height loop
-    RET
-
-cglobal hevc_put_hevc_bi_qpel_hv%1_%2, 8, 10, 16, dst, dststride, src, srcstride, src2, height, mx, my, r3src, rfilter
-%if cpuflag(avx2)
-%assign %%shift  4
-%else
-%assign %%shift  3
-%endif
-    sub              mxq, 1
-    sub              myq, 1
-    shl              mxq, %%shift                ; multiply by 32
-    shl              myq, %%shift                ; multiply by 32
-    lea           r3srcq, [srcstrideq*3]
-    sub             srcq, r3srcq
-    QPEL_H_LOAD       %2, srcq, %1, 15
-    QPEL_HV_COMPUTE   %1, %2, mx, ackssdw
-    SWAP              m8, m0
-    add             srcq, srcstrideq
-    QPEL_H_LOAD       %2, srcq, %1, 15
-    QPEL_HV_COMPUTE   %1, %2, mx, ackssdw
-    SWAP              m9, m0
-    add             srcq, srcstrideq
-    QPEL_H_LOAD       %2, srcq, %1, 15
-    QPEL_HV_COMPUTE   %1, %2, mx, ackssdw
-    SWAP             m10, m0
-    add             srcq, srcstrideq
-    QPEL_H_LOAD       %2, srcq, %1, 15
-    QPEL_HV_COMPUTE   %1, %2, mx, ackssdw
-    SWAP             m11, m0
-    add             srcq, srcstrideq
-    QPEL_H_LOAD       %2, srcq, %1, 15
-    QPEL_HV_COMPUTE   %1, %2, mx, ackssdw
-    SWAP             m12, m0
-    add             srcq, srcstrideq
-    QPEL_H_LOAD       %2, srcq, %1, 15
-    QPEL_HV_COMPUTE   %1, %2, mx, ackssdw
-    SWAP             m13, m0
-    add             srcq, srcstrideq
-    QPEL_H_LOAD       %2, srcq, %1, 15
-    QPEL_HV_COMPUTE   %1, %2, mx, ackssdw
-    SWAP             m14, m0
-    add             srcq, srcstrideq
-.loop:
-    QPEL_H_LOAD       %2, srcq, %1, 15
-    QPEL_HV_COMPUTE   %1, %2, mx, ackssdw
-    SWAP             m15, m0
-    punpcklwd         m0, m8, m9
-    punpcklwd         m2, m10, m11
-    punpcklwd         m4, m12, m13
-    punpcklwd         m6, m14, m15
-%if %1 > 4
-    punpckhwd         m1, m8, m9
-    punpckhwd         m3, m10, m11
-    punpckhwd         m5, m12, m13
-    punpckhwd         m7, m14, m15
-%endif
-    QPEL_HV_COMPUTE   %1, 14, my, ackssdw
-    SIMPLE_BILOAD     %1, src2q, m8, m9 ;m9 not used in this case
-    BI_COMPUTE        %1, %2, m0, m1, m8, m9, [pw_bi_%2]
-    PEL_%2STORE%1   dstq, m0, m1
-
-%if %1 <= 4
-    movq              m8, m9
-    movq              m9, m10
-    movq             m10, m11
-    movq             m11, m12
-    movq             m12, m13
-    movq             m13, m14
-    movq             m14, m15
-%else
-    movdqa            m8, m9
-    movdqa            m9, m10
-    movdqa           m10, m11
-    movdqa           m11, m12
-    movdqa           m12, m13
-    movdqa           m13, m14
-    movdqa           m14, m15
-%endif
-    add             dstq, dststrideq             ; dst += dststride
-    add             srcq, srcstrideq             ; src += srcstride
-    add            src2q, 2*MAX_PB_SIZE          ; src += srcstride
-    dec          heightd                         ; cmp height
-    jnz               .loop                      ; height loop
-    RET
-%endmacro
-
-%macro WEIGHTING_FUNCS 2
-%if WIN64 || ARCH_X86_32
-cglobal hevc_put_hevc_uni_w%1_%2, 4, 5, 7, dst, dststride, src, height, denom, wx, ox
-    mov             r4d, denomm
-%define SHIFT  r4d
-%else
-cglobal hevc_put_hevc_uni_w%1_%2, 6, 6, 7, dst, dststride, src, height, denom, wx, ox
-%define SHIFT  denomd
-%endif
-    lea           SHIFT, [SHIFT+14-%2]          ; shift = 14 - bitd + denom
-%if %1 <= 4
-    pxor             m1, m1
-%endif
-    movd             m2, wxm        ; WX
-    movd             m4, SHIFT      ; shift
-%if %1 <= 4
-    punpcklwd        m2, m1
-%else
-    punpcklwd        m2, m2
-%endif
-    dec           SHIFT
-    movdqu           m5, [pd_1]
-    movd             m6, SHIFT
-    pshufd           m2, m2, 0
-    mov           SHIFT, oxm
-    pslld            m5, m6
-%if %2 != 8
-    shl           SHIFT, %2-8       ; ox << (bitd - 8)
-%endif
-    movd             m3, SHIFT      ; OX
-    pshufd           m3, m3, 0
-%if WIN64 || ARCH_X86_32
-    mov           SHIFT, heightm
-%endif
-.loop:
-   SIMPLE_LOAD        %1, 10, srcq, m0
-%if %1 <= 4
-    punpcklwd         m0, m1
-    pmaddwd           m0, m2
-    paddd             m0, m5
-    psrad             m0, m4
-    paddd             m0, m3
-%else
-    pmulhw            m6, m0, m2
-    pmullw            m0, m2
-    punpckhwd         m1, m0, m6
-    punpcklwd         m0, m6
-    paddd             m0, m5
-    paddd             m1, m5
-    psrad             m0, m4
-    psrad             m1, m4
-    paddd             m0, m3
-    paddd             m1, m3
-%endif
-    packssdw          m0, m1
-%if %2 == 8
-    packuswb          m0, m0
-%else
-    CLIPW             m0, [pb_0], [max_pixels_%2]
-%endif
-    PEL_%2STORE%1   dstq, m0, m1
-    add             dstq, dststrideq             ; dst += dststride
-    add             srcq, 2*MAX_PB_SIZE          ; src += srcstride
-    dec          heightd                         ; cmp height
-    jnz               .loop                      ; height loop
-    RET
-
-cglobal hevc_put_hevc_bi_w%1_%2, 4, 6, 10, dst, dststride, src, src2, height, denom, wx0, wx1, ox0, ox1
-    movifnidn        r5d, denomm
-%if %1 <= 4
-    pxor              m1, m1
-%endif
-    movd              m2, wx0m         ; WX0
-    lea              r5d, [r5d+14-%2]  ; shift = 14 - bitd + denom
-    movd              m3, wx1m         ; WX1
-    movd              m0, r5d          ; shift
-%if %1 <= 4
-    punpcklwd         m2, m1
-    punpcklwd         m3, m1
-%else
-    punpcklwd         m2, m2
-    punpcklwd         m3, m3
-%endif
-    inc              r5d
-    movd              m5, r5d          ; shift+1
-    pshufd            m2, m2, 0
-    mov              r5d, ox0m
-    pshufd            m3, m3, 0
-    add              r5d, ox1m
-%if %2 != 8
-    shl              r5d, %2-8         ; ox << (bitd - 8)
-%endif
-    inc              r5d
-    movd              m4, r5d          ; offset
-    pshufd            m4, m4, 0
-%if UNIX64
-%define h heightd
-%else
-    mov              r5d, heightm
-%define h r5d
-%endif
-    pslld             m4, m0
-
-.loop:
-   SIMPLE_LOAD        %1, 10, srcq,  m0
-   SIMPLE_LOAD        %1, 10, src2q, m8
-%if %1 <= 4
-    punpcklwd         m0, m1
-    punpcklwd         m8, m1
-    pmaddwd           m0, m3
-    pmaddwd           m8, m2
-    paddd             m0, m4
-    paddd             m0, m8
-    psrad             m0, m5
-%else
-    pmulhw            m6, m0, m3
-    pmullw            m0, m3
-    pmulhw            m7, m8, m2
-    pmullw            m8, m2
-    punpckhwd         m1, m0, m6
-    punpcklwd         m0, m6
-    punpckhwd         m9, m8, m7
-    punpcklwd         m8, m7
-    paddd             m0, m8
-    paddd             m1, m9
-    paddd             m0, m4
-    paddd             m1, m4
-    psrad             m0, m5
-    psrad             m1, m5
-%endif
-    packssdw          m0, m1
-%if %2 == 8
-    packuswb          m0, m0
-%else
-     CLIPW            m0, [pb_0], [max_pixels_%2]
-%endif
-    PEL_%2STORE%1   dstq, m0, m1
-    add             dstq, dststrideq             ; dst += dststride
-    add             srcq, 2*MAX_PB_SIZE          ; src += srcstride
-    add            src2q, 2*MAX_PB_SIZE          ; src2 += srcstride
-    dec                h                         ; cmp height
-    jnz               .loop                      ; height loop
-    RET
-%endmacro
-
-INIT_XMM sse4                                    ; adds ff_ and _sse4 to function name
-
-WEIGHTING_FUNCS 2, 8
-WEIGHTING_FUNCS 4, 8
-WEIGHTING_FUNCS 6, 8
-WEIGHTING_FUNCS 8, 8
-
-WEIGHTING_FUNCS 2, 10
-WEIGHTING_FUNCS 4, 10
-WEIGHTING_FUNCS 6, 10
-WEIGHTING_FUNCS 8, 10
-
-WEIGHTING_FUNCS 2, 12
-WEIGHTING_FUNCS 4, 12
-WEIGHTING_FUNCS 6, 12
-WEIGHTING_FUNCS 8, 12
-
-HEVC_PUT_HEVC_PEL_PIXELS  2, 8
-HEVC_PUT_HEVC_PEL_PIXELS  4, 8
-HEVC_PUT_HEVC_PEL_PIXELS  6, 8
-HEVC_PUT_HEVC_PEL_PIXELS  8, 8
-HEVC_PUT_HEVC_PEL_PIXELS 12, 8
-HEVC_PUT_HEVC_PEL_PIXELS 16, 8
-
-HEVC_PUT_HEVC_PEL_PIXELS 2, 10
-HEVC_PUT_HEVC_PEL_PIXELS 4, 10
-HEVC_PUT_HEVC_PEL_PIXELS 6, 10
-HEVC_PUT_HEVC_PEL_PIXELS 8, 10
-
-HEVC_PUT_HEVC_PEL_PIXELS 2, 12
-HEVC_PUT_HEVC_PEL_PIXELS 4, 12
-HEVC_PUT_HEVC_PEL_PIXELS 6, 12
-HEVC_PUT_HEVC_PEL_PIXELS 8, 12
-
-HEVC_PUT_HEVC_EPEL 2,  8
-HEVC_PUT_HEVC_EPEL 4,  8
-HEVC_PUT_HEVC_EPEL 6,  8
-HEVC_PUT_HEVC_EPEL 8,  8
-HEVC_PUT_HEVC_EPEL 12, 8
-HEVC_PUT_HEVC_EPEL 16, 8
-
-
-HEVC_PUT_HEVC_EPEL 2, 10
-HEVC_PUT_HEVC_EPEL 4, 10
-HEVC_PUT_HEVC_EPEL 6, 10
-HEVC_PUT_HEVC_EPEL 8, 10
-
-HEVC_PUT_HEVC_EPEL 2, 12
-HEVC_PUT_HEVC_EPEL 4, 12
-HEVC_PUT_HEVC_EPEL 6, 12
-HEVC_PUT_HEVC_EPEL 8, 12
-
-HEVC_PUT_HEVC_EPEL_HV 2,  8
-HEVC_PUT_HEVC_EPEL_HV 4,  8
-HEVC_PUT_HEVC_EPEL_HV 6,  8
-HEVC_PUT_HEVC_EPEL_HV 8,  8
-HEVC_PUT_HEVC_EPEL_HV 16, 8
-
-HEVC_PUT_HEVC_EPEL_HV 2, 10
-HEVC_PUT_HEVC_EPEL_HV 4, 10
-HEVC_PUT_HEVC_EPEL_HV 6, 10
-HEVC_PUT_HEVC_EPEL_HV 8, 10
-
-HEVC_PUT_HEVC_EPEL_HV 2, 12
-HEVC_PUT_HEVC_EPEL_HV 4, 12
-HEVC_PUT_HEVC_EPEL_HV 6, 12
-HEVC_PUT_HEVC_EPEL_HV 8, 12
-
-HEVC_PUT_HEVC_QPEL 4,  8
-HEVC_PUT_HEVC_QPEL 8,  8
-HEVC_PUT_HEVC_QPEL 12, 8
-HEVC_PUT_HEVC_QPEL 16, 8
-
-HEVC_PUT_HEVC_QPEL 4, 10
-HEVC_PUT_HEVC_QPEL 8, 10
-
-HEVC_PUT_HEVC_QPEL 4, 12
-HEVC_PUT_HEVC_QPEL 8, 12
-
-HEVC_PUT_HEVC_QPEL_HV 2, 8
-HEVC_PUT_HEVC_QPEL_HV 4, 8
-HEVC_PUT_HEVC_QPEL_HV 6, 8
-HEVC_PUT_HEVC_QPEL_HV 8, 8
-
-HEVC_PUT_HEVC_QPEL_HV 2, 10
-HEVC_PUT_HEVC_QPEL_HV 4, 10
-HEVC_PUT_HEVC_QPEL_HV 6, 10
-HEVC_PUT_HEVC_QPEL_HV 8, 10
-
-HEVC_PUT_HEVC_QPEL_HV 2, 12
-HEVC_PUT_HEVC_QPEL_HV 4, 12
-HEVC_PUT_HEVC_QPEL_HV 6, 12
-HEVC_PUT_HEVC_QPEL_HV 8, 12
-
-%if HAVE_AVX2_EXTERNAL
-INIT_YMM avx2  ; adds ff_ and _avx2 to function name & enables 256b registers : m0 for 256b, xm0 for 128b. cpuflag(avx2) = 1 / notcpuflag(avx) = 0
-
-HEVC_PUT_HEVC_PEL_PIXELS 32, 8
-HEVC_PUT_HEVC_PEL_PIXELS 16, 10
-
-HEVC_PUT_HEVC_EPEL 32, 8
-HEVC_PUT_HEVC_EPEL 16, 10
-
-HEVC_PUT_HEVC_EPEL_HV 16, 10
-HEVC_PUT_HEVC_EPEL_HV 32, 8
-
-HEVC_PUT_HEVC_QPEL 32, 8
-
-HEVC_PUT_HEVC_QPEL 16, 10
-
-HEVC_PUT_HEVC_QPEL_HV 16, 10
-
-%endif ;AVX2
-%endif ; ARCH_X86_64
diff -uparN ffmpeg-4.1/libavcodec/x86/hevc_sao_10bit.asm ffmpeg-y/libavcodec/x86/hevc_sao_10bit.asm
--- ffmpeg-4.1/libavcodec/x86/hevc_sao_10bit.asm	2018-11-03 08:17:29.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/hevc_sao_10bit.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,370 +0,0 @@
-;******************************************************************************
-;* SIMD optimized SAO functions for HEVC 10/12bit decoding
-;*
-;* Copyright (c) 2013 Pierre-Edouard LEPERE
-;* Copyright (c) 2014 James Almer
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA 32
-
-pw_m2:     times 16 dw -2
-pw_mask10: times 16 dw 0x03FF
-pw_mask12: times 16 dw 0x0FFF
-pb_eo:              db -1, 0, 1, 0, 0, -1, 0, 1, -1, -1, 1, 1, 1, -1, -1, 1
-cextern pw_m1
-cextern pw_1
-cextern pw_2
-
-SECTION .text
-
-;******************************************************************************
-;SAO Band Filter
-;******************************************************************************
-
-%macro HEVC_SAO_BAND_FILTER_INIT 1
-    and            leftq, 31
-    movd             xm0, leftd
-    add            leftq, 1
-    and            leftq, 31
-    movd             xm1, leftd
-    add            leftq, 1
-    and            leftq, 31
-    movd             xm2, leftd
-    add            leftq, 1
-    and            leftq, 31
-    movd             xm3, leftd
-
-    SPLATW            m0, xm0
-    SPLATW            m1, xm1
-    SPLATW            m2, xm2
-    SPLATW            m3, xm3
-%if mmsize > 16
-    SPLATW            m4, [offsetq + 2]
-    SPLATW            m5, [offsetq + 4]
-    SPLATW            m6, [offsetq + 6]
-    SPLATW            m7, [offsetq + 8]
-%else
-    movq              m7, [offsetq + 2]
-    SPLATW            m4, m7, 0
-    SPLATW            m5, m7, 1
-    SPLATW            m6, m7, 2
-    SPLATW            m7, m7, 3
-%endif
-
-%if ARCH_X86_64
-    mova             m13, [pw_mask %+ %1]
-    pxor             m14, m14
-
-%else ; ARCH_X86_32
-    mova  [rsp+mmsize*0], m0
-    mova  [rsp+mmsize*1], m1
-    mova  [rsp+mmsize*2], m2
-    mova  [rsp+mmsize*3], m3
-    mova  [rsp+mmsize*4], m4
-    mova  [rsp+mmsize*5], m5
-    mova  [rsp+mmsize*6], m6
-    mova              m1, [pw_mask %+ %1]
-    pxor              m0, m0
-    %define m14 m0
-    %define m13 m1
-    %define  m9 m2
-    %define  m8 m3
-%endif ; ARCH
-DEFINE_ARGS dst, src, dststride, srcstride, offset, height
-    mov          heightd, r7m
-%endmacro
-
-;void ff_hevc_sao_band_filter_<width>_<depth>_<opt>(uint8_t *_dst, uint8_t *_src, ptrdiff_t _stride_dst, ptrdiff_t _stride_src,
-;                                                   int16_t *sao_offset_val, int sao_left_class, int width, int height);
-%macro HEVC_SAO_BAND_FILTER 3
-cglobal hevc_sao_band_filter_%2_%1, 6, 6, 15, 7*mmsize*ARCH_X86_32, dst, src, dststride, srcstride, offset, left
-    HEVC_SAO_BAND_FILTER_INIT %1
-
-align 16
-.loop:
-
-%assign i 0
-%assign j 0
-%rep %3
-%assign k 8+(j&1)
-%assign l 9-(j&1)
-    mova          m %+ k, [srcq + i]
-    psraw         m %+ l, m %+ k, %1-5
-%if ARCH_X86_64
-    pcmpeqw          m10, m %+ l, m0
-    pcmpeqw          m11, m %+ l, m1
-    pcmpeqw          m12, m %+ l, m2
-    pcmpeqw       m %+ l, m3
-    pand             m10, m4
-    pand             m11, m5
-    pand             m12, m6
-    pand          m %+ l, m7
-    por              m10, m11
-    por              m12, m %+ l
-    por              m10, m12
-    paddw         m %+ k, m10
-%else ; ARCH_X86_32
-    pcmpeqw           m4, m %+ l, [rsp+mmsize*0]
-    pcmpeqw           m5, m %+ l, [rsp+mmsize*1]
-    pcmpeqw           m6, m %+ l, [rsp+mmsize*2]
-    pcmpeqw       m %+ l, [rsp+mmsize*3]
-    pand              m4, [rsp+mmsize*4]
-    pand              m5, [rsp+mmsize*5]
-    pand              m6, [rsp+mmsize*6]
-    pand          m %+ l, m7
-    por               m4, m5
-    por               m6, m %+ l
-    por               m4, m6
-    paddw         m %+ k, m4
-%endif ; ARCH
-    CLIPW             m %+ k, m14, m13
-    mova      [dstq + i], m %+ k
-%assign i i+mmsize
-%assign j j+1
-%endrep
-
-    add             dstq, dststrideq
-    add             srcq, srcstrideq
-    dec          heightd
-    jg .loop
-    REP_RET
-%endmacro
-
-%macro HEVC_SAO_BAND_FILTER_FUNCS 0
-HEVC_SAO_BAND_FILTER 10,  8, 1
-HEVC_SAO_BAND_FILTER 10, 16, 2
-HEVC_SAO_BAND_FILTER 10, 32, 4
-HEVC_SAO_BAND_FILTER 10, 48, 6
-HEVC_SAO_BAND_FILTER 10, 64, 8
-
-HEVC_SAO_BAND_FILTER 12,  8, 1
-HEVC_SAO_BAND_FILTER 12, 16, 2
-HEVC_SAO_BAND_FILTER 12, 32, 4
-HEVC_SAO_BAND_FILTER 12, 48, 6
-HEVC_SAO_BAND_FILTER 12, 64, 8
-%endmacro
-
-INIT_XMM sse2
-HEVC_SAO_BAND_FILTER_FUNCS
-INIT_XMM avx
-HEVC_SAO_BAND_FILTER_FUNCS
-
-%if HAVE_AVX2_EXTERNAL
-INIT_XMM avx2
-HEVC_SAO_BAND_FILTER 10,  8, 1
-INIT_YMM avx2
-HEVC_SAO_BAND_FILTER 10, 16, 1
-HEVC_SAO_BAND_FILTER 10, 32, 2
-HEVC_SAO_BAND_FILTER 10, 48, 3
-HEVC_SAO_BAND_FILTER 10, 64, 4
-
-INIT_XMM avx2
-HEVC_SAO_BAND_FILTER 12,  8, 1
-INIT_YMM avx2
-HEVC_SAO_BAND_FILTER 12, 16, 1
-HEVC_SAO_BAND_FILTER 12, 32, 2
-HEVC_SAO_BAND_FILTER 12, 48, 3
-HEVC_SAO_BAND_FILTER 12, 64, 4
-%endif
-
-;******************************************************************************
-;SAO Edge Filter
-;******************************************************************************
-
-%define MAX_PB_SIZE  64
-%define PADDING_SIZE 64 ; AV_INPUT_BUFFER_PADDING_SIZE
-%define EDGE_SRCSTRIDE 2 * MAX_PB_SIZE + PADDING_SIZE
-
-%macro PMINUW 4
-%if cpuflag(sse4)
-    pminuw            %1, %2, %3
-%else
-    psubusw           %4, %2, %3
-    psubw             %1, %2, %4
-%endif
-%endmacro
-
-%macro HEVC_SAO_EDGE_FILTER_INIT 0
-%if WIN64
-    movsxd           eoq, dword eom
-%elif ARCH_X86_64
-    movsxd           eoq, eod
-%else
-    mov              eoq, r4m
-%endif
-    lea            tmp2q, [pb_eo]
-    movsx      a_strideq, byte [tmp2q+eoq*4+1]
-    movsx      b_strideq, byte [tmp2q+eoq*4+3]
-    imul       a_strideq, EDGE_SRCSTRIDE >> 1
-    imul       b_strideq, EDGE_SRCSTRIDE >> 1
-    movsx           tmpq, byte [tmp2q+eoq*4]
-    add        a_strideq, tmpq
-    movsx           tmpq, byte [tmp2q+eoq*4+2]
-    add        b_strideq, tmpq
-%endmacro
-
-;void ff_hevc_sao_edge_filter_<width>_<depth>_<opt>(uint8_t *_dst, uint8_t *_src, ptrdiff_t stride_dst, int16_t *sao_offset_val,
-;                                                   int eo, int width, int height);
-%macro HEVC_SAO_EDGE_FILTER 3
-%if ARCH_X86_64
-cglobal hevc_sao_edge_filter_%2_%1, 4, 9, 16, dst, src, dststride, offset, eo, a_stride, b_stride, height, tmp
-%define tmp2q heightq
-    HEVC_SAO_EDGE_FILTER_INIT
-    mov          heightd, r6m
-    add        a_strideq, a_strideq
-    add        b_strideq, b_strideq
-
-%else ; ARCH_X86_32
-cglobal hevc_sao_edge_filter_%2_%1, 1, 6, 8, 5*mmsize, dst, src, dststride, a_stride, b_stride, height
-%define eoq   srcq
-%define tmpq  heightq
-%define tmp2q dststrideq
-%define offsetq heightq
-%define m8 m1
-%define m9 m2
-%define m10 m3
-%define m11 m4
-%define m12 m5
-    HEVC_SAO_EDGE_FILTER_INIT
-    mov             srcq, srcm
-    mov          offsetq, r3m
-    mov       dststrideq, dststridem
-    add        a_strideq, a_strideq
-    add        b_strideq, b_strideq
-
-%endif ; ARCH
-
-%if mmsize > 16
-    SPLATW            m8, [offsetq+2]
-    SPLATW            m9, [offsetq+4]
-    SPLATW           m10, [offsetq+0]
-    SPLATW           m11, [offsetq+6]
-    SPLATW           m12, [offsetq+8]
-%else
-    movq             m10, [offsetq+0]
-    movd             m12, [offsetq+6]
-    SPLATW            m8, xm10, 1
-    SPLATW            m9, xm10, 2
-    SPLATW           m10, xm10, 0
-    SPLATW           m11, xm12, 0
-    SPLATW           m12, xm12, 1
-%endif
-    pxor              m0, m0
-%if ARCH_X86_64
-    mova             m13, [pw_m1]
-    mova             m14, [pw_1]
-    mova             m15, [pw_2]
-%else
-    mov          heightd, r6m
-    mova  [rsp+mmsize*0], m8
-    mova  [rsp+mmsize*1], m9
-    mova  [rsp+mmsize*2], m10
-    mova  [rsp+mmsize*3], m11
-    mova  [rsp+mmsize*4], m12
-%endif
-
-align 16
-.loop:
-
-%assign i 0
-%rep %3
-    mova              m1, [srcq + i]
-    movu              m2, [srcq+a_strideq + i]
-    movu              m3, [srcq+b_strideq + i]
-    PMINUW            m4, m1, m2, m6
-    PMINUW            m5, m1, m3, m7
-    pcmpeqw           m2, m4
-    pcmpeqw           m3, m5
-    pcmpeqw           m4, m1
-    pcmpeqw           m5, m1
-    psubw             m4, m2
-    psubw             m5, m3
-
-    paddw             m4, m5
-    pcmpeqw           m2, m4, [pw_m2]
-%if ARCH_X86_64
-    pcmpeqw           m3, m4, m13
-    pcmpeqw           m5, m4, m0
-    pcmpeqw           m6, m4, m14
-    pcmpeqw           m7, m4, m15
-    pand              m2, m8
-    pand              m3, m9
-    pand              m5, m10
-    pand              m6, m11
-    pand              m7, m12
-%else
-    pcmpeqw           m3, m4, [pw_m1]
-    pcmpeqw           m5, m4, m0
-    pcmpeqw           m6, m4, [pw_1]
-    pcmpeqw           m7, m4, [pw_2]
-    pand              m2, [rsp+mmsize*0]
-    pand              m3, [rsp+mmsize*1]
-    pand              m5, [rsp+mmsize*2]
-    pand              m6, [rsp+mmsize*3]
-    pand              m7, [rsp+mmsize*4]
-%endif
-    paddw             m2, m3
-    paddw             m5, m6
-    paddw             m2, m7
-    paddw             m2, m1
-    paddw             m2, m5
-    CLIPW             m2, m0, [pw_mask %+ %1]
-    mova      [dstq + i], m2
-%assign i i+mmsize
-%endrep
-
-    add             dstq, dststrideq
-    add             srcq, EDGE_SRCSTRIDE
-    dec          heightd
-    jg .loop
-    RET
-%endmacro
-
-INIT_XMM sse2
-HEVC_SAO_EDGE_FILTER 10,  8, 1
-HEVC_SAO_EDGE_FILTER 10, 16, 2
-HEVC_SAO_EDGE_FILTER 10, 32, 4
-HEVC_SAO_EDGE_FILTER 10, 48, 6
-HEVC_SAO_EDGE_FILTER 10, 64, 8
-
-HEVC_SAO_EDGE_FILTER 12,  8, 1
-HEVC_SAO_EDGE_FILTER 12, 16, 2
-HEVC_SAO_EDGE_FILTER 12, 32, 4
-HEVC_SAO_EDGE_FILTER 12, 48, 6
-HEVC_SAO_EDGE_FILTER 12, 64, 8
-
-%if HAVE_AVX2_EXTERNAL
-INIT_XMM avx2
-HEVC_SAO_EDGE_FILTER 10,  8, 1
-INIT_YMM avx2
-HEVC_SAO_EDGE_FILTER 10, 16, 1
-HEVC_SAO_EDGE_FILTER 10, 32, 2
-HEVC_SAO_EDGE_FILTER 10, 48, 3
-HEVC_SAO_EDGE_FILTER 10, 64, 4
-
-INIT_XMM avx2
-HEVC_SAO_EDGE_FILTER 12,  8, 1
-INIT_YMM avx2
-HEVC_SAO_EDGE_FILTER 12, 16, 1
-HEVC_SAO_EDGE_FILTER 12, 32, 2
-HEVC_SAO_EDGE_FILTER 12, 48, 3
-HEVC_SAO_EDGE_FILTER 12, 64, 4
-%endif
diff -uparN ffmpeg-4.1/libavcodec/x86/hevc_sao.asm ffmpeg-y/libavcodec/x86/hevc_sao.asm
--- ffmpeg-4.1/libavcodec/x86/hevc_sao.asm	2018-11-03 08:17:29.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/hevc_sao.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,340 +0,0 @@
-;******************************************************************************
-;* SIMD optimized SAO functions for HEVC 8bit decoding
-;*
-;* Copyright (c) 2013 Pierre-Edouard LEPERE
-;* Copyright (c) 2014 James Almer
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA 32
-
-pb_edge_shuffle: times 2 db 1, 2, 0, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
-pb_eo:                   db -1, 0, 1, 0, 0, -1, 0, 1, -1, -1, 1, 1, 1, -1, -1, 1
-cextern pb_1
-cextern pb_2
-
-SECTION .text
-
-;******************************************************************************
-;SAO Band Filter
-;******************************************************************************
-
-%macro HEVC_SAO_BAND_FILTER_INIT 0
-    and            leftq, 31
-    movd             xm0, leftd
-    add            leftq, 1
-    and            leftq, 31
-    movd             xm1, leftd
-    add            leftq, 1
-    and            leftq, 31
-    movd             xm2, leftd
-    add            leftq, 1
-    and            leftq, 31
-    movd             xm3, leftd
-
-    SPLATW            m0, xm0
-    SPLATW            m1, xm1
-    SPLATW            m2, xm2
-    SPLATW            m3, xm3
-%if mmsize > 16
-    SPLATW            m4, [offsetq + 2]
-    SPLATW            m5, [offsetq + 4]
-    SPLATW            m6, [offsetq + 6]
-    SPLATW            m7, [offsetq + 8]
-%else
-    movq              m7, [offsetq + 2]
-    SPLATW            m4, m7, 0
-    SPLATW            m5, m7, 1
-    SPLATW            m6, m7, 2
-    SPLATW            m7, m7, 3
-%endif
-
-%if ARCH_X86_64
-    pxor             m14, m14
-
-%else ; ARCH_X86_32
-    mova  [rsp+mmsize*0], m0
-    mova  [rsp+mmsize*1], m1
-    mova  [rsp+mmsize*2], m2
-    mova  [rsp+mmsize*3], m3
-    mova  [rsp+mmsize*4], m4
-    mova  [rsp+mmsize*5], m5
-    mova  [rsp+mmsize*6], m6
-    pxor              m0, m0
-    %assign MMSIZE mmsize
-    %define m14 m0
-    %define m13 m1
-    %define  m9 m2
-    %define  m8 m3
-%endif ; ARCH
-DEFINE_ARGS dst, src, dststride, srcstride, offset, height
-    mov          heightd, r7m
-%endmacro
-
-%macro HEVC_SAO_BAND_FILTER_COMPUTE 2
-    psraw             %1, %2, 3
-%if ARCH_X86_64
-    pcmpeqw          m10, %1, m0
-    pcmpeqw          m11, %1, m1
-    pcmpeqw          m12, %1, m2
-    pcmpeqw           %1, m3
-    pand             m10, m4
-    pand             m11, m5
-    pand             m12, m6
-    pand              %1, m7
-    por              m10, m11
-    por              m12, %1
-    por              m10, m12
-    paddw             %2, m10
-%else ; ARCH_X86_32
-    pcmpeqw           m4, %1, [rsp+MMSIZE*0]
-    pcmpeqw           m5, %1, [rsp+MMSIZE*1]
-    pcmpeqw           m6, %1, [rsp+MMSIZE*2]
-    pcmpeqw           %1, [rsp+MMSIZE*3]
-    pand              m4, [rsp+MMSIZE*4]
-    pand              m5, [rsp+MMSIZE*5]
-    pand              m6, [rsp+MMSIZE*6]
-    pand              %1, m7
-    por               m4, m5
-    por               m6, %1
-    por               m4, m6
-    paddw             %2, m4
-%endif ; ARCH
-%endmacro
-
-;void ff_hevc_sao_band_filter_<width>_8_<opt>(uint8_t *_dst, uint8_t *_src, ptrdiff_t _stride_dst, ptrdiff_t _stride_src,
-;                                             int16_t *sao_offset_val, int sao_left_class, int width, int height);
-%macro HEVC_SAO_BAND_FILTER 2
-cglobal hevc_sao_band_filter_%1_8, 6, 6, 15, 7*mmsize*ARCH_X86_32, dst, src, dststride, srcstride, offset, left
-    HEVC_SAO_BAND_FILTER_INIT
-
-align 16
-.loop:
-%if %1 == 8
-    movq              m8, [srcq]
-    punpcklbw         m8, m14
-    HEVC_SAO_BAND_FILTER_COMPUTE m9, m8
-    packuswb          m8, m14
-    movq          [dstq], m8
-%endif ; %1 == 8
-
-%assign i 0
-%rep %2
-    mova             m13, [srcq + i]
-    punpcklbw         m8, m13, m14
-    HEVC_SAO_BAND_FILTER_COMPUTE m9,  m8
-    punpckhbw        m13, m14
-    HEVC_SAO_BAND_FILTER_COMPUTE m9, m13
-    packuswb          m8, m13
-    mova      [dstq + i], m8
-%assign i i+mmsize
-%endrep
-
-%if %1 == 48
-INIT_XMM cpuname
-
-    mova             m13, [srcq + i]
-    punpcklbw         m8, m13, m14
-    HEVC_SAO_BAND_FILTER_COMPUTE m9,  m8
-    punpckhbw        m13, m14
-    HEVC_SAO_BAND_FILTER_COMPUTE m9, m13
-    packuswb          m8, m13
-    mova      [dstq + i], m8
-%if cpuflag(avx2)
-INIT_YMM cpuname
-%endif
-%endif ; %1 == 48
-
-    add             dstq, dststrideq             ; dst += dststride
-    add             srcq, srcstrideq             ; src += srcstride
-    dec          heightd                         ; cmp height
-    jnz               .loop                      ; height loop
-    REP_RET
-%endmacro
-
-
-%macro HEVC_SAO_BAND_FILTER_FUNCS 0
-HEVC_SAO_BAND_FILTER  8, 0
-HEVC_SAO_BAND_FILTER 16, 1
-HEVC_SAO_BAND_FILTER 32, 2
-HEVC_SAO_BAND_FILTER 48, 2
-HEVC_SAO_BAND_FILTER 64, 4
-%endmacro
-
-INIT_XMM sse2
-HEVC_SAO_BAND_FILTER_FUNCS
-INIT_XMM avx
-HEVC_SAO_BAND_FILTER_FUNCS
-
-%if HAVE_AVX2_EXTERNAL
-INIT_XMM avx2
-HEVC_SAO_BAND_FILTER  8, 0
-HEVC_SAO_BAND_FILTER 16, 1
-INIT_YMM avx2
-HEVC_SAO_BAND_FILTER 32, 1
-HEVC_SAO_BAND_FILTER 48, 1
-HEVC_SAO_BAND_FILTER 64, 2
-%endif
-
-;******************************************************************************
-;SAO Edge Filter
-;******************************************************************************
-
-%define MAX_PB_SIZE  64
-%define PADDING_SIZE 64 ; AV_INPUT_BUFFER_PADDING_SIZE
-%define EDGE_SRCSTRIDE 2 * MAX_PB_SIZE + PADDING_SIZE
-
-%macro HEVC_SAO_EDGE_FILTER_INIT 0
-%if WIN64
-    movsxd           eoq, dword eom
-%elif ARCH_X86_64
-    movsxd           eoq, eod
-%else
-    mov              eoq, r4m
-%endif
-    lea            tmp2q, [pb_eo]
-    movsx      a_strideq, byte [tmp2q+eoq*4+1]
-    movsx      b_strideq, byte [tmp2q+eoq*4+3]
-    imul       a_strideq, EDGE_SRCSTRIDE
-    imul       b_strideq, EDGE_SRCSTRIDE
-    movsx           tmpq, byte [tmp2q+eoq*4]
-    add        a_strideq, tmpq
-    movsx           tmpq, byte [tmp2q+eoq*4+2]
-    add        b_strideq, tmpq
-%endmacro
-
-%macro HEVC_SAO_EDGE_FILTER_COMPUTE 1
-    pminub            m4, m1, m2
-    pminub            m5, m1, m3
-    pcmpeqb           m2, m4
-    pcmpeqb           m3, m5
-    pcmpeqb           m4, m1
-    pcmpeqb           m5, m1
-    psubb             m4, m2
-    psubb             m5, m3
-    paddb             m4, m6
-    paddb             m4, m5
-
-    pshufb            m2, m0, m4
-%if %1 > 8
-    punpckhbw         m5, m7, m1
-    punpckhbw         m4, m2, m7
-    punpcklbw         m3, m7, m1
-    punpcklbw         m2, m7
-    pmaddubsw         m5, m4
-    pmaddubsw         m3, m2
-    packuswb          m3, m5
-%else
-    punpcklbw         m3, m7, m1
-    punpcklbw         m2, m7
-    pmaddubsw         m3, m2
-    packuswb          m3, m3
-%endif
-%endmacro
-
-;void ff_hevc_sao_edge_filter_<width>_8_<opt>(uint8_t *_dst, uint8_t *_src, ptrdiff_t stride_dst, int16_t *sao_offset_val,
-;                                             int eo, int width, int height);
-%macro HEVC_SAO_EDGE_FILTER 2-3
-%if ARCH_X86_64
-cglobal hevc_sao_edge_filter_%1_8, 4, 9, 8, dst, src, dststride, offset, eo, a_stride, b_stride, height, tmp
-%define tmp2q heightq
-    HEVC_SAO_EDGE_FILTER_INIT
-    mov          heightd, r6m
-
-%else ; ARCH_X86_32
-cglobal hevc_sao_edge_filter_%1_8, 1, 6, 8, dst, src, dststride, a_stride, b_stride, height
-%define eoq   srcq
-%define tmpq  heightq
-%define tmp2q dststrideq
-%define offsetq heightq
-    HEVC_SAO_EDGE_FILTER_INIT
-    mov             srcq, srcm
-    mov          offsetq, r3m
-    mov       dststrideq, dststridem
-%endif ; ARCH
-
-%if mmsize > 16
-    vbroadcasti128    m0, [offsetq]
-%else
-    movu              m0, [offsetq]
-%endif
-    mova              m1, [pb_edge_shuffle]
-    packsswb          m0, m0
-    mova              m7, [pb_1]
-    pshufb            m0, m1
-    mova              m6, [pb_2]
-%if ARCH_X86_32
-    mov          heightd, r6m
-%endif
-
-align 16
-.loop:
-
-%if %1 == 8
-    movq              m1, [srcq]
-    movq              m2, [srcq + a_strideq]
-    movq              m3, [srcq + b_strideq]
-    HEVC_SAO_EDGE_FILTER_COMPUTE %1
-    movq          [dstq], m3
-%endif
-
-%assign i 0
-%rep %2
-    mova              m1, [srcq + i]
-    movu              m2, [srcq + a_strideq + i]
-    movu              m3, [srcq + b_strideq + i]
-    HEVC_SAO_EDGE_FILTER_COMPUTE %1
-    mov%3     [dstq + i], m3
-%assign i i+mmsize
-%endrep
-
-%if %1 == 48
-INIT_XMM cpuname
-
-    mova              m1, [srcq + i]
-    movu              m2, [srcq + a_strideq + i]
-    movu              m3, [srcq + b_strideq + i]
-    HEVC_SAO_EDGE_FILTER_COMPUTE %1
-    mova      [dstq + i], m3
-%if cpuflag(avx2)
-INIT_YMM cpuname
-%endif
-%endif
-
-    add             dstq, dststrideq
-    add             srcq, EDGE_SRCSTRIDE
-    dec          heightd
-    jg .loop
-    RET
-%endmacro
-
-INIT_XMM ssse3
-HEVC_SAO_EDGE_FILTER  8, 0
-HEVC_SAO_EDGE_FILTER 16, 1, a
-HEVC_SAO_EDGE_FILTER 32, 2, a
-HEVC_SAO_EDGE_FILTER 48, 2, a
-HEVC_SAO_EDGE_FILTER 64, 4, a
-
-%if HAVE_AVX2_EXTERNAL
-INIT_YMM avx2
-HEVC_SAO_EDGE_FILTER 32, 1, a
-HEVC_SAO_EDGE_FILTER 48, 1, u
-HEVC_SAO_EDGE_FILTER 64, 2, a
-%endif
diff -uparN ffmpeg-4.1/libavcodec/x86/hpeldsp.asm ffmpeg-y/libavcodec/x86/hpeldsp.asm
--- ffmpeg-4.1/libavcodec/x86/hpeldsp.asm	2018-11-02 02:34:25.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/hpeldsp.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,591 +0,0 @@
-;******************************************************************************
-;*
-;* Copyright (c) 2000-2001 Fabrice Bellard <fabrice@bellard.org>
-;* Copyright (c)      Nick Kurshev <nickols_k@mail.ru>
-;* Copyright (c) 2002 Michael Niedermayer <michaelni@gmx.at>
-;* Copyright (c) 2002 Zdenek Kabelac <kabi@informatics.muni.cz>
-;* Copyright (c) 2013 Daniel Kang
-;*
-;* SIMD-optimized halfpel functions
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-cextern pb_1
-cextern pw_2
-pb_interleave16: db 0, 8, 1, 9, 2, 10, 3, 11, 4, 12, 5, 13, 6, 14, 7, 15
-pb_interleave8:  db 0, 4, 1, 5, 2, 6, 3, 7
-
-cextern pw_8192
-
-SECTION .text
-
-; void ff_put_pixels8_x2(uint8_t *block, const uint8_t *pixels, ptrdiff_t line_size, int h)
-%macro PUT_PIXELS8_X2 0
-%if cpuflag(sse2)
-cglobal put_pixels16_x2, 4,5,4
-%else
-cglobal put_pixels8_x2, 4,5
-%endif
-    lea          r4, [r2*2]
-.loop:
-    movu         m0, [r1+1]
-    movu         m1, [r1+r2+1]
-%if cpuflag(sse2)
-    movu         m2, [r1]
-    movu         m3, [r1+r2]
-    pavgb        m0, m2
-    pavgb        m1, m3
-%else
-    PAVGB        m0, [r1]
-    PAVGB        m1, [r1+r2]
-%endif
-    mova       [r0], m0
-    mova    [r0+r2], m1
-    add          r1, r4
-    add          r0, r4
-    movu         m0, [r1+1]
-    movu         m1, [r1+r2+1]
-%if cpuflag(sse2)
-    movu         m2, [r1]
-    movu         m3, [r1+r2]
-    pavgb        m0, m2
-    pavgb        m1, m3
-%else
-    PAVGB        m0, [r1]
-    PAVGB        m1, [r1+r2]
-%endif
-    add          r1, r4
-    mova       [r0], m0
-    mova    [r0+r2], m1
-    add          r0, r4
-    sub         r3d, 4
-    jne .loop
-    REP_RET
-%endmacro
-
-INIT_MMX mmxext
-PUT_PIXELS8_X2
-INIT_MMX 3dnow
-PUT_PIXELS8_X2
-
-
-; void ff_put_pixels16_x2(uint8_t *block, const uint8_t *pixels, ptrdiff_t line_size, int h)
-%macro PUT_PIXELS_16 0
-cglobal put_pixels16_x2, 4,5
-    lea          r4, [r2*2]
-.loop:
-    mova         m0, [r1]
-    mova         m1, [r1+r2]
-    mova         m2, [r1+8]
-    mova         m3, [r1+r2+8]
-    PAVGB        m0, [r1+1]
-    PAVGB        m1, [r1+r2+1]
-    PAVGB        m2, [r1+9]
-    PAVGB        m3, [r1+r2+9]
-    mova       [r0], m0
-    mova    [r0+r2], m1
-    mova     [r0+8], m2
-    mova  [r0+r2+8], m3
-    add          r1, r4
-    add          r0, r4
-    mova         m0, [r1]
-    mova         m1, [r1+r2]
-    mova         m2, [r1+8]
-    mova         m3, [r1+r2+8]
-    PAVGB        m0, [r1+1]
-    PAVGB        m1, [r1+r2+1]
-    PAVGB        m2, [r1+9]
-    PAVGB        m3, [r1+r2+9]
-    add          r1, r4
-    mova       [r0], m0
-    mova    [r0+r2], m1
-    mova     [r0+8], m2
-    mova  [r0+r2+8], m3
-    add          r0, r4
-    sub         r3d, 4
-    jne .loop
-    REP_RET
-%endmacro
-
-INIT_MMX mmxext
-PUT_PIXELS_16
-INIT_MMX 3dnow
-PUT_PIXELS_16
-; The 8_X2 macro can easily be used here
-INIT_XMM sse2
-PUT_PIXELS8_X2
-
-
-; void ff_put_no_rnd_pixels8_x2(uint8_t *block, const uint8_t *pixels, ptrdiff_t line_size, int h)
-%macro PUT_NO_RND_PIXELS8_X2 0
-cglobal put_no_rnd_pixels8_x2, 4,5
-    mova         m6, [pb_1]
-    lea          r4, [r2*2]
-.loop:
-    mova         m0, [r1]
-    mova         m2, [r1+r2]
-    mova         m1, [r1+1]
-    mova         m3, [r1+r2+1]
-    add          r1, r4
-    psubusb      m0, m6
-    psubusb      m2, m6
-    PAVGB        m0, m1
-    PAVGB        m2, m3
-    mova       [r0], m0
-    mova    [r0+r2], m2
-    mova         m0, [r1]
-    mova         m1, [r1+1]
-    mova         m2, [r1+r2]
-    mova         m3, [r1+r2+1]
-    add          r0, r4
-    add          r1, r4
-    psubusb      m0, m6
-    psubusb      m2, m6
-    PAVGB        m0, m1
-    PAVGB        m2, m3
-    mova       [r0], m0
-    mova    [r0+r2], m2
-    add          r0, r4
-    sub         r3d, 4
-    jne .loop
-    REP_RET
-%endmacro
-
-INIT_MMX mmxext
-PUT_NO_RND_PIXELS8_X2
-INIT_MMX 3dnow
-PUT_NO_RND_PIXELS8_X2
-
-
-; void ff_put_pixels8_y2(uint8_t *block, const uint8_t *pixels, ptrdiff_t line_size, int h)
-%macro PUT_PIXELS8_Y2 0
-%if cpuflag(sse2)
-cglobal put_pixels16_y2, 4,5,3
-%else
-cglobal put_pixels8_y2, 4,5
-%endif
-    lea          r4, [r2*2]
-    movu         m0, [r1]
-    sub          r0, r2
-.loop:
-    movu         m1, [r1+r2]
-    movu         m2, [r1+r4]
-    add          r1, r4
-    PAVGB        m0, m1
-    PAVGB        m1, m2
-    mova    [r0+r2], m0
-    mova    [r0+r4], m1
-    movu         m1, [r1+r2]
-    movu         m0, [r1+r4]
-    add          r0, r4
-    add          r1, r4
-    PAVGB        m2, m1
-    PAVGB        m1, m0
-    mova    [r0+r2], m2
-    mova    [r0+r4], m1
-    add          r0, r4
-    sub         r3d, 4
-    jne .loop
-    REP_RET
-%endmacro
-
-INIT_MMX mmxext
-PUT_PIXELS8_Y2
-INIT_MMX 3dnow
-PUT_PIXELS8_Y2
-; actually, put_pixels16_y2_sse2
-INIT_XMM sse2
-PUT_PIXELS8_Y2
-
-
-; void ff_put_no_rnd_pixels8_y2(uint8_t *block, const uint8_t *pixels, ptrdiff_t line_size, int h)
-%macro PUT_NO_RND_PIXELS8_Y2 0
-cglobal put_no_rnd_pixels8_y2, 4,5
-    mova         m6, [pb_1]
-    lea          r4, [r2+r2]
-    mova         m0, [r1]
-    sub          r0, r2
-.loop:
-    mova         m1, [r1+r2]
-    mova         m2, [r1+r4]
-    add          r1, r4
-    psubusb      m1, m6
-    PAVGB        m0, m1
-    PAVGB        m1, m2
-    mova    [r0+r2], m0
-    mova    [r0+r4], m1
-    mova         m1, [r1+r2]
-    mova         m0, [r1+r4]
-    add          r0, r4
-    add          r1, r4
-    psubusb      m1, m6
-    PAVGB        m2, m1
-    PAVGB        m1, m0
-    mova    [r0+r2], m2
-    mova    [r0+r4], m1
-    add          r0, r4
-    sub         r3d, 4
-    jne .loop
-    REP_RET
-%endmacro
-
-INIT_MMX mmxext
-PUT_NO_RND_PIXELS8_Y2
-INIT_MMX 3dnow
-PUT_NO_RND_PIXELS8_Y2
-
-
-; void ff_avg_pixels8(uint8_t *block, const uint8_t *pixels, ptrdiff_t line_size, int h)
-%macro AVG_PIXELS8 0
-cglobal avg_pixels8, 4,5
-    lea          r4, [r2*2]
-.loop:
-    mova         m0, [r0]
-    mova         m1, [r0+r2]
-    PAVGB        m0, [r1]
-    PAVGB        m1, [r1+r2]
-    mova       [r0], m0
-    mova    [r0+r2], m1
-    add          r1, r4
-    add          r0, r4
-    mova         m0, [r0]
-    mova         m1, [r0+r2]
-    PAVGB        m0, [r1]
-    PAVGB        m1, [r1+r2]
-    add          r1, r4
-    mova       [r0], m0
-    mova    [r0+r2], m1
-    add          r0, r4
-    sub         r3d, 4
-    jne .loop
-    REP_RET
-%endmacro
-
-INIT_MMX 3dnow
-AVG_PIXELS8
-
-
-; void ff_avg_pixels8_x2(uint8_t *block, const uint8_t *pixels, ptrdiff_t line_size, int h)
-%macro AVG_PIXELS8_X2 0
-%if cpuflag(sse2)
-cglobal avg_pixels16_x2, 4,5,4
-%else
-cglobal avg_pixels8_x2, 4,5
-%endif
-    lea          r4, [r2*2]
-%if notcpuflag(mmxext)
-    pcmpeqd      m5, m5
-    paddb        m5, m5
-%endif
-.loop:
-    movu         m0, [r1]
-    movu         m2, [r1+r2]
-%if cpuflag(sse2)
-    movu         m1, [r1+1]
-    movu         m3, [r1+r2+1]
-    pavgb        m0, m1
-    pavgb        m2, m3
-%else
-    PAVGB        m0, [r1+1], m3, m5
-    PAVGB        m2, [r1+r2+1], m4, m5
-%endif
-    PAVGB        m0, [r0], m3, m5
-    PAVGB        m2, [r0+r2], m4, m5
-    add          r1, r4
-    mova       [r0], m0
-    mova    [r0+r2], m2
-    movu         m0, [r1]
-    movu         m2, [r1+r2]
-%if cpuflag(sse2)
-    movu         m1, [r1+1]
-    movu         m3, [r1+r2+1]
-    pavgb        m0, m1
-    pavgb        m2, m3
-%else
-    PAVGB        m0, [r1+1], m3, m5
-    PAVGB        m2, [r1+r2+1], m4, m5
-%endif
-    add          r0, r4
-    add          r1, r4
-    PAVGB        m0, [r0], m3, m5
-    PAVGB        m2, [r0+r2], m4, m5
-    mova       [r0], m0
-    mova    [r0+r2], m2
-    add          r0, r4
-    sub         r3d, 4
-    jne .loop
-    REP_RET
-%endmacro
-
-INIT_MMX mmx
-AVG_PIXELS8_X2
-INIT_MMX mmxext
-AVG_PIXELS8_X2
-INIT_MMX 3dnow
-AVG_PIXELS8_X2
-; actually avg_pixels16_x2
-INIT_XMM sse2
-AVG_PIXELS8_X2
-
-
-; void ff_avg_pixels8_y2(uint8_t *block, const uint8_t *pixels, ptrdiff_t line_size, int h)
-%macro AVG_PIXELS8_Y2 0
-%if cpuflag(sse2)
-cglobal avg_pixels16_y2, 4,5,3
-%else
-cglobal avg_pixels8_y2, 4,5
-%endif
-    lea          r4, [r2*2]
-    movu         m0, [r1]
-    sub          r0, r2
-.loop:
-    movu         m1, [r1+r2]
-    movu         m2, [r1+r4]
-    add          r1, r4
-    PAVGB        m0, m1
-    PAVGB        m1, m2
-    PAVGB        m0, [r0+r2]
-    PAVGB        m1, [r0+r4]
-    mova    [r0+r2], m0
-    mova    [r0+r4], m1
-    movu         m1, [r1+r2]
-    movu         m0, [r1+r4]
-    PAVGB        m2, m1
-    PAVGB        m1, m0
-    add          r0, r4
-    add          r1, r4
-    PAVGB        m2, [r0+r2]
-    PAVGB        m1, [r0+r4]
-    mova    [r0+r2], m2
-    mova    [r0+r4], m1
-    add          r0, r4
-    sub         r3d, 4
-    jne .loop
-    REP_RET
-%endmacro
-
-INIT_MMX mmxext
-AVG_PIXELS8_Y2
-INIT_MMX 3dnow
-AVG_PIXELS8_Y2
-; actually avg_pixels16_y2
-INIT_XMM sse2
-AVG_PIXELS8_Y2
-
-
-; void ff_avg_pixels8_xy2(uint8_t *block, const uint8_t *pixels, ptrdiff_t line_size, int h)
-; Note this is not correctly rounded, and is therefore used for
-; not-bitexact output
-%macro AVG_APPROX_PIXELS8_XY2 0
-cglobal avg_approx_pixels8_xy2, 4,5
-    mova         m6, [pb_1]
-    lea          r4, [r2*2]
-    mova         m0, [r1]
-    PAVGB        m0, [r1+1]
-.loop:
-    mova         m2, [r1+r4]
-    mova         m1, [r1+r2]
-    psubusb      m2, m6
-    PAVGB        m1, [r1+r2+1]
-    PAVGB        m2, [r1+r4+1]
-    add          r1, r4
-    PAVGB        m0, m1
-    PAVGB        m1, m2
-    PAVGB        m0, [r0]
-    PAVGB        m1, [r0+r2]
-    mova       [r0], m0
-    mova    [r0+r2], m1
-    mova         m1, [r1+r2]
-    mova         m0, [r1+r4]
-    PAVGB        m1, [r1+r2+1]
-    PAVGB        m0, [r1+r4+1]
-    add          r0, r4
-    add          r1, r4
-    PAVGB        m2, m1
-    PAVGB        m1, m0
-    PAVGB        m2, [r0]
-    PAVGB        m1, [r0+r2]
-    mova       [r0], m2
-    mova    [r0+r2], m1
-    add          r0, r4
-    sub         r3d, 4
-    jne .loop
-    REP_RET
-%endmacro
-
-INIT_MMX mmxext
-AVG_APPROX_PIXELS8_XY2
-INIT_MMX 3dnow
-AVG_APPROX_PIXELS8_XY2
-
-
-; void ff_avg_pixels16_xy2(uint8_t *block, const uint8_t *pixels, ptrdiff_t line_size, int h)
-%macro SET_PIXELS_XY2 1
-%if cpuflag(sse2)
-cglobal %1_pixels16_xy2, 4,5,8
-%else
-cglobal %1_pixels8_xy2, 4,5
-%endif
-    pxor        m7, m7
-    mova        m6, [pw_2]
-    movu        m0, [r1]
-    movu        m4, [r1+1]
-    mova        m1, m0
-    mova        m5, m4
-    punpcklbw   m0, m7
-    punpcklbw   m4, m7
-    punpckhbw   m1, m7
-    punpckhbw   m5, m7
-    paddusw     m4, m0
-    paddusw     m5, m1
-    xor         r4, r4
-    add         r1, r2
-.loop:
-    movu        m0, [r1+r4]
-    movu        m2, [r1+r4+1]
-    mova        m1, m0
-    mova        m3, m2
-    punpcklbw   m0, m7
-    punpcklbw   m2, m7
-    punpckhbw   m1, m7
-    punpckhbw   m3, m7
-    paddusw     m0, m2
-    paddusw     m1, m3
-    paddusw     m4, m6
-    paddusw     m5, m6
-    paddusw     m4, m0
-    paddusw     m5, m1
-    psrlw       m4, 2
-    psrlw       m5, 2
-%ifidn %1, avg
-    mova        m3, [r0+r4]
-    packuswb    m4, m5
-    PAVGB       m4, m3
-%else
-    packuswb    m4, m5
-%endif
-    mova   [r0+r4], m4
-    add         r4, r2
-
-    movu        m2, [r1+r4]
-    movu        m4, [r1+r4+1]
-    mova        m3, m2
-    mova        m5, m4
-    punpcklbw   m2, m7
-    punpcklbw   m4, m7
-    punpckhbw   m3, m7
-    punpckhbw   m5, m7
-    paddusw     m4, m2
-    paddusw     m5, m3
-    paddusw     m0, m6
-    paddusw     m1, m6
-    paddusw     m0, m4
-    paddusw     m1, m5
-    psrlw       m0, 2
-    psrlw       m1, 2
-%ifidn %1, avg
-    mova        m3, [r0+r4]
-    packuswb    m0, m1
-    PAVGB       m0, m3
-%else
-    packuswb    m0, m1
-%endif
-    mova   [r0+r4], m0
-    add         r4, r2
-    sub        r3d, 2
-    jnz .loop
-    REP_RET
-%endmacro
-
-INIT_MMX mmxext
-SET_PIXELS_XY2 avg
-INIT_MMX 3dnow
-SET_PIXELS_XY2 avg
-INIT_XMM sse2
-SET_PIXELS_XY2 put
-SET_PIXELS_XY2 avg
-
-%macro SSSE3_PIXELS_XY2 1-2
-%if %0 == 2 ; sse2
-cglobal %1_pixels16_xy2, 4,5,%2
-    mova        m4, [pb_interleave16]
-%else
-cglobal %1_pixels8_xy2, 4,5
-    mova        m4, [pb_interleave8]
-%endif
-    mova        m5, [pb_1]
-    movu        m0, [r1]
-    movu        m1, [r1+1]
-    pmaddubsw   m0, m5
-    pmaddubsw   m1, m5
-    xor         r4, r4
-    add         r1, r2
-.loop:
-    movu        m2, [r1+r4]
-    movu        m3, [r1+r4+1]
-    pmaddubsw   m2, m5
-    pmaddubsw   m3, m5
-    paddusw     m0, m2
-    paddusw     m1, m3
-    pmulhrsw    m0, [pw_8192]
-    pmulhrsw    m1, [pw_8192]
-%ifidn %1, avg
-    mova        m6, [r0+r4]
-    packuswb    m0, m1
-    pshufb      m0, m4
-    pavgb       m0, m6
-%else
-    packuswb    m0, m1
-    pshufb      m0, m4
-%endif
-    mova   [r0+r4], m0
-    add         r4, r2
-
-    movu        m0, [r1+r4]
-    movu        m1, [r1+r4+1]
-    pmaddubsw   m0, m5
-    pmaddubsw   m1, m5
-    paddusw     m2, m0
-    paddusw     m3, m1
-    pmulhrsw    m2, [pw_8192]
-    pmulhrsw    m3, [pw_8192]
-%ifidn %1, avg
-    mova        m6, [r0+r4]
-    packuswb    m2, m3
-    pshufb      m2, m4
-    pavgb       m2, m6
-%else
-    packuswb    m2, m3
-    pshufb      m2, m4
-%endif
-    mova   [r0+r4], m2
-    add         r4, r2
-    sub        r3d, 2
-    jnz .loop
-    REP_RET
-%endmacro
-
-INIT_MMX ssse3
-SSSE3_PIXELS_XY2 put
-SSSE3_PIXELS_XY2 avg
-INIT_XMM ssse3
-SSSE3_PIXELS_XY2 put, 6
-SSSE3_PIXELS_XY2 avg, 7
diff -uparN ffmpeg-4.1/libavcodec/x86/hpeldsp_vp3.asm ffmpeg-y/libavcodec/x86/hpeldsp_vp3.asm
--- ffmpeg-4.1/libavcodec/x86/hpeldsp_vp3.asm	2018-11-02 02:34:25.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/hpeldsp_vp3.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,111 +0,0 @@
-;******************************************************************************
-;* SIMD-optimized halfpel functions for VP3
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION .text
-
-; void ff_put_no_rnd_pixels8_x2_exact(uint8_t *block, const uint8_t *pixels, ptrdiff_t line_size, int h)
-%macro PUT_NO_RND_PIXELS8_X2_EXACT 0
-cglobal put_no_rnd_pixels8_x2_exact, 4,5
-    lea          r4, [r2*3]
-    pcmpeqb      m6, m6
-.loop:
-    mova         m0, [r1]
-    mova         m2, [r1+r2]
-    mova         m1, [r1+1]
-    mova         m3, [r1+r2+1]
-    pxor         m0, m6
-    pxor         m2, m6
-    pxor         m1, m6
-    pxor         m3, m6
-    PAVGB        m0, m1
-    PAVGB        m2, m3
-    pxor         m0, m6
-    pxor         m2, m6
-    mova       [r0], m0
-    mova    [r0+r2], m2
-    mova         m0, [r1+r2*2]
-    mova         m1, [r1+r2*2+1]
-    mova         m2, [r1+r4]
-    mova         m3, [r1+r4+1]
-    pxor         m0, m6
-    pxor         m1, m6
-    pxor         m2, m6
-    pxor         m3, m6
-    PAVGB        m0, m1
-    PAVGB        m2, m3
-    pxor         m0, m6
-    pxor         m2, m6
-    mova  [r0+r2*2], m0
-    mova    [r0+r4], m2
-    lea          r1, [r1+r2*4]
-    lea          r0, [r0+r2*4]
-    sub         r3d, 4
-    jg .loop
-    REP_RET
-%endmacro
-
-INIT_MMX mmxext
-PUT_NO_RND_PIXELS8_X2_EXACT
-INIT_MMX 3dnow
-PUT_NO_RND_PIXELS8_X2_EXACT
-
-
-; void ff_put_no_rnd_pixels8_y2_exact(uint8_t *block, const uint8_t *pixels, ptrdiff_t line_size, int h)
-%macro PUT_NO_RND_PIXELS8_Y2_EXACT 0
-cglobal put_no_rnd_pixels8_y2_exact, 4,5
-    lea          r4, [r2*3]
-    mova         m0, [r1]
-    pcmpeqb      m6, m6
-    add          r1, r2
-    pxor         m0, m6
-.loop:
-    mova         m1, [r1]
-    mova         m2, [r1+r2]
-    pxor         m1, m6
-    pxor         m2, m6
-    PAVGB        m0, m1
-    PAVGB        m1, m2
-    pxor         m0, m6
-    pxor         m1, m6
-    mova       [r0], m0
-    mova    [r0+r2], m1
-    mova         m1, [r1+r2*2]
-    mova         m0, [r1+r4]
-    pxor         m1, m6
-    pxor         m0, m6
-    PAVGB        m2, m1
-    PAVGB        m1, m0
-    pxor         m2, m6
-    pxor         m1, m6
-    mova  [r0+r2*2], m2
-    mova    [r0+r4], m1
-    lea          r1, [r1+r2*4]
-    lea          r0, [r0+r2*4]
-    sub         r3d, 4
-    jg .loop
-    REP_RET
-%endmacro
-
-INIT_MMX mmxext
-PUT_NO_RND_PIXELS8_Y2_EXACT
-INIT_MMX 3dnow
-PUT_NO_RND_PIXELS8_Y2_EXACT
diff -uparN ffmpeg-4.1/libavcodec/x86/huffyuvdsp.asm ffmpeg-y/libavcodec/x86/huffyuvdsp.asm
--- ffmpeg-4.1/libavcodec/x86/huffyuvdsp.asm	2018-11-03 08:17:29.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/huffyuvdsp.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,164 +0,0 @@
-;******************************************************************************
-;* SIMD-optimized HuffYUV functions
-;* Copyright (c) 2008 Loren Merritt
-;* Copyright (c) 2014 Christophe Gisquet
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION .text
-
-%include "libavcodec/x86/huffyuvdsp_template.asm"
-
-;------------------------------------------------------------------------------
-; void (*add_int16)(uint16_t *dst, const uint16_t *src, unsigned mask, int w);
-;------------------------------------------------------------------------------
-
-%macro ADD_INT16 0
-cglobal add_int16, 4,4,5, dst, src, mask, w, tmp
-%if mmsize > 8
-    test srcq, mmsize-1
-    jnz .unaligned
-    test dstq, mmsize-1
-    jnz .unaligned
-%endif
-    INT16_LOOP a, add
-%if mmsize > 8
-.unaligned:
-    INT16_LOOP u, add
-%endif
-%endmacro
-
-%if ARCH_X86_32
-INIT_MMX mmx
-ADD_INT16
-%endif
-
-INIT_XMM sse2
-ADD_INT16
-
-%if HAVE_AVX2_EXTERNAL
-INIT_YMM avx2
-ADD_INT16
-%endif
-
-; void add_hfyu_left_pred_bgr32(uint8_t *dst, const uint8_t *src,
-;                               intptr_t w, uint8_t *left)
-%macro LEFT_BGR32 0
-cglobal add_hfyu_left_pred_bgr32, 4,4,3, dst, src, w, left
-    shl           wq, 2
-    movd          m0, [leftq]
-    lea         dstq, [dstq + wq]
-    lea         srcq, [srcq + wq]
-    LSHIFT        m0, mmsize-4
-    neg           wq
-.loop:
-    movu          m1, [srcq+wq]
-    mova          m2, m1
-%if mmsize == 8
-    punpckhdq     m0, m0
-%endif
-    LSHIFT        m1, 4
-    paddb         m1, m2
-%if mmsize == 16
-    pshufd        m0, m0, q3333
-    mova          m2, m1
-    LSHIFT        m1, 8
-    paddb         m1, m2
-%endif
-    paddb         m0, m1
-    movu   [dstq+wq], m0
-    add           wq, mmsize
-    jl         .loop
-    movd          m0, [dstq-4]
-    movd     [leftq], m0
-    REP_RET
-%endmacro
-
-%if ARCH_X86_32
-INIT_MMX mmx
-LEFT_BGR32
-%endif
-INIT_XMM sse2
-LEFT_BGR32
-
-; void add_hfyu_median_prediction_mmxext(uint8_t *dst, const uint8_t *top, const uint8_t *diff, int mask, int w, int *left, int *left_top)
-INIT_MMX mmxext
-cglobal add_hfyu_median_pred_int16, 7,7,0, dst, top, diff, mask, w, left, left_top
-    add      wd, wd
-    movd    mm6, maskd
-    SPLATW  mm6, mm6
-    movq    mm0, [topq]
-    movq    mm2, mm0
-    movd    mm4, [left_topq]
-    psllq   mm2, 16
-    movq    mm1, mm0
-    por     mm4, mm2
-    movd    mm3, [leftq]
-    psubw   mm0, mm4 ; t-tl
-    add    dstq, wq
-    add    topq, wq
-    add   diffq, wq
-    neg      wq
-    jmp .skip
-.loop:
-    movq    mm4, [topq+wq]
-    movq    mm0, mm4
-    psllq   mm4, 16
-    por     mm4, mm1
-    movq    mm1, mm0 ; t
-    psubw   mm0, mm4 ; t-tl
-.skip:
-    movq    mm2, [diffq+wq]
-%assign i 0
-%rep 4
-    movq    mm4, mm0
-    paddw   mm4, mm3 ; t-tl+l
-    pand    mm4, mm6
-    movq    mm5, mm3
-    pmaxsw  mm3, mm1
-    pminsw  mm5, mm1
-    pminsw  mm3, mm4
-    pmaxsw  mm3, mm5 ; median
-    paddw   mm3, mm2 ; +residual
-    pand    mm3, mm6
-%if i==0
-    movq    mm7, mm3
-    psllq   mm7, 48
-%else
-    movq    mm4, mm3
-    psrlq   mm7, 16
-    psllq   mm4, 48
-    por     mm7, mm4
-%endif
-%if i<3
-    psrlq   mm0, 16
-    psrlq   mm1, 16
-    psrlq   mm2, 16
-%endif
-%assign i i+1
-%endrep
-    movq [dstq+wq], mm7
-    add      wq, 8
-    jl .loop
-    movzx   r2d, word [dstq-2]
-    mov [leftq], r2d
-    movzx   r2d, word [topq-2]
-    mov [left_topq], r2d
-    RET
diff -uparN ffmpeg-4.1/libavcodec/x86/huffyuvdsp_template.asm ffmpeg-y/libavcodec/x86/huffyuvdsp_template.asm
--- ffmpeg-4.1/libavcodec/x86/huffyuvdsp_template.asm	2018-11-03 08:17:29.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/huffyuvdsp_template.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,76 +0,0 @@
-;******************************************************************************
-;* SIMD-optimized HuffYUV functions
-;* Copyright (c) 2008 Loren Merritt
-;* Copyright (c) 2014 Christophe Gisquet
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%macro INT16_LOOP 2 ; %1 = a/u (aligned/unaligned), %2 = add/sub
-    movd    xm4, maskd
-    SPLATW  m4, xm4
-    add     wd, wd
-    test    wq, 2*mmsize - 1
-    jz %%.tomainloop
-    push  tmpq
-%%.wordloop:
-    sub     wq, 2
-%ifidn %2, add
-    mov   tmpw, [srcq+wq]
-    add   tmpw, [dstq+wq]
-%else
-    mov   tmpw, [src1q+wq]
-    sub   tmpw, [src2q+wq]
-%endif
-    and   tmpw, maskw
-    mov     [dstq+wq], tmpw
-    test    wq, 2*mmsize - 1
-    jnz %%.wordloop
-    pop   tmpq
-%%.tomainloop:
-%ifidn %2, add
-    add     srcq, wq
-%else
-    add     src1q, wq
-    add     src2q, wq
-%endif
-    add     dstq, wq
-    neg     wq
-    jz      %%.end
-%%.loop:
-%ifidn %2, add
-    mov%1   m0, [srcq+wq]
-    mov%1   m1, [dstq+wq]
-    mov%1   m2, [srcq+wq+mmsize]
-    mov%1   m3, [dstq+wq+mmsize]
-%else
-    mov%1   m0, [src1q+wq]
-    mov%1   m1, [src2q+wq]
-    mov%1   m2, [src1q+wq+mmsize]
-    mov%1   m3, [src2q+wq+mmsize]
-%endif
-    p%2w    m0, m1
-    p%2w    m2, m3
-    pand    m0, m4
-    pand    m2, m4
-    mov%1   [dstq+wq]       , m0
-    mov%1   [dstq+wq+mmsize], m2
-    add     wq, 2*mmsize
-    jl %%.loop
-%%.end:
-    RET
-%endmacro
diff -uparN ffmpeg-4.1/libavcodec/x86/huffyuvencdsp.asm ffmpeg-y/libavcodec/x86/huffyuvencdsp.asm
--- ffmpeg-4.1/libavcodec/x86/huffyuvencdsp.asm	2018-11-03 08:17:29.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/huffyuvencdsp.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,105 +0,0 @@
-;************************************************************************
-;* SIMD-optimized HuffYUV encoding functions
-;* Copyright (c) 2000, 2001 Fabrice Bellard
-;* Copyright (c) 2002-2004 Michael Niedermayer <michaelni@gmx.at>
-;*
-;* MMX optimization by Nick Kurshev <nickols_k@mail.ru>
-;* Conversion to NASM format by Tiancheng "Timothy" Gu <timothygu99@gmail.com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* 51, Inc., Foundation Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION .text
-
-%include "libavcodec/x86/huffyuvdsp_template.asm"
-
-;------------------------------------------------------------------------------
-; void ff_diff_int16(uint8_t *dst, const uint8_t *src1, const uint8_t *src2,
-;                    unsigned mask, int w);
-;------------------------------------------------------------------------------
-
-%macro DIFF_INT16 0
-cglobal diff_int16, 5,5,5, dst, src1, src2, mask, w, tmp
-%if mmsize > 8
-    test src1q, mmsize-1
-    jnz .unaligned
-    test src2q, mmsize-1
-    jnz .unaligned
-    test dstq, mmsize-1
-    jnz .unaligned
-%endif
-    INT16_LOOP a, sub
-%if mmsize > 8
-.unaligned:
-    INT16_LOOP u, sub
-%endif
-%endmacro
-
-%if ARCH_X86_32
-INIT_MMX mmx
-DIFF_INT16
-%endif
-
-INIT_XMM sse2
-DIFF_INT16
-
-%if HAVE_AVX2_EXTERNAL
-INIT_YMM avx2
-DIFF_INT16
-%endif
-
-INIT_MMX mmxext
-cglobal sub_hfyu_median_pred_int16, 7,7,0, dst, src1, src2, mask, w, left, left_top
-    add      wd, wd
-    movd    mm7, maskd
-    SPLATW  mm7, mm7
-    movq    mm0, [src1q]
-    movq    mm2, [src2q]
-    psllq   mm0, 16
-    psllq   mm2, 16
-    movd    mm6, [left_topq]
-    por     mm0, mm6
-    movd    mm6, [leftq]
-    por     mm2, mm6
-    xor     maskq, maskq
-.loop:
-    movq    mm1, [src1q + maskq]
-    movq    mm3, [src2q + maskq]
-    movq    mm4, mm2
-    psubw   mm2, mm0
-    paddw   mm2, mm1
-    pand    mm2, mm7
-    movq    mm5, mm4
-    pmaxsw  mm4, mm1
-    pminsw  mm1, mm5
-    pminsw  mm4, mm2
-    pmaxsw  mm4, mm1
-    psubw   mm3, mm4
-    pand    mm3, mm7
-    movq    [dstq + maskq], mm3
-    add     maskq, 8
-    movq    mm0, [src1q + maskq - 2]
-    movq    mm2, [src2q + maskq - 2]
-    cmp     maskq, wq
-        jb .loop
-    movzx maskd, word [src1q + wq - 2]
-    mov [left_topq], maskd
-    movzx maskd, word [src2q + wq - 2]
-    mov [leftq], maskd
-    RET
diff -uparN ffmpeg-4.1/libavcodec/x86/idctdsp.asm ffmpeg-y/libavcodec/x86/idctdsp.asm
--- ffmpeg-4.1/libavcodec/x86/idctdsp.asm	2018-01-01 06:35:48.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/idctdsp.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,183 +0,0 @@
-;******************************************************************************
-;* SIMD-optimized IDCT-related routines
-;* Copyright (c) 2008 Loren Merritt
-;* Copyright (c) 2003-2013 Michael Niedermayer
-;* Copyright (c) 2013 Daniel Kang
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-
-cextern pb_80
-
-SECTION .text
-
-;--------------------------------------------------------------------------
-;void ff_put_signed_pixels_clamped(const int16_t *block, uint8_t *pixels,
-;                                  ptrdiff_t line_size)
-;--------------------------------------------------------------------------
-
-%macro PUT_SIGNED_PIXELS_CLAMPED_HALF 1
-    mova     m1, [blockq+mmsize*0+%1]
-    mova     m2, [blockq+mmsize*2+%1]
-%if mmsize == 8
-    mova     m3, [blockq+mmsize*4+%1]
-    mova     m4, [blockq+mmsize*6+%1]
-%endif
-    packsswb m1, [blockq+mmsize*1+%1]
-    packsswb m2, [blockq+mmsize*3+%1]
-%if mmsize == 8
-    packsswb m3, [blockq+mmsize*5+%1]
-    packsswb m4, [blockq+mmsize*7+%1]
-%endif
-    paddb    m1, m0
-    paddb    m2, m0
-%if mmsize == 8
-    paddb    m3, m0
-    paddb    m4, m0
-    movq     [pixelsq+lsizeq*0], m1
-    movq     [pixelsq+lsizeq*1], m2
-    movq     [pixelsq+lsizeq*2], m3
-    movq     [pixelsq+lsize3q ], m4
-%else
-    movq     [pixelsq+lsizeq*0], m1
-    movhps   [pixelsq+lsizeq*1], m1
-    movq     [pixelsq+lsizeq*2], m2
-    movhps   [pixelsq+lsize3q ], m2
-%endif
-%endmacro
-
-%macro PUT_SIGNED_PIXELS_CLAMPED 1
-cglobal put_signed_pixels_clamped, 3, 4, %1, block, pixels, lsize, lsize3
-    mova     m0, [pb_80]
-    lea      lsize3q, [lsizeq*3]
-    PUT_SIGNED_PIXELS_CLAMPED_HALF 0
-    lea      pixelsq, [pixelsq+lsizeq*4]
-    PUT_SIGNED_PIXELS_CLAMPED_HALF 64
-    RET
-%endmacro
-
-INIT_MMX mmx
-PUT_SIGNED_PIXELS_CLAMPED 0
-INIT_XMM sse2
-PUT_SIGNED_PIXELS_CLAMPED 3
-
-;--------------------------------------------------------------------------
-; void ff_put_pixels_clamped(const int16_t *block, uint8_t *pixels,
-;                            ptrdiff_t line_size);
-;--------------------------------------------------------------------------
-; %1 = block offset
-%macro PUT_PIXELS_CLAMPED_HALF 1
-    mova     m0, [blockq+mmsize*0+%1]
-    mova     m1, [blockq+mmsize*2+%1]
-%if mmsize == 8
-    mova     m2, [blockq+mmsize*4+%1]
-    mova     m3, [blockq+mmsize*6+%1]
-%endif
-    packuswb m0, [blockq+mmsize*1+%1]
-    packuswb m1, [blockq+mmsize*3+%1]
-%if mmsize == 8
-    packuswb m2, [blockq+mmsize*5+%1]
-    packuswb m3, [blockq+mmsize*7+%1]
-    movq           [pixelsq], m0
-    movq    [lsizeq+pixelsq], m1
-    movq  [2*lsizeq+pixelsq], m2
-    movq   [lsize3q+pixelsq], m3
-%else
-    movq           [pixelsq], m0
-    movhps  [lsizeq+pixelsq], m0
-    movq  [2*lsizeq+pixelsq], m1
-    movhps [lsize3q+pixelsq], m1
-%endif
-%endmacro
-
-%macro PUT_PIXELS_CLAMPED 0
-cglobal put_pixels_clamped, 3, 4, 2, block, pixels, lsize, lsize3
-    lea lsize3q, [lsizeq*3]
-    PUT_PIXELS_CLAMPED_HALF 0
-    lea pixelsq, [pixelsq+lsizeq*4]
-    PUT_PIXELS_CLAMPED_HALF 64
-    RET
-%endmacro
-
-INIT_MMX mmx
-PUT_PIXELS_CLAMPED
-INIT_XMM sse2
-PUT_PIXELS_CLAMPED
-
-;--------------------------------------------------------------------------
-; void ff_add_pixels_clamped(const int16_t *block, uint8_t *pixels,
-;                            ptrdiff_t line_size);
-;--------------------------------------------------------------------------
-; %1 = block offset
-%macro ADD_PIXELS_CLAMPED 1
-    mova       m0, [blockq+mmsize*0+%1]
-    mova       m1, [blockq+mmsize*1+%1]
-%if mmsize == 8
-    mova       m5, [blockq+mmsize*2+%1]
-    mova       m6, [blockq+mmsize*3+%1]
-%endif
-    movq       m2, [pixelsq]
-    movq       m3, [pixelsq+lsizeq]
-%if mmsize == 8
-    mova       m7, m2
-    punpcklbw  m2, m4
-    punpckhbw  m7, m4
-    paddsw     m0, m2
-    paddsw     m1, m7
-    mova       m7, m3
-    punpcklbw  m3, m4
-    punpckhbw  m7, m4
-    paddsw     m5, m3
-    paddsw     m6, m7
-%else
-    punpcklbw  m2, m4
-    punpcklbw  m3, m4
-    paddsw     m0, m2
-    paddsw     m1, m3
-%endif
-    packuswb   m0, m1
-%if mmsize == 8
-    packuswb   m5, m6
-    movq       [pixelsq], m0
-    movq       [pixelsq+lsizeq], m5
-%else
-    movq       [pixelsq], m0
-    movhps     [pixelsq+lsizeq], m0
-%endif
-%endmacro
-
-%macro ADD_PIXELS_CLAMPED 0
-cglobal add_pixels_clamped, 3, 3, 5, block, pixels, lsize
-    pxor       m4, m4
-    ADD_PIXELS_CLAMPED 0
-    lea        pixelsq, [pixelsq+lsizeq*2]
-    ADD_PIXELS_CLAMPED 32
-    lea        pixelsq, [pixelsq+lsizeq*2]
-    ADD_PIXELS_CLAMPED 64
-    lea        pixelsq, [pixelsq+lsizeq*2]
-    ADD_PIXELS_CLAMPED 96
-    RET
-%endmacro
-
-INIT_MMX mmx
-ADD_PIXELS_CLAMPED
-INIT_XMM sse2
-ADD_PIXELS_CLAMPED
diff -uparN ffmpeg-4.1/libavcodec/x86/imdct36.asm ffmpeg-y/libavcodec/x86/imdct36.asm
--- ffmpeg-4.1/libavcodec/x86/imdct36.asm	2018-11-03 08:17:29.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/imdct36.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,741 +0,0 @@
-;******************************************************************************
-;* 36 point SSE-optimized IMDCT transform
-;* Copyright (c) 2011 Vitor Sessak
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-
-ps_mask:  dd 0, ~0, ~0, ~0
-ps_mask2: dd 0, ~0,  0, ~0
-ps_mask3: dd 0,  0,  0, ~0
-ps_mask4: dd 0, ~0,  0,  0
-
-ps_val1:  dd          -0.5,          -0.5, -0.8660254038, -0.8660254038
-ps_val2:  dd           1.0,           1.0,  0.8660254038,  0.8660254038
-ps_val3:  dd  0.1736481777,  0.1736481777,  0.3420201433,  0.3420201433
-ps_val4:  dd -0.7660444431, -0.7660444431,  0.8660254038,  0.8660254038
-ps_val5:  dd -0.9396926208, -0.9396926208, -0.9848077530, -0.9848077530
-ps_val6:  dd           0.5,           0.5, -0.6427876097, -0.6427876097
-ps_val7:  dd           1.0,           1.0, -0.6427876097, -0.6427876097
-
-ps_p1p1m1m1: dd 0,          0, 0x80000000, 0x80000000
-ps_p1m1p1m1: dd 0, 0x80000000,          0, 0x80000000
-
-ps_cosh:       dd 1.0, 0.50190991877167369479,  1.0,  5.73685662283492756461
-               dd 1.0, 0.51763809020504152469,  1.0,  1.93185165257813657349
-               dd 1.0, 0.55168895948124587824, -1.0, -1.18310079157624925896
-               dd 1.0, 0.61038729438072803416, -1.0, -0.87172339781054900991
-               dd 1.0, 0.70710678118654752439,  0.0,  0.0
-
-ps_cosh_sse3:  dd 1.0, -0.50190991877167369479,  1.0, -5.73685662283492756461
-               dd 1.0, -0.51763809020504152469,  1.0, -1.93185165257813657349
-               dd 1.0, -0.55168895948124587824, -1.0,  1.18310079157624925896
-               dd 1.0, -0.61038729438072803416, -1.0,  0.87172339781054900991
-               dd 1.0, -0.70710678118654752439,  0.0,  0.0
-
-costabs:  times 4 dd  0.98480773
-          times 4 dd  0.93969262
-          times 4 dd  0.86602539
-          times 4 dd -0.76604444
-          times 4 dd -0.64278764
-          times 4 dd  0.50000000
-          times 4 dd -0.50000000
-          times 4 dd -0.34202015
-          times 4 dd -0.17364818
-          times 4 dd  0.50190992
-          times 4 dd  0.51763808
-          times 4 dd  0.55168896
-          times 4 dd  0.61038726
-          times 4 dd  0.70710677
-          times 4 dd  0.87172341
-          times 4 dd  1.18310082
-          times 4 dd  1.93185163
-          times 4 dd  5.73685646
-
-%define SBLIMIT 32
-SECTION .text
-
-%macro PSHUFD 3
-%if cpuflag(sse2) && notcpuflag(avx)
-    pshufd %1, %2, %3
-%else
-    shufps %1, %2, %2, %3
-%endif
-%endmacro
-
-; input  %2={x1,x2,x3,x4}, %3={y1,y2,y3,y4}
-; output %1={x3,x4,y1,y2}
-%macro BUILDINVHIGHLOW 3
-%if cpuflag(avx)
-    shufps %1, %2, %3, 0x4e
-%else
-    movlhps %1, %3
-    movhlps %1, %2
-%endif
-%endmacro
-
-; input  %2={x1,x2,x3,x4}, %3={y1,y2,y3,y4}
-; output %1={x4,y1,y2,y3}
-%macro ROTLEFT 3
-%if cpuflag(ssse3)
-    palignr  %1, %3, %2, 12
-%else
-    BUILDINVHIGHLOW %1, %2, %3
-    shufps  %1, %1, %3, 0x99
-%endif
-%endmacro
-
-%macro INVERTHL 2
-%if cpuflag(sse2)
-    PSHUFD  %1, %2, 0x4e
-%else
-    movhlps %1, %2
-    movlhps %1, %2
-%endif
-%endmacro
-
-%macro BUTTERF 3
-    INVERTHL %2, %1
-    xorps    %1, [ps_p1p1m1m1]
-    addps    %1, %2
-%if cpuflag(sse3)
-    mulps    %1, %1, [ps_cosh_sse3 + %3]
-    PSHUFD   %2, %1, 0xb1
-    addsubps %1, %1, %2
-%else
-    mulps    %1, [ps_cosh + %3]
-    PSHUFD   %2, %1, 0xb1
-    xorps    %1, [ps_p1m1p1m1]
-    addps    %1, %2
-%endif
-%endmacro
-
-%macro BUTTERF2 3
-%if cpuflag(sse3)
-    mulps    %1, %1, [ps_cosh_sse3 + %3]
-    PSHUFD   %2, %1, 0xe1
-    addsubps %1, %1, %2
-%else
-    mulps    %1, [ps_cosh + %3]
-    PSHUFD   %2, %1, 0xe1
-    xorps    %1, [ps_p1m1p1m1]
-    addps    %1, %2
-%endif
-%endmacro
-
-%macro STORE 4
-%if cpuflag(sse4)
-    movss     [%3       ], %1
-    extractps dword [%3 +   %4], %1, 1
-    extractps dword [%3 + 2*%4], %1, 2
-    extractps dword [%3 + 3*%4], %1, 3
-%else
-    movhlps %2, %1
-    movss   [%3       ], %1
-    movss   [%3 + 2*%4], %2
-    shufps  %1, %1, 0xb1
-    movss   [%3 +   %4], %1
-    movhlps %2, %1
-    movss   [%3 + 3*%4], %2
-%endif
-%endmacro
-
-%macro LOAD 4
-    movlps  %1, [%3       ]
-    movhps  %1, [%3 +   %4]
-    movlps  %2, [%3 + 2*%4]
-    movhps  %2, [%3 + 3*%4]
-    shufps  %1, %2, 0x88
-%endmacro
-
-%macro LOADA64 2
-%if cpuflag(avx)
-   movu     %1, [%2]
-%else
-   movlps   %1, [%2]
-   movhps   %1, [%2 + 8]
-%endif
-%endmacro
-
-%macro DEFINE_IMDCT 0
-cglobal imdct36_float, 4,4,9, out, buf, in, win
-
-    ; for(i=17;i>=1;i--) in[i] += in[i-1];
-    LOADA64 m0, inq
-    LOADA64 m1, inq + 16
-
-    ROTLEFT m5, m0, m1
-
-    PSHUFD  m6, m0, 0x93
-    andps   m6, m6, [ps_mask]
-    addps   m0, m0, m6
-
-    LOADA64 m2, inq + 32
-
-    ROTLEFT m7, m1, m2
-
-    addps   m1, m1, m5
-    LOADA64 m3, inq + 48
-
-    ROTLEFT m5, m2, m3
-
-    xorps   m4, m4, m4
-    movlps  m4, [inq+64]
-    BUILDINVHIGHLOW m6, m3, m4
-    shufps  m6, m6, m4, 0xa9
-
-    addps   m4, m4, m6
-    addps   m2, m2, m7
-    addps   m3, m3, m5
-
-    ; for(i=17;i>=3;i-=2) in[i] += in[i-2];
-    movlhps m5, m5, m0
-    andps   m5, m5, [ps_mask3]
-
-    BUILDINVHIGHLOW m7, m0, m1
-    andps   m7, m7, [ps_mask2]
-
-    addps   m0, m0, m5
-
-    BUILDINVHIGHLOW m6, m1, m2
-    andps   m6, m6, [ps_mask2]
-
-    addps  m1, m1, m7
-
-    BUILDINVHIGHLOW m7, m2, m3
-    andps   m7, m7, [ps_mask2]
-
-    addps   m2, m2, m6
-
-    movhlps m6, m6, m3
-    andps   m6, m6, [ps_mask4]
-
-    addps  m3, m3, m7
-    addps  m4, m4, m6
-
-    ; Populate tmp[]
-    movlhps m6, m1, m5    ; zero out high values
-    subps   m6, m6, m4
-
-    subps  m5, m0, m3
-
-%if ARCH_X86_64
-    SWAP   m5, m8
-%endif
-
-    mulps  m7, m2, [ps_val1]
-
-%if ARCH_X86_64
-    mulps  m5, m8, [ps_val2]
-%else
-    mulps  m5, m5, [ps_val2]
-%endif
-    addps  m7, m7, m5
-
-    mulps  m5, m6, [ps_val1]
-    subps  m7, m7, m5
-
-%if ARCH_X86_64
-    SWAP   m5, m8
-%else
-    subps  m5, m0, m3
-%endif
-
-    subps  m5, m5, m6
-    addps  m5, m5, m2
-
-    shufps m6, m4, m3, 0xe4
-    subps  m6, m6, m2
-    mulps  m6, m6, [ps_val3]
-
-    addps  m4, m4, m1
-    mulps  m4, m4, [ps_val4]
-
-    shufps m1, m1, m0, 0xe4
-    addps  m1, m1, m2
-    mulps  m1, m1, [ps_val5]
-
-    mulps  m3, m3, [ps_val6]
-    mulps  m0, m0, [ps_val7]
-    addps  m0, m0, m3
-
-    xorps  m2, m1, [ps_p1p1m1m1]
-    subps  m2, m2, m4
-    addps  m2, m2, m0
-
-    addps  m3, m4, m0
-    subps  m3, m3, m6
-    xorps  m3, m3, [ps_p1p1m1m1]
-
-    shufps m0, m0, m4, 0xe4
-    subps  m0, m0, m1
-    addps  m0, m0, m6
-
-    BUILDINVHIGHLOW m4, m2, m3
-    shufps  m3, m3, m2, 0x4e
-
-    ; we have tmp = {SwAPLH(m0), SwAPLH(m7), m3, m4, m5}
-
-    BUTTERF  m0, m1, 0
-    BUTTERF  m7, m2, 16
-    BUTTERF  m3, m6, 32
-    BUTTERF  m4, m1, 48
-    BUTTERF2 m5, m1, 64
-
-    ; permutates:
-    ; m0    0  1  2  3     =>     2  6 10 14   m1
-    ; m7    4  5  6  7     =>     3  7 11 15   m2
-    ; m3    8  9 10 11     =>    17 13  9  5   m3
-    ; m4   12 13 14 15     =>    16 12  8  4   m5
-    ; m5   16 17 xx xx     =>     0  1 xx xx   m0
-
-    unpckhps m1, m0, m7
-    unpckhps m6, m3, m4
-    movhlps  m2, m6, m1
-    movlhps  m1, m1, m6
-
-    unpcklps m5, m5, m4
-    unpcklps m3, m3, m7
-    movhlps  m4, m3, m5
-    movlhps  m5, m5, m3
-    SWAP m4, m3
-    ; permutation done
-
-    PSHUFD  m6, m2, 0xb1
-    movss   m4, [bufq + 4*68]
-    movss   m7, [bufq + 4*64]
-    unpcklps  m7, m7, m4
-    mulps   m6, m6, [winq + 16*4]
-    addps   m6, m6, m7
-    movss   [outq + 64*SBLIMIT], m6
-    shufps  m6, m6, m6, 0xb1
-    movss   [outq + 68*SBLIMIT], m6
-
-    mulps   m6, m3, [winq + 4*4]
-    LOAD    m4, m7, bufq + 4*16, 16
-    addps   m6, m6, m4
-    STORE   m6, m7, outq + 16*SBLIMIT, 4*SBLIMIT
-
-    shufps  m4, m0, m3, 0xb5
-    mulps   m4, m4, [winq + 8*4]
-    LOAD    m7, m6, bufq + 4*32, 16
-    addps   m4, m4, m7
-    STORE   m4, m6, outq + 32*SBLIMIT, 4*SBLIMIT
-
-    shufps  m3, m3, m2, 0xb1
-    mulps   m3, m3, [winq + 12*4]
-    LOAD    m7, m6, bufq + 4*48, 16
-    addps   m3, m3, m7
-    STORE   m3, m7, outq + 48*SBLIMIT, 4*SBLIMIT
-
-    mulps   m2, m2, [winq]
-    LOAD    m6, m7, bufq, 16
-    addps   m2, m2, m6
-    STORE   m2, m7, outq, 4*SBLIMIT
-
-    mulps    m4, m1, [winq + 20*4]
-    STORE    m4, m7, bufq, 16
-
-    mulps    m3, m5, [winq + 24*4]
-    STORE    m3, m7, bufq + 4*16, 16
-
-    shufps   m0, m0, m5, 0xb0
-    mulps    m0, m0, [winq + 28*4]
-    STORE    m0, m7, bufq + 4*32, 16
-
-    shufps   m5, m5, m1, 0xb1
-    mulps    m5, m5, [winq + 32*4]
-    STORE    m5, m7, bufq + 4*48, 16
-
-    shufps   m1, m1, m1, 0xb1
-    mulps    m1, m1, [winq + 36*4]
-    movss    [bufq + 4*64], m1
-    shufps   m1, m1, 0xb1
-    movss    [bufq + 4*68], m1
-    RET
-%endmacro
-
-%if ARCH_X86_32
-INIT_XMM sse
-DEFINE_IMDCT
-%endif
-
-INIT_XMM sse2
-DEFINE_IMDCT
-
-INIT_XMM sse3
-DEFINE_IMDCT
-
-INIT_XMM ssse3
-DEFINE_IMDCT
-
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-DEFINE_IMDCT
-%endif
-
-INIT_XMM sse
-
-%if ARCH_X86_64
-%define SPILL SWAP
-%define UNSPILL SWAP
-%define SPILLED(x) m %+ x
-%else
-%define SPILLED(x) [tmpq+(x-8)*16 + 32*4]
-%macro SPILL 2 ; xmm#, mempos
-    movaps SPILLED(%2), m%1
-%endmacro
-%macro UNSPILL 2
-    movaps m%1, SPILLED(%2)
-%endmacro
-%endif
-
-%macro DEFINE_FOUR_IMDCT 0
-cglobal four_imdct36_float, 5,5,16, out, buf, in, win, tmp
-    movlps  m0, [inq+64]
-    movhps  m0, [inq+64 +   72]
-    movlps  m3, [inq+64 + 2*72]
-    movhps  m3, [inq+64 + 3*72]
-
-    shufps  m5, m0, m3, 0xdd
-    shufps  m0, m0, m3, 0x88
-
-    mova     m1, [inq+48]
-    movu     m6, [inq+48 +   72]
-    mova     m7, [inq+48 + 2*72]
-    movu     m3, [inq+48 + 3*72]
-
-    TRANSPOSE4x4PS 1, 6, 7, 3, 4
-
-    addps   m4, m6, m7
-    mova    [tmpq+4*28], m4
-
-    addps    m7, m3
-    addps    m6, m1
-    addps    m3, m0
-    addps    m0, m5
-    addps    m0, m7
-    addps    m7, m6
-    mova    [tmpq+4*12], m7
-    SPILL   3, 12
-
-    mova     m4, [inq+32]
-    movu     m5, [inq+32 +   72]
-    mova     m2, [inq+32 + 2*72]
-    movu     m7, [inq+32 + 3*72]
-
-    TRANSPOSE4x4PS 4, 5, 2, 7, 3
-
-    addps   m1, m7
-    SPILL   1, 11
-
-    addps   m3, m5, m2
-    SPILL   3, 13
-
-    addps    m7, m2
-    addps    m5, m4
-    addps    m6, m7
-    mova    [tmpq], m6
-    addps   m7, m5
-    mova    [tmpq+4*16], m7
-
-    mova    m2, [inq+16]
-    movu    m7, [inq+16 +   72]
-    mova    m1, [inq+16 + 2*72]
-    movu    m6, [inq+16 + 3*72]
-
-    TRANSPOSE4x4PS 2, 7, 1, 6, 3
-
-    addps   m4, m6
-    addps   m6, m1
-    addps   m1, m7
-    addps   m7, m2
-    addps   m5, m6
-    SPILL   5, 15
-    addps   m6, m7
-    mulps   m6, [costabs + 16*2]
-    mova    [tmpq+4*8], m6
-    SPILL   1, 10
-    SPILL   0, 14
-
-    mova    m1, [inq]
-    movu    m6, [inq +   72]
-    mova    m3, [inq + 2*72]
-    movu    m5, [inq + 3*72]
-
-    TRANSPOSE4x4PS 1, 6, 3, 5, 0
-
-    addps    m2, m5
-    addps    m5, m3
-    addps    m7, m5
-    addps    m3, m6
-    addps    m6, m1
-    SPILL    7, 8
-    addps    m5, m6
-    SPILL    6, 9
-    addps    m6, m4, SPILLED(12)
-    subps    m6, m2
-    UNSPILL  7, 11
-    SPILL    5, 11
-    subps    m5, m1, m7
-    mulps    m7, [costabs + 16*5]
-    addps    m7, m1
-    mulps    m0, m6, [costabs + 16*6]
-    addps    m0, m5
-    mova     [tmpq+4*24], m0
-    addps    m6, m5
-    mova     [tmpq+4*4], m6
-    addps    m6, m4, m2
-    mulps    m6, [costabs + 16*1]
-    subps    m4, SPILLED(12)
-    mulps    m4, [costabs + 16*8]
-    addps    m2, SPILLED(12)
-    mulps    m2, [costabs + 16*3]
-    subps    m5, m7, m6
-    subps    m5, m2
-    addps    m6, m7
-    addps    m6, m4
-    addps    m7, m2
-    subps    m7, m4
-    mova     [tmpq+4*20], m7
-    mova     m2, [tmpq+4*28]
-    mova     [tmpq+4*28], m5
-    UNSPILL  7, 13
-    subps    m5, m7, m2
-    mulps    m5, [costabs + 16*7]
-    UNSPILL  1, 10
-    mulps    m1, [costabs + 16*2]
-    addps    m4, m3, m2
-    mulps    m4, [costabs + 16*4]
-    addps    m2, m7
-    addps    m7, m3
-    mulps    m7, [costabs]
-    subps    m3, m2
-    mulps    m3, [costabs + 16*2]
-    addps    m2, m7, m5
-    addps    m2, m1
-    SPILL    2, 10
-    addps    m7, m4
-    subps    m7, m1
-    SPILL    7, 12
-    subps    m5, m4
-    subps    m5, m1
-    UNSPILL  0, 14
-    SPILL    5, 13
-    addps    m1, m0, SPILLED(15)
-    subps    m1, SPILLED(8)
-    mova     m4, [costabs + 16*5]
-    mulps    m4, [tmpq]
-    UNSPILL  2, 9
-    addps    m4, m2
-    subps    m2, [tmpq]
-    mulps    m5, m1, [costabs + 16*6]
-    addps    m5, m2
-    SPILL    5, 9
-    addps    m2, m1
-    SPILL    2, 14
-    UNSPILL  5, 15
-    subps    m7, m5, m0
-    addps    m5, SPILLED(8)
-    mulps    m5, [costabs + 16*1]
-    mulps    m7, [costabs + 16*8]
-    addps    m0, SPILLED(8)
-    mulps    m0, [costabs + 16*3]
-    subps    m2, m4, m5
-    subps    m2, m0
-    SPILL    2, 15
-    addps    m5, m4
-    addps    m5, m7
-    addps    m4, m0
-    subps    m4, m7
-    SPILL    4, 8
-    mova     m7, [tmpq+4*16]
-    mova     m2, [tmpq+4*12]
-    addps    m0, m7, m2
-    subps    m0, SPILLED(11)
-    mulps    m0, [costabs + 16*2]
-    addps    m4, m7, SPILLED(11)
-    mulps    m4, [costabs]
-    subps    m7, m2
-    mulps    m7, [costabs + 16*7]
-    addps    m2, SPILLED(11)
-    mulps    m2, [costabs + 16*4]
-    addps    m1, m7, [tmpq+4*8]
-    addps    m1, m4
-    addps    m4, m2
-    subps    m4, [tmpq+4*8]
-    SPILL    4, 11
-    subps    m7, m2
-    subps    m7, [tmpq+4*8]
-    addps    m4, m6, SPILLED(10)
-    subps    m6, SPILLED(10)
-    addps    m2, m5, m1
-    mulps    m2, [costabs + 16*9]
-    subps    m5, m1
-    mulps    m5, [costabs + 16*17]
-    subps    m1, m4, m2
-    addps    m4, m2
-    mulps    m2, m1, [winq+4*36]
-    addps    m2, [bufq+4*36]
-    mova     [outq+1152], m2
-    mulps    m1, [winq+4*32]
-    addps    m1, [bufq+4*32]
-    mova     [outq+1024], m1
-    mulps    m1, m4, [winq+4*116]
-    mova     [bufq+4*36], m1
-    mulps    m4, [winq+4*112]
-    mova     [bufq+4*32], m4
-    addps    m2, m6, m5
-    subps    m6, m5
-    mulps    m1, m6, [winq+4*68]
-    addps    m1, [bufq+4*68]
-    mova     [outq+2176], m1
-    mulps    m6, [winq]
-    addps    m6, [bufq]
-    mova     [outq], m6
-    mulps    m1, m2, [winq+4*148]
-    mova     [bufq+4*68], m1
-    mulps    m2, [winq+4*80]
-    mova     [bufq], m2
-    addps    m5, m3, [tmpq+4*24]
-    mova     m2, [tmpq+4*24]
-    subps    m2, m3
-    mova     m1, SPILLED(9)
-    subps    m1, m0
-    mulps    m1, [costabs + 16*10]
-    addps    m0, SPILLED(9)
-    mulps    m0, [costabs + 16*16]
-    addps    m6, m5, m1
-    subps    m5, m1
-    mulps    m3, m5, [winq+4*40]
-    addps    m3, [bufq+4*40]
-    mova     [outq+1280], m3
-    mulps    m5, [winq+4*28]
-    addps    m5, [bufq+4*28]
-    mova     [outq+896], m5
-    mulps    m1, m6, [winq+4*120]
-    mova     [bufq+4*40], m1
-    mulps    m6, [winq+4*108]
-    mova     [bufq+4*28], m6
-    addps    m1, m2, m0
-    subps    m2, m0
-    mulps    m5, m2, [winq+4*64]
-    addps    m5, [bufq+4*64]
-    mova     [outq+2048], m5
-    mulps    m2, [winq+4*4]
-    addps    m2, [bufq+4*4]
-    mova     [outq+128], m2
-    mulps    m0, m1, [winq+4*144]
-    mova     [bufq+4*64], m0
-    mulps    m1, [winq+4*84]
-    mova     [bufq+4*4], m1
-    mova     m1, [tmpq+4*28]
-    mova     m5, m1
-    addps    m1, SPILLED(13)
-    subps    m5, SPILLED(13)
-    UNSPILL  3, 15
-    addps    m2, m7, m3
-    mulps    m2, [costabs + 16*11]
-    subps    m3, m7
-    mulps    m3, [costabs + 16*15]
-    addps    m0, m2, m1
-    subps    m1, m2
-    SWAP     m0, m2
-    mulps    m6, m1, [winq+4*44]
-    addps    m6, [bufq+4*44]
-    mova     [outq+1408], m6
-    mulps    m1, [winq+4*24]
-    addps    m1, [bufq+4*24]
-    mova     [outq+768], m1
-    mulps    m0, m2, [winq+4*124]
-    mova     [bufq+4*44], m0
-    mulps    m2, [winq+4*104]
-    mova     [bufq+4*24], m2
-    addps    m0, m5, m3
-    subps    m5, m3
-    mulps    m1, m5, [winq+4*60]
-    addps    m1, [bufq+4*60]
-    mova     [outq+1920], m1
-    mulps    m5, [winq+4*8]
-    addps    m5, [bufq+4*8]
-    mova     [outq+256], m5
-    mulps    m1, m0, [winq+4*140]
-    mova     [bufq+4*60], m1
-    mulps    m0, [winq+4*88]
-    mova     [bufq+4*8], m0
-    mova     m1, [tmpq+4*20]
-    addps    m1, SPILLED(12)
-    mova     m2, [tmpq+4*20]
-    subps    m2, SPILLED(12)
-    UNSPILL  7, 8
-    subps    m0, m7, SPILLED(11)
-    addps    m7, SPILLED(11)
-    mulps    m4, m7, [costabs + 16*12]
-    mulps    m0, [costabs + 16*14]
-    addps    m5, m1, m4
-    subps    m1, m4
-    mulps    m7, m1, [winq+4*48]
-    addps    m7, [bufq+4*48]
-    mova     [outq+1536], m7
-    mulps    m1, [winq+4*20]
-    addps    m1, [bufq+4*20]
-    mova     [outq+640], m1
-    mulps    m1, m5, [winq+4*128]
-    mova     [bufq+4*48], m1
-    mulps    m5, [winq+4*100]
-    mova     [bufq+4*20], m5
-    addps    m6, m2, m0
-    subps    m2, m0
-    mulps    m1, m2, [winq+4*56]
-    addps    m1, [bufq+4*56]
-    mova     [outq+1792], m1
-    mulps    m2, [winq+4*12]
-    addps    m2, [bufq+4*12]
-    mova     [outq+384], m2
-    mulps    m0, m6, [winq+4*136]
-    mova    [bufq+4*56], m0
-    mulps    m6, [winq+4*92]
-    mova     [bufq+4*12], m6
-    UNSPILL  0, 14
-    mulps    m0, [costabs + 16*13]
-    mova     m3, [tmpq+4*4]
-    addps    m2, m0, m3
-    subps    m3, m0
-    mulps    m0, m3, [winq+4*52]
-    addps    m0, [bufq+4*52]
-    mova     [outq+1664], m0
-    mulps    m3, [winq+4*16]
-    addps    m3, [bufq+4*16]
-    mova     [outq+512], m3
-    mulps    m0, m2, [winq+4*132]
-    mova     [bufq+4*52], m0
-    mulps    m2, [winq+4*96]
-    mova     [bufq+4*16], m2
-    RET
-%endmacro
-
-INIT_XMM sse
-DEFINE_FOUR_IMDCT
-
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-DEFINE_FOUR_IMDCT
-%endif
diff -uparN ffmpeg-4.1/libavcodec/x86/jpeg2000dsp.asm ffmpeg-y/libavcodec/x86/jpeg2000dsp.asm
--- ffmpeg-4.1/libavcodec/x86/jpeg2000dsp.asm	2018-11-03 08:17:29.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/jpeg2000dsp.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,164 +0,0 @@
-;******************************************************************************
-;* SIMD-optimized JPEG2000 DSP functions
-;* Copyright (c) 2014 Nicolas Bertrand
-;* Copyright (c) 2015 James Almer
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA 32
-
-pf_ict0: times 8 dd 1.402
-pf_ict1: times 8 dd 0.34413
-pf_ict2: times 8 dd 0.71414
-pf_ict3: times 8 dd 1.772
-
-SECTION .text
-
-;***********************************************************************
-; ff_ict_float_<opt>(float *src0, float *src1, float *src2, int csize)
-;***********************************************************************
-%macro ICT_FLOAT 1
-cglobal ict_float, 4, 4, %1, src0, src1, src2, csize
-    shl  csized, 2
-    add   src0q, csizeq
-    add   src1q, csizeq
-    add   src2q, csizeq
-    neg  csizeq
-    movaps   m6, [pf_ict0]
-    movaps   m7, [pf_ict1]
-    %define ICT0 m6
-    %define ICT1 m7
-
-%if ARCH_X86_64
-    movaps   m8, [pf_ict2]
-    %define ICT2 m8
-%if cpuflag(avx)
-    movaps   m3, [pf_ict3]
-    %define ICT3 m3
-%else
-    movaps   m9, [pf_ict3]
-    %define ICT3 m9
-%endif
-
-%else ; ARCH_X86_32
-    %define ICT2 [pf_ict2]
-%if cpuflag(avx)
-    movaps   m3, [pf_ict3]
-    %define ICT3 m3
-%else
-    %define ICT3 [pf_ict3]
-%endif
-
-%endif ; ARCH
-
-align 16
-.loop:
-    movaps   m0, [src0q+csizeq]
-    movaps   m1, [src1q+csizeq]
-    movaps   m2, [src2q+csizeq]
-
-%if cpuflag(fma4) || cpuflag(fma3)
-%if cpuflag(fma4)
-    fnmaddps  m5, m1, ICT1, m0
-    fmaddps   m4, m2, ICT0, m0
-%else ; fma3
-    movaps    m5, m1
-    movaps    m4, m2
-    fnmaddps  m5, m5, ICT1, m0
-    fmaddps   m4, m4, ICT0, m0
-%endif
-    fmaddps   m0, m1, ICT3, m0
-    fnmaddps  m5, m2, ICT2, m5
-%else ; non FMA
-%if cpuflag(avx)
-    mulps    m5, m1, ICT1
-    mulps    m4, m2, ICT0
-    mulps    m1, m1, ICT3
-    mulps    m2, m2, ICT2
-    subps    m5, m0, m5
-%else ; sse
-    movaps   m3, m1
-    movaps   m4, m2
-    movaps   m5, m0
-    mulps    m3, ICT1
-    mulps    m4, ICT0
-    mulps    m1, ICT3
-    mulps    m2, ICT2
-    subps    m5, m3
-%endif
-    addps    m4, m4, m0
-    addps    m0, m0, m1
-    subps    m5, m5, m2
-%endif
-
-    movaps   [src0q+csizeq], m4
-    movaps   [src2q+csizeq], m0
-    movaps   [src1q+csizeq], m5
-    add  csizeq, mmsize
-    jl .loop
-    REP_RET
-%endmacro
-
-INIT_XMM sse
-ICT_FLOAT 10
-INIT_YMM avx
-ICT_FLOAT 9
-%if HAVE_FMA4_EXTERNAL
-INIT_XMM fma4
-ICT_FLOAT 9
-%endif
-INIT_YMM fma3
-ICT_FLOAT 9
-
-;***************************************************************************
-; ff_rct_int_<opt>(int32_t *src0, int32_t *src1, int32_t *src2, int csize)
-;***************************************************************************
-%macro RCT_INT 0
-cglobal rct_int, 4, 4, 4, src0, src1, src2, csize
-    shl  csized, 2
-    add   src0q, csizeq
-    add   src1q, csizeq
-    add   src2q, csizeq
-    neg  csizeq
-
-align 16
-.loop:
-    mova   m1, [src1q+csizeq]
-    mova   m2, [src2q+csizeq]
-    mova   m0, [src0q+csizeq]
-    paddd  m3, m1, m2
-    psrad  m3, 2
-    psubd  m0, m3
-    paddd  m1, m0
-    paddd  m2, m0
-    mova   [src1q+csizeq], m0
-    mova   [src2q+csizeq], m1
-    mova   [src0q+csizeq], m2
-    add  csizeq, mmsize
-    jl .loop
-    REP_RET
-%endmacro
-
-INIT_XMM sse2
-RCT_INT
-%if HAVE_AVX2_EXTERNAL
-INIT_YMM avx2
-RCT_INT
-%endif
diff -uparN ffmpeg-4.1/libavcodec/x86/lossless_audiodsp.asm ffmpeg-y/libavcodec/x86/lossless_audiodsp.asm
--- ffmpeg-4.1/libavcodec/x86/lossless_audiodsp.asm	2018-11-02 02:34:25.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/lossless_audiodsp.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,190 +0,0 @@
-;******************************************************************************
-;* Copyright (c) 2008 Loren Merritt
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION .text
-
-%macro SCALARPRODUCT 0
-; int ff_scalarproduct_and_madd_int16(int16_t *v1, int16_t *v2, int16_t *v3,
-;                                     int order, int mul)
-cglobal scalarproduct_and_madd_int16, 4,4,8, v1, v2, v3, order, mul
-    shl orderq, 1
-    movd    m7, mulm
-%if mmsize == 16
-    pshuflw m7, m7, 0
-    punpcklqdq m7, m7
-%else
-    pshufw  m7, m7, 0
-%endif
-    pxor    m6, m6
-    add v1q, orderq
-    add v2q, orderq
-    add v3q, orderq
-    neg orderq
-.loop:
-    movu    m0, [v2q + orderq]
-    movu    m1, [v2q + orderq + mmsize]
-    mova    m4, [v1q + orderq]
-    mova    m5, [v1q + orderq + mmsize]
-    movu    m2, [v3q + orderq]
-    movu    m3, [v3q + orderq + mmsize]
-    pmaddwd m0, m4
-    pmaddwd m1, m5
-    pmullw  m2, m7
-    pmullw  m3, m7
-    paddd   m6, m0
-    paddd   m6, m1
-    paddw   m2, m4
-    paddw   m3, m5
-    mova    [v1q + orderq], m2
-    mova    [v1q + orderq + mmsize], m3
-    add     orderq, mmsize*2
-    jl .loop
-    HADDD   m6, m0
-    movd   eax, m6
-    RET
-%endmacro
-
-INIT_MMX mmxext
-SCALARPRODUCT
-INIT_XMM sse2
-SCALARPRODUCT
-
-INIT_XMM sse4
-; int ff_scalarproduct_and_madd_int32(int16_t *v1, int32_t *v2, int16_t *v3,
-;                                     int order, int mul)
-cglobal scalarproduct_and_madd_int32, 4,4,8, v1, v2, v3, order, mul
-    shl orderq, 1
-    movd    m7, mulm
-    SPLATW  m7, m7
-    pxor    m6, m6
-    add v1q, orderq
-    lea v2q, [v2q + 2*orderq]
-    add v3q, orderq
-    neg orderq
-.loop:
-    mova    m3, [v1q + orderq]
-    movu    m0, [v2q + 2*orderq]
-    pmovsxwd m4, m3
-    movu    m1, [v2q + 2*orderq + mmsize]
-    movhlps m5, m3
-    movu    m2, [v3q + orderq]
-    pmovsxwd m5, m5
-    pmullw  m2, m7
-    pmulld  m0, m4
-    pmulld  m1, m5
-    paddw   m2, m3
-    paddd   m6, m0
-    paddd   m6, m1
-    mova    [v1q + orderq], m2
-    add     orderq, 16
-    jl .loop
-    HADDD   m6, m0
-    movd   eax, m6
-    RET
-
-%macro SCALARPRODUCT_LOOP 1
-align 16
-.loop%1:
-    sub     orderq, mmsize*2
-%if %1
-    mova    m1, m4
-    mova    m4, [v2q + orderq]
-    mova    m0, [v2q + orderq + mmsize]
-    palignr m1, m0, %1
-    palignr m0, m4, %1
-    mova    m3, m5
-    mova    m5, [v3q + orderq]
-    mova    m2, [v3q + orderq + mmsize]
-    palignr m3, m2, %1
-    palignr m2, m5, %1
-%else
-    mova    m0, [v2q + orderq]
-    mova    m1, [v2q + orderq + mmsize]
-    mova    m2, [v3q + orderq]
-    mova    m3, [v3q + orderq + mmsize]
-%endif
-    %define t0  [v1q + orderq]
-    %define t1  [v1q + orderq + mmsize]
-%if ARCH_X86_64
-    mova    m8, t0
-    mova    m9, t1
-    %define t0  m8
-    %define t1  m9
-%endif
-    pmaddwd m0, t0
-    pmaddwd m1, t1
-    pmullw  m2, m7
-    pmullw  m3, m7
-    paddw   m2, t0
-    paddw   m3, t1
-    paddd   m6, m0
-    paddd   m6, m1
-    mova    [v1q + orderq], m2
-    mova    [v1q + orderq + mmsize], m3
-    jg .loop%1
-%if %1
-    jmp .end
-%endif
-%endmacro
-
-; int ff_scalarproduct_and_madd_int16(int16_t *v1, int16_t *v2, int16_t *v3,
-;                                     int order, int mul)
-INIT_XMM ssse3
-cglobal scalarproduct_and_madd_int16, 4,5,10, v1, v2, v3, order, mul
-    shl orderq, 1
-    movd    m7, mulm
-    pshuflw m7, m7, 0
-    punpcklqdq m7, m7
-    pxor    m6, m6
-    mov    r4d, v2d
-    and    r4d, 15
-    and    v2q, ~15
-    and    v3q, ~15
-    mova    m4, [v2q + orderq]
-    mova    m5, [v3q + orderq]
-    ; linear is faster than branch tree or jump table, because the branches taken are cyclic (i.e. predictable)
-    cmp    r4d, 0
-    je .loop0
-    cmp    r4d, 2
-    je .loop2
-    cmp    r4d, 4
-    je .loop4
-    cmp    r4d, 6
-    je .loop6
-    cmp    r4d, 8
-    je .loop8
-    cmp    r4d, 10
-    je .loop10
-    cmp    r4d, 12
-    je .loop12
-SCALARPRODUCT_LOOP 14
-SCALARPRODUCT_LOOP 12
-SCALARPRODUCT_LOOP 10
-SCALARPRODUCT_LOOP 8
-SCALARPRODUCT_LOOP 6
-SCALARPRODUCT_LOOP 4
-SCALARPRODUCT_LOOP 2
-SCALARPRODUCT_LOOP 0
-.end:
-    HADDD   m6, m0
-    movd   eax, m6
-    RET
diff -uparN ffmpeg-4.1/libavcodec/x86/lossless_videodsp.asm ffmpeg-y/libavcodec/x86/lossless_videodsp.asm
--- ffmpeg-4.1/libavcodec/x86/lossless_videodsp.asm	2018-11-03 08:17:29.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/lossless_videodsp.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,406 +0,0 @@
-;******************************************************************************
-;* SIMD lossless video DSP utils
-;* Copyright (c) 2008 Loren Merritt
-;* Copyright (c) 2014 Michael Niedermayer
-;* Copyright (c) 2017 Jokyo Images
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-
-cextern pb_15
-pb_zzzzzzzz77777777: times 8 db -1
-pb_7: times 8 db 7
-pb_ef: times 8 db 14,15
-pb_67: times 8 db  6, 7
-pb_zzzz3333zzzzbbbb: db -1,-1,-1,-1,3,3,3,3,-1,-1,-1,-1,11,11,11,11
-pb_zz11zz55zz99zzdd: db -1,-1,1,1,-1,-1,5,5,-1,-1,9,9,-1,-1,13,13
-pb_zzzz2323zzzzabab: db -1,-1,-1,-1, 2, 3, 2, 3,-1,-1,-1,-1,10,11,10,11
-pb_zzzzzzzz67676767: db -1,-1,-1,-1,-1,-1,-1,-1, 6, 7, 6, 7, 6, 7, 6, 7
-
-SECTION .text
-
-;------------------------------------------------------------------------------
-; void ff_add_median_pred_mmxext(uint8_t *dst, const uint8_t *top,
-;                                const uint8_t *diff, int w,
-;                                int *left, int *left_top)
-;------------------------------------------------------------------------------
-%macro MEDIAN_PRED 0
-cglobal add_median_pred, 6,6,8, dst, top, diff, w, left, left_top
-    movu    m0, [topq]
-    mova    m2, m0
-    movd    m4, [left_topq]
-    LSHIFT  m2, 1
-    mova    m1, m0
-    por     m4, m2
-    movd    m3, [leftq]
-    psubb   m0, m4 ; t-tl
-    add    dstq, wq
-    add    topq, wq
-    add   diffq, wq
-    neg      wq
-    jmp .skip
-.loop:
-    movu    m4, [topq+wq]
-    mova    m0, m4
-    LSHIFT  m4, 1
-    por     m4, m1
-    mova    m1, m0 ; t
-    psubb   m0, m4 ; t-tl
-.skip:
-    movu    m2, [diffq+wq]
-%assign i 0
-%rep mmsize
-    mova    m4, m0
-    paddb   m4, m3 ; t-tl+l
-    mova    m5, m3
-    pmaxub  m3, m1
-    pminub  m5, m1
-    pminub  m3, m4
-    pmaxub  m3, m5 ; median
-    paddb   m3, m2 ; +residual
-%if i==0
-    mova    m7, m3
-    LSHIFT  m7, mmsize-1
-%else
-    mova    m6, m3
-    RSHIFT  m7, 1
-    LSHIFT  m6, mmsize-1
-    por     m7, m6
-%endif
-%if i<mmsize-1
-    RSHIFT  m0, 1
-    RSHIFT  m1, 1
-    RSHIFT  m2, 1
-%endif
-%assign i i+1
-%endrep
-    movu [dstq+wq], m7
-    add      wq, mmsize
-    jl .loop
-    movzx   r2d, byte [dstq-1]
-    mov [leftq], r2d
-    movzx   r2d, byte [topq-1]
-    mov [left_topq], r2d
-    RET
-%endmacro
-
-%if ARCH_X86_32
-INIT_MMX mmxext
-MEDIAN_PRED
-%endif
-INIT_XMM sse2
-MEDIAN_PRED
-
-
-%macro ADD_LEFT_LOOP 2 ; %1 = dst_is_aligned, %2 = src_is_aligned
-    add     srcq, wq
-    add     dstq, wq
-    neg     wq
-%%.loop:
-    pshufb  xm0, xm5
-%if %2
-    mova    m1, [srcq+wq]
-%else
-    movu    m1, [srcq+wq]
-%endif
-    psllw   m2, m1, 8
-    paddb   m1, m2
-    pshufb  m2, m1, m3
-    paddb   m1, m2
-    pshufb  m2, m1, m4
-    paddb   m1, m2
-%if mmsize >= 16
-    pshufb  m2, m1, m6
-    paddb   m1, m2
-%endif
-    paddb   xm0, xm1
-%if %1
-    mova    [dstq+wq], xm0
-%else
-    movq    [dstq+wq], xm0
-    movhps  [dstq+wq+8], xm0
-%endif
-
-%if mmsize == 32
-    vextracti128    xm2, m1, 1 ; get second lane of the ymm
-    pshufb          xm0, xm5   ; set alls val to last val of the first lane
-    paddb           xm0, xm2
-;store val
-%if %1
-    mova    [dstq+wq+16], xm0
-%else;
-    movq    [dstq+wq+16], xm0
-    movhps  [dstq+wq+16+8], xm0
-%endif
-%endif
-    add     wq, mmsize
-    jl %%.loop
-%if mmsize == 32
-    movzx   eax, byte [dstq - 1]
-%else;
-    mov     eax, mmsize-1
-    sub     eax, wd
-    movd    m1, eax
-    pshufb  m0, m1
-    movd    eax, m0
-%endif
-    RET
-%endmacro
-
-;------------------------------------------------------------------------------
-; int ff_add_left_pred(uint8_t *dst, const uint8_t *src, int w, int left)
-;------------------------------------------------------------------------------
-INIT_MMX ssse3
-cglobal add_left_pred, 3,3,7, dst, src, w, left
-.skip_prologue:
-    mova    m5, [pb_7]
-    mova    m4, [pb_zzzz3333zzzzbbbb]
-    mova    m3, [pb_zz11zz55zz99zzdd]
-    movd    m0, leftm
-    psllq   m0, 56
-    ADD_LEFT_LOOP 1, 1
-
-%macro ADD_LEFT_PRED_UNALIGNED 0
-cglobal add_left_pred_unaligned, 3,3,7, dst, src, w, left
-    mova    xm5, [pb_15]
-    VBROADCASTI128    m6, [pb_zzzzzzzz77777777]
-    VBROADCASTI128    m4, [pb_zzzz3333zzzzbbbb]
-    VBROADCASTI128    m3, [pb_zz11zz55zz99zzdd]
-    movd    xm0, leftm
-    pslldq  xm0, 15
-    test    srcq, mmsize - 1
-    jnz .src_unaligned
-    test    dstq, mmsize - 1
-    jnz .dst_unaligned
-    ADD_LEFT_LOOP 1, 1
-.dst_unaligned:
-    ADD_LEFT_LOOP 0, 1
-.src_unaligned:
-    ADD_LEFT_LOOP 0, 0
-%endmacro
-
-INIT_XMM ssse3
-ADD_LEFT_PRED_UNALIGNED
-
-%if HAVE_AVX2_EXTERNAL
-INIT_YMM avx2
-ADD_LEFT_PRED_UNALIGNED
-%endif
-
-;------------------------------------------------------------------------------
-; void ff_add_bytes(uint8_t *dst, uint8_t *src, ptrdiff_t w);
-;------------------------------------------------------------------------------
-%macro ADD_BYTES 0
-cglobal add_bytes, 3,4,2, dst, src, w, size
-    mov  sizeq, wq
-    and  sizeq, -2*mmsize
-    jz  .2
-    add   dstq, sizeq
-    add   srcq, sizeq
-    neg  sizeq
-.1:
-    mova    m0, [srcq + sizeq]
-    mova    m1, [srcq + sizeq + mmsize]
-    paddb   m0, [dstq + sizeq]
-    paddb   m1, [dstq + sizeq + mmsize]
-    mova   [dstq + sizeq], m0
-    mova   [dstq + sizeq + mmsize], m1
-    add  sizeq, 2*mmsize
-    jl .1
-.2:
-    and     wq, 2*mmsize-1
-    jz    .end
-    add   dstq, wq
-    add   srcq, wq
-    neg     wq
-.3:
-    mov  sizeb, [srcq + wq]
-    add [dstq + wq], sizeb
-    inc     wq
-    jl .3
-.end:
-    REP_RET
-%endmacro
-
-%if ARCH_X86_32
-INIT_MMX mmx
-ADD_BYTES
-%endif
-INIT_XMM sse2
-ADD_BYTES
-
-%if HAVE_AVX2_EXTERNAL
-INIT_YMM avx2
-ADD_BYTES
-%endif
-
-%macro ADD_HFYU_LEFT_LOOP_INT16 2 ; %1 = dst alignment (a/u), %2 = src alignment (a/u)
-    add     wd, wd
-    add     srcq, wq
-    add     dstq, wq
-    neg     wq
-%%.loop:
-    mov%2   m1, [srcq+wq]
-    mova    m2, m1
-    pslld   m1, 16
-    paddw   m1, m2
-    mova    m2, m1
-
-    pshufb  m1, m3
-    paddw   m1, m2
-    pshufb  m0, m5
-%if mmsize == 16
-    mova    m2, m1
-    pshufb  m1, m4
-    paddw   m1, m2
-%endif
-    paddw   m0, m1
-    pand    m0, m7
-%ifidn %1, a
-    mova    [dstq+wq], m0
-%else
-    movq    [dstq+wq], m0
-    movhps  [dstq+wq+8], m0
-%endif
-    add     wq, mmsize
-    jl %%.loop
-    mov     eax, mmsize-1
-    sub     eax, wd
-    mov     wd, eax
-    shl     wd, 8
-    lea     eax, [wd+eax-1]
-    movd    m1, eax
-    pshufb  m0, m1
-    movd    eax, m0
-    RET
-%endmacro
-
-;---------------------------------------------------------------------------------------------
-; int add_left_pred_int16(uint16_t *dst, const uint16_t *src, unsigned mask, int w, int left)
-;---------------------------------------------------------------------------------------------
-INIT_MMX ssse3
-cglobal add_left_pred_int16, 4,4,8, dst, src, mask, w, left
-.skip_prologue:
-    mova    m5, [pb_67]
-    mova    m3, [pb_zzzz2323zzzzabab]
-    movd    m0, leftm
-    psllq   m0, 48
-    movd    m7, maskm
-    SPLATW  m7 ,m7
-    ADD_HFYU_LEFT_LOOP_INT16 a, a
-
-INIT_XMM ssse3
-cglobal add_left_pred_int16_unaligned, 4,4,8, dst, src, mask, w, left
-    mova    m5, [pb_ef]
-    mova    m4, [pb_zzzzzzzz67676767]
-    mova    m3, [pb_zzzz2323zzzzabab]
-    movd    m0, leftm
-    pslldq  m0, 14
-    movd    m7, maskm
-    SPLATW  m7 ,m7
-    test    srcq, 15
-    jnz .src_unaligned
-    test    dstq, 15
-    jnz .dst_unaligned
-    ADD_HFYU_LEFT_LOOP_INT16 a, a
-.dst_unaligned:
-    ADD_HFYU_LEFT_LOOP_INT16 u, a
-.src_unaligned:
-    ADD_HFYU_LEFT_LOOP_INT16 u, u
-
-
-;---------------------------------------------------------------------------------------------
-; void add_gradient_pred(uint8_t *src, const ptrdiff_t stride, const ptrdiff_t width)
-;---------------------------------------------------------------------------------------------
-%macro ADD_GRADIENT_PRED 0
-cglobal add_gradient_pred, 3,4,5, src, stride, width, tmp
-    mova         xm0, [pb_15]
-
-;load src - 1 in xm1
-    movd         xm1, [srcq-1]
-%if cpuflag(avx2)
-    vpbroadcastb xm1, xm1
-%else
-    pxor         xm2, xm2
-    pshufb       xm1, xm2
-%endif
-
-    add    srcq, widthq
-    neg  widthq
-    neg strideq
-
-.loop:
-    lea    tmpq, [srcq + strideq]
-    mova     m2, [tmpq + widthq] ; A = src[x-stride]
-    movu     m3, [tmpq + widthq - 1] ; B = src[x - (stride + 1)]
-    mova     m4, [srcq + widthq] ; current val (src[x])
-
-    psubb    m2, m3; A - B
-
-; prefix sum A-B
-    pslldq   m3, m2, 1
-    paddb    m2, m3
-    pslldq   m3, m2, 2
-    paddb    m2, m3
-    pslldq   m3, m2, 4
-    paddb    m2, m3
-    pslldq   m3, m2, 8
-    paddb    m2, m3
-
-; prefix sum current val
-    pslldq   m3, m4, 1
-    paddb    m4, m3
-    pslldq   m3, m4, 2
-    paddb    m4, m3
-    pslldq   m3, m4, 4
-    paddb    m4, m3
-    pslldq   m3, m4, 8
-    paddb    m4, m3
-
-; last sum
-    paddb                    m2, m4 ; current + (A - B)
-
-    paddb                   xm1, xm2 ; += C
-    mova        [srcq + widthq], xm1 ; store
-
-    pshufb                  xm1, xm0 ; put last val in all val of xm1
-
-%if mmsize == 32
-    vextracti128            xm2, m2, 1 ; get second lane of the ymm
-    paddb                   xm1, xm2; += C
-
-    mova   [srcq + widthq + 16], xm1 ; store
-    pshufb                  xm1, xm0 ; put last val in all val of m1
-%endif
-
-    add         widthq, mmsize
-    jl .loop
-    RET
-
-%endmacro
-
-INIT_XMM ssse3
-ADD_GRADIENT_PRED
-
-%if HAVE_AVX2_EXTERNAL
-INIT_YMM avx2
-ADD_GRADIENT_PRED
-%endif
diff -uparN ffmpeg-4.1/libavcodec/x86/lossless_videoencdsp.asm ffmpeg-y/libavcodec/x86/lossless_videoencdsp.asm
--- ffmpeg-4.1/libavcodec/x86/lossless_videoencdsp.asm	2018-11-03 08:17:29.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/lossless_videoencdsp.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,194 +0,0 @@
-;************************************************************************
-;* SIMD-optimized lossless video encoding functions
-;* Copyright (c) 2000, 2001 Fabrice Bellard
-;* Copyright (c) 2002-2004 Michael Niedermayer <michaelni@gmx.at>
-;*
-;* MMX optimization by Nick Kurshev <nickols_k@mail.ru>
-;* Conversion to NASM format by Tiancheng "Timothy" Gu <timothygu99@gmail.com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* 51, Inc., Foundation Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-cextern pb_80
-
-SECTION .text
-
-; void ff_diff_bytes(uint8_t *dst, const uint8_t *src1, const uint8_t *src2,
-;                    intptr_t w);
-%macro DIFF_BYTES_PROLOGUE 0
-%if ARCH_X86_32
-cglobal diff_bytes, 3,5,2, dst, src1, src2
-%define wq r4q
-    DECLARE_REG_TMP 3
-    mov               wq, r3mp
-%else
-cglobal diff_bytes, 4,5,2, dst, src1, src2, w
-    DECLARE_REG_TMP 4
-%endif ; ARCH_X86_32
-%define i t0q
-%endmacro
-
-; labels to jump to if w < regsize and w < 0
-%macro DIFF_BYTES_LOOP_PREP 2
-    mov                i, wq
-    and                i, -2 * regsize
-        js            %2
-        jz            %1
-    add             dstq, i
-    add            src1q, i
-    add            src2q, i
-    neg                i
-%endmacro
-
-; mov type used for src1q, dstq, first reg, second reg
-%macro DIFF_BYTES_LOOP_CORE 4
-%if mmsize != 16
-    mov%1             %3, [src1q + i]
-    mov%1             %4, [src1q + i + regsize]
-    psubb             %3, [src2q + i]
-    psubb             %4, [src2q + i + regsize]
-    mov%2           [dstq + i], %3
-    mov%2 [regsize + dstq + i], %4
-%else
-    ; SSE enforces alignment of psubb operand
-    mov%1             %3, [src1q + i]
-    movu              %4, [src2q + i]
-    psubb             %3, %4
-    mov%2     [dstq + i], %3
-    mov%1             %3, [src1q + i + regsize]
-    movu              %4, [src2q + i + regsize]
-    psubb             %3, %4
-    mov%2 [regsize + dstq + i], %3
-%endif
-%endmacro
-
-%macro DIFF_BYTES_BODY 2 ; mov type used for src1q, for dstq
-    %define regsize mmsize
-.loop_%1%2:
-    DIFF_BYTES_LOOP_CORE %1, %2, m0, m1
-    add                i, 2 * regsize
-        jl    .loop_%1%2
-.skip_main_%1%2:
-    and               wq, 2 * regsize - 1
-        jz     .end_%1%2
-%if mmsize > 16
-    ; fall back to narrower xmm
-    %define regsize (mmsize / 2)
-    DIFF_BYTES_LOOP_PREP .setup_loop_gpr_aa, .end_aa
-.loop2_%1%2:
-    DIFF_BYTES_LOOP_CORE %1, %2, xm0, xm1
-    add                i, 2 * regsize
-        jl   .loop2_%1%2
-.setup_loop_gpr_%1%2:
-    and               wq, 2 * regsize - 1
-        jz     .end_%1%2
-%endif
-    add             dstq, wq
-    add            src1q, wq
-    add            src2q, wq
-    neg               wq
-.loop_gpr_%1%2:
-    mov              t0b, [src1q + wq]
-    sub              t0b, [src2q + wq]
-    mov      [dstq + wq], t0b
-    inc               wq
-        jl .loop_gpr_%1%2
-.end_%1%2:
-    REP_RET
-%endmacro
-
-%if ARCH_X86_32
-INIT_MMX mmx
-DIFF_BYTES_PROLOGUE
-    %define regsize mmsize
-    DIFF_BYTES_LOOP_PREP .skip_main_aa, .end_aa
-    DIFF_BYTES_BODY    a, a
-%undef i
-%endif
-
-INIT_XMM sse2
-DIFF_BYTES_PROLOGUE
-    %define regsize mmsize
-    DIFF_BYTES_LOOP_PREP .skip_main_aa, .end_aa
-    test            dstq, regsize - 1
-        jnz     .loop_uu
-    test           src1q, regsize - 1
-        jnz     .loop_ua
-    DIFF_BYTES_BODY    a, a
-    DIFF_BYTES_BODY    u, a
-    DIFF_BYTES_BODY    u, u
-%undef i
-
-%if HAVE_AVX2_EXTERNAL
-INIT_YMM avx2
-DIFF_BYTES_PROLOGUE
-    %define regsize mmsize
-    ; Directly using unaligned SSE2 version is marginally faster than
-    ; branching based on arguments.
-    DIFF_BYTES_LOOP_PREP .skip_main_uu, .end_uu
-    test            dstq, regsize - 1
-        jnz     .loop_uu
-    test           src1q, regsize - 1
-        jnz     .loop_ua
-    DIFF_BYTES_BODY    a, a
-    DIFF_BYTES_BODY    u, a
-    DIFF_BYTES_BODY    u, u
-%undef i
-%endif
-
-
-;--------------------------------------------------------------------------------------------------
-;void sub_left_predict(uint8_t *dst, uint8_t *src, ptrdiff_t stride, ptrdiff_t width, int height)
-;--------------------------------------------------------------------------------------------------
-
-INIT_XMM avx
-cglobal sub_left_predict, 5,6,5, dst, src, stride, width, height, x
-    mova             m1, [pb_80] ; prev initial
-    add            dstq, widthq
-    add            srcq, widthq
-    lea              xd, [widthq-1]
-    neg          widthq
-    and              xd, 15
-    pinsrb           m4, m1, xd, 15
-    mov              xq, widthq
-
-    .loop:
-        movu                     m0, [srcq + widthq]
-        palignr                  m2, m0, m1, 15
-        movu                     m1, [srcq + widthq + 16]
-        palignr                  m3, m1, m0, 15
-        psubb                    m2, m0, m2
-        psubb                    m3, m1, m3
-        movu        [dstq + widthq], m2
-        movu   [dstq + widthq + 16], m3
-        add                  widthq, 2 * 16
-        jl .loop
-
-    add   srcq, strideq
-    sub   dstq, xq ; dst + width
-    test    xd, 16
-    jz .mod32
-    mova    m1, m0
-
-.mod32:
-    pshufb    m1, m4
-    mov   widthq, xq
-    dec  heightd
-    jg .loop
-    RET
diff -uparN ffmpeg-4.1/libavcodec/x86/mdct15.asm ffmpeg-y/libavcodec/x86/mdct15.asm
--- ffmpeg-4.1/libavcodec/x86/mdct15.asm	2018-11-06 07:22:26.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/mdct15.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,221 +0,0 @@
-;******************************************************************************
-;* SIMD optimized non-power-of-two MDCT functions
-;*
-;* Copyright (C) 2017 Rostislav Pehlivanov <atomnuker@gmail.com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA 32
-
-perm_neg: dd 2, 5, 3, 4, 6, 1, 7, 0
-perm_pos: dd 0, 7, 1, 6, 4, 3, 5, 2
-sign_adjust_r: times 4 dd 0x80000000, 0x00000000
-
-sign_adjust_5: dd 0x00000000, 0x80000000, 0x80000000, 0x00000000
-
-SECTION .text
-
-%if ARCH_X86_64
-
-;*****************************************************************************************
-;void ff_fft15_avx(FFTComplex *out, FFTComplex *in, FFTComplex *exptab, ptrdiff_t stride);
-;*****************************************************************************************
-%macro FFT5 3 ; %1 - in_offset, %2 - dst1 (64bit used), %3 - dst2
-    VBROADCASTSD m0, [inq + %1]         ; in[ 0].re, in[ 0].im, in[ 0].re, in[ 0].im
-    movsd   xm1, [inq + 1*16 +  8 + %1] ; in[ 3].re, in[ 3].im,         0,         0
-    movsd   xm4, [inq + 6*16 +  0 + %1] ; in[12].re, in[12].im,         0,         0
-    movhps  xm1, [inq + 3*16 +  0 + %1] ; in[ 3].re, in[ 3].im, in[ 6].re, in[ 6].im
-    movhps  xm4, [inq + 4*16 +  8 + %1] ; in[12].re, in[12].im, in[ 9].re, in[ 9].im
-
-    subps       xm2,  xm1, xm4          ; t[2].im, t[2].re, t[3].im, t[3].re
-    addps       xm1,  xm4               ; t[0].re, t[0].im, t[1].re, t[1].im
-
-    movhlps     %2,   xm1               ; t[0].re, t[1].re, t[0].im, t[1].im
-    addps       %2,   xm1
-    addps       %2,   xm0               ; DC[0].re, DC[0].im, junk...
-    movlhps     %2,   %2                ; DC[0].re, DC[0].im, DC[0].re, DC[0].im
-
-    shufps      xm3,  xm1, xm2, q0110   ; t[0].re, t[0].im, t[2].re, t[2].im
-    shufps      xm1,  xm2, q2332        ; t[1].re, t[1].im, t[3].re, t[3].im
-
-    mulps       xm%3, xm1, xm5
-    mulps       xm4,  xm3, xm6
-    mulps       xm1,  xm6
-
-    xorps       xm1,  xm7
-    mulps       xm3,  xm5
-    addsubps    xm3,  xm1               ; t[0].re, t[0].im, t[2].re, t[2].im
-    subps       xm%3, xm4               ; t[4].re, t[4].im, t[5].re, t[5].im
-
-    movhlps     xm2, xm%3, xm3          ; t[2].re, t[2].im, t[5].re, t[5].im
-    movlhps     xm3, xm%3               ; t[0].re, t[0].im, t[4].re, t[4].im
-
-    xorps       xm2,  xm7
-    addps       xm%3, xm2, xm3
-    subps       xm3,  xm2
-
-    shufps      xm3,  xm3, q1032
-    vinsertf128 m%3,  m%3, xm3, 1       ; All ACs (tmp[1] through to tmp[4])
-    addps       m%3,  m%3,  m0          ; Finally offset with DCs
-%endmacro
-
-%macro BUTTERFLIES_DC 1 ; %1 - exptab_offset
-    mulps xm0,  xm9, [exptabq + %1 + 16*0]
-    mulps xm1, xm10, [exptabq + %1 + 16*1]
-
-    haddps  xm0,  xm1
-    movhlps xm1,  xm0                   ; t[0].re, t[1].re, t[0].im, t[1].im
-
-    addps   xm0,  xm1
-    addps   xm0,  xm8
-
-    movsd [outq], xm0
-%endmacro
-
-%macro BUTTERFLIES_AC 1 ; %1 - exptab_offset
-    mulps  m0, m12, [exptabq + 64*0 + 0*mmsize + %1]
-    mulps  m1, m12, [exptabq + 64*0 + 1*mmsize + %1]
-    mulps  m2, m13, [exptabq + 64*1 + 0*mmsize + %1]
-    mulps  m3, m13, [exptabq + 64*1 + 1*mmsize + %1]
-
-    addps  m0, m0, m2
-    addps  m1, m1, m3
-    addps  m0, m0, m11
-
-    shufps m1, m1, m1, q2301
-    addps  m0, m0, m1
-
-    vextractf128 xm1, m0, 1
-
-    movlps [outq + strideq*1], xm0
-    movhps [outq + strideq*2], xm0
-    movlps [outq +  stride3q], xm1
-    movhps [outq + strideq*4], xm1
-%endmacro
-
-INIT_YMM avx
-cglobal fft15, 4, 5, 14, out, in, exptab, stride, stride5
-    shl strideq, 3
-
-    movaps xm5, [exptabq + 480 + 16*0]
-    movaps xm6, [exptabq + 480 + 16*1]
-    movaps xm7, [sign_adjust_5]
-
-    FFT5  0,  xm8, 11
-    FFT5  8,  xm9, 12
-    FFT5 16, xm10, 13
-
-%define stride3q inq
-    lea stride3q, [strideq + strideq*2]
-    lea stride5q, [strideq + strideq*4]
-
-    BUTTERFLIES_DC (8*6 + 4*0)*2*4
-    BUTTERFLIES_AC (8*0 + 0*0)*2*4
-
-    add outq, stride5q
-    BUTTERFLIES_DC (8*6 + 4*1)*2*4
-    BUTTERFLIES_AC (8*2 + 0*0)*2*4
-
-    add outq, stride5q
-    BUTTERFLIES_DC (8*6 + 4*2)*2*4
-    BUTTERFLIES_AC (8*4 + 0*0)*2*4
-
-    RET
-
-%endif ; ARCH_X86_64
-
-;*******************************************************************************************************
-;void ff_mdct15_postreindex(FFTComplex *out, FFTComplex *in, FFTComplex *exp, int *lut, ptrdiff_t len8);
-;*******************************************************************************************************
-%macro LUT_LOAD_4D 3
-    mov      r4d, [lutq + %3q*4 +  0]
-    movsd  xmm%1, [inq +  r4q*8]
-    mov      r4d, [lutq + %3q*4 +  4]
-    movhps xmm%1, [inq +  r4q*8]
-%if cpuflag(avx2)
-    mov      r4d, [lutq + %3q*4 +  8]
-    movsd     %2, [inq +  r4q*8]
-    mov      r4d, [lutq + %3q*4 + 12]
-    movhps    %2, [inq +  r4q*8]
-    vinsertf128 %1, %1, %2, 1
-%endif
-%endmacro
-
-%macro POSTROTATE_FN 1
-cglobal mdct15_postreindex, 5, 7, 8 + cpuflag(avx2)*2, out, in, exp, lut, len8, offset_p, offset_n
-
-    xor offset_nq, offset_nq
-    lea offset_pq, [len8q*2 - %1]
-
-    movaps m7,  [sign_adjust_r]
-
-%if cpuflag(avx2)
-    movaps   m8, [perm_pos]
-    movaps   m9, [perm_neg]
-%endif
-
-.loop:
-    movups m0, [expq + offset_pq*8]     ; exp[p0].re, exp[p0].im, exp[p1].re, exp[p1].im, exp[p2].re, exp[p2].im, exp[p3].re, exp[p3].im
-    movups m1, [expq + offset_nq*8]     ; exp[n3].re, exp[n3].im, exp[n2].re, exp[n2].im, exp[n1].re, exp[n1].im, exp[n0].re, exp[n0].im
-
-    LUT_LOAD_4D m3, xm4, offset_p       ; in[p0].re, in[p0].im, in[p1].re, in[p1].im, in[p2].re, in[p2].im, in[p3].re, in[p3].im
-    LUT_LOAD_4D m4, xm5, offset_n       ; in[n3].re, in[n3].im, in[n2].re, in[n2].im, in[n1].re, in[n1].im, in[n0].re, in[n0].im
-
-    mulps  m5, m3, m0                   ; in[p].reim * exp[p].reim
-    mulps  m6, m4, m1                   ; in[n].reim * exp[n].reim
-
-    xorps  m5, m7                       ; in[p].re *= -1, in[p].im *= 1
-    xorps  m6, m7                       ; in[n].re *= -1, in[n].im *= 1
-
-    shufps m3, m3, m3, q2301            ; in[p].imre
-    shufps m4, m4, m4, q2301            ; in[n].imre
-
-    mulps  m3, m0                       ; in[p].imre * exp[p].reim
-    mulps  m4, m1                       ; in[n].imre * exp[n].reim
-
-    haddps m3, m6                       ; out[n0].im, out[n1].im, out[n3].re, out[n2].re, out[n2].im, out[n3].im, out[n1].re, out[n0].re
-    haddps m5, m4                       ; out[p0].re, out[p1].re, out[p3].im, out[p2].im, out[p2].re, out[p3].re, out[p1].im, out[p0].im
-
-%if cpuflag(avx2)
-    vpermps m3, m9, m3                  ; out[n3].im, out[n3].re, out[n2].im, out[n2].re, out[n1].im, out[n1].re, out[n0].im, out[n0].re
-    vpermps m5, m8, m5                  ; out[p0].re, out[p0].im, out[p1].re, out[p1].im, out[p2].re, out[p2].im, out[p3].re, out[p3].im
-%else
-    shufps m3, m3, m3, q0312
-    shufps m5, m5, m5, q2130
-%endif
-
-    movups [outq + offset_nq*8], m3
-    movups [outq + offset_pq*8], m5
-
-    sub offset_pq, %1
-    add offset_nq, %1
-    cmp offset_nq, offset_pq
-    jle .loop
-
-    REP_RET
-%endmacro
-
-INIT_XMM sse3
-POSTROTATE_FN 2
-
-%if ARCH_X86_64 && HAVE_AVX2_EXTERNAL
-INIT_YMM avx2
-POSTROTATE_FN 4
-%endif
diff -uparN ffmpeg-4.1/libavcodec/x86/me_cmp.asm ffmpeg-y/libavcodec/x86/me_cmp.asm
--- ffmpeg-4.1/libavcodec/x86/me_cmp.asm	2018-01-01 06:35:48.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/me_cmp.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,934 +0,0 @@
-;*****************************************************************************
-;* SIMD-optimized motion compensation estimation
-;*****************************************************************************
-;* Copyright (c) 2000, 2001 Fabrice Bellard
-;* Copyright (c) 2002-2004 Michael Niedermayer <michaelni@gmx.at>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;*****************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-
-cextern pb_1
-cextern pb_80
-
-SECTION .text
-
-%macro DIFF_PIXELS_1 4
-    movh            %1, %3
-    movh            %2, %4
-    punpcklbw       %2, %1
-    punpcklbw       %1, %1
-    psubw           %1, %2
-%endmacro
-
-; %1=uint8_t *pix1, %2=uint8_t *pix2, %3=static offset, %4=stride, %5=stride*3
-; %6=temporary storage location
-; this macro requires $mmsize stack space (aligned) on %6 (except on SSE+x86-64)
-%macro DIFF_PIXELS_8 6
-    DIFF_PIXELS_1   m0, m7, [%1     +%3], [%2     +%3]
-    DIFF_PIXELS_1   m1, m7, [%1+%4  +%3], [%2+%4  +%3]
-    DIFF_PIXELS_1   m2, m7, [%1+%4*2+%3], [%2+%4*2+%3]
-    add             %1, %5
-    add             %2, %5
-    DIFF_PIXELS_1   m3, m7, [%1     +%3], [%2     +%3]
-    DIFF_PIXELS_1   m4, m7, [%1+%4  +%3], [%2+%4  +%3]
-    DIFF_PIXELS_1   m5, m7, [%1+%4*2+%3], [%2+%4*2+%3]
-    DIFF_PIXELS_1   m6, m7, [%1+%5  +%3], [%2+%5  +%3]
-%ifdef m8
-    DIFF_PIXELS_1   m7, m8, [%1+%4*4+%3], [%2+%4*4+%3]
-%else
-    mova          [%6], m0
-    DIFF_PIXELS_1   m7, m0, [%1+%4*4+%3], [%2+%4*4+%3]
-    mova            m0, [%6]
-%endif
-    sub             %1, %5
-    sub             %2, %5
-%endmacro
-
-%macro HADAMARD8 0
-    SUMSUB_BADC       w, 0, 1, 2, 3
-    SUMSUB_BADC       w, 4, 5, 6, 7
-    SUMSUB_BADC       w, 0, 2, 1, 3
-    SUMSUB_BADC       w, 4, 6, 5, 7
-    SUMSUB_BADC       w, 0, 4, 1, 5
-    SUMSUB_BADC       w, 2, 6, 3, 7
-%endmacro
-
-%macro ABS1_SUM 3
-    ABS1            %1, %2
-    paddusw         %3, %1
-%endmacro
-
-%macro ABS2_SUM 6
-    ABS2            %1, %2, %3, %4
-    paddusw         %5, %1
-    paddusw         %6, %2
-%endmacro
-
-%macro ABS_SUM_8x8_64 1
-    ABS2            m0, m1, m8, m9
-    ABS2_SUM        m2, m3, m8, m9, m0, m1
-    ABS2_SUM        m4, m5, m8, m9, m0, m1
-    ABS2_SUM        m6, m7, m8, m9, m0, m1
-    paddusw         m0, m1
-%endmacro
-
-%macro ABS_SUM_8x8_32 1
-    mova          [%1], m7
-    ABS1            m0, m7
-    ABS1            m1, m7
-    ABS1_SUM        m2, m7, m0
-    ABS1_SUM        m3, m7, m1
-    ABS1_SUM        m4, m7, m0
-    ABS1_SUM        m5, m7, m1
-    ABS1_SUM        m6, m7, m0
-    mova            m2, [%1]
-    ABS1_SUM        m2, m7, m1
-    paddusw         m0, m1
-%endmacro
-
-; FIXME: HSUM saturates at 64k, while an 8x8 hadamard or dct block can get up to
-; about 100k on extreme inputs. But that's very unlikely to occur in natural video,
-; and it's even more unlikely to not have any alternative mvs/modes with lower cost.
-%macro HSUM 3
-%if cpuflag(sse2)
-    movhlps         %2, %1
-    paddusw         %1, %2
-    pshuflw         %2, %1, 0xE
-    paddusw         %1, %2
-    pshuflw         %2, %1, 0x1
-    paddusw         %1, %2
-    movd            %3, %1
-%elif cpuflag(mmxext)
-    pshufw          %2, %1, 0xE
-    paddusw         %1, %2
-    pshufw          %2, %1, 0x1
-    paddusw         %1, %2
-    movd            %3, %1
-%elif cpuflag(mmx)
-    mova            %2, %1
-    psrlq           %1, 32
-    paddusw         %1, %2
-    mova            %2, %1
-    psrlq           %1, 16
-    paddusw         %1, %2
-    movd            %3, %1
-%endif
-%endmacro
-
-%macro STORE4 5
-    mova [%1+mmsize*0], %2
-    mova [%1+mmsize*1], %3
-    mova [%1+mmsize*2], %4
-    mova [%1+mmsize*3], %5
-%endmacro
-
-%macro LOAD4 5
-    mova            %2, [%1+mmsize*0]
-    mova            %3, [%1+mmsize*1]
-    mova            %4, [%1+mmsize*2]
-    mova            %5, [%1+mmsize*3]
-%endmacro
-
-%macro hadamard8_16_wrapper 2
-cglobal hadamard8_diff, 4, 4, %1
-%ifndef m8
-    %assign pad %2*mmsize-(4+stack_offset&(mmsize-1))
-    SUB            rsp, pad
-%endif
-    call hadamard8x8_diff %+ SUFFIX
-%ifndef m8
-    ADD            rsp, pad
-%endif
-    RET
-
-cglobal hadamard8_diff16, 5, 6, %1
-%ifndef m8
-    %assign pad %2*mmsize-(4+stack_offset&(mmsize-1))
-    SUB            rsp, pad
-%endif
-
-    call hadamard8x8_diff %+ SUFFIX
-    mov            r5d, eax
-
-    add             r1, 8
-    add             r2, 8
-    call hadamard8x8_diff %+ SUFFIX
-    add            r5d, eax
-
-    cmp            r4d, 16
-    jne .done
-
-    lea             r1, [r1+r3*8-8]
-    lea             r2, [r2+r3*8-8]
-    call hadamard8x8_diff %+ SUFFIX
-    add            r5d, eax
-
-    add             r1, 8
-    add             r2, 8
-    call hadamard8x8_diff %+ SUFFIX
-    add            r5d, eax
-
-.done:
-    mov            eax, r5d
-%ifndef m8
-    ADD            rsp, pad
-%endif
-    RET
-%endmacro
-
-%macro HADAMARD8_DIFF 0-1
-%if cpuflag(sse2)
-hadamard8x8_diff %+ SUFFIX:
-    lea                          r0, [r3*3]
-    DIFF_PIXELS_8                r1, r2,  0, r3, r0, rsp+gprsize
-    HADAMARD8
-%if ARCH_X86_64
-    TRANSPOSE8x8W                 0,  1,  2,  3,  4,  5,  6,  7,  8
-%else
-    TRANSPOSE8x8W                 0,  1,  2,  3,  4,  5,  6,  7, [rsp+gprsize], [rsp+mmsize+gprsize]
-%endif
-    HADAMARD8
-    ABS_SUM_8x8         rsp+gprsize
-    HSUM                        m0, m1, eax
-    and                         eax, 0xFFFF
-    ret
-
-hadamard8_16_wrapper %1, 3
-%elif cpuflag(mmx)
-ALIGN 16
-; int ff_hadamard8_diff_ ## cpu(MpegEncContext *s, uint8_t *src1,
-;                               uint8_t *src2, ptrdiff_t stride, int h)
-; r0 = void *s = unused, int h = unused (always 8)
-; note how r1, r2 and r3 are not clobbered in this function, so 16x16
-; can simply call this 2x2x (and that's why we access rsp+gprsize
-; everywhere, which is rsp of calling func
-hadamard8x8_diff %+ SUFFIX:
-    lea                          r0, [r3*3]
-
-    ; first 4x8 pixels
-    DIFF_PIXELS_8                r1, r2,  0, r3, r0, rsp+gprsize+0x60
-    HADAMARD8
-    mova         [rsp+gprsize+0x60], m7
-    TRANSPOSE4x4W                 0,  1,  2,  3,  7
-    STORE4              rsp+gprsize, m0, m1, m2, m3
-    mova                         m7, [rsp+gprsize+0x60]
-    TRANSPOSE4x4W                 4,  5,  6,  7,  0
-    STORE4         rsp+gprsize+0x40, m4, m5, m6, m7
-
-    ; second 4x8 pixels
-    DIFF_PIXELS_8                r1, r2,  4, r3, r0, rsp+gprsize+0x60
-    HADAMARD8
-    mova         [rsp+gprsize+0x60], m7
-    TRANSPOSE4x4W                 0,  1,  2,  3,  7
-    STORE4         rsp+gprsize+0x20, m0, m1, m2, m3
-    mova                         m7, [rsp+gprsize+0x60]
-    TRANSPOSE4x4W                 4,  5,  6,  7,  0
-
-    LOAD4          rsp+gprsize+0x40, m0, m1, m2, m3
-    HADAMARD8
-    ABS_SUM_8x8_32 rsp+gprsize+0x60
-    mova         [rsp+gprsize+0x60], m0
-
-    LOAD4          rsp+gprsize     , m0, m1, m2, m3
-    LOAD4          rsp+gprsize+0x20, m4, m5, m6, m7
-    HADAMARD8
-    ABS_SUM_8x8_32 rsp+gprsize
-    paddusw                      m0, [rsp+gprsize+0x60]
-
-    HSUM                         m0, m1, eax
-    and                         rax, 0xFFFF
-    ret
-
-hadamard8_16_wrapper 0, 14
-%endif
-%endmacro
-
-INIT_MMX mmx
-HADAMARD8_DIFF
-
-INIT_MMX mmxext
-HADAMARD8_DIFF
-
-INIT_XMM sse2
-%if ARCH_X86_64
-%define ABS_SUM_8x8 ABS_SUM_8x8_64
-%else
-%define ABS_SUM_8x8 ABS_SUM_8x8_32
-%endif
-HADAMARD8_DIFF 10
-
-INIT_XMM ssse3
-%define ABS_SUM_8x8 ABS_SUM_8x8_64
-HADAMARD8_DIFF 9
-
-; int ff_sse*_*(MpegEncContext *v, uint8_t *pix1, uint8_t *pix2,
-;               ptrdiff_t line_size, int h)
-
-%macro SUM_SQUARED_ERRORS 1
-cglobal sse%1, 5,5,8, v, pix1, pix2, lsize, h
-%if %1 == mmsize
-    shr       hd, 1
-%endif
-    pxor      m0, m0         ; mm0 = 0
-    pxor      m7, m7         ; mm7 holds the sum
-
-.next2lines: ; FIXME why are these unaligned movs? pix1[] is aligned
-    movu      m1, [pix1q]    ; m1 = pix1[0][0-15], [0-7] for mmx
-    movu      m2, [pix2q]    ; m2 = pix2[0][0-15], [0-7] for mmx
-%if %1 == mmsize
-    movu      m3, [pix1q+lsizeq] ; m3 = pix1[1][0-15], [0-7] for mmx
-    movu      m4, [pix2q+lsizeq] ; m4 = pix2[1][0-15], [0-7] for mmx
-%else  ; %1 / 2 == mmsize; mmx only
-    mova      m3, [pix1q+8]  ; m3 = pix1[0][8-15]
-    mova      m4, [pix2q+8]  ; m4 = pix2[0][8-15]
-%endif
-
-    ; todo: mm1-mm2, mm3-mm4
-    ; algo: subtract mm1 from mm2 with saturation and vice versa
-    ;       OR the result to get the absolute difference
-    mova      m5, m1
-    mova      m6, m3
-    psubusb   m1, m2
-    psubusb   m3, m4
-    psubusb   m2, m5
-    psubusb   m4, m6
-
-    por       m2, m1
-    por       m4, m3
-
-    ; now convert to 16-bit vectors so we can square them
-    mova      m1, m2
-    mova      m3, m4
-
-    punpckhbw m2, m0
-    punpckhbw m4, m0
-    punpcklbw m1, m0         ; mm1 not spread over (mm1,mm2)
-    punpcklbw m3, m0         ; mm4 not spread over (mm3,mm4)
-
-    pmaddwd   m2, m2
-    pmaddwd   m4, m4
-    pmaddwd   m1, m1
-    pmaddwd   m3, m3
-
-    paddd     m1, m2
-    paddd     m3, m4
-    paddd     m7, m1
-    paddd     m7, m3
-
-%if %1 == mmsize
-    lea    pix1q, [pix1q + 2*lsizeq]
-    lea    pix2q, [pix2q + 2*lsizeq]
-%else
-    add    pix1q, lsizeq
-    add    pix2q, lsizeq
-%endif
-    dec       hd
-    jnz .next2lines
-
-    HADDD     m7, m1
-    movd     eax, m7         ; return value
-    RET
-%endmacro
-
-INIT_MMX mmx
-SUM_SQUARED_ERRORS 8
-
-INIT_MMX mmx
-SUM_SQUARED_ERRORS 16
-
-INIT_XMM sse2
-SUM_SQUARED_ERRORS 16
-
-;-----------------------------------------------
-;int ff_sum_abs_dctelem(int16_t *block)
-;-----------------------------------------------
-; %1 = number of xmm registers used
-; %2 = number of inline loops
-
-%macro SUM_ABS_DCTELEM 2
-cglobal sum_abs_dctelem, 1, 1, %1, block
-    pxor    m0, m0
-    pxor    m1, m1
-%assign %%i 0
-%rep %2
-    mova      m2, [blockq+mmsize*(0+%%i)]
-    mova      m3, [blockq+mmsize*(1+%%i)]
-    mova      m4, [blockq+mmsize*(2+%%i)]
-    mova      m5, [blockq+mmsize*(3+%%i)]
-    ABS1_SUM  m2, m6, m0
-    ABS1_SUM  m3, m6, m1
-    ABS1_SUM  m4, m6, m0
-    ABS1_SUM  m5, m6, m1
-%assign %%i %%i+4
-%endrep
-    paddusw m0, m1
-    HSUM    m0, m1, eax
-    and     eax, 0xFFFF
-    RET
-%endmacro
-
-INIT_MMX mmx
-SUM_ABS_DCTELEM 0, 4
-INIT_MMX mmxext
-SUM_ABS_DCTELEM 0, 4
-INIT_XMM sse2
-SUM_ABS_DCTELEM 7, 2
-INIT_XMM ssse3
-SUM_ABS_DCTELEM 6, 2
-
-;------------------------------------------------------------------------------
-; int ff_hf_noise*_mmx(uint8_t *pix1, ptrdiff_t lsize, int h)
-;------------------------------------------------------------------------------
-; %1 = 8/16. %2-5=m#
-%macro HF_NOISE_PART1 5
-    mova      m%2, [pix1q]
-%if %1 == 8
-    mova      m%3, m%2
-    psllq     m%2, 8
-    psrlq     m%3, 8
-    psrlq     m%2, 8
-%else
-    mova      m%3, [pix1q+1]
-%endif
-    mova      m%4, m%2
-    mova      m%5, m%3
-    punpcklbw m%2, m7
-    punpcklbw m%3, m7
-    punpckhbw m%4, m7
-    punpckhbw m%5, m7
-    psubw     m%2, m%3
-    psubw     m%4, m%5
-%endmacro
-
-; %1-2 = m#
-%macro HF_NOISE_PART2 4
-    psubw     m%1, m%3
-    psubw     m%2, m%4
-    pxor       m3, m3
-    pxor       m1, m1
-    pcmpgtw    m3, m%1
-    pcmpgtw    m1, m%2
-    pxor      m%1, m3
-    pxor      m%2, m1
-    psubw     m%1, m3
-    psubw     m%2, m1
-    paddw     m%2, m%1
-    paddw      m6, m%2
-%endmacro
-
-; %1 = 8/16
-%macro HF_NOISE 1
-cglobal hf_noise%1, 3,3,0, pix1, lsize, h
-    sub        hd, 2
-    pxor       m7, m7
-    pxor       m6, m6
-    HF_NOISE_PART1 %1, 0, 1, 2, 3
-    add     pix1q, lsizeq
-    HF_NOISE_PART1 %1, 4, 1, 5, 3
-    HF_NOISE_PART2     0, 2, 4, 5
-    add     pix1q, lsizeq
-.loop:
-    HF_NOISE_PART1 %1, 0, 1, 2, 3
-    HF_NOISE_PART2     4, 5, 0, 2
-    add     pix1q, lsizeq
-    HF_NOISE_PART1 %1, 4, 1, 5, 3
-    HF_NOISE_PART2     0, 2, 4, 5
-    add     pix1q, lsizeq
-    sub        hd, 2
-        jne .loop
-
-    mova       m0, m6
-    punpcklwd  m0, m7
-    punpckhwd  m6, m7
-    paddd      m6, m0
-    mova       m0, m6
-    psrlq      m6, 32
-    paddd      m0, m6
-    movd      eax, m0   ; eax = result of hf_noise8;
-    REP_RET                 ; return eax;
-%endmacro
-
-INIT_MMX mmx
-HF_NOISE 8
-HF_NOISE 16
-
-;---------------------------------------------------------------------------------------
-;int ff_sad_<opt>(MpegEncContext *v, uint8_t *pix1, uint8_t *pix2, ptrdiff_t stride, int h);
-;---------------------------------------------------------------------------------------
-;%1 = 8/16
-%macro SAD 1
-cglobal sad%1, 5, 5, 3, v, pix1, pix2, stride, h
-    movu      m2, [pix2q]
-    movu      m1, [pix2q+strideq]
-    psadbw    m2, [pix1q]
-    psadbw    m1, [pix1q+strideq]
-    paddw     m2, m1
-%if %1 != mmsize
-    movu      m0, [pix2q+8]
-    movu      m1, [pix2q+strideq+8]
-    psadbw    m0, [pix1q+8]
-    psadbw    m1, [pix1q+strideq+8]
-    paddw     m2, m0
-    paddw     m2, m1
-%endif
-    sub       hd, 2
-
-align 16
-.loop:
-    lea    pix1q, [pix1q+strideq*2]
-    lea    pix2q, [pix2q+strideq*2]
-    movu      m0, [pix2q]
-    movu      m1, [pix2q+strideq]
-    psadbw    m0, [pix1q]
-    psadbw    m1, [pix1q+strideq]
-    paddw     m2, m0
-    paddw     m2, m1
-%if %1 != mmsize
-    movu      m0, [pix2q+8]
-    movu      m1, [pix2q+strideq+8]
-    psadbw    m0, [pix1q+8]
-    psadbw    m1, [pix1q+strideq+8]
-    paddw     m2, m0
-    paddw     m2, m1
-%endif
-    sub       hd, 2
-    jg .loop
-%if mmsize == 16
-    movhlps   m0, m2
-    paddw     m2, m0
-%endif
-    movd     eax, m2
-    RET
-%endmacro
-
-INIT_MMX mmxext
-SAD 8
-SAD 16
-INIT_XMM sse2
-SAD 16
-
-;------------------------------------------------------------------------------------------
-;int ff_sad_x2_<opt>(MpegEncContext *v, uint8_t *pix1, uint8_t *pix2, ptrdiff_t stride, int h);
-;------------------------------------------------------------------------------------------
-;%1 = 8/16
-%macro SAD_X2 1
-cglobal sad%1_x2, 5, 5, 5, v, pix1, pix2, stride, h
-    movu      m0, [pix2q]
-    movu      m2, [pix2q+strideq]
-%if mmsize == 16
-    movu      m3, [pix2q+1]
-    movu      m4, [pix2q+strideq+1]
-    pavgb     m0, m3
-    pavgb     m2, m4
-%else
-    pavgb     m0, [pix2q+1]
-    pavgb     m2, [pix2q+strideq+1]
-%endif
-    psadbw    m0, [pix1q]
-    psadbw    m2, [pix1q+strideq]
-    paddw     m0, m2
-%if %1 != mmsize
-    movu      m1, [pix2q+8]
-    movu      m2, [pix2q+strideq+8]
-    pavgb     m1, [pix2q+9]
-    pavgb     m2, [pix2q+strideq+9]
-    psadbw    m1, [pix1q+8]
-    psadbw    m2, [pix1q+strideq+8]
-    paddw     m0, m1
-    paddw     m0, m2
-%endif
-    sub       hd, 2
-
-align 16
-.loop:
-    lea    pix1q, [pix1q+2*strideq]
-    lea    pix2q, [pix2q+2*strideq]
-    movu      m1, [pix2q]
-    movu      m2, [pix2q+strideq]
-%if mmsize == 16
-    movu      m3, [pix2q+1]
-    movu      m4, [pix2q+strideq+1]
-    pavgb     m1, m3
-    pavgb     m2, m4
-%else
-    pavgb     m1, [pix2q+1]
-    pavgb     m2, [pix2q+strideq+1]
-%endif
-    psadbw    m1, [pix1q]
-    psadbw    m2, [pix1q+strideq]
-    paddw     m0, m1
-    paddw     m0, m2
-%if %1 != mmsize
-    movu      m1, [pix2q+8]
-    movu      m2, [pix2q+strideq+8]
-    pavgb     m1, [pix2q+9]
-    pavgb     m2, [pix2q+strideq+9]
-    psadbw    m1, [pix1q+8]
-    psadbw    m2, [pix1q+strideq+8]
-    paddw     m0, m1
-    paddw     m0, m2
-%endif
-    sub       hd, 2
-    jg .loop
-%if mmsize == 16
-    movhlps   m1, m0
-    paddw     m0, m1
-%endif
-    movd     eax, m0
-    RET
-%endmacro
-
-INIT_MMX mmxext
-SAD_X2 8
-SAD_X2 16
-INIT_XMM sse2
-SAD_X2 16
-
-;------------------------------------------------------------------------------------------
-;int ff_sad_y2_<opt>(MpegEncContext *v, uint8_t *pix1, uint8_t *pix2, ptrdiff_t stride, int h);
-;------------------------------------------------------------------------------------------
-;%1 = 8/16
-%macro SAD_Y2 1
-cglobal sad%1_y2, 5, 5, 4, v, pix1, pix2, stride, h
-    movu      m1, [pix2q]
-    movu      m0, [pix2q+strideq]
-    movu      m3, [pix2q+2*strideq]
-    pavgb     m1, m0
-    pavgb     m0, m3
-    psadbw    m1, [pix1q]
-    psadbw    m0, [pix1q+strideq]
-    paddw     m0, m1
-    mova      m1, m3
-%if %1 != mmsize
-    movu      m4, [pix2q+8]
-    movu      m5, [pix2q+strideq+8]
-    movu      m6, [pix2q+2*strideq+8]
-    pavgb     m4, m5
-    pavgb     m5, m6
-    psadbw    m4, [pix1q+8]
-    psadbw    m5, [pix1q+strideq+8]
-    paddw     m0, m4
-    paddw     m0, m5
-    mova      m4, m6
-%endif
-    add    pix2q, strideq
-    sub       hd, 2
-
-align 16
-.loop:
-    lea    pix1q, [pix1q+2*strideq]
-    lea    pix2q, [pix2q+2*strideq]
-    movu      m2, [pix2q]
-    movu      m3, [pix2q+strideq]
-    pavgb     m1, m2
-    pavgb     m2, m3
-    psadbw    m1, [pix1q]
-    psadbw    m2, [pix1q+strideq]
-    paddw     m0, m1
-    paddw     m0, m2
-    mova      m1, m3
-%if %1 != mmsize
-    movu      m5, [pix2q+8]
-    movu      m6, [pix2q+strideq+8]
-    pavgb     m4, m5
-    pavgb     m5, m6
-    psadbw    m4, [pix1q+8]
-    psadbw    m5, [pix1q+strideq+8]
-    paddw     m0, m4
-    paddw     m0, m5
-    mova      m4, m6
-%endif
-    sub       hd, 2
-    jg .loop
-%if mmsize == 16
-    movhlps   m1, m0
-    paddw     m0, m1
-%endif
-    movd     eax, m0
-    RET
-%endmacro
-
-INIT_MMX mmxext
-SAD_Y2 8
-SAD_Y2 16
-INIT_XMM sse2
-SAD_Y2 16
-
-;-------------------------------------------------------------------------------------------
-;int ff_sad_approx_xy2_<opt>(MpegEncContext *v, uint8_t *pix1, uint8_t *pix2, ptrdiff_t stride, int h);
-;-------------------------------------------------------------------------------------------
-;%1 = 8/16
-%macro SAD_APPROX_XY2 1
-cglobal sad%1_approx_xy2, 5, 5, 7, v, pix1, pix2, stride, h
-    mova      m4, [pb_1]
-    movu      m1, [pix2q]
-    movu      m0, [pix2q+strideq]
-    movu      m3, [pix2q+2*strideq]
-%if mmsize == 16
-    movu      m5, [pix2q+1]
-    movu      m6, [pix2q+strideq+1]
-    movu      m2, [pix2q+2*strideq+1]
-    pavgb     m1, m5
-    pavgb     m0, m6
-    pavgb     m3, m2
-%else
-    pavgb     m1, [pix2q+1]
-    pavgb     m0, [pix2q+strideq+1]
-    pavgb     m3, [pix2q+2*strideq+1]
-%endif
-    psubusb   m0, m4
-    pavgb     m1, m0
-    pavgb     m0, m3
-    psadbw    m1, [pix1q]
-    psadbw    m0, [pix1q+strideq]
-    paddw     m0, m1
-    mova      m1, m3
-%if %1 != mmsize
-    movu      m5, [pix2q+8]
-    movu      m6, [pix2q+strideq+8]
-    movu      m7, [pix2q+2*strideq+8]
-    pavgb     m5, [pix2q+1+8]
-    pavgb     m6, [pix2q+strideq+1+8]
-    pavgb     m7, [pix2q+2*strideq+1+8]
-    psubusb   m6, m4
-    pavgb     m5, m6
-    pavgb     m6, m7
-    psadbw    m5, [pix1q+8]
-    psadbw    m6, [pix1q+strideq+8]
-    paddw     m0, m5
-    paddw     m0, m6
-    mova      m5, m7
-%endif
-    add    pix2q, strideq
-    sub       hd, 2
-
-align 16
-.loop:
-    lea    pix1q, [pix1q+2*strideq]
-    lea    pix2q, [pix2q+2*strideq]
-    movu      m2, [pix2q]
-    movu      m3, [pix2q+strideq]
-%if mmsize == 16
-    movu      m5, [pix2q+1]
-    movu      m6, [pix2q+strideq+1]
-    pavgb     m2, m5
-    pavgb     m3, m6
-%else
-    pavgb     m2, [pix2q+1]
-    pavgb     m3, [pix2q+strideq+1]
-%endif
-    psubusb   m2, m4
-    pavgb     m1, m2
-    pavgb     m2, m3
-    psadbw    m1, [pix1q]
-    psadbw    m2, [pix1q+strideq]
-    paddw     m0, m1
-    paddw     m0, m2
-    mova      m1, m3
-%if %1 != mmsize
-    movu      m6, [pix2q+8]
-    movu      m7, [pix2q+strideq+8]
-    pavgb     m6, [pix2q+8+1]
-    pavgb     m7, [pix2q+strideq+8+1]
-    psubusb   m6, m4
-    pavgb     m5, m6
-    pavgb     m6, m7
-    psadbw    m5, [pix1q+8]
-    psadbw    m6, [pix1q+strideq+8]
-    paddw     m0, m5
-    paddw     m0, m6
-    mova      m5, m7
-%endif
-    sub       hd, 2
-    jg .loop
-%if mmsize == 16
-    movhlps   m1, m0
-    paddw     m0, m1
-%endif
-    movd     eax, m0
-    RET
-%endmacro
-
-INIT_MMX mmxext
-SAD_APPROX_XY2 8
-SAD_APPROX_XY2 16
-INIT_XMM sse2
-SAD_APPROX_XY2 16
-
-;--------------------------------------------------------------------
-;int ff_vsad_intra(MpegEncContext *v, uint8_t *pix1, uint8_t *pix2,
-;                  ptrdiff_t line_size, int h);
-;--------------------------------------------------------------------
-; %1 = 8/16
-%macro VSAD_INTRA 1
-cglobal vsad_intra%1, 5, 5, 3, v, pix1, pix2, lsize, h
-    mova      m0, [pix1q]
-%if %1 == mmsize
-    mova      m2, [pix1q+lsizeq]
-    psadbw    m0, m2
-%else
-    mova      m2, [pix1q+lsizeq]
-    mova      m3, [pix1q+8]
-    mova      m4, [pix1q+lsizeq+8]
-    psadbw    m0, m2
-    psadbw    m3, m4
-    paddw     m0, m3
-%endif
-    sub       hd, 2
-
-.loop:
-    lea    pix1q, [pix1q + 2*lsizeq]
-%if %1 == mmsize
-    mova      m1, [pix1q]
-    psadbw    m2, m1
-    paddw     m0, m2
-    mova      m2, [pix1q+lsizeq]
-    psadbw    m1, m2
-    paddw     m0, m1
-%else
-    mova      m1, [pix1q]
-    mova      m3, [pix1q+8]
-    psadbw    m2, m1
-    psadbw    m4, m3
-    paddw     m0, m2
-    paddw     m0, m4
-    mova      m2, [pix1q+lsizeq]
-    mova      m4, [pix1q+lsizeq+8]
-    psadbw    m1, m2
-    psadbw    m3, m4
-    paddw     m0, m1
-    paddw     m0, m3
-%endif
-    sub       hd, 2
-    jg     .loop
-
-%if mmsize == 16
-    pshufd m1, m0, 0xe
-    paddd  m0, m1
-%endif
-    movd eax, m0
-    RET
-%endmacro
-
-INIT_MMX mmxext
-VSAD_INTRA 8
-VSAD_INTRA 16
-INIT_XMM sse2
-VSAD_INTRA 16
-
-;---------------------------------------------------------------------
-;int ff_vsad_approx(MpegEncContext *v, uint8_t *pix1, uint8_t *pix2,
-;                   ptrdiff_t line_size, int h);
-;---------------------------------------------------------------------
-; %1 = 8/16
-%macro VSAD_APPROX 1
-cglobal vsad%1_approx, 5, 5, 5, v, pix1, pix2, lsize, h
-    mova   m1, [pb_80]
-    mova   m0, [pix1q]
-%if %1 == mmsize ; vsad8_mmxext, vsad16_sse2
-    mova   m4, [pix1q+lsizeq]
-%if mmsize == 16
-    movu   m3, [pix2q]
-    movu   m2, [pix2q+lsizeq]
-    psubb  m0, m3
-    psubb  m4, m2
-%else
-    psubb  m0, [pix2q]
-    psubb  m4, [pix2q+lsizeq]
-%endif
-    pxor   m0, m1
-    pxor   m4, m1
-    psadbw m0, m4
-%else ; vsad16_mmxext
-    mova   m3, [pix1q+8]
-    psubb  m0, [pix2q]
-    psubb  m3, [pix2q+8]
-    pxor   m0, m1
-    pxor   m3, m1
-    mova   m4, [pix1q+lsizeq]
-    mova   m5, [pix1q+lsizeq+8]
-    psubb  m4, [pix2q+lsizeq]
-    psubb  m5, [pix2q+lsizeq+8]
-    pxor   m4, m1
-    pxor   m5, m1
-    psadbw m0, m4
-    psadbw m3, m5
-    paddw  m0, m3
-%endif
-    sub    hd, 2
-
-.loop:
-    lea pix1q, [pix1q + 2*lsizeq]
-    lea pix2q, [pix2q + 2*lsizeq]
-    mova   m2, [pix1q]
-%if %1 == mmsize ; vsad8_mmxext, vsad16_sse2
-%if mmsize == 16
-    movu   m3, [pix2q]
-    psubb  m2, m3
-%else
-    psubb  m2, [pix2q]
-%endif
-    pxor   m2, m1
-    psadbw m4, m2
-    paddw  m0, m4
-    mova   m4, [pix1q+lsizeq]
-    movu   m3, [pix2q+lsizeq]
-    psubb  m4, m3
-    pxor   m4, m1
-    psadbw m2, m4
-    paddw  m0, m2
-%else ; vsad16_mmxext
-    mova   m3, [pix1q+8]
-    psubb  m2, [pix2q]
-    psubb  m3, [pix2q+8]
-    pxor   m2, m1
-    pxor   m3, m1
-    psadbw m4, m2
-    psadbw m5, m3
-    paddw  m0, m4
-    paddw  m0, m5
-    mova   m4, [pix1q+lsizeq]
-    mova   m5, [pix1q+lsizeq+8]
-    psubb  m4, [pix2q+lsizeq]
-    psubb  m5, [pix2q+lsizeq+8]
-    pxor   m4, m1
-    pxor   m5, m1
-    psadbw m2, m4
-    psadbw m3, m5
-    paddw  m0, m2
-    paddw  m0, m3
-%endif
-    sub    hd, 2
-    jg  .loop
-
-%if mmsize == 16
-    pshufd m1, m0, 0xe
-    paddd  m0, m1
-%endif
-    movd  eax, m0
-    RET
-%endmacro
-
-INIT_MMX mmxext
-VSAD_APPROX 8
-VSAD_APPROX 16
-INIT_XMM sse2
-VSAD_APPROX 16
diff -uparN ffmpeg-4.1/libavcodec/x86/mlpdsp.asm ffmpeg-y/libavcodec/x86/mlpdsp.asm
--- ffmpeg-4.1/libavcodec/x86/mlpdsp.asm	2018-01-01 06:35:48.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/mlpdsp.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,196 +0,0 @@
-;******************************************************************************
-;* SIMD-optimized MLP DSP functions
-;* Copyright (c) 2014 James Almer <jamrial@gmail.com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION .text
-
-%if ARCH_X86_64
-
-%macro SHLX 2
-%if cpuflag(bmi2)
-   shlx %1, %1, %2q
-%else
-   shl  %1, %2b
-%endif
-%endmacro
-
-%macro REMATRIX 0
-    movdqa        m0, [samplesq]
-    movdqa        m1, [coeffsq ]
-    pshufd        m2, m0, q2301
-    pshufd        m3, m1, q2301
-    pmuldq        m0, m1
-    pmuldq        m3, m2
-    paddq         m0, m3
-%if notcpuflag(avx2)
-    movdqa        m1, [samplesq + 16]
-    movdqa        m2, [coeffsq  + 16]
-    pshufd        m3, m1, q2301
-    pshufd        m4, m2, q2301
-    pmuldq        m1, m2
-    pmuldq        m4, m3
-    paddq         m0, m1
-    paddq         m0, m4
-%else
-    vextracti128 xm1, m0, 1
-    paddq        xm0, xm1
-%endif
-%endmacro
-
-%macro LOOP_END 0
-    pshufd       xm1, xm0, q0032
-    paddq        xm0, xm1
-    movq      accumq, xm0
-    movzx     blsbsd, byte [blsbs_ptrq]             ; load *bypassed_lsbs
-    sar       accumq, 14                            ; accum >>= 14
-    and       accumd, maskd                         ; accum &= mask
-    add       accumd, blsbsd                        ; accum += *bypassed_lsbs
-    mov   [samplesq + dest_chq], accumd             ; samples[dest_ch] = accum
-    add   blsbs_ptrq, 8                             ; bypassed_lsbs += MAX_CHANNELS;
-    add     samplesq, 32                            ; samples += MAX_CHANNELS;
-    cmp   blsbs_ptrq, cntq
-%endmacro
-
-%macro LOOP_SHIFT_END 0
-    pshufd       xm1, xm0, q0032
-    paddq        xm0, xm1
-    movq      accumq, xm0
-    and       indexd, auspd                         ; index &= access_unit_size_pow2;
-    movsx     noiseq, byte [noise_bufferq + indexq] ; load noise_buffer[index]
-    add       indexd, index2d                       ; index += index2
-    SHLX      noiseq, mns                           ; noise_buffer[index] <<= matrix_noise_shift
-    add       accumq, noiseq                        ; accum += noise_buffer[index]
-    movzx     noised, byte [blsbs_ptrq]             ; load *bypassed_lsbs (reuse tmp noise register)
-    sar       accumq, 14                            ; accum >>= 14
-    and       accumd, maskd                         ; accum &= mask
-    add       accumd, noised                        ; accum += *bypassed_lsbs
-    mov   [samplesq + dest_chq], accumd             ; samples[dest_ch] = accum
-    add   blsbs_ptrq, 8                             ; bypassed_lsbs += MAX_CHANNELS;
-    add     samplesq, 32                            ; samples += MAX_CHANNELS;
-    cmp   blsbs_ptrq, cntq
-%endmacro
-
-;void ff_mlp_rematrix_channel(int32_t *samples, const int32_t *coeffs,
-;                             const uint8_t *bypassed_lsbs, const int8_t *noise_buffer,
-;                             int index, unsigned int dest_ch, uint16_t blockpos,
-;                             unsigned int maxchan, int matrix_noise_shift,
-;                             int access_unit_size_pow2, int32_t mask)
-%macro MLP_REMATRIX_CHANNEL 0
-cglobal mlp_rematrix_channel, 0, 13, 5, samples, coeffs, blsbs_ptr, blsbs, \
-                                        index, dest_ch, blockpos, maxchan, mns, \
-                                        accum, mask, cnt
-    mov         mnsd, mnsm                          ; load matrix_noise_shift
-    movzx  blockposq, word blockposm                ; load and zero extend blockpos (16bit)
-    mov     maxchand, maxchanm                      ; load maxchan
-    mov        maskd, maskm                         ; load mask
-%if WIN64
-    mov     dest_chd, dest_chm                      ; load dest_chd (not needed on UNIX64)
-%endif
-    shl     dest_chd, 2
-    lea         cntq, [blsbs_ptrq + blockposq*8]
-    test        mnsd, mnsd                          ; is matrix_noise_shift != 0?
-    jne .shift                                      ; jump if true
-    cmp     maxchand, 4                             ; is maxchan < 4?
-    jl .loop4                                       ; jump if true
-
-align 16
-.loop8:
-    ; Process 5 or more channels
-    REMATRIX
-    LOOP_END
-    jne .loop8
-    RET
-
-align 16
-.loop4:
-    ; Process up to 4 channels
-    movdqa       xm0, [samplesq]
-    movdqa       xm1, [coeffsq ]
-    pshufd       xm2, xm0, q2301
-    pshufd       xm3, xm1, q2301
-    pmuldq       xm0, xm1
-    pmuldq       xm3, xm2
-    paddq        xm0, xm3
-    LOOP_END
-    jne .loop4
-    RET
-
-.shift:
-%if WIN64
-    mov       indexd, indexm         ; load index (not needed on UNIX64)
-%endif
-    mov          r9d, r9m            ; load access_unit_size_pow2
-%if cpuflag(bmi2)
-    ; bmi2 has shift functions that accept any gpr, not just cl, so keep things in place.
-    DEFINE_ARGS samples, coeffs, blsbs_ptr, noise_buffer, \
-                index, dest_ch, accum, index2, mns, \
-                ausp, mask, cnt, noise
-    add         mnsd, 7              ; matrix_noise_shift += 7
-%else ; sse4
-    mov           r6, rcx            ; move rcx elsewhere so we can use cl for matrix_noise_shift
-%if WIN64
-    ; r0 = rcx
-    DEFINE_ARGS mns, coeffs, blsbs_ptr, noise_buffer, index, dest_ch, samples, \
-                index2, accum, ausp, mask, cnt, noise
-%else ; UNIX64
-    ; r3 = rcx
-    DEFINE_ARGS samples, coeffs, blsbs_ptr, mns, index, dest_ch, noise_buffer, \
-                index2, accum, ausp, mask, cnt, noise
-%endif
-    lea         mnsd, [r8 + 7]       ; rcx = matrix_noise_shift + 7
-%endif ; cpuflag
-    sub        auspd, 1              ; access_unit_size_pow2 -= 1
-    cmp          r7d, 4              ; is maxchan < 4?
-    lea      index2q, [indexq*2 + 1] ; index2 = 2 * index + 1;
-    jl .loop4_shift                  ; jump if maxchan < 4
-
-align 16
-.loop8_shift:
-    ; Process 5 or more channels
-    REMATRIX
-    LOOP_SHIFT_END
-    jne .loop8_shift
-    RET
-
-align 16
-.loop4_shift:
-    ; Process up to 4 channels
-    movdqa       xm0, [samplesq]
-    movdqa       xm1, [coeffsq ]
-    pshufd       xm2, xm0, q2301
-    pshufd       xm3, xm1, q2301
-    pmuldq       xm0, xm1
-    pmuldq       xm3, xm2
-    paddq        xm0, xm3
-    LOOP_SHIFT_END
-    jne .loop4_shift
-    RET
-%endmacro
-
-INIT_XMM sse4
-MLP_REMATRIX_CHANNEL
-%if HAVE_AVX2_EXTERNAL
-INIT_YMM avx2, bmi2
-MLP_REMATRIX_CHANNEL
-%endif
-
-%endif ; ARCH_X86_64
diff -uparN ffmpeg-4.1/libavcodec/x86/mpegvideoencdsp.asm ffmpeg-y/libavcodec/x86/mpegvideoencdsp.asm
--- ffmpeg-4.1/libavcodec/x86/mpegvideoencdsp.asm	2018-01-01 06:35:48.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/mpegvideoencdsp.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,154 +0,0 @@
-;*****************************************************************************
-;* SIMD-optimized MPEG encoding functions
-;*****************************************************************************
-;* Copyright (c) 2000, 2001 Fabrice Bellard
-;* Copyright (c) 2002-2004 Michael Niedermayer <michaelni@gmx.at>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;*****************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-
-cextern pw_1
-
-SECTION .text
-; int ff_pix_sum16_mmx(uint8_t *pix, int line_size)
-; %1 = number of loops
-; %2 = number of GPRs used
-%macro PIX_SUM16 3
-cglobal pix_sum16, 2, %2, 6
-    movsxdifnidn r1, r1d
-    mov          r2, %1
-%if mmsize == 16
-    lea          r3, [r1*3]
-%endif
-%if notcpuflag(xop)
-    pxor         m5, m5
-%endif
-    pxor         m4, m4
-.loop:
-%if cpuflag(xop)
-    vphaddubq    m0, [r0]
-    vphaddubq    m1, [r0+r1]
-    vphaddubq    m2, [r0+r1*2]
-    vphaddubq    m3, [r0+r3]
-%else
-    mova         m0, [r0]
-%if mmsize == 8
-    mova         m1, [r0+8]
-%if cpuflag(mmxext)
-    mova         m2, [r0+r1]
-    mova         m3, [r0+r1+8]
-%endif
-%else ; sse2
-    mova         m1, [r0+r1]
-    mova         m2, [r0+r1*2]
-    mova         m3, [r0+r3]
-%endif
-%if cpuflag(mmxext)
-    psadbw       m0, m5
-    psadbw       m1, m5
-    psadbw       m2, m5
-    psadbw       m3, m5
-%else ; mmx
-    punpckhbw    m2, m0, m5
-    punpcklbw    m0, m5
-    punpckhbw    m3, m1, m5
-    punpcklbw    m1, m5
-%endif ; cpuflag(mmxext)
-%endif ; cpuflag(xop)
-    paddw        m1, m0
-    paddw        m3, m2
-    paddw        m3, m1
-    paddw        m4, m3
-%if cpuflag(mmxext)
-    lea          r0, [r0+r1*%3]
-%else
-    add          r0, r1
-%endif
-    dec r2
-    jne .loop
-%if mmsize == 16
-    pshufd       m0, m4, q0032
-    paddd        m4, m0
-%elif notcpuflag(mmxext)
-    HADDW        m4, m5
-%endif
-    movd        eax, m4
-    RET
-%endmacro
-
-%if ARCH_X86_32
-INIT_MMX mmx
-PIX_SUM16 16, 3, 0
-INIT_MMX mmxext
-PIX_SUM16  8, 4, 2
-%endif
-INIT_XMM sse2
-PIX_SUM16  4, 4, 4
-%if HAVE_XOP_EXTERNAL
-INIT_XMM xop
-PIX_SUM16  4, 4, 4
-%endif
-
-; int ff_pix_norm1_mmx(uint8_t *pix, int line_size)
-; %1 = number of xmm registers used
-; %2 = number of loops
-%macro PIX_NORM1 2
-cglobal pix_norm1, 2, 3, %1
-    movsxdifnidn r1, r1d
-    mov          r2, %2
-    pxor         m0, m0
-    pxor         m5, m5
-.loop:
-    mova         m2, [r0+0]
-%if mmsize == 8
-    mova         m3, [r0+8]
-%else
-    mova         m3, [r0+r1]
-%endif
-    punpckhbw    m1, m2, m0
-    punpcklbw    m2, m0
-    punpckhbw    m4, m3, m0
-    punpcklbw    m3, m0
-    pmaddwd      m1, m1
-    pmaddwd      m2, m2
-    pmaddwd      m3, m3
-    pmaddwd      m4, m4
-    paddd        m2, m1
-    paddd        m4, m3
-    paddd        m5, m2
-    paddd        m5, m4
-%if mmsize == 8
-    add          r0, r1
-%else
-    lea          r0, [r0+r1*2]
-%endif
-    dec r2
-    jne .loop
-    HADDD        m5, m1
-    movd        eax, m5
-    RET
-%endmacro
-
-INIT_MMX mmx
-PIX_NORM1 0, 16
-INIT_XMM sse2
-PIX_NORM1 6, 8
-
diff -uparN ffmpeg-4.1/libavcodec/x86/opus_pvq_search.asm ffmpeg-y/libavcodec/x86/opus_pvq_search.asm
--- ffmpeg-4.1/libavcodec/x86/opus_pvq_search.asm	2018-11-02 02:34:25.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/opus_pvq_search.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,385 +0,0 @@
-;******************************************************************************
-;* SIMD optimized Opus encoder DSP function
-;*
-;* Copyright (C) 2017 Ivan Kalvachev <ikalvachev@gmail.com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "config.asm"
-%include "libavutil/x86/x86util.asm"
-
-%ifdef __NASM_VER__
-%use "smartalign"
-ALIGNMODE p6
-%endif
-
-SECTION_RODATA 64
-
-const_float_abs_mask:   times 8 dd 0x7fffffff
-const_align_abs_edge:   times 8 dd 0
-
-const_float_0_5:        times 8 dd 0.5
-const_float_1:          times 8 dd 1.0
-const_float_sign_mask:  times 8 dd 0x80000000
-
-const_int32_offsets:
-                        %rep 8
-                                dd $-const_int32_offsets
-                        %endrep
-SECTION .text
-
-;
-;   Setup High Register to be used
-;   for holding memory constants
-;
-; %1 - the register to be used, assmues it is >= mm8
-; %2 - name of the constant.
-;
-; Subsequent opcodes are going to use the constant in the form
-; "addps m0, mm_const_name" and it would be turned into:
-; "addps m0, [const_name]" on 32 bit arch or
-; "addps m0, m8" on 64 bit arch
-%macro SET_HI_REG_MM_CONSTANT 3 ; movop, reg, const_name
-%if num_mmregs > 8
-    %define  mm_%3   %2
-    %{1}        %2, [%3]    ; movaps m8, [const_name]
-%else
-    %define  mm_%3  [%3]
-%endif
-%endmacro
-
-;
-;   Set Position Independent Code
-;       Base address of a constant
-; %1 - the register to be used, if PIC is set
-; %2 - name of the constant.
-;
-; Subsequent opcode are going to use the base address in the form
-; "movaps m0, [pic_base_constant_name+r4]" and it would be turned into
-; "movaps m0, [r5 + r4]" if PIC is enabled
-; "movaps m0, [constant_name + r4]" if texrel are used
-%macro SET_PIC_BASE 3; reg, const_label
-%ifdef PIC
-    %{1}     %2, [%3]      ; lea r5, [rip+const]
-    %define  pic_base_%3 %2
-%else
-    %define  pic_base_%3 %3
-%endif
-%endmacro
-
-%macro PULSES_SEARCH 1
-; m6 Syy_norm
-; m7 Sxy_norm
-    addps          m6, mm_const_float_0_5   ; Syy_norm += 1.0/2
-    pxor           m1, m1                   ; max_idx
-    xorps          m3, m3                   ; p_max
-    xor           r4d, r4d
-align 16
-%%distortion_search:
-    movd          xm2, dword r4d    ; movd zero extends
-%ifidn %1,add
-    movaps         m4, [tmpY + r4]  ; y[i]
-    movaps         m5, [tmpX + r4]  ; X[i]
-
-  %if USE_APPROXIMATION == 1
-    xorps          m0, m0
-    cmpps          m0, m0, m5, 4    ; m0 = (X[i] != 0.0)
-  %endif
-
-    addps          m4, m6           ; m4 = Syy_new = y[i] + Syy_norm
-    addps          m5, m7           ; m5 = Sxy_new = X[i] + Sxy_norm
-
-  %if USE_APPROXIMATION == 1
-    andps          m5, m0           ; if(X[i] == 0) Sxy_new = 0; Prevent aproximation error from setting pulses in array padding.
-  %endif
-
-%else
-    movaps         m5, [tmpY + r4]      ; m5 = y[i]
-
-    xorps          m0, m0               ; m0 = 0;
-    cmpps          m0, m0, m5, 1        ; m0 = (0<y)
-
-    subps          m4, m6, m5           ; m4 = Syy_new = Syy_norm - y[i]
-    subps          m5, m7, [tmpX + r4]  ; m5 = Sxy_new = Sxy_norm - X[i]
-    andps          m5, m0               ; (0<y)?m5:0
-%endif
-
-%if USE_APPROXIMATION == 1
-    rsqrtps        m4, m4
-    mulps          m5, m4           ; m5 = p = Sxy_new*approx(1/sqrt(Syy) )
-%else
-    mulps          m5, m5
-    divps          m5, m4           ; m5 = p = Sxy_new*Sxy_new/Syy
-%endif
-    VPBROADCASTD   m2, xm2          ; m2=i (all lanes get same values, we add the offset-per-lane, later)
-
-    cmpps          m0, m3, m5, 1    ; m0 = (m3 < m5) ; (p_max < p) ; (p > p_max)
-    maxps          m3, m5           ; m3=max(p_max,p)
-                                    ; maxps here is faster than blendvps, despite blend having lower latency.
-
-    pand           m2, m0           ; This version seems faster than sse41 pblendvb
-    pmaxsw         m1, m2           ; SSE2 signed word, so it would work for N < 32768/4
-
-    add           r4d, mmsize
-    cmp           r4d, Nd
-    jb   %%distortion_search
-
-    por            m1, mm_const_int32_offsets  ; max_idx offsets per individual lane (skipped in the inner loop)
-    movdqa         m4, m1                      ; needed for the aligned y[max_idx]+=1; processing
-
-%if mmsize >= 32
-; Merge parallel maximums round 8 (4 vs 4)
-
-    vextractf128  xm5, ym3, 1       ; xmm5 = ymm3[1x128] = ymm3[255..128b]
-    cmpps         xm0, xm3, xm5, 1  ; m0 = (m3 < m5) = ( p[0x128] < p[1x128] )
-
-    vextracti128  xm2, ym1, 1       ; xmm2 = ymm1[1x128] = ymm1[255..128b]
-    BLENDVPS      xm3, xm5, xm0     ; max_idx = m0 ? max_idx[1x128] : max_idx[0x128]
-    PBLENDVB      xm1, xm2, xm0     ; p       = m0 ? p[1x128]       : p[0x128]
-%endif
-
-; Merge parallel maximums round 4 (2 vs 2)
-                                    ; m3=p[3210]
-    movhlps       xm5, xm3          ; m5=p[xx32]
-    cmpps         xm0, xm3, xm5, 1  ; m0 = (m3 < m5) = ( p[1,0] < p[3,2] )
-
-    pshufd        xm2, xm1, q3232
-    BLENDVPS      xm3, xm5, xm0     ; max_idx = m0 ? max_idx[3,2] : max_idx[1,0]
-    PBLENDVB      xm1, xm2, xm0     ; p       = m0 ? p[3,2]       : p[1,0]
-
-; Merge parallel maximums final round (1 vs 1)
-    shufps        xm0, xm3, xm3, q1111  ; m0 = m3[1] = p[1]
-    cmpss         xm0, xm3, 5           ; m0 = !(m0 >= m3) = !( p[1] >= p[0] )
-
-    pshufd        xm2, xm1, q1111
-    PBLENDVB      xm1, xm2, xm0
-
-    movd    dword r4d, xm1          ; zero extends to the rest of r4q
-
-    VBROADCASTSS   m3, [tmpX + r4]
-    %{1}ps         m7, m3           ; Sxy += X[max_idx]
-
-    VBROADCASTSS   m5, [tmpY + r4]
-    %{1}ps         m6, m5           ; Syy += Y[max_idx]
-
-    ; We have to update a single element in Y[i]
-    ; However writing 4 bytes and then doing 16 byte load in the inner loop
-    ; could cause a stall due to breaking write forwarding.
-    VPBROADCASTD   m1, xm1
-    pcmpeqd        m1, m1, m4           ; exactly 1 element matches max_idx and this finds it
-
-    and           r4d, ~(mmsize-1)      ; align address down, so the value pointed by max_idx is inside a mmsize load
-    movaps         m5, [tmpY + r4]      ; m5 = Y[y3...ym...y0]
-    andps          m1, mm_const_float_1 ; m1 =  [ 0...1.0...0]
-    %{1}ps         m5, m1               ; m5 = Y[y3...ym...y0] +/- [0...1.0...0]
-    movaps [tmpY + r4], m5              ; Y[max_idx] +-= 1.0;
-%endmacro
-
-;
-; We need one more register for
-; PIC relative addressing. Use this
-; to count it in cglobal
-;
-%ifdef PIC
-  %define num_pic_regs 1
-%else
-  %define num_pic_regs 0
-%endif
-
-;
-; Pyramid Vector Quantization Search implementation
-;
-; float * inX   - Unaligned (SIMD) access, it will be overread,
-;                 but extra data is masked away.
-; int32 * outY  - Should be aligned and padded buffer.
-;                 It is used as temp buffer.
-; uint32 K      - Number of pulses to have after quantizations.
-; uint32 N      - Number of vector elements. Must be 0 < N < 256
-;
-%macro PVQ_FAST_SEARCH 1
-cglobal pvq_search%1, 4, 5+num_pic_regs, 11, 256*4, inX, outY, K, N
-%define tmpX rsp
-%define tmpY outYq
-
-    movaps     m0, [const_float_abs_mask]
-    shl        Nd, 2    ; N *= sizeof(float); also 32 bit operation zeroes the high 32 bits in 64 bit mode.
-    mov       r4d, Nd
-
-    neg       r4d
-    and       r4d, mmsize-1
-
-    SET_PIC_BASE lea, r5, const_align_abs_edge  ; rip+const
-    movups     m2, [pic_base_const_align_abs_edge + r4 - mmsize]
-
-    add        Nd, r4d              ; N = align(N, mmsize)
-
-    lea       r4d, [Nd - mmsize]    ; N is rounded up (aligned up) to mmsize, so r4 can't become negative here, unless N=0.
-    movups     m1, [inXq + r4]
-    andps      m1, m2
-    movaps  [tmpX + r4], m1         ; Sx = abs( X[N-1] )
-
-align 16
-%%loop_abs_sum:
-    sub       r4d, mmsize
-    jc   %%end_loop_abs_sum
-
-    movups     m2, [inXq + r4]
-    andps      m2, m0
-
-    movaps  [tmpX + r4], m2 ; tmpX[i]=abs(X[i])
-    addps      m1, m2       ; Sx += abs(X[i])
-    jmp  %%loop_abs_sum
-
-align 16
-%%end_loop_abs_sum:
-
-    HSUMPS     m1, m2       ; m1  = Sx
-
-    xorps      m0, m0
-    comiss    xm0, xm1      ;
-    jz   %%zero_input       ; if (Sx==0) goto zero_input
-
-    cvtsi2ss  xm0, dword Kd ; m0 = K
-%if USE_APPROXIMATION == 1
-    rcpss     xm1, xm1      ; m1 = approx(1/Sx)
-    mulss     xm0, xm1      ; m0 = K*(1/Sx)
-%else
-    divss     xm0, xm1      ; b = K/Sx
-                            ; b = K/max_x
-%endif
-
-    VBROADCASTSS  m0, xm0
-
-    lea       r4d, [Nd - mmsize]
-    pxor       m5, m5             ; Sy    ( Sum of abs( y[i]) )
-    xorps      m6, m6             ; Syy   ( Sum of y[i]*y[i]  )
-    xorps      m7, m7             ; Sxy   ( Sum of X[i]*y[i]  )
-align 16
-%%loop_guess:
-    movaps     m1, [tmpX + r4]    ; m1   = X[i]
-    mulps      m2, m0, m1         ; m2   = res*X[i]
-    cvtps2dq   m2, m2             ; yt   = (int)lrintf( res*X[i] )
-    paddd      m5, m2             ; Sy  += yt
-    cvtdq2ps   m2, m2             ; yt   = (float)yt
-    mulps      m1, m2             ; m1   = X[i]*yt
-    movaps  [tmpY + r4], m2       ; y[i] = m2
-    addps      m7, m1             ; Sxy += m1;
-    mulps      m2, m2             ; m2   = yt*yt
-    addps      m6, m2             ; Syy += m2
-
-    sub       r4d, mmsize
-    jnc  %%loop_guess
-
-    HSUMPS     m6, m1       ; Syy_norm
-    HADDD      m5, m4       ; pulses
-
-    movd  dword r4d, xm5    ; zero extends to the rest of r4q
-
-    sub        Kd, r4d      ; K -= pulses , also 32 bit operation zeroes high 32 bit in 64 bit mode.
-    jz   %%finish           ; K - pulses == 0
-
-    SET_HI_REG_MM_CONSTANT movaps,  m8, const_float_0_5
-    SET_HI_REG_MM_CONSTANT movaps,  m9, const_float_1
-    SET_HI_REG_MM_CONSTANT movdqa, m10, const_int32_offsets
-    ; Use Syy/2 in distortion parameter calculations.
-    ; Saves pre and post-caclulation to correct Y[] values.
-    ; Same precision, since float mantisa is normalized.
-    ; The SQRT approximation does differ.
-    HSUMPS     m7, m0         ; Sxy_norm
-    mulps      m6, mm_const_float_0_5
-
-    jc   %%remove_pulses_loop   ; K - pulses < 0
-
-align 16                        ; K - pulses > 0
-%%add_pulses_loop:
-
-    PULSES_SEARCH add   ; m6 Syy_norm ; m7 Sxy_norm
-
-    sub        Kd, 1
-    jnz  %%add_pulses_loop
-
-    addps      m6, m6 ; Syy*=2
-
-    jmp  %%finish
-
-align 16
-%%remove_pulses_loop:
-
-    PULSES_SEARCH sub   ; m6 Syy_norm ; m7 Sxy_norm
-
-    add        Kd, 1
-    jnz  %%remove_pulses_loop
-
-    addps      m6, m6 ; Syy*=2
-
-align 16
-%%finish:
-    lea       r4d, [Nd - mmsize]
-    movaps     m2, [const_float_sign_mask]
-
-align 16
-%%restore_sign_loop:
-    movaps     m0, [tmpY + r4]    ; m0 = Y[i]
-    movups     m1, [inXq + r4]    ; m1 = X[i]
-    andps      m1, m2             ; m1 = sign(X[i])
-    orps       m0, m1             ; m0 = Y[i]*sign
-    cvtps2dq   m3, m0             ; m3 = (int)m0
-    movaps  [outYq + r4], m3
-
-    sub       r4d, mmsize
-    jnc  %%restore_sign_loop
-%%return:
-
-%if ARCH_X86_64 == 0    ; sbrdsp
-    movss     r0m, xm6  ; return (float)Syy_norm
-    fld dword r0m
-%else
-    movaps     m0, m6   ; return (float)Syy_norm
-%endif
-
-    RET
-
-align 16
-%%zero_input:
-    lea       r4d, [Nd - mmsize]
-    xorps      m0, m0
-%%zero_loop:
-    movaps  [outYq + r4], m0
-    sub       r4d, mmsize
-    jnc  %%zero_loop
-
-    movaps     m6, [const_float_1]
-    jmp  %%return
-%endmacro
-
-; if 1, use a float op that give half precision but execute for around 3 cycles.
-; On Skylake & Ryzen the division is much faster (around 11c/3),
-; that makes the full precision code about 2% slower.
-; Opus also does use rsqrt approximation in their intrinsics code.
-%define USE_APPROXIMATION   1
-
-INIT_XMM sse2
-PVQ_FAST_SEARCH _approx
-
-INIT_XMM sse4
-PVQ_FAST_SEARCH _approx
-
-%define USE_APPROXIMATION   0
-
-INIT_XMM avx
-PVQ_FAST_SEARCH _exact
diff -uparN ffmpeg-4.1/libavcodec/x86/pixblockdsp.asm ffmpeg-y/libavcodec/x86/pixblockdsp.asm
--- ffmpeg-4.1/libavcodec/x86/pixblockdsp.asm	2018-11-02 02:34:25.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/pixblockdsp.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,128 +0,0 @@
-;*****************************************************************************
-;* SIMD-optimized pixel operations
-;*****************************************************************************
-;* Copyright (c) 2000, 2001 Fabrice Bellard
-;* Copyright (c) 2002-2004 Michael Niedermayer <michaelni@gmx.at>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;*****************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION .text
-
-INIT_MMX mmx
-; void ff_get_pixels_mmx(int16_t *block, const uint8_t *pixels, ptrdiff_t stride)
-cglobal get_pixels, 3,4
-    add          r0, 128
-    mov          r3, -128
-    pxor         m7, m7
-.loop:
-    mova         m0, [r1]
-    mova         m2, [r1+r2]
-    mova         m1, m0
-    mova         m3, m2
-    punpcklbw    m0, m7
-    punpckhbw    m1, m7
-    punpcklbw    m2, m7
-    punpckhbw    m3, m7
-    mova [r0+r3+ 0], m0
-    mova [r0+r3+ 8], m1
-    mova [r0+r3+16], m2
-    mova [r0+r3+24], m3
-    lea          r1, [r1+r2*2]
-    add          r3, 32
-    js .loop
-    REP_RET
-
-INIT_XMM sse2
-cglobal get_pixels, 3, 4, 5
-    lea          r3, [r2*3]
-    pxor         m4, m4
-    movh         m0, [r1]
-    movh         m1, [r1+r2]
-    movh         m2, [r1+r2*2]
-    movh         m3, [r1+r3]
-    lea          r1, [r1+r2*4]
-    punpcklbw    m0, m4
-    punpcklbw    m1, m4
-    punpcklbw    m2, m4
-    punpcklbw    m3, m4
-    mova       [r0], m0
-    mova  [r0+0x10], m1
-    mova  [r0+0x20], m2
-    mova  [r0+0x30], m3
-    movh         m0, [r1]
-    movh         m1, [r1+r2*1]
-    movh         m2, [r1+r2*2]
-    movh         m3, [r1+r3]
-    punpcklbw    m0, m4
-    punpcklbw    m1, m4
-    punpcklbw    m2, m4
-    punpcklbw    m3, m4
-    mova  [r0+0x40], m0
-    mova  [r0+0x50], m1
-    mova  [r0+0x60], m2
-    mova  [r0+0x70], m3
-    RET
-
-; void ff_diff_pixels_mmx(int16_t *block, const uint8_t *s1, const uint8_t *s2,
-;                         ptrdiff_t stride);
-%macro DIFF_PIXELS 0
-cglobal diff_pixels, 4,5,5
-    pxor         m4, m4
-    add          r0,  128
-    mov          r4, -128
-.loop:
-    movq         m0, [r1]
-    movq         m2, [r2]
-%if mmsize == 8
-    movq         m1, m0
-    movq         m3, m2
-    punpcklbw    m0, m4
-    punpckhbw    m1, m4
-    punpcklbw    m2, m4
-    punpckhbw    m3, m4
-%else
-    movq         m1, [r1+r3]
-    movq         m3, [r2+r3]
-    punpcklbw    m0, m4
-    punpcklbw    m1, m4
-    punpcklbw    m2, m4
-    punpcklbw    m3, m4
-%endif
-    psubw        m0, m2
-    psubw        m1, m3
-    mova  [r0+r4+0], m0
-    mova  [r0+r4+mmsize], m1
-%if mmsize == 8
-    add          r1, r3
-    add          r2, r3
-%else
-    lea          r1, [r1+r3*2]
-    lea          r2, [r2+r3*2]
-%endif
-    add          r4, 2 * mmsize
-    jne .loop
-    RET
-%endmacro
-
-INIT_MMX mmx
-DIFF_PIXELS
-
-INIT_XMM sse2
-DIFF_PIXELS
diff -uparN ffmpeg-4.1/libavcodec/x86/pngdsp.asm ffmpeg-y/libavcodec/x86/pngdsp.asm
--- ffmpeg-4.1/libavcodec/x86/pngdsp.asm	2018-01-01 06:35:48.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/pngdsp.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,173 +0,0 @@
-;******************************************************************************
-;* x86 optimizations for PNG decoding
-;*
-;* Copyright (c) 2008 Loren Merritt <lorenm@u.washington.edu>
-;* Copyright (c) 2012 Ronald S. Bultje <rsbultje@gmail.com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* 51, Inc., Foundation Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-
-cextern pw_255
-
-SECTION .text
-
-; %1 = nr. of xmm registers used
-%macro ADD_BYTES_FN 1
-cglobal add_bytes_l2, 4, 6, %1, dst, src1, src2, wa, w, i
-%if ARCH_X86_64
-    movsxd             waq, wad
-%endif
-    xor                 iq, iq
-
-    ; vector loop
-    mov                 wq, waq
-    and                waq, ~(mmsize*2-1)
-    jmp .end_v
-.loop_v:
-    movu                m0, [src2q+iq]
-    movu                m1, [src2q+iq+mmsize]
-    paddb               m0, [src1q+iq]
-    paddb               m1, [src1q+iq+mmsize]
-    movu  [dstq+iq       ], m0
-    movu  [dstq+iq+mmsize], m1
-    add                 iq, mmsize*2
-.end_v:
-    cmp                 iq, waq
-    jl .loop_v
-
-%if mmsize == 16
-    ; vector loop
-    mov                waq, wq
-    and                waq, ~7
-    jmp .end_l
-.loop_l:
-    movq               mm0, [src1q+iq]
-    paddb              mm0, [src2q+iq]
-    movq  [dstq+iq       ], mm0
-    add                 iq, 8
-.end_l:
-    cmp                 iq, waq
-    jl .loop_l
-%endif
-
-    ; scalar loop for leftover
-    jmp .end_s
-.loop_s:
-    mov                wab, [src1q+iq]
-    add                wab, [src2q+iq]
-    mov          [dstq+iq], wab
-    inc                 iq
-.end_s:
-    cmp                 iq, wq
-    jl .loop_s
-    REP_RET
-%endmacro
-
-%if ARCH_X86_32
-INIT_MMX mmx
-ADD_BYTES_FN 0
-%endif
-
-INIT_XMM sse2
-ADD_BYTES_FN 2
-
-%macro ADD_PAETH_PRED_FN 1
-cglobal add_png_paeth_prediction, 5, 7, %1, dst, src, top, w, bpp, end, cntr
-%if ARCH_X86_64
-    movsxd            bppq, bppd
-    movsxd              wq, wd
-%endif
-    lea               endq, [dstq+wq-(mmsize/2-1)]
-    sub               topq, dstq
-    sub               srcq, dstq
-    sub               dstq, bppq
-    pxor                m7, m7
-
-    PUSH              dstq
-    lea              cntrq, [bppq-1]
-    shr              cntrq, 2 + mmsize/16
-.bpp_loop:
-    lea               dstq, [dstq+cntrq*(mmsize/2)]
-    movh                m0, [dstq]
-    movh                m1, [topq+dstq]
-    punpcklbw           m0, m7
-    punpcklbw           m1, m7
-    add               dstq, bppq
-.loop:
-    mova                m2, m1
-    movh                m1, [topq+dstq]
-    mova                m3, m2
-    punpcklbw           m1, m7
-    mova                m4, m2
-    psubw               m3, m1
-    psubw               m4, m0
-    mova                m5, m3
-    paddw               m5, m4
-%if cpuflag(ssse3)
-    pabsw               m3, m3
-    pabsw               m4, m4
-    pabsw               m5, m5
-%else ; !cpuflag(ssse3)
-    psubw               m7, m5
-    pmaxsw              m5, m7
-    pxor                m6, m6
-    pxor                m7, m7
-    psubw               m6, m3
-    psubw               m7, m4
-    pmaxsw              m3, m6
-    pmaxsw              m4, m7
-    pxor                m7, m7
-%endif ; cpuflag(ssse3)
-    mova                m6, m4
-    pminsw              m6, m5
-    pcmpgtw             m3, m6
-    pcmpgtw             m4, m5
-    mova                m6, m4
-    pand                m4, m3
-    pandn               m6, m3
-    pandn               m3, m0
-    movh                m0, [srcq+dstq]
-    pand                m6, m1
-    pand                m2, m4
-    punpcklbw           m0, m7
-    paddw               m0, m6
-    paddw               m3, m2
-    paddw               m0, m3
-    pand                m0, [pw_255]
-    mova                m3, m0
-    packuswb            m3, m3
-    movh            [dstq], m3
-    add               dstq, bppq
-    cmp               dstq, endq
-    jl .loop
-
-    mov               dstq, [rsp]
-    dec              cntrq
-    jge .bpp_loop
-    POP               dstq
-    RET
-%endmacro
-
-INIT_MMX mmxext
-ADD_PAETH_PRED_FN 0
-
-INIT_MMX ssse3
-ADD_PAETH_PRED_FN 0
diff -uparN ffmpeg-4.1/libavcodec/x86/proresdsp.asm ffmpeg-y/libavcodec/x86/proresdsp.asm
--- ffmpeg-4.1/libavcodec/x86/proresdsp.asm	2018-11-02 02:34:25.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/proresdsp.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,68 +0,0 @@
-;******************************************************************************
-;* x86-SIMD-optimized IDCT for prores
-;* this is identical to "simple" IDCT written by Michael Niedermayer
-;* except for the clip range
-;*
-;* Copyright (c) 2011 Ronald S. Bultje <rsbultje@gmail.com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* 51, Inc., Foundation Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-%if ARCH_X86_64
-
-SECTION_RODATA
-
-pw_88:      times 8 dw 0x2008
-cextern pw_1
-cextern pw_4
-cextern pw_1019
-; Below are defined in simple_idct10.asm built from selecting idctdsp
-cextern w4_plus_w2_hi
-cextern w4_min_w2_hi
-cextern w4_plus_w6_hi
-cextern w4_min_w6_hi
-cextern w1_plus_w3_hi
-cextern w3_min_w1_hi
-cextern w7_plus_w3_hi
-cextern w3_min_w7_hi
-cextern w1_plus_w5
-cextern w5_min_w1
-cextern w5_plus_w7
-cextern w7_min_w5
-
-%include "libavcodec/x86/simple_idct10_template.asm"
-
-SECTION .text
-
-define_constants _hi
-
-%macro idct_fn 0
-cglobal prores_idct_put_10, 4, 4, 15, pixels, lsize, block, qmat
-    IDCT_FN    pw_1, 15, pw_88, 18, "put", pw_4, pw_1019, r3
-    RET
-%endmacro
-
-INIT_XMM sse2
-idct_fn
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-idct_fn
-%endif
-
-%endif
diff -uparN ffmpeg-4.1/libavcodec/x86/qpel.asm ffmpeg-y/libavcodec/x86/qpel.asm
--- ffmpeg-4.1/libavcodec/x86/qpel.asm	2016-03-29 10:25:25.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/qpel.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,179 +0,0 @@
-;******************************************************************************
-;* SIMD-optimized quarterpel functions
-;* Copyright (c) 2008 Loren Merritt
-;* Copyright (c) 2003-2013 Michael Niedermayer
-;* Copyright (c) 2013 Daniel Kang
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION .text
-
-%macro op_avgh 3
-    movh   %3, %2
-    pavgb  %1, %3
-    movh   %2, %1
-%endmacro
-
-%macro op_avg 2
-    pavgb  %1, %2
-    mova   %2, %1
-%endmacro
-
-%macro op_puth 2-3
-    movh   %2, %1
-%endmacro
-
-%macro op_put 2
-    mova   %2, %1
-%endmacro
-
-; void ff_put/avg_pixels4_l2_mmxext(uint8_t *dst, uint8_t *src1, uint8_t *src2,
-;                                   int dstStride, int src1Stride, int h)
-%macro PIXELS4_L2 1
-%define OP op_%1h
-cglobal %1_pixels4_l2, 6,6
-    movsxdifnidn r3, r3d
-    movsxdifnidn r4, r4d
-    test        r5d, 1
-    je        .loop
-    movd         m0, [r1]
-    movd         m1, [r2]
-    add          r1, r4
-    add          r2, 4
-    pavgb        m0, m1
-    OP           m0, [r0], m3
-    add          r0, r3
-    dec         r5d
-.loop:
-    mova         m0, [r1]
-    mova         m1, [r1+r4]
-    lea          r1, [r1+2*r4]
-    pavgb        m0, [r2]
-    pavgb        m1, [r2+4]
-    OP           m0, [r0], m3
-    OP           m1, [r0+r3], m3
-    lea          r0, [r0+2*r3]
-    mova         m0, [r1]
-    mova         m1, [r1+r4]
-    lea          r1, [r1+2*r4]
-    pavgb        m0, [r2+8]
-    pavgb        m1, [r2+12]
-    OP           m0, [r0], m3
-    OP           m1, [r0+r3], m3
-    lea          r0, [r0+2*r3]
-    add          r2, 16
-    sub         r5d, 4
-    jne       .loop
-    REP_RET
-%endmacro
-
-INIT_MMX mmxext
-PIXELS4_L2 put
-PIXELS4_L2 avg
-
-; void ff_put/avg_pixels8_l2_mmxext(uint8_t *dst, uint8_t *src1, uint8_t *src2,
-;                                   int dstStride, int src1Stride, int h)
-%macro PIXELS8_L2 1
-%define OP op_%1
-cglobal %1_pixels8_l2, 6,6
-    movsxdifnidn r3, r3d
-    movsxdifnidn r4, r4d
-    test        r5d, 1
-    je        .loop
-    mova         m0, [r1]
-    mova         m1, [r2]
-    add          r1, r4
-    add          r2, 8
-    pavgb        m0, m1
-    OP           m0, [r0]
-    add          r0, r3
-    dec         r5d
-.loop:
-    mova         m0, [r1]
-    mova         m1, [r1+r4]
-    lea          r1, [r1+2*r4]
-    pavgb        m0, [r2]
-    pavgb        m1, [r2+8]
-    OP           m0, [r0]
-    OP           m1, [r0+r3]
-    lea          r0, [r0+2*r3]
-    mova         m0, [r1]
-    mova         m1, [r1+r4]
-    lea          r1, [r1+2*r4]
-    pavgb        m0, [r2+16]
-    pavgb        m1, [r2+24]
-    OP           m0, [r0]
-    OP           m1, [r0+r3]
-    lea          r0, [r0+2*r3]
-    add          r2, 32
-    sub         r5d, 4
-    jne       .loop
-    REP_RET
-%endmacro
-
-INIT_MMX mmxext
-PIXELS8_L2 put
-PIXELS8_L2 avg
-
-; void ff_put/avg_pixels16_l2_mmxext(uint8_t *dst, uint8_t *src1, uint8_t *src2,
-;                                    int dstStride, int src1Stride, int h)
-%macro PIXELS16_L2 1
-%define OP op_%1
-cglobal %1_pixels16_l2, 6,6
-    movsxdifnidn r3, r3d
-    movsxdifnidn r4, r4d
-    test        r5d, 1
-    je        .loop
-    mova         m0, [r1]
-    mova         m1, [r1+8]
-    pavgb        m0, [r2]
-    pavgb        m1, [r2+8]
-    add          r1, r4
-    add          r2, 16
-    OP           m0, [r0]
-    OP           m1, [r0+8]
-    add          r0, r3
-    dec         r5d
-.loop:
-    mova         m0, [r1]
-    mova         m1, [r1+8]
-    add          r1, r4
-    pavgb        m0, [r2]
-    pavgb        m1, [r2+8]
-    OP           m0, [r0]
-    OP           m1, [r0+8]
-    add          r0, r3
-    mova         m0, [r1]
-    mova         m1, [r1+8]
-    add          r1, r4
-    pavgb        m0, [r2+16]
-    pavgb        m1, [r2+24]
-    OP           m0, [r0]
-    OP           m1, [r0+8]
-    add          r0, r3
-    add          r2, 32
-    sub         r5d, 2
-    jne       .loop
-    REP_RET
-%endmacro
-
-INIT_MMX mmxext
-PIXELS16_L2 put
-PIXELS16_L2 avg
diff -uparN ffmpeg-4.1/libavcodec/x86/qpeldsp.asm ffmpeg-y/libavcodec/x86/qpeldsp.asm
--- ffmpeg-4.1/libavcodec/x86/qpeldsp.asm	2018-01-01 06:35:48.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/qpeldsp.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,560 +0,0 @@
-;******************************************************************************
-;* mpeg4 qpel
-;* Copyright (c) 2003 Michael Niedermayer <michaelni@gmx.at>
-;* Copyright (c) 2008 Loren Merritt
-;* Copyright (c) 2013 Daniel Kang
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-cextern pb_1
-cextern pw_3
-cextern pw_15
-cextern pw_16
-cextern pw_20
-
-
-SECTION .text
-
-; void ff_put_no_rnd_pixels8_l2(uint8_t *dst, uint8_t *src1, uint8_t *src2, int dstStride, int src1Stride, int h)
-%macro PUT_NO_RND_PIXELS8_L2 0
-cglobal put_no_rnd_pixels8_l2, 6,6
-    movsxdifnidn r4, r4d
-    movsxdifnidn r3, r3d
-    pcmpeqb      m6, m6
-    test        r5d, 1
-    je .loop
-    mova         m0, [r1]
-    mova         m1, [r2]
-    add          r1, r4
-    add          r2, 8
-    pxor         m0, m6
-    pxor         m1, m6
-    PAVGB        m0, m1
-    pxor         m0, m6
-    mova       [r0], m0
-    add          r0, r3
-    dec r5d
-.loop:
-    mova         m0, [r1]
-    add          r1, r4
-    mova         m1, [r1]
-    add          r1, r4
-    mova         m2, [r2]
-    mova         m3, [r2+8]
-    pxor         m0, m6
-    pxor         m1, m6
-    pxor         m2, m6
-    pxor         m3, m6
-    PAVGB        m0, m2
-    PAVGB        m1, m3
-    pxor         m0, m6
-    pxor         m1, m6
-    mova       [r0], m0
-    add          r0, r3
-    mova       [r0], m1
-    add          r0, r3
-    mova         m0, [r1]
-    add          r1, r4
-    mova         m1, [r1]
-    add          r1, r4
-    mova         m2, [r2+16]
-    mova         m3, [r2+24]
-    pxor         m0, m6
-    pxor         m1, m6
-    pxor         m2, m6
-    pxor         m3, m6
-    PAVGB        m0, m2
-    PAVGB        m1, m3
-    pxor         m0, m6
-    pxor         m1, m6
-    mova       [r0], m0
-    add          r0, r3
-    mova       [r0], m1
-    add          r0, r3
-    add          r2, 32
-    sub         r5d, 4
-    jne .loop
-    REP_RET
-%endmacro
-
-INIT_MMX mmxext
-PUT_NO_RND_PIXELS8_L2
-
-
-; void ff_put_no_rnd_pixels16_l2(uint8_t *dst, uint8_t *src1, uint8_t *src2, int dstStride, int src1Stride, int h)
-%macro PUT_NO_RND_PIXELS16_l2 0
-cglobal put_no_rnd_pixels16_l2, 6,6
-    movsxdifnidn r3, r3d
-    movsxdifnidn r4, r4d
-    pcmpeqb      m6, m6
-    test        r5d, 1
-    je .loop
-    mova         m0, [r1]
-    mova         m1, [r1+8]
-    mova         m2, [r2]
-    mova         m3, [r2+8]
-    pxor         m0, m6
-    pxor         m1, m6
-    pxor         m2, m6
-    pxor         m3, m6
-    PAVGB        m0, m2
-    PAVGB        m1, m3
-    pxor         m0, m6
-    pxor         m1, m6
-    add          r1, r4
-    add          r2, 16
-    mova       [r0], m0
-    mova     [r0+8], m1
-    add          r0, r3
-    dec r5d
-.loop:
-    mova         m0, [r1]
-    mova         m1, [r1+8]
-    add          r1, r4
-    mova         m2, [r2]
-    mova         m3, [r2+8]
-    pxor         m0, m6
-    pxor         m1, m6
-    pxor         m2, m6
-    pxor         m3, m6
-    PAVGB        m0, m2
-    PAVGB        m1, m3
-    pxor         m0, m6
-    pxor         m1, m6
-    mova       [r0], m0
-    mova     [r0+8], m1
-    add          r0, r3
-    mova         m0, [r1]
-    mova         m1, [r1+8]
-    add          r1, r4
-    mova         m2, [r2+16]
-    mova         m3, [r2+24]
-    pxor         m0, m6
-    pxor         m1, m6
-    pxor         m2, m6
-    pxor         m3, m6
-    PAVGB        m0, m2
-    PAVGB        m1, m3
-    pxor         m0, m6
-    pxor         m1, m6
-    mova       [r0], m0
-    mova     [r0+8], m1
-    add          r0, r3
-    add          r2, 32
-    sub         r5d, 2
-    jne .loop
-    REP_RET
-%endmacro
-
-INIT_MMX mmxext
-PUT_NO_RND_PIXELS16_l2
-INIT_MMX 3dnow
-PUT_NO_RND_PIXELS16_l2
-
-%macro MPEG4_QPEL16_H_LOWPASS 1
-cglobal %1_mpeg4_qpel16_h_lowpass, 5, 5, 0, 16
-    movsxdifnidn r2, r2d
-    movsxdifnidn r3, r3d
-    pxor         m7, m7
-.loop:
-    mova         m0, [r1]
-    mova         m1, m0
-    mova         m2, m0
-    punpcklbw    m0, m7
-    punpckhbw    m1, m7
-    pshufw       m5, m0, 0x90
-    pshufw       m6, m0, 0x41
-    mova         m3, m2
-    mova         m4, m2
-    psllq        m2, 8
-    psllq        m3, 16
-    psllq        m4, 24
-    punpckhbw    m2, m7
-    punpckhbw    m3, m7
-    punpckhbw    m4, m7
-    paddw        m5, m3
-    paddw        m6, m2
-    paddw        m5, m5
-    psubw        m6, m5
-    pshufw       m5, m0, 6
-    pmullw       m6, [pw_3]
-    paddw        m0, m4
-    paddw        m5, m1
-    pmullw       m0, [pw_20]
-    psubw        m0, m5
-    paddw        m6, [PW_ROUND]
-    paddw        m0, m6
-    psraw        m0, 5
-    mova    [rsp+8], m0
-    mova         m0, [r1+5]
-    mova         m5, m0
-    mova         m6, m0
-    psrlq        m0, 8
-    psrlq        m5, 16
-    punpcklbw    m0, m7
-    punpcklbw    m5, m7
-    paddw        m2, m0
-    paddw        m3, m5
-    paddw        m2, m2
-    psubw        m3, m2
-    mova         m2, m6
-    psrlq        m6, 24
-    punpcklbw    m2, m7
-    punpcklbw    m6, m7
-    pmullw       m3, [pw_3]
-    paddw        m1, m2
-    paddw        m4, m6
-    pmullw       m1, [pw_20]
-    psubw        m3, m4
-    paddw        m1, [PW_ROUND]
-    paddw        m3, m1
-    psraw        m3, 5
-    mova         m1, [rsp+8]
-    packuswb     m1, m3
-    OP_MOV     [r0], m1, m4
-    mova         m1, [r1+9]
-    mova         m4, m1
-    mova         m3, m1
-    psrlq        m1, 8
-    psrlq        m4, 16
-    punpcklbw    m1, m7
-    punpcklbw    m4, m7
-    paddw        m5, m1
-    paddw        m0, m4
-    paddw        m5, m5
-    psubw        m0, m5
-    mova         m5, m3
-    psrlq        m3, 24
-    pmullw       m0, [pw_3]
-    punpcklbw    m3, m7
-    paddw        m2, m3
-    psubw        m0, m2
-    mova         m2, m5
-    punpcklbw    m2, m7
-    punpckhbw    m5, m7
-    paddw        m6, m2
-    pmullw       m6, [pw_20]
-    paddw        m0, [PW_ROUND]
-    paddw        m0, m6
-    psraw        m0, 5
-    paddw        m3, m5
-    pshufw       m6, m5, 0xf9
-    paddw        m6, m4
-    pshufw       m4, m5, 0xbe
-    pshufw       m5, m5, 0x6f
-    paddw        m4, m1
-    paddw        m5, m2
-    paddw        m6, m6
-    psubw        m4, m6
-    pmullw       m3, [pw_20]
-    pmullw       m4, [pw_3]
-    psubw        m3, m5
-    paddw        m4, [PW_ROUND]
-    paddw        m4, m3
-    psraw        m4, 5
-    packuswb     m0, m4
-    OP_MOV   [r0+8], m0, m4
-    add          r1, r3
-    add          r0, r2
-    dec r4d
-    jne .loop
-    REP_RET
-%endmacro
-
-%macro PUT_OP 2-3
-    mova %1, %2
-%endmacro
-
-%macro AVG_OP 2-3
-    mova  %3, %1
-    pavgb %2, %3
-    mova  %1, %2
-%endmacro
-
-INIT_MMX mmxext
-%define PW_ROUND pw_16
-%define OP_MOV PUT_OP
-MPEG4_QPEL16_H_LOWPASS put
-%define PW_ROUND pw_16
-%define OP_MOV AVG_OP
-MPEG4_QPEL16_H_LOWPASS avg
-%define PW_ROUND pw_15
-%define OP_MOV PUT_OP
-MPEG4_QPEL16_H_LOWPASS put_no_rnd
-
-
-
-%macro MPEG4_QPEL8_H_LOWPASS 1
-cglobal %1_mpeg4_qpel8_h_lowpass, 5, 5, 0, 8
-    movsxdifnidn r2, r2d
-    movsxdifnidn r3, r3d
-    pxor         m7, m7
-.loop:
-    mova         m0, [r1]
-    mova         m1, m0
-    mova         m2, m0
-    punpcklbw    m0, m7
-    punpckhbw    m1, m7
-    pshufw       m5, m0, 0x90
-    pshufw       m6, m0, 0x41
-    mova         m3, m2
-    mova         m4, m2
-    psllq        m2, 8
-    psllq        m3, 16
-    psllq        m4, 24
-    punpckhbw    m2, m7
-    punpckhbw    m3, m7
-    punpckhbw    m4, m7
-    paddw        m5, m3
-    paddw        m6, m2
-    paddw        m5, m5
-    psubw        m6, m5
-    pshufw       m5, m0, 0x6
-    pmullw       m6, [pw_3]
-    paddw        m0, m4
-    paddw        m5, m1
-    pmullw       m0, [pw_20]
-    psubw        m0, m5
-    paddw        m6, [PW_ROUND]
-    paddw        m0, m6
-    psraw        m0, 5
-    movh         m5, [r1+5]
-    punpcklbw    m5, m7
-    pshufw       m6, m5, 0xf9
-    paddw        m1, m5
-    paddw        m2, m6
-    pshufw       m6, m5, 0xbe
-    pshufw       m5, m5, 0x6f
-    paddw        m3, m6
-    paddw        m4, m5
-    paddw        m2, m2
-    psubw        m3, m2
-    pmullw       m1, [pw_20]
-    pmullw       m3, [pw_3]
-    psubw        m3, m4
-    paddw        m1, [PW_ROUND]
-    paddw        m3, m1
-    psraw        m3, 5
-    packuswb     m0, m3
-    OP_MOV     [r0], m0, m4
-    add          r1, r3
-    add          r0, r2
-    dec r4d
-    jne .loop
-    REP_RET
-%endmacro
-
-INIT_MMX mmxext
-%define PW_ROUND pw_16
-%define OP_MOV PUT_OP
-MPEG4_QPEL8_H_LOWPASS put
-%define PW_ROUND pw_16
-%define OP_MOV AVG_OP
-MPEG4_QPEL8_H_LOWPASS avg
-%define PW_ROUND pw_15
-%define OP_MOV PUT_OP
-MPEG4_QPEL8_H_LOWPASS put_no_rnd
-
-
-
-%macro QPEL_V_LOW 5
-    paddw      m0, m1
-    mova       m4, [pw_20]
-    pmullw     m4, m0
-    mova       m0, %4
-    mova       m5, %1
-    paddw      m5, m0
-    psubw      m4, m5
-    mova       m5, %2
-    mova       m6, %3
-    paddw      m5, m3
-    paddw      m6, m2
-    paddw      m6, m6
-    psubw      m5, m6
-    pmullw     m5, [pw_3]
-    paddw      m4, [PW_ROUND]
-    paddw      m5, m4
-    psraw      m5, 5
-    packuswb   m5, m5
-    OP_MOV     %5, m5, m7
-    SWAP 0,1,2,3
-%endmacro
-
-%macro MPEG4_QPEL16_V_LOWPASS 1
-cglobal %1_mpeg4_qpel16_v_lowpass, 4, 6, 0, 544
-    movsxdifnidn r2, r2d
-    movsxdifnidn r3, r3d
-
-    mov         r4d, 17
-    mov          r5, rsp
-    pxor         m7, m7
-.looph:
-    mova         m0, [r1]
-    mova         m1, [r1]
-    mova         m2, [r1+8]
-    mova         m3, [r1+8]
-    punpcklbw    m0, m7
-    punpckhbw    m1, m7
-    punpcklbw    m2, m7
-    punpckhbw    m3, m7
-    mova       [r5], m0
-    mova  [r5+0x88], m1
-    mova [r5+0x110], m2
-    mova [r5+0x198], m3
-    add          r5, 8
-    add          r1, r3
-    dec r4d
-    jne .looph
-
-
-    ; NOTE: r1 CHANGES VALUES: r1 -> 4 - 14*dstStride
-    mov         r4d, 4
-    mov          r1, 4
-    neg          r2
-    lea          r1, [r1+r2*8]
-    lea          r1, [r1+r2*4]
-    lea          r1, [r1+r2*2]
-    neg          r2
-    mov          r5, rsp
-.loopv:
-    pxor         m7, m7
-    mova         m0, [r5+ 0x0]
-    mova         m1, [r5+ 0x8]
-    mova         m2, [r5+0x10]
-    mova         m3, [r5+0x18]
-    QPEL_V_LOW [r5+0x10], [r5+ 0x8], [r5+ 0x0], [r5+0x20], [r0]
-    QPEL_V_LOW [r5+ 0x8], [r5+ 0x0], [r5+ 0x0], [r5+0x28], [r0+r2]
-    lea    r0, [r0+r2*2]
-    QPEL_V_LOW [r5+ 0x0], [r5+ 0x0], [r5+ 0x8], [r5+0x30], [r0]
-    QPEL_V_LOW [r5+ 0x0], [r5+ 0x8], [r5+0x10], [r5+0x38], [r0+r2]
-    lea    r0, [r0+r2*2]
-    QPEL_V_LOW [r5+ 0x8], [r5+0x10], [r5+0x18], [r5+0x40], [r0]
-    QPEL_V_LOW [r5+0x10], [r5+0x18], [r5+0x20], [r5+0x48], [r0+r2]
-    lea    r0, [r0+r2*2]
-    QPEL_V_LOW [r5+0x18], [r5+0x20], [r5+0x28], [r5+0x50], [r0]
-    QPEL_V_LOW [r5+0x20], [r5+0x28], [r5+0x30], [r5+0x58], [r0+r2]
-    lea    r0, [r0+r2*2]
-    QPEL_V_LOW [r5+0x28], [r5+0x30], [r5+0x38], [r5+0x60], [r0]
-    QPEL_V_LOW [r5+0x30], [r5+0x38], [r5+0x40], [r5+0x68], [r0+r2]
-    lea    r0, [r0+r2*2]
-    QPEL_V_LOW [r5+0x38], [r5+0x40], [r5+0x48], [r5+0x70], [r0]
-    QPEL_V_LOW [r5+0x40], [r5+0x48], [r5+0x50], [r5+0x78], [r0+r2]
-    lea    r0, [r0+r2*2]
-    QPEL_V_LOW [r5+0x48], [r5+0x50], [r5+0x58], [r5+0x80], [r0]
-    QPEL_V_LOW [r5+0x50], [r5+0x58], [r5+0x60], [r5+0x80], [r0+r2]
-    lea    r0, [r0+r2*2]
-    QPEL_V_LOW [r5+0x58], [r5+0x60], [r5+0x68], [r5+0x78], [r0]
-    QPEL_V_LOW [r5+0x60], [r5+0x68], [r5+0x70], [r5+0x70], [r0+r2]
-
-    add    r5, 0x88
-    add    r0, r1
-    dec r4d
-    jne .loopv
-    REP_RET
-%endmacro
-
-%macro PUT_OPH 2-3
-    movh %1, %2
-%endmacro
-
-%macro AVG_OPH 2-3
-    movh  %3, %1
-    pavgb %2, %3
-    movh  %1, %2
-%endmacro
-
-INIT_MMX mmxext
-%define PW_ROUND pw_16
-%define OP_MOV PUT_OPH
-MPEG4_QPEL16_V_LOWPASS put
-%define PW_ROUND pw_16
-%define OP_MOV AVG_OPH
-MPEG4_QPEL16_V_LOWPASS avg
-%define PW_ROUND pw_15
-%define OP_MOV PUT_OPH
-MPEG4_QPEL16_V_LOWPASS put_no_rnd
-
-
-
-%macro MPEG4_QPEL8_V_LOWPASS 1
-cglobal %1_mpeg4_qpel8_v_lowpass, 4, 6, 0, 288
-    movsxdifnidn r2, r2d
-    movsxdifnidn r3, r3d
-
-    mov         r4d, 9
-    mov          r5, rsp
-    pxor         m7, m7
-.looph:
-    mova         m0, [r1]
-    mova         m1, [r1]
-    punpcklbw    m0, m7
-    punpckhbw    m1, m7
-    mova       [r5], m0
-    mova  [r5+0x48], m1
-    add          r5, 8
-    add          r1, r3
-    dec r4d
-    jne .looph
-
-
-    ; NOTE: r1 CHANGES VALUES: r1 -> 4 - 6*dstStride
-    mov         r4d, 2
-    mov          r1, 4
-    neg          r2
-    lea          r1, [r1+r2*4]
-    lea          r1, [r1+r2*2]
-    neg          r2
-    mov          r5, rsp
-.loopv:
-    pxor         m7, m7
-    mova         m0, [r5+ 0x0]
-    mova         m1, [r5+ 0x8]
-    mova         m2, [r5+0x10]
-    mova         m3, [r5+0x18]
-    QPEL_V_LOW [r5+0x10], [r5+ 0x8], [r5+ 0x0], [r5+0x20], [r0]
-    QPEL_V_LOW [r5+ 0x8], [r5+ 0x0], [r5+ 0x0], [r5+0x28], [r0+r2]
-    lea    r0, [r0+r2*2]
-    QPEL_V_LOW [r5+ 0x0], [r5+ 0x0], [r5+ 0x8], [r5+0x30], [r0]
-    QPEL_V_LOW [r5+ 0x0], [r5+ 0x8], [r5+0x10], [r5+0x38], [r0+r2]
-    lea    r0, [r0+r2*2]
-    QPEL_V_LOW [r5+ 0x8], [r5+0x10], [r5+0x18], [r5+0x40], [r0]
-    QPEL_V_LOW [r5+0x10], [r5+0x18], [r5+0x20], [r5+0x40], [r0+r2]
-    lea    r0, [r0+r2*2]
-    QPEL_V_LOW [r5+0x18], [r5+0x20], [r5+0x28], [r5+0x38], [r0]
-    QPEL_V_LOW [r5+0x20], [r5+0x28], [r5+0x30], [r5+0x30], [r0+r2]
-
-    add    r5, 0x48
-    add    r0, r1
-    dec r4d
-    jne .loopv
-    REP_RET
-%endmacro
-
-INIT_MMX mmxext
-%define PW_ROUND pw_16
-%define OP_MOV PUT_OPH
-MPEG4_QPEL8_V_LOWPASS put
-%define PW_ROUND pw_16
-%define OP_MOV AVG_OPH
-MPEG4_QPEL8_V_LOWPASS avg
-%define PW_ROUND pw_15
-%define OP_MOV PUT_OPH
-MPEG4_QPEL8_V_LOWPASS put_no_rnd
diff -uparN ffmpeg-4.1/libavcodec/x86/rv34dsp.asm ffmpeg-y/libavcodec/x86/rv34dsp.asm
--- ffmpeg-4.1/libavcodec/x86/rv34dsp.asm	2018-11-02 02:34:25.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/rv34dsp.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,213 +0,0 @@
-;******************************************************************************
-;* MMX/SSE2-optimized functions for the RV30 and RV40 decoders
-;* Copyright (C) 2012 Christophe Gisquet <christophe.gisquet@gmail.com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-pw_row_coeffs:  times 4 dw 13
-                times 4 dw 17
-                times 4 dw  7
-pd_512: times 2 dd 0x200
-pw_col_coeffs:  dw 13,  13,  13, -13
-                dw 17,   7,   7, -17
-                dw 13, -13,  13,  13
-                dw -7,  17, -17,  -7
-
-SECTION .text
-
-%macro IDCT_DC_NOROUND 1
-    imul   %1, 13*13*3
-    sar    %1, 11
-%endmacro
-
-%macro IDCT_DC_ROUND 1
-    imul   %1, 13*13
-    add    %1, 0x200
-    sar    %1, 10
-%endmacro
-
-%macro rv34_idct 1
-cglobal rv34_idct_%1, 1, 2, 0
-    movsx   r1, word [r0]
-    IDCT_DC r1
-    movd    m0, r1d
-    pshufw  m0, m0, 0
-    movq    [r0+ 0], m0
-    movq    [r0+ 8], m0
-    movq    [r0+16], m0
-    movq    [r0+24], m0
-    REP_RET
-%endmacro
-
-INIT_MMX mmxext
-%define IDCT_DC IDCT_DC_ROUND
-rv34_idct dc
-%define IDCT_DC IDCT_DC_NOROUND
-rv34_idct dc_noround
-
-; ff_rv34_idct_dc_add_mmx(uint8_t *dst, int stride, int dc);
-%if ARCH_X86_32
-INIT_MMX mmx
-cglobal rv34_idct_dc_add, 3, 3
-    ; calculate DC
-    IDCT_DC_ROUND r2
-    pxor       m1, m1
-    movd       m0, r2d
-    psubw      m1, m0
-    packuswb   m0, m0
-    packuswb   m1, m1
-    punpcklbw  m0, m0
-    punpcklbw  m1, m1
-    punpcklwd  m0, m0
-    punpcklwd  m1, m1
-
-    ; add DC
-    lea        r2, [r0+r1*2]
-    movh       m2, [r0]
-    movh       m3, [r0+r1]
-    movh       m4, [r2]
-    movh       m5, [r2+r1]
-    paddusb    m2, m0
-    paddusb    m3, m0
-    paddusb    m4, m0
-    paddusb    m5, m0
-    psubusb    m2, m1
-    psubusb    m3, m1
-    psubusb    m4, m1
-    psubusb    m5, m1
-    movh       [r0], m2
-    movh       [r0+r1], m3
-    movh       [r2], m4
-    movh       [r2+r1], m5
-    RET
-%endif
-
-; Load coeffs and perform row transform
-; Output: coeffs in mm[0467], rounder in mm5
-%macro ROW_TRANSFORM  1
-    pxor        mm7, mm7
-    mova        mm0, [%1+ 0*8]
-    mova        mm1, [%1+ 1*8]
-    mova        mm2, [%1+ 2*8]
-    mova        mm3, [%1+ 3*8]
-    mova  [%1+ 0*8], mm7
-    mova  [%1+ 1*8], mm7
-    mova  [%1+ 2*8], mm7
-    mova  [%1+ 3*8], mm7
-    mova        mm4, mm0
-    mova        mm6, [pw_row_coeffs+ 0]
-    paddsw      mm0, mm2                ; b0 + b2
-    psubsw      mm4, mm2                ; b0 - b2
-    pmullw      mm0, mm6                ; *13 = z0
-    pmullw      mm4, mm6                ; *13 = z1
-    mova        mm5, mm1
-    pmullw      mm1, [pw_row_coeffs+ 8] ; b1*17
-    pmullw      mm5, [pw_row_coeffs+16] ; b1* 7
-    mova        mm7, mm3
-    pmullw      mm3, [pw_row_coeffs+ 8] ; b3*17
-    pmullw      mm7, [pw_row_coeffs+16] ; b3* 7
-    paddsw      mm1, mm7                ; z3 = b1*17 + b3* 7
-    psubsw      mm5, mm3                ; z2 = b1* 7 - b3*17
-    mova        mm7, mm0
-    mova        mm6, mm4
-    paddsw      mm0, mm1                ; z0 + z3
-    psubsw      mm7, mm1                ; z0 - z3
-    paddsw      mm4, mm5                ; z1 + z2
-    psubsw      mm6, mm5                ; z1 - z2
-    mova        mm5, [pd_512]           ; 0x200
-%endmacro
-
-; ff_rv34_idct_add_mmxext(uint8_t *dst, ptrdiff_t stride, int16_t *block);
-%macro COL_TRANSFORM  4
-    pshufw      mm3, %2, 0xDD        ; col. 1,3,1,3
-    pshufw       %2, %2, 0x88        ; col. 0,2,0,2
-    pmaddwd      %2, %3              ; 13*c0+13*c2 | 13*c0-13*c2 = z0 | z1
-    pmaddwd     mm3, %4              ; 17*c1+ 7*c3 |  7*c1-17*c3 = z3 | z2
-    paddd        %2, mm5
-    pshufw      mm1,  %2, 01001110b  ;    z1 | z0
-    pshufw      mm2, mm3, 01001110b  ;    z2 | z3
-    paddd        %2, mm3             ; z0+z3 | z1+z2
-    psubd       mm1, mm2             ; z1-z2 | z0-z3
-    movd        mm3, %1
-    psrad        %2, 10
-    pxor        mm2, mm2
-    psrad       mm1, 10
-    punpcklbw   mm3, mm2
-    packssdw     %2, mm1
-    paddw        %2, mm3
-    packuswb     %2, %2
-    movd         %1, %2
-%endmacro
-INIT_MMX mmxext
-cglobal rv34_idct_add, 3,3,0, d, s, b
-    ROW_TRANSFORM       bq
-    COL_TRANSFORM     [dq], mm0, [pw_col_coeffs+ 0], [pw_col_coeffs+ 8]
-    mova               mm0, [pw_col_coeffs+ 0]
-    COL_TRANSFORM  [dq+sq], mm4, mm0, [pw_col_coeffs+ 8]
-    mova               mm4, [pw_col_coeffs+ 8]
-    lea                 dq, [dq + 2*sq]
-    COL_TRANSFORM     [dq], mm6, mm0, mm4
-    COL_TRANSFORM  [dq+sq], mm7, mm0, mm4
-    ret
-
-; ff_rv34_idct_dc_add_sse4(uint8_t *dst, int stride, int dc);
-%macro RV34_IDCT_DC_ADD 0
-cglobal rv34_idct_dc_add, 3, 3, 6
-    ; load data
-    IDCT_DC_ROUND r2
-    pxor       m1, m1
-
-    ; calculate DC
-    movd       m0, r2d
-    lea        r2, [r0+r1*2]
-    movd       m2, [r0]
-    movd       m3, [r0+r1]
-    pshuflw    m0, m0, 0
-    movd       m4, [r2]
-    movd       m5, [r2+r1]
-    punpcklqdq m0, m0
-    punpckldq  m2, m3
-    punpckldq  m4, m5
-    punpcklbw  m2, m1
-    punpcklbw  m4, m1
-    paddw      m2, m0
-    paddw      m4, m0
-    packuswb   m2, m4
-    movd      [r0], m2
-%if cpuflag(sse4)
-    pextrd [r0+r1], m2, 1
-    pextrd    [r2], m2, 2
-    pextrd [r2+r1], m2, 3
-%else
-    psrldq     m2, 4
-    movd   [r0+r1], m2
-    psrldq     m2, 4
-    movd      [r2], m2
-    psrldq     m2, 4
-    movd   [r2+r1], m2
-%endif
-    RET
-%endmacro
-
-INIT_XMM sse2
-RV34_IDCT_DC_ADD
-INIT_XMM sse4
-RV34_IDCT_DC_ADD
diff -uparN ffmpeg-4.1/libavcodec/x86/rv40dsp.asm ffmpeg-y/libavcodec/x86/rv40dsp.asm
--- ffmpeg-4.1/libavcodec/x86/rv40dsp.asm	2018-11-03 08:17:29.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/rv40dsp.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,500 +0,0 @@
-;******************************************************************************
-;* MMX/SSE2-optimized functions for the RV40 decoder
-;* Copyright (c) 2010 Ronald S. Bultje <rsbultje@gmail.com>
-;* Copyright (c) 2010 Fiona Glaser <fiona@x264.com>
-;* Copyright (C) 2012 Christophe Gisquet <christophe.gisquet@gmail.com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-
-pw_1024:   times 8 dw 1 << (16 - 6) ; pw_1024
-
-sixtap_filter_hb_m:  times 8 db   1, -5
-                     times 8 db  52, 20
-                     ; multiplied by 2 to have the same shift
-                     times 8 db   2, -10
-                     times 8 db  40,  40
-                     ; back to normal
-                     times 8 db   1, -5
-                     times 8 db  20, 52
-
-sixtap_filter_v_m:   times 8 dw   1
-                     times 8 dw  -5
-                     times 8 dw  52
-                     times 8 dw  20
-                     ; multiplied by 2 to have the same shift
-                     times 8 dw   2
-                     times 8 dw -10
-                     times 8 dw  40
-                     times 8 dw  40
-                     ; back to normal
-                     times 8 dw   1
-                     times 8 dw  -5
-                     times 8 dw  20
-                     times 8 dw  52
-
-%ifdef PIC
-%define sixtap_filter_hw   picregq
-%define sixtap_filter_hb   picregq
-%define sixtap_filter_v    picregq
-%define npicregs 1
-%else
-%define sixtap_filter_hw   sixtap_filter_hw_m
-%define sixtap_filter_hb   sixtap_filter_hb_m
-%define sixtap_filter_v    sixtap_filter_v_m
-%define npicregs 0
-%endif
-
-filter_h6_shuf1: db 0, 1, 1, 2, 2, 3, 3, 4, 4, 5,  5, 6,  6,  7,  7,  8
-filter_h6_shuf2: db 2, 3, 3, 4, 4, 5, 5, 6, 6, 7,  7, 8,  8,  9,  9, 10
-filter_h6_shuf3: db 5, 4, 6, 5, 7, 6, 8, 7, 9, 8, 10, 9, 11, 10, 12, 11
-
-cextern  pw_32
-cextern  pw_16
-cextern  pw_512
-
-SECTION .text
-
-;-----------------------------------------------------------------------------
-; subpel MC functions:
-;
-; void ff_[put|rv40]_rv40_qpel_[h|v]_<opt>(uint8_t *dst, int deststride,
-;                                          uint8_t *src, int srcstride,
-;                                          int len, int m);
-;----------------------------------------------------------------------
-%macro LOAD  2
-%if WIN64
-   movsxd   %1q, %1d
-%endif
-%ifdef PIC
-   add      %1q, picregq
-%else
-   add      %1q, %2
-%endif
-%endmacro
-
-%macro STORE 3
-%ifidn %3, avg
-    movh      %2, [dstq]
-%endif
-    packuswb  %1, %1
-%ifidn %3, avg
-    PAVGB     %1, %2
-%endif
-    movh  [dstq], %1
-%endmacro
-
-%macro FILTER_V 1
-cglobal %1_rv40_qpel_v, 6,6+npicregs,12, dst, dststride, src, srcstride, height, my, picreg
-%ifdef PIC
-    lea  picregq, [sixtap_filter_v_m]
-%endif
-    pxor      m7, m7
-    LOAD      my, sixtap_filter_v
-
-    ; read 5 lines
-    sub     srcq, srcstrideq
-    sub     srcq, srcstrideq
-    movh      m0, [srcq]
-    movh      m1, [srcq+srcstrideq]
-    movh      m2, [srcq+srcstrideq*2]
-    lea     srcq, [srcq+srcstrideq*2]
-    add     srcq, srcstrideq
-    movh      m3, [srcq]
-    movh      m4, [srcq+srcstrideq]
-    punpcklbw m0, m7
-    punpcklbw m1, m7
-    punpcklbw m2, m7
-    punpcklbw m3, m7
-    punpcklbw m4, m7
-
-%ifdef m8
-    mova      m8, [myq+ 0]
-    mova      m9, [myq+16]
-    mova     m10, [myq+32]
-    mova     m11, [myq+48]
-%define COEFF05  m8
-%define COEFF14  m9
-%define COEFF2   m10
-%define COEFF3   m11
-%else
-%define COEFF05  [myq+ 0]
-%define COEFF14  [myq+16]
-%define COEFF2   [myq+32]
-%define COEFF3   [myq+48]
-%endif
-.nextrow:
-    mova      m6, m1
-    movh      m5, [srcq+2*srcstrideq]      ; read new row
-    paddw     m6, m4
-    punpcklbw m5, m7
-    pmullw    m6, COEFF14
-    paddw     m0, m5
-    pmullw    m0, COEFF05
-    paddw     m6, m0
-    mova      m0, m1
-    paddw     m6, [pw_32]
-    mova      m1, m2
-    pmullw    m2, COEFF2
-    paddw     m6, m2
-    mova      m2, m3
-    pmullw    m3, COEFF3
-    paddw     m6, m3
-
-    ; round/clip/store
-    mova      m3, m4
-    psraw     m6, 6
-    mova      m4, m5
-    STORE     m6, m5, %1
-
-    ; go to next line
-    add     dstq, dststrideq
-    add     srcq, srcstrideq
-    dec  heightd                           ; next row
-    jg .nextrow
-    REP_RET
-%endmacro
-
-%macro FILTER_H  1
-cglobal %1_rv40_qpel_h, 6, 6+npicregs, 12, dst, dststride, src, srcstride, height, mx, picreg
-%ifdef PIC
-    lea  picregq, [sixtap_filter_v_m]
-%endif
-    pxor      m7, m7
-    LOAD      mx, sixtap_filter_v
-    mova      m6, [pw_32]
-%ifdef m8
-    mova      m8, [mxq+ 0]
-    mova      m9, [mxq+16]
-    mova     m10, [mxq+32]
-    mova     m11, [mxq+48]
-%define COEFF05  m8
-%define COEFF14  m9
-%define COEFF2   m10
-%define COEFF3   m11
-%else
-%define COEFF05  [mxq+ 0]
-%define COEFF14  [mxq+16]
-%define COEFF2   [mxq+32]
-%define COEFF3   [mxq+48]
-%endif
-.nextrow:
-    movq      m0, [srcq-2]
-    movq      m5, [srcq+3]
-    movq      m1, [srcq-1]
-    movq      m4, [srcq+2]
-    punpcklbw m0, m7
-    punpcklbw m5, m7
-    punpcklbw m1, m7
-    punpcklbw m4, m7
-    movq      m2, [srcq-0]
-    movq      m3, [srcq+1]
-    paddw     m0, m5
-    paddw     m1, m4
-    punpcklbw m2, m7
-    punpcklbw m3, m7
-    pmullw    m0, COEFF05
-    pmullw    m1, COEFF14
-    pmullw    m2, COEFF2
-    pmullw    m3, COEFF3
-    paddw     m0, m6
-    paddw     m1, m2
-    paddw     m0, m3
-    paddw     m0, m1
-    psraw     m0, 6
-    STORE     m0, m1, %1
-
-    ; go to next line
-    add     dstq, dststrideq
-    add     srcq, srcstrideq
-    dec  heightd            ; next row
-    jg .nextrow
-    REP_RET
-%endmacro
-
-%if ARCH_X86_32
-INIT_MMX  mmx
-FILTER_V  put
-FILTER_H  put
-
-INIT_MMX  mmxext
-FILTER_V  avg
-FILTER_H  avg
-
-INIT_MMX  3dnow
-FILTER_V  avg
-FILTER_H  avg
-%endif
-
-INIT_XMM  sse2
-FILTER_H  put
-FILTER_H  avg
-FILTER_V  put
-FILTER_V  avg
-
-%macro FILTER_SSSE3 1
-cglobal %1_rv40_qpel_v, 6,6+npicregs,8, dst, dststride, src, srcstride, height, my, picreg
-%ifdef PIC
-    lea  picregq, [sixtap_filter_hb_m]
-%endif
-
-    ; read 5 lines
-    sub     srcq, srcstrideq
-    LOAD      my, sixtap_filter_hb
-    sub     srcq, srcstrideq
-    movh      m0, [srcq]
-    movh      m1, [srcq+srcstrideq]
-    movh      m2, [srcq+srcstrideq*2]
-    lea     srcq, [srcq+srcstrideq*2]
-    add     srcq, srcstrideq
-    mova      m5, [myq]
-    movh      m3, [srcq]
-    movh      m4, [srcq+srcstrideq]
-    lea     srcq, [srcq+2*srcstrideq]
-
-.nextrow:
-    mova      m6, m2
-    punpcklbw m0, m1
-    punpcklbw m6, m3
-    pmaddubsw m0, m5
-    pmaddubsw m6, [myq+16]
-    movh      m7, [srcq]      ; read new row
-    paddw     m6, m0
-    mova      m0, m1
-    mova      m1, m2
-    mova      m2, m3
-    mova      m3, m4
-    mova      m4, m7
-    punpcklbw m7, m3
-    pmaddubsw m7, m5
-    paddw     m6, m7
-    pmulhrsw  m6, [pw_512]
-    STORE     m6, m7, %1
-
-    ; go to next line
-    add     dstq, dststrideq
-    add     srcq, srcstrideq
-    dec       heightd                          ; next row
-    jg       .nextrow
-    REP_RET
-
-cglobal %1_rv40_qpel_h, 6,6+npicregs,8, dst, dststride, src, srcstride, height, mx, picreg
-%ifdef PIC
-    lea  picregq, [sixtap_filter_hb_m]
-%endif
-    mova      m3, [filter_h6_shuf2]
-    mova      m4, [filter_h6_shuf3]
-    LOAD      mx, sixtap_filter_hb
-    mova      m5, [mxq] ; set up 6tap filter in bytes
-    mova      m6, [mxq+16]
-    mova      m7, [filter_h6_shuf1]
-
-.nextrow:
-    movu      m0, [srcq-2]
-    mova      m1, m0
-    mova      m2, m0
-    pshufb    m0, m7
-    pshufb    m1, m3
-    pshufb    m2, m4
-    pmaddubsw m0, m5
-    pmaddubsw m1, m6
-    pmaddubsw m2, m5
-    paddw     m0, m1
-    paddw     m0, m2
-    pmulhrsw  m0, [pw_512]
-    STORE     m0, m1, %1
-
-    ; go to next line
-    add     dstq, dststrideq
-    add     srcq, srcstrideq
-    dec  heightd            ; next row
-    jg .nextrow
-    REP_RET
-%endmacro
-
-INIT_XMM ssse3
-FILTER_SSSE3  put
-FILTER_SSSE3  avg
-
-; %1=5-bit weights?, %2=dst %3=src1 %4=src3 %5=stride if SSE2
-%macro RV40_WCORE  4-5
-    movh       m4, [%3 + r6 + 0]
-    movh       m5, [%4 + r6 + 0]
-%if %0 == 4
-%define OFFSET r6 + mmsize / 2
-%else
-    ; 8x8 block and SSE2, stride was provided
-%define OFFSET r6
-    add        r6, r5
-%endif
-    movh       m6, [%3 + OFFSET]
-    movh       m7, [%4 + OFFSET]
-
-%if %1 == 0
-    ; 14-bit weights
-    punpcklbw  m4, m0
-    punpcklbw  m5, m0
-    punpcklbw  m6, m0
-    punpcklbw  m7, m0
-
-    psllw      m4, 7
-    psllw      m5, 7
-    psllw      m6, 7
-    psllw      m7, 7
-    pmulhw     m4, m3
-    pmulhw     m5, m2
-    pmulhw     m6, m3
-    pmulhw     m7, m2
-
-    paddw      m4, m5
-    paddw      m6, m7
-%else
-    ; 5-bit weights
-%if cpuflag(ssse3)
-    punpcklbw  m4, m5
-    punpcklbw  m6, m7
-
-    pmaddubsw  m4, m3
-    pmaddubsw  m6, m3
-%else
-    punpcklbw  m4, m0
-    punpcklbw  m5, m0
-    punpcklbw  m6, m0
-    punpcklbw  m7, m0
-
-    pmullw     m4, m3
-    pmullw     m5, m2
-    pmullw     m6, m3
-    pmullw     m7, m2
-    paddw      m4, m5
-    paddw      m6, m7
-%endif
-
-%endif
-
-    ; bias and shift down
-%if cpuflag(ssse3)
-    pmulhrsw   m4, m1
-    pmulhrsw   m6, m1
-%else
-    paddw      m4, m1
-    paddw      m6, m1
-    psrlw      m4, 5
-    psrlw      m6, 5
-%endif
-
-    packuswb   m4, m6
-%if %0 == 5
-    ; Only called for 8x8 blocks and SSE2
-    sub        r6, r5
-    movh       [%2 + r6], m4
-    add        r6, r5
-    movhps     [%2 + r6], m4
-%else
-    mova       [%2 + r6], m4
-%endif
-%endmacro
-
-
-%macro MAIN_LOOP   2
-%if mmsize == 8
-    RV40_WCORE %2, r0, r1, r2
-%if %1 == 16
-    RV40_WCORE %2, r0 + 8, r1 + 8, r2 + 8
-%endif
-
-    ; Prepare for next loop
-    add        r6, r5
-%else
-%ifidn %1, 8
-    RV40_WCORE %2, r0, r1, r2, r5
-    ; Prepare 2 next lines
-    add        r6, r5
-%else
-    RV40_WCORE %2, r0, r1, r2
-    ; Prepare single next line
-    add        r6, r5
-%endif
-%endif
-
-%endmacro
-
-; void ff_rv40_weight_func_%1(uint8_t *dst, uint8_t *src1, uint8_t *src2, int w1, int w2, int stride)
-; %1=size  %2=num of xmm regs
-; The weights are FP0.14 notation of fractions depending on pts.
-; For timebases without rounding error (i.e. PAL), the fractions
-; can be simplified, and several operations can be avoided.
-; Therefore, we check here whether they are multiples of 2^9 for
-; those simplifications to occur.
-%macro RV40_WEIGHT  3
-cglobal rv40_weight_func_%1_%2, 6, 7, 8
-%if cpuflag(ssse3)
-    mova       m1, [pw_1024]
-%else
-    mova       m1, [pw_16]
-%endif
-    pxor       m0, m0
-    ; Set loop counter and increments
-    mov        r6, r5
-    shl        r6, %3
-    add        r0, r6
-    add        r1, r6
-    add        r2, r6
-    neg        r6
-
-    movd       m2, r3d
-    movd       m3, r4d
-%ifidn %1,rnd
-%define  RND   0
-    SPLATW     m2, m2
-%else
-%define  RND   1
-%if cpuflag(ssse3)
-    punpcklbw  m3, m2
-%else
-    SPLATW     m2, m2
-%endif
-%endif
-    SPLATW     m3, m3
-
-.loop:
-    MAIN_LOOP  %2, RND
-    jnz        .loop
-    REP_RET
-%endmacro
-
-INIT_MMX mmxext
-RV40_WEIGHT   rnd,    8, 3
-RV40_WEIGHT   rnd,   16, 4
-RV40_WEIGHT   nornd,  8, 3
-RV40_WEIGHT   nornd, 16, 4
-
-INIT_XMM sse2
-RV40_WEIGHT   rnd,    8, 3
-RV40_WEIGHT   rnd,   16, 4
-RV40_WEIGHT   nornd,  8, 3
-RV40_WEIGHT   nornd, 16, 4
-
-INIT_XMM ssse3
-RV40_WEIGHT   rnd,    8, 3
-RV40_WEIGHT   rnd,   16, 4
-RV40_WEIGHT   nornd,  8, 3
-RV40_WEIGHT   nornd, 16, 4
diff -uparN ffmpeg-4.1/libavcodec/x86/sbcdsp.asm ffmpeg-y/libavcodec/x86/sbcdsp.asm
--- ffmpeg-4.1/libavcodec/x86/sbcdsp.asm	2018-11-03 08:17:29.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/sbcdsp.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,168 +0,0 @@
-;******************************************************************************
-;* SIMD optimized SBC encoder DSP functions
-;*
-;* Copyright (C) 2017  Aurelien Jacobs <aurel@gnuage.org>
-;* Copyright (C) 2008-2010  Nokia Corporation
-;* Copyright (C) 2004-2010  Marcel Holtmann <marcel@holtmann.org>
-;* Copyright (C) 2004-2005  Henryk Ploetz <henryk@ploetzli.ch>
-;* Copyright (C) 2005-2006  Brad Midgley <bmidgley@xmission.com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-
-scale_mask: times 2 dd 0x8000    ; 1 << (SBC_PROTO_FIXED_SCALE - 1)
-
-SECTION .text
-
-%macro NIDN 3
-%ifnidn %2, %3
-    %1            %2, %3
-%endif
-%endmacro
-
-%macro ANALYZE_MAC 9 ; out1, out2, in1, in2, tmp1, tmp2, add1, add2, offset
-    NIDN movq,    %5, %3
-    NIDN movq,    %6, %4
-    pmaddwd       %5, [constsq+%9]
-    pmaddwd       %6, [constsq+%9+8]
-    NIDN paddd,   %1, %7
-    NIDN paddd,   %2, %8
-%endmacro
-
-%macro ANALYZE_MAC_IN 7 ; out1, out2, tmp1, tmp2, add1, add2, offset
-    ANALYZE_MAC   %1, %2, [inq+%7], [inq+%7+8], %3, %4, %5, %6, %7
-%endmacro
-
-%macro ANALYZE_MAC_REG 7 ; out1, out2, in, tmp1, tmp2, offset, pack
-%ifidn %7, pack
-    psrad         %3, 16    ; SBC_PROTO_FIXED_SCALE
-    packssdw      %3, %3
-%endif
-    ANALYZE_MAC   %1, %2, %3, %3, %4, %5, %4, %5, %6
-%endmacro
-
-;*******************************************************************
-;void ff_sbc_analyze_4(const int16_t *in, int32_t *out, const int16_t *consts);
-;*******************************************************************
-INIT_MMX mmx
-cglobal sbc_analyze_4, 3, 3, 4, in, out, consts
-    ANALYZE_MAC_IN   m0, m1, m0, m1, [scale_mask], [scale_mask], 0
-    ANALYZE_MAC_IN   m0, m1, m2, m3, m2, m3, 16
-    ANALYZE_MAC_IN   m0, m1, m2, m3, m2, m3, 32
-    ANALYZE_MAC_IN   m0, m1, m2, m3, m2, m3, 48
-    ANALYZE_MAC_IN   m0, m1, m2, m3, m2, m3, 64
-
-    ANALYZE_MAC_REG  m0, m2, m0, m0, m2, 80, pack
-    ANALYZE_MAC_REG  m0, m2, m1, m1, m3, 96, pack
-
-    movq          [outq  ], m0
-    movq          [outq+8], m2
-
-    RET
-
-
-;*******************************************************************
-;void ff_sbc_analyze_8(const int16_t *in, int32_t *out, const int16_t *consts);
-;*******************************************************************
-INIT_MMX mmx
-cglobal sbc_analyze_8, 3, 3, 4, in, out, consts
-    ANALYZE_MAC_IN   m0, m1, m0, m1, [scale_mask], [scale_mask],  0
-    ANALYZE_MAC_IN   m2, m3, m2, m3, [scale_mask], [scale_mask], 16
-    ANALYZE_MAC_IN   m0, m1, m4, m5, m4, m5,  32
-    ANALYZE_MAC_IN   m2, m3, m6, m7, m6, m7,  48
-    ANALYZE_MAC_IN   m0, m1, m4, m5, m4, m5,  64
-    ANALYZE_MAC_IN   m2, m3, m6, m7, m6, m7,  80
-    ANALYZE_MAC_IN   m0, m1, m4, m5, m4, m5,  96
-    ANALYZE_MAC_IN   m2, m3, m6, m7, m6, m7, 112
-    ANALYZE_MAC_IN   m0, m1, m4, m5, m4, m5, 128
-    ANALYZE_MAC_IN   m2, m3, m6, m7, m6, m7, 144
-
-    ANALYZE_MAC_REG  m4, m5, m0, m4, m5, 160, pack
-    ANALYZE_MAC_REG  m4, m5, m1, m6, m7, 192, pack
-    ANALYZE_MAC_REG  m4, m5, m2, m6, m7, 224, pack
-    ANALYZE_MAC_REG  m4, m5, m3, m6, m7, 256, pack
-
-    movq          [outq  ], m4
-    movq          [outq+8], m5
-
-    ANALYZE_MAC_REG  m0, m5, m0, m0, m5, 176, no
-    ANALYZE_MAC_REG  m0, m5, m1, m1, m7, 208, no
-    ANALYZE_MAC_REG  m0, m5, m2, m2, m7, 240, no
-    ANALYZE_MAC_REG  m0, m5, m3, m3, m7, 272, no
-
-    movq          [outq+16], m0
-    movq          [outq+24], m5
-
-    RET
-
-
-;*******************************************************************
-;void ff_sbc_calc_scalefactors(int32_t sb_sample_f[16][2][8],
-;                              uint32_t scale_factor[2][8],
-;                              int blocks, int channels, int subbands)
-;*******************************************************************
-INIT_MMX mmx
-cglobal sbc_calc_scalefactors, 5, 7, 4, sb_sample_f, scale_factor, blocks, channels, subbands, ptr, blk
-    ; subbands = 4 * subbands * channels
-    movq          m3, [scale_mask]
-    shl           subbandsd, 2
-    cmp           channelsd, 2
-    jl            .loop_1
-    shl           subbandsd, 1
-
-.loop_1:
-    sub           subbandsq, 8
-    lea           ptrq, [sb_sample_fq + subbandsq]
-
-    ; blk = (blocks - 1) * 64;
-    lea           blkq, [blocksq - 1]
-    shl           blkd, 6
-
-    movq          m0, m3
-.loop_2:
-    movq          m1, [ptrq+blkq]
-    pxor          m2, m2
-    pcmpgtd       m1, m2
-    paddd         m1, [ptrq+blkq]
-    pcmpgtd       m2, m1
-    pxor          m1, m2
-
-    por           m0, m1
-
-    sub           blkq, 64
-    jns           .loop_2
-
-    movd          blkd, m0
-    psrlq         m0,   32
-    bsr           blkd, blkd
-    sub           blkd, 15    ; SCALE_OUT_BITS
-    mov           [scale_factorq + subbandsq], blkd
-
-    movd          blkd, m0
-    bsr           blkd, blkd
-    sub           blkd, 15    ; SCALE_OUT_BITS
-    mov           [scale_factorq + subbandsq + 4], blkd
-
-    cmp           subbandsq, 0
-    jg            .loop_1
-
-    emms
-    RET
diff -uparN ffmpeg-4.1/libavcodec/x86/sbrdsp.asm ffmpeg-y/libavcodec/x86/sbrdsp.asm
--- ffmpeg-4.1/libavcodec/x86/sbrdsp.asm	2018-11-02 02:34:25.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/sbrdsp.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,548 +0,0 @@
-;******************************************************************************
-;* AAC Spectral Band Replication decoding functions
-;* Copyright (C) 2012 Christophe Gisquet <christophe.gisquet@gmail.com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-; mask equivalent for multiply by -1.0 1.0
-ps_mask         times 2 dd 1<<31, 0
-ps_mask2        times 2 dd 0, 1<<31
-ps_mask3        dd  0, 0, 0, 1<<31
-ps_noise0       times 2 dd  1.0,  0.0,
-ps_noise2       times 2 dd -1.0,  0.0
-ps_noise13      dd  0.0,  1.0, 0.0, -1.0
-                dd  0.0, -1.0, 0.0,  1.0
-                dd  0.0,  1.0, 0.0, -1.0
-cextern         sbr_noise_table
-cextern         ps_neg
-
-SECTION .text
-
-INIT_XMM sse
-cglobal sbr_sum_square, 2, 3, 6
-    mov        r2d, r1d
-    xorps       m0, m0
-    xorps       m1, m1
-    sar         r2, 3
-    jz          .prepare
-.loop:
-    movu        m2, [r0 +  0]
-    movu        m3, [r0 + 16]
-    movu        m4, [r0 + 32]
-    movu        m5, [r0 + 48]
-    mulps       m2, m2
-    mulps       m3, m3
-    mulps       m4, m4
-    mulps       m5, m5
-    addps       m0, m2
-    addps       m1, m3
-    addps       m0, m4
-    addps       m1, m5
-    add         r0, 64
-    dec         r2
-    jnz         .loop
-.prepare:
-    and         r1, 7
-    sar         r1, 1
-    jz          .end
-; len is a multiple of 2, thus there are at least 4 elements to process
-.endloop:
-    movu        m2, [r0]
-    add         r0, 16
-    mulps       m2, m2
-    dec         r1
-    addps       m0, m2
-    jnz         .endloop
-.end:
-    addps       m0, m1
-    movhlps     m2, m0
-    addps       m0, m2
-    movss       m1, m0
-    shufps      m0, m0, 1
-    addss       m0, m1
-%if ARCH_X86_64 == 0
-    movss       r0m,  m0
-    fld         dword r0m
-%endif
-    RET
-
-%define STEP  40*4*2
-cglobal sbr_hf_g_filt, 5, 6, 5
-    lea         r1, [r1 + 8*r4] ; offset by ixh elements into X_high
-    mov         r5, r3
-    and         r3, 0xFC
-    lea         r2, [r2 + r3*4]
-    lea         r0, [r0 + r3*8]
-    neg         r3
-    jz          .loop1
-.loop4:
-    movlps      m0, [r2 + 4*r3 + 0]
-    movlps      m1, [r2 + 4*r3 + 8]
-    movlps      m2, [r1 + 0*STEP]
-    movlps      m3, [r1 + 2*STEP]
-    movhps      m2, [r1 + 1*STEP]
-    movhps      m3, [r1 + 3*STEP]
-    unpcklps    m0, m0
-    unpcklps    m1, m1
-    mulps       m0, m2
-    mulps       m1, m3
-    movu        [r0 + 8*r3 +  0], m0
-    movu        [r0 + 8*r3 + 16], m1
-    add         r1, 4*STEP
-    add         r3, 4
-    jnz         .loop4
-    and         r5, 3 ; number of single element loops
-    jz          .end
-.loop1: ; element 0 and 1 can be computed at the same time
-    movss       m0, [r2]
-    movlps      m2, [r1]
-    unpcklps    m0, m0
-    mulps       m2, m0
-    movlps    [r0], m2
-    add         r0, 8
-    add         r2, 4
-    add         r1, STEP
-    dec         r5
-    jnz         .loop1
-.end:
-    RET
-
-; void ff_sbr_hf_gen_sse(float (*X_high)[2], const float (*X_low)[2],
-;                        const float alpha0[2], const float alpha1[2],
-;                        float bw, int start, int end)
-;
-cglobal sbr_hf_gen, 4,4,8, X_high, X_low, alpha0, alpha1, BW, S, E
-    ; load alpha factors
-%define bw m0
-%if ARCH_X86_64 == 0 || WIN64
-    movss      bw, BWm
-%endif
-    movlps     m2, [alpha1q]
-    movlps     m1, [alpha0q]
-    shufps     bw, bw, 0
-    mulps      m2, bw             ; (a1[0] a1[1])*bw
-    mulps      m1, bw             ; (a0[0] a0[1])*bw    = (a2 a3)
-    mulps      m2, bw             ; (a1[0] a1[1])*bw*bw = (a0 a1)
-    mova       m3, m1
-    mova       m4, m2
-
-    ; Set pointers
-%if ARCH_X86_64 == 0 || WIN64
-    ; start and end 6th and 7th args on stack
-    mov        r2d, Sm
-    mov        r3d, Em
-    DEFINE_ARGS X_high, X_low, start, end
-%else
-; BW does not actually occupy a register, so shift by 1
-    DEFINE_ARGS X_high, X_low, alpha0, alpha1, start, end
-    movsxd  startq, startd
-    movsxd    endq, endd
-%endif
-    sub     startq, endq         ; neg num of loops
-    lea    X_highq, [X_highq + endq*2*4]
-    lea     X_lowq, [X_lowq  + endq*2*4 - 2*2*4]
-    shl     startq, 3            ; offset from num loops
-
-    mova        m0, [X_lowq + startq]
-    shufps      m3, m3, q1111
-    shufps      m4, m4, q1111
-    xorps       m3, [ps_mask]
-    shufps      m1, m1, q0000
-    shufps      m2, m2, q0000
-    xorps       m4, [ps_mask]
-.loop2:
-    movu        m7, [X_lowq + startq + 8]       ; BbCc
-    mova        m6, m0
-    mova        m5, m7
-    shufps      m0, m0, q2301                   ; aAbB
-    shufps      m7, m7, q2301                   ; bBcC
-    mulps       m0, m4
-    mulps       m7, m3
-    mulps       m6, m2
-    mulps       m5, m1
-    addps       m7, m0
-    mova        m0, [X_lowq + startq + 16]      ; CcDd
-    addps       m7, m0
-    addps       m6, m5
-    addps       m7, m6
-    mova  [X_highq + startq], m7
-    add     startq, 16
-    jnz         .loop2
-    RET
-
-cglobal sbr_sum64x5, 1,2,4,z
-    lea    r1q, [zq+ 256]
-.loop:
-    mova    m0, [zq+   0]
-    mova    m2, [zq+  16]
-    mova    m1, [zq+ 256]
-    mova    m3, [zq+ 272]
-    addps   m0, [zq+ 512]
-    addps   m2, [zq+ 528]
-    addps   m1, [zq+ 768]
-    addps   m3, [zq+ 784]
-    addps   m0, [zq+1024]
-    addps   m2, [zq+1040]
-    addps   m0, m1
-    addps   m2, m3
-    mova  [zq], m0
-    mova  [zq+16], m2
-    add     zq, 32
-    cmp     zq, r1q
-    jne  .loop
-    REP_RET
-
-INIT_XMM sse
-cglobal sbr_qmf_post_shuffle, 2,3,4,W,z
-    lea              r2q, [zq + (64-4)*4]
-    mova              m3, [ps_neg]
-.loop:
-    mova              m1, [zq]
-    xorps             m0, m3, [r2q]
-    shufps            m0, m0, m0, q0123
-    unpcklps          m2, m0, m1
-    unpckhps          m0, m0, m1
-    mova       [Wq +  0], m2
-    mova       [Wq + 16], m0
-    add               Wq, 32
-    sub              r2q, 16
-    add               zq, 16
-    cmp               zq, r2q
-    jl             .loop
-    REP_RET
-
-INIT_XMM sse
-cglobal sbr_neg_odd_64, 1,2,4,z
-    lea        r1q, [zq+256]
-.loop:
-    mova        m0, [zq+ 0]
-    mova        m1, [zq+16]
-    mova        m2, [zq+32]
-    mova        m3, [zq+48]
-    xorps       m0, [ps_mask2]
-    xorps       m1, [ps_mask2]
-    xorps       m2, [ps_mask2]
-    xorps       m3, [ps_mask2]
-    mova   [zq+ 0], m0
-    mova   [zq+16], m1
-    mova   [zq+32], m2
-    mova   [zq+48], m3
-    add         zq, 64
-    cmp         zq, r1q
-    jne      .loop
-    REP_RET
-
-; void ff_sbr_qmf_deint_bfly_sse2(float *v, const float *src0, const float *src1)
-%macro SBR_QMF_DEINT_BFLY  0
-cglobal sbr_qmf_deint_bfly, 3,5,8, v,src0,src1,vrev,c
-    mov               cq, 64*4-2*mmsize
-    lea            vrevq, [vq + 64*4]
-.loop:
-    mova              m0, [src0q+cq]
-    mova              m1, [src1q]
-    mova              m4, [src0q+cq+mmsize]
-    mova              m5, [src1q+mmsize]
-%if cpuflag(sse2)
-    pshufd            m2, m0, q0123
-    pshufd            m3, m1, q0123
-    pshufd            m6, m4, q0123
-    pshufd            m7, m5, q0123
-%else
-    shufps            m2, m0, m0, q0123
-    shufps            m3, m1, m1, q0123
-    shufps            m6, m4, m4, q0123
-    shufps            m7, m5, m5, q0123
-%endif
-    addps             m5, m2
-    subps             m0, m7
-    addps             m1, m6
-    subps             m4, m3
-    mova         [vrevq], m1
-    mova  [vrevq+mmsize], m5
-    mova         [vq+cq], m0
-    mova  [vq+cq+mmsize], m4
-    add            src1q, 2*mmsize
-    add            vrevq, 2*mmsize
-    sub               cq, 2*mmsize
-    jge            .loop
-    REP_RET
-%endmacro
-
-INIT_XMM sse
-SBR_QMF_DEINT_BFLY
-
-INIT_XMM sse2
-SBR_QMF_DEINT_BFLY
-
-INIT_XMM sse2
-cglobal sbr_qmf_pre_shuffle, 1,4,6,z
-%define OFFSET  (32*4-2*mmsize)
-    mov       r3q, OFFSET
-    lea       r1q, [zq + (32+1)*4]
-    lea       r2q, [zq + 64*4]
-    mova       m5, [ps_neg]
-.loop:
-    movu       m0, [r1q]
-    movu       m2, [r1q + mmsize]
-    movu       m1, [zq + r3q + 4 + mmsize]
-    movu       m3, [zq + r3q + 4]
-
-    pxor       m2, m5
-    pxor       m0, m5
-    pshufd     m2, m2, q0123
-    pshufd     m0, m0, q0123
-    SBUTTERFLY dq, 2, 3, 4
-    SBUTTERFLY dq, 0, 1, 4
-    mova  [r2q + 2*r3q + 0*mmsize], m2
-    mova  [r2q + 2*r3q + 1*mmsize], m3
-    mova  [r2q + 2*r3q + 2*mmsize], m0
-    mova  [r2q + 2*r3q + 3*mmsize], m1
-    add       r1q, 2*mmsize
-    sub       r3q, 2*mmsize
-    jge      .loop
-    movq       m2, [zq]
-    movq    [r2q], m2
-    REP_RET
-
-%ifdef PIC
-%define NREGS 1
-%if UNIX64
-%define NOISE_TABLE r6q ; r5q is m_max
-%else
-%define NOISE_TABLE r5q
-%endif
-%else
-%define NREGS 0
-%define NOISE_TABLE sbr_noise_table
-%endif
-
-%macro LOAD_NST  1
-%ifdef PIC
-    lea  NOISE_TABLE, [%1]
-    mova          m0, [kxq + NOISE_TABLE]
-%else
-    mova          m0, [kxq + %1]
-%endif
-%endmacro
-
-INIT_XMM sse2
-; sbr_hf_apply_noise_0(float (*Y)[2], const float *s_m,
-;                      const float *q_filt, int noise,
-;                      int kx, int m_max)
-cglobal sbr_hf_apply_noise_0, 5,5+NREGS+UNIX64,8, Y,s_m,q_filt,noise,kx,m_max
-    mova       m0, [ps_noise0]
-    jmp apply_noise_main
-
-; sbr_hf_apply_noise_1(float (*Y)[2], const float *s_m,
-;                      const float *q_filt, int noise,
-;                      int kx, int m_max)
-cglobal sbr_hf_apply_noise_1, 5,5+NREGS+UNIX64,8, Y,s_m,q_filt,noise,kx,m_max
-    and       kxq, 1
-    shl       kxq, 4
-    LOAD_NST  ps_noise13
-    jmp apply_noise_main
-
-; sbr_hf_apply_noise_2(float (*Y)[2], const float *s_m,
-;                      const float *q_filt, int noise,
-;                      int kx, int m_max)
-cglobal sbr_hf_apply_noise_2, 5,5+NREGS+UNIX64,8, Y,s_m,q_filt,noise,kx,m_max
-    mova       m0, [ps_noise2]
-    jmp apply_noise_main
-
-; sbr_hf_apply_noise_3(float (*Y)[2], const float *s_m,
-;                      const float *q_filt, int noise,
-;                      int kx, int m_max)
-cglobal sbr_hf_apply_noise_3, 5,5+NREGS+UNIX64,8, Y,s_m,q_filt,noise,kx,m_max
-    and       kxq, 1
-    shl       kxq, 4
-    LOAD_NST  ps_noise13+16
-
-apply_noise_main:
-%if ARCH_X86_64 == 0 || WIN64
-    mov       kxd, m_maxm
-    DEFINE_ARGS Y, s_m, q_filt, noise, count
-%else
-    DEFINE_ARGS Y, s_m, q_filt, noise, kx, count
-%endif
-    movsxdifnidn    noiseq, noised
-    dec    noiseq
-    shl    countd, 2
-%ifdef PIC
-    lea NOISE_TABLE, [sbr_noise_table]
-%endif
-    lea        Yq, [Yq + 2*countq]
-    add      s_mq, countq
-    add   q_filtq, countq
-    shl    noiseq, 3
-    pxor       m5, m5
-    neg    countq
-.loop:
-    mova       m1, [q_filtq + countq]
-    movu       m3, [noiseq + NOISE_TABLE + 1*mmsize]
-    movu       m4, [noiseq + NOISE_TABLE + 2*mmsize]
-    add    noiseq, 2*mmsize
-    and    noiseq, 0x1ff<<3
-    punpckhdq  m2, m1, m1
-    punpckldq  m1, m1
-    mulps      m1, m3 ; m2 = q_filt[m] * ff_sbr_noise_table[noise]
-    mulps      m2, m4 ; m2 = q_filt[m] * ff_sbr_noise_table[noise]
-    mova       m3, [s_mq + countq]
-    ; TODO: replace by a vpermd in AVX2
-    punpckhdq  m4, m3, m3
-    punpckldq  m3, m3
-    pcmpeqd    m6, m3, m5 ; m6 == 0
-    pcmpeqd    m7, m4, m5 ; m7 == 0
-    mulps      m3, m0 ; s_m[m] * phi_sign
-    mulps      m4, m0 ; s_m[m] * phi_sign
-    pand       m1, m6
-    pand       m2, m7
-    movu       m6, [Yq + 2*countq]
-    movu       m7, [Yq + 2*countq + mmsize]
-    addps      m3, m1
-    addps      m4, m2
-    addps      m6, m3
-    addps      m7, m4
-    movu    [Yq + 2*countq], m6
-    movu    [Yq + 2*countq + mmsize], m7
-    add    countq, mmsize
-    jl      .loop
-    RET
-
-INIT_XMM sse
-cglobal sbr_qmf_deint_neg, 2,4,4,v,src,vrev,c
-%define COUNT  32*4
-%define OFFSET 32*4
-    mov        cq, -COUNT
-    lea     vrevq, [vq + OFFSET + COUNT]
-    add        vq, OFFSET-mmsize
-    add      srcq, 2*COUNT
-    mova       m3, [ps_neg]
-.loop:
-    mova       m0, [srcq + 2*cq + 0*mmsize]
-    mova       m1, [srcq + 2*cq + 1*mmsize]
-    shufps     m2, m0, m1, q2020
-    shufps     m1, m0, q1313
-    xorps      m2, m3
-    mova     [vq], m1
-    mova  [vrevq + cq], m2
-    sub        vq, mmsize
-    add        cq, mmsize
-    jl      .loop
-    REP_RET
-
-%macro SBR_AUTOCORRELATE 0
-cglobal sbr_autocorrelate, 2,3,8,32, x, phi, cnt
-    mov   cntq, 37*8
-    add     xq, cntq
-    neg   cntq
-
-%if cpuflag(sse3)
-%define   MOVH  movsd
-    movddup m5, [xq+cntq]
-%else
-%define   MOVH  movlps
-    movlps  m5, [xq+cntq]
-    movlhps m5, m5
-%endif
-    MOVH    m7, [xq+cntq+8 ]
-    MOVH    m1, [xq+cntq+16]
-    shufps  m7, m7, q0110
-    shufps  m1, m1, q0110
-    mulps   m3, m5, m7   ;              x[0][0] * x[1][0], x[0][1] * x[1][1], x[0][0] * x[1][1], x[0][1] * x[1][0]
-    mulps   m4, m5, m5   ;              x[0][0] * x[0][0], x[0][1] * x[0][1];
-    mulps   m5, m1       ; real_sum2  = x[0][0] * x[2][0], x[0][1] * x[2][1]; imag_sum2 = x[0][0] * x[2][1], x[0][1] * x[2][0]
-    movaps  [rsp   ], m3
-    movaps  [rsp+16], m4
-    add   cntq, 8
-
-    MOVH    m2, [xq+cntq+16]
-    movlhps m7, m7
-    shufps  m2, m2, q0110
-    mulps   m6, m7, m1   ; real_sum1  = x[1][0] * x[2][0], x[1][1] * x[2][1]; imag_sum1 += x[1][0] * x[2][1], x[1][1] * x[2][0]
-    mulps   m4, m7, m2
-    mulps   m7, m7       ; real_sum0  = x[1][0] * x[1][0], x[1][1] * x[1][1];
-    addps   m5, m4       ; real_sum2 += x[1][0] * x[3][0], x[1][1] * x[3][1]; imag_sum2 += x[1][0] * x[3][1], x[1][1] * x[3][0]
-
-align 16
-.loop:
-    add   cntq, 8
-    MOVH    m0, [xq+cntq+16]
-    movlhps m1, m1
-    shufps  m0, m0, q0110
-    mulps   m3, m1, m2
-    mulps   m4, m1, m0
-    mulps   m1, m1
-    addps   m6, m3       ; real_sum1 += x[i][0] * x[i + 1][0], x[i][1] * x[i + 1][1]; imag_sum1 += x[i][0] * x[i + 1][1], x[i][1] * x[i + 1][0];
-    addps   m5, m4       ; real_sum2 += x[i][0] * x[i + 2][0], x[i][1] * x[i + 2][1]; imag_sum2 += x[i][0] * x[i + 2][1], x[i][1] * x[i + 2][0];
-    addps   m7, m1       ; real_sum0 += x[i][0] * x[i][0],     x[i][1] * x[i][1];
-    add   cntq, 8
-    MOVH    m1, [xq+cntq+16]
-    movlhps m2, m2
-    shufps  m1, m1, q0110
-    mulps   m3, m2, m0
-    mulps   m4, m2, m1
-    mulps   m2, m2
-    addps   m6, m3       ; real_sum1 += x[i][0] * x[i + 1][0], x[i][1] * x[i + 1][1]; imag_sum1 += x[i][0] * x[i + 1][1], x[i][1] * x[i + 1][0];
-    addps   m5, m4       ; real_sum2 += x[i][0] * x[i + 2][0], x[i][1] * x[i + 2][1]; imag_sum2 += x[i][0] * x[i + 2][1], x[i][1] * x[i + 2][0];
-    addps   m7, m2       ; real_sum0 += x[i][0] * x[i][0],     x[i][1] * x[i][1];
-    add   cntq, 8
-    MOVH    m2, [xq+cntq+16]
-    movlhps m0, m0
-    shufps  m2, m2, q0110
-    mulps   m3, m0, m1
-    mulps   m4, m0, m2
-    mulps   m0, m0
-    addps   m6, m3       ; real_sum1 += x[i][0] * x[i + 1][0], x[i][1] * x[i + 1][1]; imag_sum1 += x[i][0] * x[i + 1][1], x[i][1] * x[i + 1][0];
-    addps   m5, m4       ; real_sum2 += x[i][0] * x[i + 2][0], x[i][1] * x[i + 2][1]; imag_sum2 += x[i][0] * x[i + 2][1], x[i][1] * x[i + 2][0];
-    addps   m7, m0       ; real_sum0 += x[i][0] * x[i][0],     x[i][1] * x[i][1];
-    jl .loop
-
-    movlhps m1, m1
-    mulps   m2, m1
-    mulps   m1, m1
-    addps   m2, m6       ; real_sum1 + x[38][0] * x[39][0], x[38][1] * x[39][1]; imag_sum1 + x[38][0] * x[39][1], x[38][1] * x[39][0];
-    addps   m1, m7       ; real_sum0 + x[38][0] * x[38][0], x[38][1] * x[38][1];
-    addps   m6, [rsp   ] ; real_sum1 + x[ 0][0] * x[ 1][0], x[ 0][1] * x[ 1][1]; imag_sum1 + x[ 0][0] * x[ 1][1], x[ 0][1] * x[ 1][0];
-    addps   m7, [rsp+16] ; real_sum0 + x[ 0][0] * x[ 0][0], x[ 0][1] * x[ 0][1];
-
-    xorps   m2, [ps_mask3]
-    xorps   m5, [ps_mask3]
-    xorps   m6, [ps_mask3]
-    HADDPS  m2, m5, m3
-    HADDPS  m7, m6, m4
-%if cpuflag(sse3)
-    movshdup m0, m1
-%else
-    movss   m0, m1
-    shufps  m1, m1, q0001
-%endif
-    addss   m1, m0
-    movaps  [phiq     ], m2
-    movhps  [phiq+0x18], m7
-    movss   [phiq+0x28], m7
-    movss   [phiq+0x10], m1
-    RET
-%endmacro
-
-INIT_XMM sse
-SBR_AUTOCORRELATE
-INIT_XMM sse3
-SBR_AUTOCORRELATE
diff -uparN ffmpeg-4.1/libavcodec/x86/simple_idct10.asm ffmpeg-y/libavcodec/x86/simple_idct10.asm
--- ffmpeg-4.1/libavcodec/x86/simple_idct10.asm	2018-11-02 02:34:25.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/simple_idct10.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,205 +0,0 @@
-;******************************************************************************
-;* x86-SIMD-optimized IDCT for prores
-;* this is identical to "simple" IDCT written by Michael Niedermayer
-;* except for the clip range
-;*
-;* Copyright (c) 2011 Ronald S. Bultje <rsbultje@gmail.com>
-;* Copyright (c) 2015 Christophe Gisquet
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* 51, Inc., Foundation Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-%if ARCH_X86_64
-
-SECTION_RODATA
-
-cextern pw_2
-cextern pw_16
-cextern pw_32
-cextern pw_1023
-cextern pw_4095
-pd_round_11: times 4 dd 1<<(11-1)
-pd_round_12: times 4 dd 1<<(12-1)
-pd_round_15: times 4 dd 1<<(15-1)
-pd_round_19: times 4 dd 1<<(19-1)
-pd_round_20: times 4 dd 1<<(20-1)
-
-%macro CONST_DEC  3
-const %1
-times 4 dw %2, %3
-%endmacro
-
-%define W1sh2 22725 ; W1 = 90901 = 22725<<2 + 1
-%define W2sh2 21407 ; W2 = 85627 = 21407<<2 - 1
-%define W3sh2 19265 ; W3 = 77062 = 19265<<2 + 2
-%define W4sh2 16384 ; W4 = 65535 = 16384<<2 - 1
-%define W3sh2_lo 19266
-%define W4sh2_lo 16383
-%define W5sh2 12873 ; W5 = 51491 = 12873<<2 - 1
-%define W6sh2  8867 ; W6 = 35468 =  8867<<2
-%define W7sh2  4520 ; W7 = 18081 =  4520<<2 + 1
-
-CONST_DEC  w4_plus_w2_hi,   W4sh2, +W2sh2
-CONST_DEC  w4_min_w2_hi,    W4sh2, -W2sh2
-CONST_DEC  w4_plus_w6_hi,   W4sh2, +W6sh2
-CONST_DEC  w4_min_w6_hi,    W4sh2, -W6sh2
-CONST_DEC  w1_plus_w3_hi,   W1sh2, +W3sh2
-CONST_DEC  w3_min_w1_hi,    W3sh2, -W1sh2
-CONST_DEC  w7_plus_w3_hi,   W7sh2, +W3sh2
-CONST_DEC  w3_min_w7_hi,    W3sh2, -W7sh2
-CONST_DEC  w1_plus_w5,   W1sh2, +W5sh2
-CONST_DEC  w5_min_w1,    W5sh2, -W1sh2
-CONST_DEC  w5_plus_w7,   W5sh2, +W7sh2
-CONST_DEC  w7_min_w5,    W7sh2, -W5sh2
-CONST_DEC  w4_plus_w2_lo,   W4sh2_lo, +W2sh2
-CONST_DEC  w4_min_w2_lo,    W4sh2_lo, -W2sh2
-CONST_DEC  w4_plus_w6_lo,   W4sh2_lo, +W6sh2
-CONST_DEC  w4_min_w6_lo,    W4sh2_lo, -W6sh2
-CONST_DEC  w1_plus_w3_lo,   W1sh2,    +W3sh2_lo
-CONST_DEC  w3_min_w1_lo,    W3sh2_lo, -W1sh2
-CONST_DEC  w7_plus_w3_lo,   W7sh2,    +W3sh2_lo
-CONST_DEC  w3_min_w7_lo,    W3sh2_lo, -W7sh2
-
-%include "libavcodec/x86/simple_idct10_template.asm"
-
-SECTION .text
-
-%macro STORE_HI_LO 12
-    movq   %1, %9
-    movq   %3, %10
-    movq   %5, %11
-    movq   %7, %12
-    movhps %2, %9
-    movhps %4, %10
-    movhps %6, %11
-    movhps %8, %12
-%endmacro
-
-%macro LOAD_ZXBW_8 16
-    pmovzxbw %1, %9
-    pmovzxbw %2, %10
-    pmovzxbw %3, %11
-    pmovzxbw %4, %12
-    pmovzxbw %5, %13
-    pmovzxbw %6, %14
-    pmovzxbw %7, %15
-    pmovzxbw %8, %16
-%endmacro
-
-%macro LOAD_ZXBW_4 9
-    movh %1, %5
-    movh %2, %6
-    movh %3, %7
-    movh %4, %8
-    punpcklbw %1, %9
-    punpcklbw %2, %9
-    punpcklbw %3, %9
-    punpcklbw %4, %9
-%endmacro
-
-%define PASS4ROWS(base, stride, stride3) \
-    [base], [base + stride], [base + 2*stride], [base + stride3]
-
-%macro idct_fn 0
-
-define_constants _lo
-
-cglobal simple_idct8, 1, 1, 16, 32, block
-    IDCT_FN    "", 11, pw_32, 20, "store"
-RET
-
-cglobal simple_idct8_put, 3, 4, 16, 32, pixels, lsize, block
-    IDCT_FN    "", 11, pw_32, 20
-    lea       r3, [3*lsizeq]
-    lea       r2, [pixelsq + r3]
-    packuswb  m8, m0
-    packuswb  m1, m2
-    packuswb  m4, m11
-    packuswb  m9, m10
-    STORE_HI_LO PASS8ROWS(pixelsq, r2, lsizeq, r3), m8, m1, m4, m9
-RET
-
-cglobal simple_idct8_add, 3, 4, 16, 32, pixels, lsize, block
-    IDCT_FN    "", 11, pw_32, 20
-    lea r2, [3*lsizeq]
-    %if cpuflag(sse4)
-        lea r3, [pixelsq + r2]
-        LOAD_ZXBW_8 m3, m5, m6, m7, m12, m13, m14, m15, PASS8ROWS(pixelsq, r3, lsizeq, r2)
-        paddsw m8, m3
-        paddsw m0, m5
-        paddsw m1, m6
-        paddsw m2, m7
-        paddsw m4, m12
-        paddsw m11, m13
-        paddsw m9, m14
-        paddsw m10, m15
-    %else
-        pxor m12, m12
-        LOAD_ZXBW_4 m3, m5, m6, m7, PASS4ROWS(pixelsq, lsizeq, r2), m12
-        paddsw m8, m3
-        paddsw m0, m5
-        paddsw m1, m6
-        paddsw m2, m7
-        lea r3, [pixelsq + 4*lsizeq]
-        LOAD_ZXBW_4 m3, m5, m6, m7, PASS4ROWS(r3, lsizeq, r2), m12
-        paddsw m4, m3
-        paddsw m11, m5
-        paddsw m9, m6
-        paddsw m10, m7
-        lea r3, [pixelsq + r2]
-    %endif
-    packuswb  m8, m0
-    packuswb  m1, m2
-    packuswb  m4, m11
-    packuswb  m9, m10
-    STORE_HI_LO PASS8ROWS(pixelsq, r3, lsizeq, r2), m8, m1, m4, m9
-RET
-
-define_constants _hi
-
-cglobal simple_idct10, 1, 1, 16, block
-    IDCT_FN    "", 12, "", 19, "store"
-    RET
-
-cglobal simple_idct10_put, 3, 3, 16, pixels, lsize, block
-    IDCT_FN    "", 12, "", 19, "put", 0, pw_1023
-    RET
-
-cglobal simple_idct12, 1, 1, 16, block
-    ; coeffs are already 15bits, adding the offset would cause
-    ; overflow in the input
-    IDCT_FN    "", 15, pw_2, 16, "store"
-    RET
-
-cglobal simple_idct12_put, 3, 3, 16, pixels, lsize, block
-    ; range isn't known, so the C simple_idct range is used
-    ; Also, using a bias on input overflows, so use the bias
-    ; on output of the first butterfly instead
-    IDCT_FN    "", 15, pw_2, 16, "put", 0, pw_4095
-    RET
-%endmacro
-
-INIT_XMM sse2
-idct_fn
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-idct_fn
-%endif
-
-%endif
diff -uparN ffmpeg-4.1/libavcodec/x86/simple_idct10_template.asm ffmpeg-y/libavcodec/x86/simple_idct10_template.asm
--- ffmpeg-4.1/libavcodec/x86/simple_idct10_template.asm	2018-11-02 02:34:25.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/simple_idct10_template.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,369 +0,0 @@
-;******************************************************************************
-;* x86-SIMD-optimized IDCT for prores
-;* this is identical to "simple" IDCT written by Michael Niedermayer
-;* except for the clip range
-;*
-;* Copyright (c) 2011 Ronald S. Bultje <rsbultje@gmail.com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* 51, Inc., Foundation Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-; add SECTION_RODATA and proper include before including this file!
-
-%if ARCH_X86_64
-
-%macro define_constants 1
-    %undef w4_plus_w2
-    %undef w4_min_w2
-    %undef w4_plus_w6
-    %undef w4_min_w6
-    %undef w1_plus_w3
-    %undef w3_min_w1
-    %undef w7_plus_w3
-    %undef w3_min_w7
-    %define w4_plus_w2 w4_plus_w2%1
-    %define w4_min_w2  w4_min_w2%1
-    %define w4_plus_w6 w4_plus_w6%1
-    %define w4_min_w6  w4_min_w6%1
-    %define w1_plus_w3 w1_plus_w3%1
-    %define w3_min_w1  w3_min_w1%1
-    %define w7_plus_w3 w7_plus_w3%1
-    %define w3_min_w7  w3_min_w7%1
-%endmacro
-
-; interleave data while maintaining source
-; %1=type, %2=dstlo, %3=dsthi, %4=src, %5=interleave
-%macro SBUTTERFLY3 5
-    punpckl%1   m%2, m%4, m%5
-    punpckh%1   m%3, m%4, m%5
-%endmacro
-
-; %1/%2=src1/dst1, %3/%4=dst2, %5/%6=src2, %7=shift
-; action: %3/%4 = %1/%2 - %5/%6; %1/%2 += %5/%6
-;         %1/%2/%3/%4 >>= %7; dword -> word (in %1/%3)
-%macro SUMSUB_SHPK 7
-    psubd       %3,  %1,  %5       ; { a0 - b0 }[0-3]
-    psubd       %4,  %2,  %6       ; { a0 - b0 }[4-7]
-    paddd       %1,  %5            ; { a0 + b0 }[0-3]
-    paddd       %2,  %6            ; { a0 + b0 }[4-7]
-    psrad       %1,  %7
-    psrad       %2,  %7
-    psrad       %3,  %7
-    psrad       %4,  %7
-    packssdw    %1,  %2            ; row[0]
-    packssdw    %3,  %4            ; row[7]
-%endmacro
-
-; %1 = initial bias ("" if nop)
-; %2 = number of bits to shift at the end
-; %3 = qmat (for prores)
-%macro IDCT_1D 2-3
-    ; a0 = (W4 * row[0]) + (1 << (15 - 1));
-    ; a1 = a0;
-    ; a2 = a0;
-    ; a3 = a0;
-    ; a0 += W2 * row[2];
-    ; a1 += W6 * row[2];
-    ; a2 -= W6 * row[2];
-    ; a3 -= W2 * row[2];
-%ifstr %1
-    mova        m15, [pd_round_ %+ %2]
-%else
-    paddw       m10, [%1]
-%endif
-    SBUTTERFLY3 wd,  0,  1, 10,  8 ; { row[0], row[2] }[0-3]/[4-7]
-    pmaddwd     m2,  m0, [w4_plus_w6]
-    pmaddwd     m3,  m1, [w4_plus_w6]
-    pmaddwd     m4,  m0, [w4_min_w6]
-    pmaddwd     m5,  m1, [w4_min_w6]
-    pmaddwd     m6,  m0, [w4_min_w2]
-    pmaddwd     m7,  m1, [w4_min_w2]
-    pmaddwd     m0, [w4_plus_w2]
-    pmaddwd     m1, [w4_plus_w2]
-%ifstr %1
-    ; Adding 1<<(%2-1) for >=15 bits values
-    paddd       m2, m15
-    paddd       m3, m15
-    paddd       m4, m15
-    paddd       m5, m15
-    paddd       m6, m15
-    paddd       m7, m15
-    paddd       m0, m15
-    paddd       m1, m15
-%endif
-
-    ; a0: -1*row[0]-1*row[2]
-    ; a1: -1*row[0]
-    ; a2: -1*row[0]
-    ; a3: -1*row[0]+1*row[2]
-
-    ; a0 +=   W4*row[4] + W6*row[6]; i.e. -1*row[4]
-    ; a1 -=   W4*row[4] + W2*row[6]; i.e. -1*row[4]-1*row[6]
-    ; a2 -=   W4*row[4] - W2*row[6]; i.e. -1*row[4]+1*row[6]
-    ; a3 +=   W4*row[4] - W6*row[6]; i.e. -1*row[4]
-    SBUTTERFLY3 wd,  8,  9, 13, 12 ; { row[4], row[6] }[0-3]/[4-7]
-    pmaddwd     m10, m8, [w4_plus_w6]
-    pmaddwd     m11, m9, [w4_plus_w6]
-    paddd       m0,  m10            ; a0[0-3]
-    paddd       m1,  m11            ; a0[4-7]
-    pmaddwd     m10, m8, [w4_min_w6]
-    pmaddwd     m11, m9, [w4_min_w6]
-    paddd       m6,  m10           ; a3[0-3]
-    paddd       m7,  m11           ; a3[4-7]
-    pmaddwd     m10, m8, [w4_min_w2]
-    pmaddwd     m11, m9, [w4_min_w2]
-    pmaddwd     m8, [w4_plus_w2]
-    pmaddwd     m9, [w4_plus_w2]
-    psubd       m4,  m10           ; a2[0-3] intermediate
-    psubd       m5,  m11           ; a2[4-7] intermediate
-    psubd       m2,  m8            ; a1[0-3] intermediate
-    psubd       m3,  m9            ; a1[4-7] intermediate
-
-    ; load/store
-    mova   [blockq+  0], m0
-    mova   [blockq+ 32], m2
-    mova   [blockq+ 64], m4
-    mova   [blockq+ 96], m6
-    mova        m10,[blockq+ 16]       ; { row[1] }[0-7]
-    mova        m8, [blockq+ 48]       ; { row[3] }[0-7]
-    mova        m13,[blockq+ 80]       ; { row[5] }[0-7]
-    mova        m14,[blockq+112]       ; { row[7] }[0-7]
-    mova   [blockq+ 16], m1
-    mova   [blockq+ 48], m3
-    mova   [blockq+ 80], m5
-    mova   [blockq+112], m7
-%if %0 == 3
-    pmullw      m10,[%3+ 16]
-    pmullw      m8, [%3+ 48]
-    pmullw      m13,[%3+ 80]
-    pmullw      m14,[%3+112]
-%endif
-
-    ; b0 = MUL(W1, row[1]);
-    ; MAC(b0, W3, row[3]);
-    ; b1 = MUL(W3, row[1]);
-    ; MAC(b1, -W7, row[3]);
-    ; b2 = MUL(W5, row[1]);
-    ; MAC(b2, -W1, row[3]);
-    ; b3 = MUL(W7, row[1]);
-    ; MAC(b3, -W5, row[3]);
-    SBUTTERFLY3 wd,  0,  1, 10, 8  ; { row[1], row[3] }[0-3]/[4-7]
-    pmaddwd     m2,  m0, [w3_min_w7]
-    pmaddwd     m3,  m1, [w3_min_w7]
-    pmaddwd     m4,  m0, [w5_min_w1]
-    pmaddwd     m5,  m1, [w5_min_w1]
-    pmaddwd     m6,  m0, [w7_min_w5]
-    pmaddwd     m7,  m1, [w7_min_w5]
-    pmaddwd     m0, [w1_plus_w3]
-    pmaddwd     m1, [w1_plus_w3]
-
-    ; b0: +1*row[1]+2*row[3]
-    ; b1: +2*row[1]-1*row[3]
-    ; b2: -1*row[1]-1*row[3]
-    ; b3: +1*row[1]+1*row[3]
-
-    ; MAC(b0,  W5, row[5]);
-    ; MAC(b0,  W7, row[7]);
-    ; MAC(b1, -W1, row[5]);
-    ; MAC(b1, -W5, row[7]);
-    ; MAC(b2,  W7, row[5]);
-    ; MAC(b2,  W3, row[7]);
-    ; MAC(b3,  W3, row[5]);
-    ; MAC(b3, -W1, row[7]);
-    SBUTTERFLY3 wd,  8,  9, 13, 14 ; { row[5], row[7] }[0-3]/[4-7]
-
-    ; b0: -1*row[5]+1*row[7]
-    ; b1: -1*row[5]+1*row[7]
-    ; b2: +1*row[5]+2*row[7]
-    ; b3: +2*row[5]-1*row[7]
-
-    pmaddwd     m10, m8, [w1_plus_w5]
-    pmaddwd     m11, m9, [w1_plus_w5]
-    pmaddwd     m12, m8, [w5_plus_w7]
-    pmaddwd     m13, m9, [w5_plus_w7]
-    psubd       m2,  m10           ; b1[0-3]
-    psubd       m3,  m11           ; b1[4-7]
-    paddd       m0,  m12            ; b0[0-3]
-    paddd       m1,  m13            ; b0[4-7]
-    pmaddwd     m12, m8, [w7_plus_w3]
-    pmaddwd     m13, m9, [w7_plus_w3]
-    pmaddwd     m8, [w3_min_w1]
-    pmaddwd     m9, [w3_min_w1]
-    paddd       m4,  m12           ; b2[0-3]
-    paddd       m5,  m13           ; b2[4-7]
-    paddd       m6,  m8            ; b3[0-3]
-    paddd       m7,  m9            ; b3[4-7]
-
-    ; row[0] = (a0 + b0) >> 15;
-    ; row[7] = (a0 - b0) >> 15;
-    ; row[1] = (a1 + b1) >> 15;
-    ; row[6] = (a1 - b1) >> 15;
-    ; row[2] = (a2 + b2) >> 15;
-    ; row[5] = (a2 - b2) >> 15;
-    ; row[3] = (a3 + b3) >> 15;
-    ; row[4] = (a3 - b3) >> 15;
-    mova        m8, [blockq+ 0]        ; a0[0-3]
-    mova        m9, [blockq+16]        ; a0[4-7]
-    SUMSUB_SHPK m8,  m9,  m10, m11, m0,  m1,  %2
-    mova        m0, [blockq+32]        ; a1[0-3]
-    mova        m1, [blockq+48]        ; a1[4-7]
-    SUMSUB_SHPK m0,  m1,  m9,  m11, m2,  m3,  %2
-    mova        m1, [blockq+64]        ; a2[0-3]
-    mova        m2, [blockq+80]        ; a2[4-7]
-    SUMSUB_SHPK m1,  m2,  m11, m3,  m4,  m5,  %2
-    mova        m2, [blockq+96]        ; a3[0-3]
-    mova        m3, [blockq+112]       ; a3[4-7]
-    SUMSUB_SHPK m2,  m3,  m4,  m5,  m6,  m7,  %2
-%endmacro
-
-; void ff_prores_idct_put_10_<opt>(uint8_t *pixels, ptrdiff_t stride,
-;                                  int16_t *block, const int16_t *qmat);
-
-; %1 = row shift
-; %2 = row bias macro
-; %3 = column shift
-; %4 = column bias macro
-; %5 = final action (nothing, "store", "put", "add")
-; %6 = min pixel value
-; %7 = max pixel value
-; %8 = qmat (for prores)
-
-%macro IDCT_FN 4-8
-    ; for (i = 0; i < 8; i++)
-    ;     idctRowCondDC(block + i*8);
-    mova        m10,[blockq+ 0]        ; { row[0] }[0-7]
-    mova        m8, [blockq+32]        ; { row[2] }[0-7]
-    mova        m13,[blockq+64]        ; { row[4] }[0-7]
-    mova        m12,[blockq+96]        ; { row[6] }[0-7]
-
-%if %0 == 8
-    pmullw      m10,[%8+ 0]
-    pmullw      m8, [%8+32]
-    pmullw      m13,[%8+64]
-    pmullw      m12,[%8+96]
-
-    IDCT_1D     %1, %2, %8
-%elif %2 == 11
-    ; This copies the DC-only shortcut.  When there is only a DC coefficient the
-    ; C shifts the value and splats it to all coeffs rather than multiplying and
-    ; doing the full IDCT.  This causes a difference on 8-bit because the
-    ; coefficient is 16383 rather than 16384 (which you can get with shifting).
-    por      m1,  m8, m13
-    por      m1,  m12
-    por      m1, [blockq+ 16]       ; { row[1] }[0-7]
-    por      m1, [blockq+ 48]       ; { row[3] }[0-7]
-    por      m1, [blockq+ 80]       ; { row[5] }[0-7]
-    por      m1, [blockq+112]       ; { row[7] }[0-7]
-    pxor     m2,  m2
-    pcmpeqw  m1,  m2
-    psllw    m2,  m10, 3
-    pand     m2,  m1
-    pcmpeqb  m3,  m3
-    pxor     m1,  m3
-    mova    [rsp],    m1
-    mova    [rsp+16], m2
-
-    IDCT_1D  %1,  %2
-
-    mova     m5, [rsp]
-    mova     m6, [rsp+16]
-    pand     m8,  m5
-    por      m8,  m6
-    pand     m0,  m5
-    por      m0,  m6
-    pand     m1,  m5
-    por      m1,  m6
-    pand     m2,  m5
-    por      m2,  m6
-    pand     m4,  m5
-    por      m4,  m6
-    pand     m11, m5
-    por      m11, m6
-    pand     m9,  m5
-    por      m9,  m6
-    pand     m10, m5
-    por      m10, m6
-%else
-    IDCT_1D     %1, %2
-%endif
-
-    ; transpose for second part of IDCT
-    TRANSPOSE8x8W 8, 0, 1, 2, 4, 11, 9, 10, 3
-    mova   [blockq+ 16], m0
-    mova   [blockq+ 48], m2
-    mova   [blockq+ 80], m11
-    mova   [blockq+112], m10
-    SWAP         8,  10
-    SWAP         1,   8
-    SWAP         4,  13
-    SWAP         9,  12
-
-    ; for (i = 0; i < 8; i++)
-    ;     idctSparseColAdd(dest + i, line_size, block + i);
-    IDCT_1D     %3, %4
-
-    ; clip/store
-%if %0 >= 5
-%ifidn %5,"store"
-    ; No clamping, means pure idct
-    mova  [blockq+  0], m8
-    mova  [blockq+ 16], m0
-    mova  [blockq+ 32], m1
-    mova  [blockq+ 48], m2
-    mova  [blockq+ 64], m4
-    mova  [blockq+ 80], m11
-    mova  [blockq+ 96], m9
-    mova  [blockq+112], m10
-%elifidn %5,"put"
-%ifidn %6, 0
-    pxor        m3, m3
-%else
-    mova        m3, [%6]
-%endif ; ifidn %6, 0
-    mova        m5, [%7]
-    pmaxsw      m8,  m3
-    pmaxsw      m0,  m3
-    pmaxsw      m1,  m3
-    pmaxsw      m2,  m3
-    pmaxsw      m4,  m3
-    pmaxsw      m11, m3
-    pmaxsw      m9,  m3
-    pmaxsw      m10, m3
-    pminsw      m8,  m5
-    pminsw      m0,  m5
-    pminsw      m1,  m5
-    pminsw      m2,  m5
-    pminsw      m4,  m5
-    pminsw      m11, m5
-    pminsw      m9,  m5
-    pminsw      m10, m5
-
-    lea         r2, [r1*3]
-    mova  [r0     ], m8
-    mova  [r0+r1  ], m0
-    mova  [r0+r1*2], m1
-    mova  [r0+r2  ], m2
-    lea         r0, [r0+r1*4]
-    mova  [r0     ], m4
-    mova  [r0+r1  ], m11
-    mova  [r0+r1*2], m9
-    mova  [r0+r2  ], m10
-%endif ; %5 action
-%endif; if %0 >= 5
-%endmacro
-
-%endif
diff -uparN ffmpeg-4.1/libavcodec/x86/simple_idct.asm ffmpeg-y/libavcodec/x86/simple_idct.asm
--- ffmpeg-4.1/libavcodec/x86/simple_idct.asm	2018-11-02 02:34:25.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/simple_idct.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,889 +0,0 @@
-;
-; Simple IDCT MMX
-;
-; Copyright (c) 2001, 2002 Michael Niedermayer <michaelni@gmx.at>
-;
-; Conversion from gcc syntax to x264asm syntax with minimal modifications
-; by James Darnley <jdarnley@obe.tv>.
-;
-; This file is part of FFmpeg.
-;
-; FFmpeg is free software; you can redistribute it and/or
-; modify it under the terms of the GNU Lesser General Public
-; License as published by the Free Software Foundation; either
-; version 2.1 of the License, or (at your option) any later version.
-;
-; FFmpeg is distributed in the hope that it will be useful,
-; but WITHOUT ANY WARRANTY; without even the implied warranty of
-; MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-; Lesser General Public License for more details.
-;
-; You should have received a copy of the GNU Lesser General Public
-; License along with FFmpeg; if not, write to the Free Software
-; Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;/
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-
-cextern pb_80
-
-wm1010: dw 0, 0xffff, 0, 0xffff
-d40000: dd 4 << 16, 0
-
-; 23170.475006
-; 22725.260826
-; 21406.727617
-; 19265.545870
-; 16384.000000
-; 12872.826198
-; 8866.956905
-; 4520.335430
-
-%define C0 23170 ; cos(i*M_PI/16)*sqrt(2)*(1<<14) + 0.5
-%define C1 22725 ; cos(i*M_PI/16)*sqrt(2)*(1<<14) + 0.5
-%define C2 21407 ; cos(i*M_PI/16)*sqrt(2)*(1<<14) + 0.5
-%define C3 19266 ; cos(i*M_PI/16)*sqrt(2)*(1<<14) + 0.5
-%define C4 16383 ; cos(i*M_PI/16)*sqrt(2)*(1<<14) - 0.5
-%define C5 12873 ; cos(i*M_PI/16)*sqrt(2)*(1<<14) + 0.5
-%define C6 8867  ; cos(i*M_PI/16)*sqrt(2)*(1<<14) + 0.5
-%define C7 4520  ; cos(i*M_PI/16)*sqrt(2)*(1<<14) + 0.5
-
-%define ROW_SHIFT 11
-%define COL_SHIFT 20 ; 6
-
-coeffs:
-    dw 1 << (ROW_SHIFT - 1), 0
-    dw 1 << (ROW_SHIFT - 1), 0
-    dw 1 << (ROW_SHIFT - 1), 1
-    dw 1 << (ROW_SHIFT - 1), 0
-
-    dw C4,  C4,  C4,  C4
-    dw C4, -C4,  C4, -C4
-
-    dw C2,  C6,  C2,  C6
-    dw C6, -C2,  C6, -C2
-
-    dw C1,  C3,  C1,  C3
-    dw C5,  C7,  C5,  C7
-
-    dw C3, -C7,  C3, -C7
-    dw -C1, -C5, -C1, -C5
-
-    dw C5, -C1,  C5, -C1
-    dw C7,  C3,  C7,  C3
-
-    dw C7, -C5,  C7, -C5
-    dw C3, -C1,  C3, -C1
-
-SECTION .text
-
-%macro DC_COND_IDCT 7
-    movq            mm0, [blockq + %1]  ; R4     R0      r4      r0
-    movq            mm1, [blockq + %2]  ; R6     R2      r6      r2
-    movq            mm2, [blockq + %3]  ; R3     R1      r3      r1
-    movq            mm3, [blockq + %4]  ; R7     R5      r7      r5
-    movq            mm4, [wm1010]
-    pand            mm4, mm0
-    por             mm4, mm1
-    por             mm4, mm2
-    por             mm4, mm3
-    packssdw        mm4, mm4
-    movd            t0d, mm4
-    or              t0d, t0d
-    jz              %%1
-    movq            mm4, [coeffs + 16]  ; C4     C4      C4      C4
-    pmaddwd         mm4, mm0            ; C4R4+C4R0      C4r4+C4r0
-    movq            mm5, [coeffs + 24]  ; -C4    C4      -C4     C4
-    pmaddwd         mm0, mm5            ; -C4R4+C4R0     -C4r4+C4r0
-    movq            mm5, [coeffs + 32]  ; C6     C2      C6      C2
-    pmaddwd         mm5, mm1            ; C6R6+C2R2      C6r6+C2r2
-    movq            mm6, [coeffs + 40]  ; -C2    C6      -C2     C6
-    pmaddwd         mm1, mm6            ; -C2R6+C6R2     -C2r6+C6r2
-    movq            mm7, [coeffs + 48]  ; C3     C1      C3      C1
-    pmaddwd         mm7, mm2            ; C3R3+C1R1      C3r3+C1r1
-    paddd           mm4, [coeffs + 8]
-    movq            mm6, mm4            ; C4R4+C4R0      C4r4+C4r0
-    paddd           mm4, mm5            ; A0             a0
-    psubd           mm6, mm5            ; A3             a3
-    movq            mm5, [coeffs + 56]  ; C7     C5      C7      C5
-    pmaddwd         mm5, mm3            ; C7R7+C5R5      C7r7+C5r5
-    paddd           mm0, [coeffs + 8]
-    paddd           mm1, mm0            ; A1             a1
-    paddd           mm0, mm0
-    psubd           mm0, mm1            ; A2             a2
-    pmaddwd         mm2, [coeffs + 64]  ; -C7R3+C3R1     -C7r3+C3r1
-    paddd           mm7, mm5            ; B0             b0
-    movq            mm5, [coeffs + 72]  ; -C5    -C1     -C5     -C1
-    pmaddwd         mm5, mm3            ; -C5R7-C1R5     -C5r7-C1r5
-    paddd           mm7, mm4            ; A0+B0          a0+b0
-    paddd           mm4, mm4            ; 2A0            2a0
-    psubd           mm4, mm7            ; A0-B0          a0-b0
-    paddd           mm5, mm2            ; B1             b1
-    psrad           mm7, %7
-    psrad           mm4, %7
-    movq            mm2, mm1            ; A1             a1
-    paddd           mm1, mm5            ; A1+B1          a1+b1
-    psubd           mm2, mm5            ; A1-B1          a1-b1
-    psrad           mm1, %7
-    psrad           mm2, %7
-    packssdw        mm7, mm1            ; A1+B1  a1+b1   A0+B0   a0+b0
-    packssdw        mm2, mm4            ; A0-B0  a0-b0   A1-B1   a1-b1
-    movq           [%5], mm7
-    movq            mm1, [blockq + %3]  ; R3     R1      r3      r1
-    movq            mm4, [coeffs + 80]  ; -C1    C5      -C1     C5
-    movq      [24 + %5], mm2
-    pmaddwd         mm4, mm1            ; -C1R3+C5R1     -C1r3+C5r1
-    movq            mm7, [coeffs + 88]  ; C3     C7      C3      C7
-    pmaddwd         mm1, [coeffs + 96]  ; -C5R3+C7R1     -C5r3+C7r1
-    pmaddwd         mm7, mm3            ; C3R7+C7R5      C3r7+C7r5
-    movq            mm2, mm0            ; A2             a2
-    pmaddwd         mm3, [coeffs + 104] ; -C1R7+C3R5     -C1r7+C3r5
-    paddd           mm4, mm7            ; B2             b2
-    paddd           mm2, mm4            ; A2+B2          a2+b2
-    psubd           mm0, mm4            ; a2-B2          a2-b2
-    psrad           mm2, %7
-    psrad           mm0, %7
-    movq            mm4, mm6            ; A3             a3
-    paddd           mm3, mm1            ; B3             b3
-    paddd           mm6, mm3            ; A3+B3          a3+b3
-    psubd           mm4, mm3            ; a3-B3          a3-b3
-    psrad           mm6, %7
-    packssdw        mm2, mm6            ; A3+B3  a3+b3   A2+B2   a2+b2
-    movq       [8 + %5], mm2
-    psrad           mm4, %7
-    packssdw        mm4, mm0            ; A2-B2  a2-b2   A3-B3   a3-b3
-    movq      [16 + %5], mm4
-    jmp             %%2
-%%1:
-    pslld           mm0, 16
-    paddd           mm0, [d40000]
-    psrad           mm0, 13
-    packssdw        mm0, mm0
-    movq           [%5], mm0
-    movq       [8 + %5], mm0
-    movq      [16 + %5], mm0
-    movq      [24 + %5], mm0
-%%2:
-%endmacro
-
-%macro Z_COND_IDCT 8
-    movq            mm0, [blockq + %1]  ; R4     R0      r4      r0
-    movq            mm1, [blockq + %2]  ; R6     R2      r6      r2
-    movq            mm2, [blockq + %3]  ; R3     R1      r3      r1
-    movq            mm3, [blockq + %4]  ; R7     R5      r7      r5
-    movq            mm4, mm0
-    por             mm4, mm1
-    por             mm4, mm2
-    por             mm4, mm3
-    packssdw        mm4, mm4
-    movd            t0d, mm4
-    or              t0d, t0d
-    jz               %8
-    movq            mm4, [coeffs + 16]  ; C4     C4      C4      C4
-    pmaddwd         mm4, mm0            ; C4R4+C4R0      C4r4+C4r0
-    movq            mm5, [coeffs + 24]  ; -C4    C4      -C4     C4
-    pmaddwd         mm0, mm5            ; -C4R4+C4R0     -C4r4+C4r0
-    movq            mm5, [coeffs + 32]  ; C6     C2      C6      C2
-    pmaddwd         mm5, mm1            ; C6R6+C2R2      C6r6+C2r2
-    movq            mm6, [coeffs + 40]  ; -C2    C6      -C2     C6
-    pmaddwd         mm1, mm6            ; -C2R6+C6R2     -C2r6+C6r2
-    movq            mm7, [coeffs + 48]  ; C3     C1      C3      C1
-    pmaddwd         mm7, mm2            ; C3R3+C1R1      C3r3+C1r1
-    paddd           mm4, [coeffs]
-    movq            mm6, mm4            ; C4R4+C4R0      C4r4+C4r0
-    paddd           mm4, mm5            ; A0             a0
-    psubd           mm6, mm5            ; A3             a3
-    movq            mm5, [coeffs + 56]  ; C7     C5      C7      C5
-    pmaddwd         mm5, mm3            ; C7R7+C5R5      C7r7+C5r5
-    paddd           mm0, [coeffs]
-    paddd           mm1, mm0            ; A1             a1
-    paddd           mm0, mm0
-    psubd           mm0, mm1            ; A2             a2
-    pmaddwd         mm2, [coeffs + 64]  ; -C7R3+C3R1     -C7r3+C3r1
-    paddd           mm7, mm5            ; B0             b0
-    movq            mm5, [coeffs + 72]  ; -C5    -C1     -C5     -C1
-    pmaddwd         mm5, mm3            ; -C5R7-C1R5     -C5r7-C1r5
-    paddd           mm7, mm4            ; A0+B0          a0+b0
-    paddd           mm4, mm4            ; 2A0            2a0
-    psubd           mm4, mm7            ; A0-B0          a0-b0
-    paddd           mm5, mm2            ; B1             b1
-    psrad           mm7, %7
-    psrad           mm4, %7
-    movq            mm2, mm1            ; A1             a1
-    paddd           mm1, mm5            ; A1+B1          a1+b1
-    psubd           mm2, mm5            ; A1-B1          a1-b1
-    psrad           mm1, %7
-    psrad           mm2, %7
-    packssdw        mm7, mm1            ; A1+B1  a1+b1   A0+B0   a0+b0
-    packssdw        mm2, mm4            ; A0-B0  a0-b0   A1-B1   a1-b1
-    movq           [%5], mm7
-    movq            mm1, [blockq + %3]  ; R3     R1      r3      r1
-    movq            mm4, [coeffs + 80]  ; -C1    C5      -C1     C5
-    movq      [24 + %5], mm2
-    pmaddwd         mm4, mm1            ; -C1R3+C5R1     -C1r3+C5r1
-    movq            mm7, [coeffs + 88]  ; C3     C7      C3      C7
-    pmaddwd         mm1, [coeffs + 96]  ; -C5R3+C7R1     -C5r3+C7r1
-    pmaddwd         mm7, mm3            ; C3R7+C7R5      C3r7+C7r5
-    movq            mm2, mm0            ; A2             a2
-    pmaddwd         mm3, [coeffs + 104] ; -C1R7+C3R5     -C1r7+C3r5
-    paddd           mm4, mm7            ; B2             b2
-    paddd           mm2, mm4            ; A2+B2          a2+b2
-    psubd           mm0, mm4            ; a2-B2          a2-b2
-    psrad           mm2, %7
-    psrad           mm0, %7
-    movq            mm4, mm6            ; A3             a3
-    paddd           mm3, mm1            ; B3             b3
-    paddd           mm6, mm3            ; A3+B3          a3+b3
-    psubd           mm4, mm3            ; a3-B3          a3-b3
-    psrad           mm6, %7
-    packssdw        mm2, mm6            ; A3+B3  a3+b3   A2+B2   a2+b2
-    movq       [8 + %5], mm2
-    psrad           mm4, %7
-    packssdw        mm4, mm0            ; A2-B2  a2-b2   A3-B3   a3-b3
-    movq      [16 + %5], mm4
-%endmacro
-
-%macro IDCT1 6
-    movq            mm0, %1             ; R4     R0      r4      r0
-    movq            mm1, %2             ; R6     R2      r6      r2
-    movq            mm2, %3             ; R3     R1      r3      r1
-    movq            mm3, %4             ; R7     R5      r7      r5
-    movq            mm4, [coeffs + 16]  ; C4     C4      C4      C4
-    pmaddwd         mm4, mm0            ; C4R4+C4R0      C4r4+C4r0
-    movq            mm5, [coeffs + 24]  ; -C4    C4      -C4     C4
-    pmaddwd         mm0, mm5            ; -C4R4+C4R0     -C4r4+C4r0
-    movq            mm5, [coeffs + 32]  ; C6     C2      C6      C2
-    pmaddwd         mm5, mm1            ; C6R6+C2R2      C6r6+C2r2
-    movq            mm6, [coeffs + 40]  ; -C2    C6      -C2     C6
-    pmaddwd         mm1, mm6            ; -C2R6+C6R2     -C2r6+C6r2
-    movq            mm6, mm4            ; C4R4+C4R0      C4r4+C4r0
-    movq            mm7, [coeffs + 48]  ; C3     C1      C3      C1
-    pmaddwd         mm7, mm2            ; C3R3+C1R1      C3r3+C1r1
-    paddd           mm4, mm5            ; A0             a0
-    psubd           mm6, mm5            ; A3             a3
-    movq            mm5, mm0            ; -C4R4+C4R0     -C4r4+C4r0
-    paddd           mm0, mm1            ; A1             a1
-    psubd           mm5, mm1            ; A2             a2
-    movq            mm1, [coeffs + 56]  ; C7     C5      C7      C5
-    pmaddwd         mm1, mm3            ; C7R7+C5R5      C7r7+C5r5
-    pmaddwd         mm2, [coeffs + 64]  ; -C7R3+C3R1     -C7r3+C3r1
-    paddd           mm7, mm1            ; B0             b0
-    movq            mm1, [coeffs + 72]  ; -C5    -C1     -C5     -C1
-    pmaddwd         mm1, mm3            ; -C5R7-C1R5     -C5r7-C1r5
-    paddd           mm7, mm4            ; A0+B0          a0+b0
-    paddd           mm4, mm4            ; 2A0            2a0
-    psubd           mm4, mm7            ; A0-B0          a0-b0
-    paddd           mm1, mm2            ; B1             b1
-    psrad           mm7, %6
-    psrad           mm4, %6
-    movq            mm2, mm0            ; A1             a1
-    paddd           mm0, mm1            ; A1+B1          a1+b1
-    psubd           mm2, mm1            ; A1-B1          a1-b1
-    psrad           mm0, %6
-    psrad           mm2, %6
-    packssdw        mm7, mm7            ; A0+B0  a0+b0
-    movd           [%5], mm7
-    packssdw        mm0, mm0            ; A1+B1  a1+b1
-    movd      [16 + %5], mm0
-    packssdw        mm2, mm2            ; A1-B1  a1-b1
-    movd      [96 + %5], mm2
-    packssdw        mm4, mm4            ; A0-B0  a0-b0
-    movd     [112 + %5], mm4
-    movq            mm0, %3             ; R3     R1      r3      r1
-    movq            mm4, [coeffs + 80]  ; -C1    C5      -C1     C5
-    pmaddwd         mm4, mm0            ; -C1R3+C5R1     -C1r3+C5r1
-    movq            mm7, [coeffs + 88]  ; C3     C7      C3      C7
-    pmaddwd         mm0, [coeffs + 96]  ; -C5R3+C7R1     -C5r3+C7r1
-    pmaddwd         mm7, mm3            ; C3R7+C7R5      C3r7+C7r5
-    movq            mm2, mm5            ; A2             a2
-    pmaddwd         mm3, [coeffs + 104] ; -C1R7+C3R5     -C1r7+C3r5
-    paddd           mm4, mm7            ; B2             b2
-    paddd           mm2, mm4            ; A2+B2          a2+b2
-    psubd           mm5, mm4            ; a2-B2          a2-b2
-    psrad           mm2, %6
-    psrad           mm5, %6
-    movq            mm4, mm6            ; A3             a3
-    paddd           mm3, mm0            ; B3             b3
-    paddd           mm6, mm3            ; A3+B3          a3+b3
-    psubd           mm4, mm3            ; a3-B3          a3-b3
-    psrad           mm6, %6
-    psrad           mm4, %6
-    packssdw        mm2, mm2            ; A2+B2  a2+b2
-    packssdw        mm6, mm6            ; A3+B3  a3+b3
-    movd      [32 + %5], mm2
-    packssdw        mm4, mm4            ; A3-B3  a3-b3
-    packssdw        mm5, mm5            ; A2-B2  a2-b2
-    movd      [48 + %5], mm6
-    movd      [64 + %5], mm4
-    movd      [80 + %5], mm5
-%endmacro
-
-%macro IDCT2 6
-    movq            mm0, %1             ; R4     R0      r4      r0
-    movq            mm1, %2             ; R6     R2      r6      r2
-    movq            mm3, %4             ; R7     R5      r7      r5
-    movq            mm4, [coeffs + 16]  ; C4     C4      C4      C4
-    pmaddwd         mm4, mm0            ; C4R4+C4R0      C4r4+C4r0
-    movq            mm5, [coeffs + 24]  ; -C4    C4      -C4     C4
-    pmaddwd         mm0, mm5            ; -C4R4+C4R0     -C4r4+C4r0
-    movq            mm5, [coeffs + 32]  ; C6     C2      C6      C2
-    pmaddwd         mm5, mm1            ; C6R6+C2R2      C6r6+C2r2
-    movq            mm6, [coeffs + 40]  ; -C2    C6      -C2     C6
-    pmaddwd         mm1, mm6            ; -C2R6+C6R2     -C2r6+C6r2
-    movq            mm6, mm4            ; C4R4+C4R0      C4r4+C4r0
-    paddd           mm4, mm5            ; A0             a0
-    psubd           mm6, mm5            ; A3             a3
-    movq            mm5, mm0            ; -C4R4+C4R0     -C4r4+C4r0
-    paddd           mm0, mm1            ; A1             a1
-    psubd           mm5, mm1            ; A2             a2
-    movq            mm1, [coeffs + 56]  ; C7     C5      C7      C5
-    pmaddwd         mm1, mm3            ; C7R7+C5R5      C7r7+C5r5
-    movq            mm7, [coeffs + 72]  ; -C5    -C1     -C5     -C1
-    pmaddwd         mm7, mm3            ; -C5R7-C1R5     -C5r7-C1r5
-    paddd           mm1, mm4            ; A0+B0          a0+b0
-    paddd           mm4, mm4            ; 2A0            2a0
-    psubd           mm4, mm1            ; A0-B0          a0-b0
-    psrad           mm1, %6
-    psrad           mm4, %6
-    movq            mm2, mm0            ; A1             a1
-    paddd           mm0, mm7            ; A1+B1          a1+b1
-    psubd           mm2, mm7            ; A1-B1          a1-b1
-    psrad           mm0, %6
-    psrad           mm2, %6
-    packssdw        mm1, mm1            ; A0+B0  a0+b0
-    movd           [%5], mm1
-    packssdw        mm0, mm0            ; A1+B1  a1+b1
-    movd      [16 + %5], mm0
-    packssdw        mm2, mm2            ; A1-B1  a1-b1
-    movd      [96 + %5], mm2
-    packssdw        mm4, mm4            ; A0-B0  a0-b0
-    movd     [112 + %5], mm4
-    movq            mm1, [coeffs + 88]  ; C3     C7      C3      C7
-    pmaddwd         mm1, mm3            ; C3R7+C7R5      C3r7+C7r5
-    movq            mm2, mm5            ; A2             a2
-    pmaddwd         mm3, [coeffs + 104] ; -C1R7+C3R5     -C1r7+C3r5
-    paddd           mm2, mm1            ; A2+B2          a2+b2
-    psubd           mm5, mm1            ; a2-B2          a2-b2
-    psrad           mm2, %6
-    psrad           mm5, %6
-    movq            mm1, mm6            ; A3             a3
-    paddd           mm6, mm3            ; A3+B3          a3+b3
-    psubd           mm1, mm3            ; a3-B3          a3-b3
-    psrad           mm6, %6
-    psrad           mm1, %6
-    packssdw        mm2, mm2            ; A2+B2  a2+b2
-    packssdw        mm6, mm6            ; A3+B3  a3+b3
-    movd      [32 + %5], mm2
-    packssdw        mm1, mm1            ; A3-B3  a3-b3
-    packssdw        mm5, mm5            ; A2-B2  a2-b2
-    movd      [48 + %5], mm6
-    movd      [64 + %5], mm1
-    movd      [80 + %5], mm5
-%endmacro
-
-%macro IDCT3 6
-    movq            mm0, %1             ; R4     R0      r4      r0
-    movq            mm3, %4             ; R7     R5      r7      r5
-    movq            mm4, [coeffs + 16]  ; C4     C4      C4      C4
-    pmaddwd         mm4, mm0            ; C4R4+C4R0      C4r4+C4r0
-    movq            mm5, [coeffs + 24]  ; -C4    C4      -C4     C4
-    pmaddwd         mm0, mm5            ; -C4R4+C4R0     -C4r4+C4r0
-    movq            mm6, mm4            ; C4R4+C4R0      C4r4+C4r0
-    movq            mm5, mm0            ; -C4R4+C4R0     -C4r4+C4r0
-    movq            mm1, [coeffs + 56]  ; C7     C5      C7      C5
-    pmaddwd         mm1, mm3            ; C7R7+C5R5      C7r7+C5r5
-    movq            mm7, [coeffs + 72]  ; -C5    -C1     -C5     -C1
-    pmaddwd         mm7, mm3            ; -C5R7-C1R5     -C5r7-C1r5
-    paddd           mm1, mm4            ; A0+B0          a0+b0
-    paddd           mm4, mm4            ; 2A0            2a0
-    psubd           mm4, mm1            ; A0-B0          a0-b0
-    psrad           mm1, %6
-    psrad           mm4, %6
-    movq            mm2, mm0            ; A1             a1
-    paddd           mm0, mm7            ; A1+B1          a1+b1
-    psubd           mm2, mm7            ; A1-B1          a1-b1
-    psrad           mm0, %6
-    psrad           mm2, %6
-    packssdw        mm1, mm1            ; A0+B0  a0+b0
-    movd           [%5], mm1
-    packssdw        mm0, mm0            ; A1+B1  a1+b1
-    movd      [16 + %5], mm0
-    packssdw        mm2, mm2            ; A1-B1  a1-b1
-    movd      [96 + %5], mm2
-    packssdw        mm4, mm4            ; A0-B0  a0-b0
-    movd     [112 + %5], mm4
-    movq            mm1, [coeffs + 88]  ; C3     C7      C3      C7
-    pmaddwd         mm1, mm3            ; C3R7+C7R5      C3r7+C7r5
-    movq            mm2, mm5            ; A2             a2
-    pmaddwd         mm3, [coeffs + 104] ; -C1R7+C3R5     -C1r7+C3r5
-    paddd           mm2, mm1            ; A2+B2          a2+b2
-    psubd           mm5, mm1            ; a2-B2          a2-b2
-    psrad           mm2, %6
-    psrad           mm5, %6
-    movq            mm1, mm6            ; A3             a3
-    paddd           mm6, mm3            ; A3+B3          a3+b3
-    psubd           mm1, mm3            ; a3-B3          a3-b3
-    psrad           mm6, %6
-    psrad           mm1, %6
-    packssdw        mm2, mm2            ; A2+B2  a2+b2
-    packssdw        mm6, mm6            ; A3+B3  a3+b3
-    movd      [32 + %5], mm2
-    packssdw        mm1, mm1            ; A3-B3  a3-b3
-    packssdw        mm5, mm5            ; A2-B2  a2-b2
-    movd      [48 + %5], mm6
-    movd      [64 + %5], mm1
-    movd      [80 + %5], mm5
-%endmacro
-
-%macro IDCT4 6
-    movq            mm0, %1             ; R4     R0      r4      r0
-    movq            mm2, %3             ; R3     R1      r3      r1
-    movq            mm3, %4             ; R7     R5      r7      r5
-    movq            mm4, [coeffs + 16]  ; C4     C4      C4      C4
-    pmaddwd         mm4, mm0            ; C4R4+C4R0      C4r4+C4r0
-    movq            mm5, [coeffs + 24]  ; -C4    C4      -C4     C4
-    pmaddwd         mm0, mm5            ; -C4R4+C4R0     -C4r4+C4r0
-    movq            mm6, mm4            ; C4R4+C4R0      C4r4+C4r0
-    movq            mm7, [coeffs + 48]  ; C3     C1      C3      C1
-    pmaddwd         mm7, mm2            ; C3R3+C1R1      C3r3+C1r1
-    movq            mm5, mm0            ; -C4R4+C4R0     -C4r4+C4r0
-    movq            mm1, [coeffs + 56]  ; C7     C5      C7      C5
-    pmaddwd         mm1, mm3            ; C7R7+C5R5      C7r7+C5r5
-    pmaddwd         mm2, [coeffs + 64]  ; -C7R3+C3R1     -C7r3+C3r1
-    paddd           mm7, mm1            ; B0             b0
-    movq            mm1, [coeffs + 72]  ; -C5    -C1     -C5     -C1
-    pmaddwd         mm1, mm3            ; -C5R7-C1R5     -C5r7-C1r5
-    paddd           mm7, mm4            ; A0+B0          a0+b0
-    paddd           mm4, mm4            ; 2A0            2a0
-    psubd           mm4, mm7            ; A0-B0          a0-b0
-    paddd           mm1, mm2            ; B1             b1
-    psrad           mm7, %6
-    psrad           mm4, %6
-    movq            mm2, mm0            ; A1             a1
-    paddd           mm0, mm1            ; A1+B1          a1+b1
-    psubd           mm2, mm1            ; A1-B1          a1-b1
-    psrad           mm0, %6
-    psrad           mm2, %6
-    packssdw        mm7, mm7            ; A0+B0  a0+b0
-    movd           [%5], mm7
-    packssdw        mm0, mm0            ; A1+B1  a1+b1
-    movd      [16 + %5], mm0
-    packssdw        mm2, mm2            ; A1-B1  a1-b1
-    movd      [96 + %5], mm2
-    packssdw        mm4, mm4            ; A0-B0  a0-b0
-    movd     [112 + %5], mm4
-    movq            mm0, %3             ; R3     R1      r3      r1
-    movq            mm4, [coeffs + 80]  ; -C1    C5      -C1     C5
-    pmaddwd         mm4, mm0            ; -C1R3+C5R1     -C1r3+C5r1
-    movq            mm7, [coeffs + 88]  ; C3     C7      C3      C7
-    pmaddwd         mm0, [coeffs + 96]  ; -C5R3+C7R1     -C5r3+C7r1
-    pmaddwd         mm7, mm3            ; C3R7+C7R5      C3r7+C7r5
-    movq            mm2, mm5            ; A2             a2
-    pmaddwd         mm3, [coeffs + 104] ; -C1R7+C3R5     -C1r7+C3r5
-    paddd           mm4, mm7            ; B2             b2
-    paddd           mm2, mm4            ; A2+B2          a2+b2
-    psubd           mm5, mm4            ; a2-B2          a2-b2
-    psrad           mm2, %6
-    psrad           mm5, %6
-    movq            mm4, mm6            ; A3             a3
-    paddd           mm3, mm0            ; B3             b3
-    paddd           mm6, mm3            ; A3+B3          a3+b3
-    psubd           mm4, mm3            ; a3-B3          a3-b3
-    psrad           mm6, %6
-    psrad           mm4, %6
-    packssdw        mm2, mm2            ; A2+B2  a2+b2
-    packssdw        mm6, mm6            ; A3+B3  a3+b3
-    movd      [32 + %5], mm2
-    packssdw        mm4, mm4            ; A3-B3  a3-b3
-    packssdw        mm5, mm5            ; A2-B2  a2-b2
-    movd      [48 + %5], mm6
-    movd      [64 + %5], mm4
-    movd      [80 + %5], mm5
-%endmacro
-
-%macro IDCT5 6
-    movq            mm0, %1             ; R4     R0      r4      r0
-    movq            mm2, %3             ; R3     R1      r3      r1
-    movq            mm4, [coeffs + 16]  ; C4     C4      C4      C4
-    pmaddwd         mm4, mm0            ; C4R4+C4R0      C4r4+C4r0
-    movq            mm5, [coeffs + 24]  ; -C4    C4      -C4     C4
-    pmaddwd         mm0, mm5            ; -C4R4+C4R0     -C4r4+C4r0
-    movq            mm6, mm4            ; C4R4+C4R0      C4r4+C4r0
-    movq            mm7, [coeffs + 48]  ; C3     C1      C3      C1
-    pmaddwd         mm7, mm2            ; C3R3+C1R1      C3r3+C1r1
-    movq            mm5, mm0            ; -C4R4+C4R0     -C4r4+C4r0
-    movq            mm3, [coeffs + 64]
-    pmaddwd         mm3, mm2            ; -C7R3+C3R1     -C7r3+C3r1
-    paddd           mm7, mm4            ; A0+B0          a0+b0
-    paddd           mm4, mm4            ; 2A0            2a0
-    psubd           mm4, mm7            ; A0-B0          a0-b0
-    psrad           mm7, %6
-    psrad           mm4, %6
-    movq            mm1, mm0            ; A1             a1
-    paddd           mm0, mm3            ; A1+B1          a1+b1
-    psubd           mm1, mm3            ; A1-B1          a1-b1
-    psrad           mm0, %6
-    psrad           mm1, %6
-    packssdw        mm7, mm7            ; A0+B0  a0+b0
-    movd           [%5], mm7
-    packssdw        mm0, mm0            ; A1+B1  a1+b1
-    movd      [16 + %5], mm0
-    packssdw        mm1, mm1            ; A1-B1  a1-b1
-    movd      [96 + %5], mm1
-    packssdw        mm4, mm4            ; A0-B0  a0-b0
-    movd     [112 + %5], mm4
-    movq            mm4, [coeffs + 80]  ; -C1    C5      -C1     C5
-    pmaddwd         mm4, mm2            ; -C1R3+C5R1     -C1r3+C5r1
-    pmaddwd         mm2, [coeffs + 96]  ; -C5R3+C7R1     -C5r3+C7r1
-    movq            mm1, mm5            ; A2             a2
-    paddd           mm1, mm4            ; A2+B2          a2+b2
-    psubd           mm5, mm4            ; a2-B2          a2-b2
-    psrad           mm1, %6
-    psrad           mm5, %6
-    movq            mm4, mm6            ; A3             a3
-    paddd           mm6, mm2            ; A3+B3          a3+b3
-    psubd           mm4, mm2            ; a3-B3          a3-b3
-    psrad           mm6, %6
-    psrad           mm4, %6
-    packssdw        mm1, mm1            ; A2+B2  a2+b2
-    packssdw        mm6, mm6            ; A3+B3  a3+b3
-    movd      [32 + %5], mm1
-    packssdw        mm4, mm4            ; A3-B3  a3-b3
-    packssdw        mm5, mm5            ; A2-B2  a2-b2
-    movd      [48 + %5], mm6
-    movd      [64 + %5], mm4
-    movd      [80 + %5], mm5
-%endmacro
-
-%macro IDCT6 6
-    movq            mm0, [%1]           ; R4     R0      r4      r0
-    movq            mm1, [%2]           ; R6     R2      r6      r2
-    movq            mm4, [coeffs + 16]  ; C4     C4      C4      C4
-    pmaddwd         mm4, mm0            ; C4R4+C4R0      C4r4+C4r0
-    movq            mm5, [coeffs + 24]  ; -C4    C4      -C4     C4
-    pmaddwd         mm0, mm5            ; -C4R4+C4R0     -C4r4+C4r0
-    movq            mm5, [coeffs + 32]  ; C6     C2      C6      C2
-    pmaddwd         mm5, mm1            ; C6R6+C2R2      C6r6+C2r2
-    movq            mm6, [coeffs + 40]  ; -C2    C6      -C2     C6
-    pmaddwd         mm1, mm6            ; -C2R6+C6R2     -C2r6+C6r2
-    movq            mm6, mm4            ; C4R4+C4R0      C4r4+C4r0
-    paddd           mm4, mm5            ; A0             a0
-    psubd           mm6, mm5            ; A3             a3
-    movq            mm5, mm0            ; -C4R4+C4R0     -C4r4+C4r0
-    paddd           mm0, mm1            ; A1             a1
-    psubd           mm5, mm1            ; A2             a2
-    movq            mm2, [8 + %1]       ; R4     R0      r4      r0
-    movq            mm3, [8 + %2]       ; R6     R2      r6      r2
-    movq            mm1, [coeffs + 16]  ; C4     C4      C4      C4
-    pmaddwd         mm1, mm2            ; C4R4+C4R0      C4r4+C4r0
-    movq            mm7, [coeffs + 24]  ; -C4    C4      -C4     C4
-    pmaddwd         mm2, mm7            ; -C4R4+C4R0     -C4r4+C4r0
-    movq            mm7, [coeffs + 32]  ; C6     C2      C6      C2
-    pmaddwd         mm7, mm3            ; C6R6+C2R2      C6r6+C2r2
-    pmaddwd         mm3, [coeffs + 40]  ; -C2R6+C6R2     -C2r6+C6r2
-    paddd           mm7, mm1            ; A0             a0
-    paddd           mm1, mm1            ; 2C0            2c0
-    psubd           mm1, mm7            ; A3             a3
-    paddd           mm3, mm2            ; A1             a1
-    paddd           mm2, mm2            ; 2C1            2c1
-    psubd           mm2, mm3            ; A2             a2
-    psrad           mm4, %6
-    psrad           mm7, %6
-    psrad           mm3, %6
-    packssdw        mm4, mm7            ; A0     a0
-    movq           [%5], mm4
-    psrad           mm0, %6
-    packssdw        mm0, mm3            ; A1     a1
-    movq      [16 + %5], mm0
-    movq      [96 + %5], mm0
-    movq     [112 + %5], mm4
-    psrad           mm5, %6
-    psrad           mm6, %6
-    psrad           mm2, %6
-    packssdw        mm5, mm2            ; A2-B2  a2-b2
-    movq      [32 + %5], mm5
-    psrad           mm1, %6
-    packssdw        mm6, mm1            ; A3+B3  a3+b3
-    movq      [48 + %5], mm6
-    movq      [64 + %5], mm6
-    movq      [80 + %5], mm5
-%endmacro
-
-%macro IDCT7 6
-    movq            mm0, %1             ; R4     R0      r4      r0
-    movq            mm1, %2             ; R6     R2      r6      r2
-    movq            mm2, %3             ; R3     R1      r3      r1
-    movq            mm4, [coeffs + 16]  ; C4     C4      C4      C4
-    pmaddwd         mm4, mm0            ; C4R4+C4R0      C4r4+C4r0
-    movq            mm5, [coeffs + 24]  ; -C4    C4      -C4     C4
-    pmaddwd         mm0, mm5            ; -C4R4+C4R0     -C4r4+C4r0
-    movq            mm5, [coeffs + 32]  ; C6     C2      C6      C2
-    pmaddwd         mm5, mm1            ; C6R6+C2R2      C6r6+C2r2
-    movq            mm6, [coeffs + 40]  ; -C2    C6      -C2     C6
-    pmaddwd         mm1, mm6            ; -C2R6+C6R2     -C2r6+C6r2
-    movq            mm6, mm4            ; C4R4+C4R0      C4r4+C4r0
-    movq            mm7, [coeffs + 48]  ; C3     C1      C3      C1
-    pmaddwd         mm7, mm2            ; C3R3+C1R1      C3r3+C1r1
-    paddd           mm4, mm5            ; A0             a0
-    psubd           mm6, mm5            ; A3             a3
-    movq            mm5, mm0            ; -C4R4+C4R0     -C4r4+C4r0
-    paddd           mm0, mm1            ; A1             a1
-    psubd           mm5, mm1            ; A2             a2
-    movq            mm1, [coeffs + 64]
-    pmaddwd         mm1, mm2            ; -C7R3+C3R1     -C7r3+C3r1
-    paddd           mm7, mm4            ; A0+B0          a0+b0
-    paddd           mm4, mm4            ; 2A0            2a0
-    psubd           mm4, mm7            ; A0-B0          a0-b0
-    psrad           mm7, %6
-    psrad           mm4, %6
-    movq            mm3, mm0            ; A1             a1
-    paddd           mm0, mm1            ; A1+B1          a1+b1
-    psubd           mm3, mm1            ; A1-B1          a1-b1
-    psrad           mm0, %6
-    psrad           mm3, %6
-    packssdw        mm7, mm7            ; A0+B0  a0+b0
-    movd           [%5], mm7
-    packssdw        mm0, mm0            ; A1+B1  a1+b1
-    movd      [16 + %5], mm0
-    packssdw        mm3, mm3            ; A1-B1  a1-b1
-    movd      [96 + %5], mm3
-    packssdw        mm4, mm4            ; A0-B0  a0-b0
-    movd     [112 + %5], mm4
-    movq            mm4, [coeffs + 80]  ; -C1    C5      -C1     C5
-    pmaddwd         mm4, mm2            ; -C1R3+C5R1     -C1r3+C5r1
-    pmaddwd         mm2, [coeffs + 96]  ; -C5R3+C7R1     -C5r3+C7r1
-    movq            mm3, mm5            ; A2             a2
-    paddd           mm3, mm4            ; A2+B2          a2+b2
-    psubd           mm5, mm4            ; a2-B2          a2-b2
-    psrad           mm3, %6
-    psrad           mm5, %6
-    movq            mm4, mm6            ; A3             a3
-    paddd           mm6, mm2            ; A3+B3          a3+b3
-    psubd           mm4, mm2            ; a3-B3          a3-b3
-    psrad           mm6, %6
-    packssdw        mm3, mm3            ; A2+B2  a2+b2
-    movd      [32 + %5], mm3
-    psrad           mm4, %6
-    packssdw        mm6, mm6            ; A3+B3  a3+b3
-    movd      [48 + %5], mm6
-    packssdw        mm4, mm4            ; A3-B3  a3-b3
-    packssdw        mm5, mm5            ; A2-B2  a2-b2
-    movd      [64 + %5], mm4
-    movd      [80 + %5], mm5
-%endmacro
-
-%macro IDCT8 6
-    movq            mm0, [%1]           ; R4     R0      r4      r0
-    movq            mm4, [coeffs + 16]  ; C4     C4      C4      C4
-    pmaddwd         mm4, mm0            ; C4R4+C4R0      C4r4+C4r0
-    movq            mm5, [coeffs + 24]  ; -C4    C4      -C4     C4
-    pmaddwd         mm0, mm5            ; -C4R4+C4R0     -C4r4+C4r0
-    psrad           mm4, %6
-    psrad           mm0, %6
-    movq            mm2, [8 + %1]       ; R4     R0      r4      r0
-    movq            mm1, [coeffs + 16]  ; C4     C4      C4      C4
-    pmaddwd         mm1, mm2            ; C4R4+C4R0      C4r4+C4r0
-    movq            mm7, [coeffs + 24]  ; -C4    C4      -C4     C4
-    pmaddwd         mm2, mm7            ; -C4R4+C4R0     -C4r4+C4r0
-    movq            mm7, [coeffs + 32]  ; C6     C2      C6      C2
-    psrad           mm1, %6
-    packssdw        mm4, mm1            ; A0     a0
-    movq           [%5], mm4
-    psrad           mm2, %6
-    packssdw        mm0, mm2            ; A1     a1
-    movq      [16 + %5], mm0
-    movq      [96 + %5], mm0
-    movq     [112 + %5], mm4
-    movq      [32 + %5], mm0
-    movq      [48 + %5], mm4
-    movq      [64 + %5], mm4
-    movq      [80 + %5], mm0
-%endmacro
-
-%macro IDCT 0
-    DC_COND_IDCT  0,   8,  16,  24, rsp +  0, null, 11
-    Z_COND_IDCT  32,  40,  48,  56, rsp + 32, null, 11, %%4
-    Z_COND_IDCT  64,  72,  80,  88, rsp + 64, null, 11, %%2
-    Z_COND_IDCT  96, 104, 112, 120, rsp + 96, null, 11, %%1
-
-    IDCT1 [rsp +  0], [rsp + 64], [rsp + 32], [rsp +  96], blockq +  0, 20
-    IDCT1 [rsp +  8], [rsp + 72], [rsp + 40], [rsp + 104], blockq +  4, 20
-    IDCT1 [rsp + 16], [rsp + 80], [rsp + 48], [rsp + 112], blockq +  8, 20
-    IDCT1 [rsp + 24], [rsp + 88], [rsp + 56], [rsp + 120], blockq + 12, 20
-    jmp %%9
-
-    ALIGN 16
-    %%4:
-    Z_COND_IDCT 64,  72,  80,  88, rsp + 64, null, 11, %%6
-    Z_COND_IDCT 96, 104, 112, 120, rsp + 96, null, 11, %%5
-
-    IDCT2 [rsp +  0], [rsp + 64], [rsp + 32], [rsp +  96], blockq +  0, 20
-    IDCT2 [rsp +  8], [rsp + 72], [rsp + 40], [rsp + 104], blockq +  4, 20
-    IDCT2 [rsp + 16], [rsp + 80], [rsp + 48], [rsp + 112], blockq +  8, 20
-    IDCT2 [rsp + 24], [rsp + 88], [rsp + 56], [rsp + 120], blockq + 12, 20
-    jmp %%9
-
-    ALIGN 16
-    %%6:
-    Z_COND_IDCT 96, 104, 112, 120, rsp + 96, null, 11, %%7
-
-    IDCT3 [rsp +  0], [rsp + 64], [rsp + 32], [rsp +  96], blockq +  0, 20
-    IDCT3 [rsp +  8], [rsp + 72], [rsp + 40], [rsp + 104], blockq +  4, 20
-    IDCT3 [rsp + 16], [rsp + 80], [rsp + 48], [rsp + 112], blockq +  8, 20
-    IDCT3 [rsp + 24], [rsp + 88], [rsp + 56], [rsp + 120], blockq + 12, 20
-    jmp %%9
-
-    ALIGN 16
-    %%2:
-    Z_COND_IDCT 96, 104, 112, 120, rsp + 96, null, 11, %%3
-
-    IDCT4 [rsp +  0], [rsp + 64], [rsp + 32], [rsp +  96], blockq +  0, 20
-    IDCT4 [rsp +  8], [rsp + 72], [rsp + 40], [rsp + 104], blockq +  4, 20
-    IDCT4 [rsp + 16], [rsp + 80], [rsp + 48], [rsp + 112], blockq +  8, 20
-    IDCT4 [rsp + 24], [rsp + 88], [rsp + 56], [rsp + 120], blockq + 12, 20
-    jmp %%9
-
-    ALIGN 16
-    %%3:
-
-    IDCT5 [rsp +  0], [rsp + 64], [rsp + 32], [rsp +  96], blockq +  0, 20
-    IDCT5 [rsp +  8], [rsp + 72], [rsp + 40], [rsp + 104], blockq +  4, 20
-    IDCT5 [rsp + 16], [rsp + 80], [rsp + 48], [rsp + 112], blockq +  8, 20
-    IDCT5 [rsp + 24], [rsp + 88], [rsp + 56], [rsp + 120], blockq + 12, 20
-    jmp %%9
-
-    ALIGN 16
-    %%5:
-
-    IDCT6 rsp +  0, rsp + 64, rsp + 32, rsp +  96, blockq +  0, 20
-    IDCT6 rsp + 16, rsp + 80, rsp + 48, rsp + 112, blockq +  8, 20
-    jmp %%9
-
-    ALIGN 16
-    %%1:
-
-    IDCT7 [rsp +  0], [rsp + 64], [rsp + 32], [rsp +  96], blockq +  0, 20
-    IDCT7 [rsp +  8], [rsp + 72], [rsp + 40], [rsp + 104], blockq +  4, 20
-    IDCT7 [rsp + 16], [rsp + 80], [rsp + 48], [rsp + 112], blockq +  8, 20
-    IDCT7 [rsp + 24], [rsp + 88], [rsp + 56], [rsp + 120], blockq + 12, 20
-    jmp %%9
-
-    ALIGN 16
-    %%7:
-
-    IDCT8 rsp +  0, rsp + 64, rsp + 32, rsp +  96, blockq +  0, 20
-    IDCT8 rsp + 16, rsp + 80, rsp + 48, rsp + 112, blockq +  8, 20
-
-    %%9:
-%endmacro
-
-%macro PUT_PIXELS_CLAMPED_HALF 1
-    mova     m0, [blockq+mmsize*0+%1]
-    mova     m1, [blockq+mmsize*2+%1]
-%if mmsize == 8
-    mova     m2, [blockq+mmsize*4+%1]
-    mova     m3, [blockq+mmsize*6+%1]
-%endif
-    packuswb m0, [blockq+mmsize*1+%1]
-    packuswb m1, [blockq+mmsize*3+%1]
-%if mmsize == 8
-    packuswb m2, [blockq+mmsize*5+%1]
-    packuswb m3, [blockq+mmsize*7+%1]
-    movq           [pixelsq], m0
-    movq    [lsizeq+pixelsq], m1
-    movq  [2*lsizeq+pixelsq], m2
-    movq   [lsize3q+pixelsq], m3
-%else
-    movq           [pixelsq], m0
-    movhps  [lsizeq+pixelsq], m0
-    movq  [2*lsizeq+pixelsq], m1
-    movhps [lsize3q+pixelsq], m1
-%endif
-%endmacro
-
-%macro ADD_PIXELS_CLAMPED 1
-    mova       m0, [blockq+mmsize*0+%1]
-    mova       m1, [blockq+mmsize*1+%1]
-%if mmsize == 8
-    mova       m5, [blockq+mmsize*2+%1]
-    mova       m6, [blockq+mmsize*3+%1]
-%endif
-    movq       m2, [pixelsq]
-    movq       m3, [pixelsq+lsizeq]
-%if mmsize == 8
-    mova       m7, m2
-    punpcklbw  m2, m4
-    punpckhbw  m7, m4
-    paddsw     m0, m2
-    paddsw     m1, m7
-    mova       m7, m3
-    punpcklbw  m3, m4
-    punpckhbw  m7, m4
-    paddsw     m5, m3
-    paddsw     m6, m7
-%else
-    punpcklbw  m2, m4
-    punpcklbw  m3, m4
-    paddsw     m0, m2
-    paddsw     m1, m3
-%endif
-    packuswb   m0, m1
-%if mmsize == 8
-    packuswb   m5, m6
-    movq       [pixelsq], m0
-    movq       [pixelsq+lsizeq], m5
-%else
-    movq       [pixelsq], m0
-    movhps     [pixelsq+lsizeq], m0
-%endif
-%endmacro
-
-INIT_MMX mmx
-
-cglobal simple_idct, 1, 2, 8, 128, block, t0
-    IDCT
-RET
-
-cglobal simple_idct_put, 3, 5, 8, 128, pixels, lsize, block, lsize3, t0
-    IDCT
-    lea lsize3q, [lsizeq*3]
-    PUT_PIXELS_CLAMPED_HALF 0
-    lea pixelsq, [pixelsq+lsizeq*4]
-    PUT_PIXELS_CLAMPED_HALF 64
-RET
-
-cglobal simple_idct_add, 3, 4, 8, 128, pixels, lsize, block, t0
-    IDCT
-    pxor       m4, m4
-    ADD_PIXELS_CLAMPED 0
-    lea        pixelsq, [pixelsq+lsizeq*2]
-    ADD_PIXELS_CLAMPED 32
-    lea        pixelsq, [pixelsq+lsizeq*2]
-    ADD_PIXELS_CLAMPED 64
-    lea        pixelsq, [pixelsq+lsizeq*2]
-    ADD_PIXELS_CLAMPED 96
-RET
-
-INIT_XMM sse2
-
-cglobal simple_idct_put, 3, 5, 8, 128, pixels, lsize, block, lsize3, t0
-    IDCT
-    lea lsize3q, [lsizeq*3]
-    PUT_PIXELS_CLAMPED_HALF 0
-    lea pixelsq, [pixelsq+lsizeq*4]
-    PUT_PIXELS_CLAMPED_HALF 64
-RET
-
-cglobal simple_idct_add, 3, 4, 8, 128, pixels, lsize, block, t0
-    IDCT
-    pxor       m4, m4
-    ADD_PIXELS_CLAMPED 0
-    lea        pixelsq, [pixelsq+lsizeq*2]
-    ADD_PIXELS_CLAMPED 32
-    lea        pixelsq, [pixelsq+lsizeq*2]
-    ADD_PIXELS_CLAMPED 64
-    lea        pixelsq, [pixelsq+lsizeq*2]
-    ADD_PIXELS_CLAMPED 96
-RET
diff -uparN ffmpeg-4.1/libavcodec/x86/svq1enc.asm ffmpeg-y/libavcodec/x86/svq1enc.asm
--- ffmpeg-4.1/libavcodec/x86/svq1enc.asm	2018-01-01 06:35:48.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/svq1enc.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,61 +0,0 @@
-;******************************************************************************
-;* SIMD-optimized SVQ1 encoder functions
-;* Copyright (c) 2007 Loren Merritt
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION .text
-
-%macro SSD_INT8_VS_INT16 0
-cglobal ssd_int8_vs_int16, 3, 3, 3, pix1, pix2, size
-    pxor m0, m0
-.loop:
-    sub       sizeq, 8
-    movq      m1, [pix1q + sizeq]
-    mova      m2, [pix2q + sizeq*2]
-%if mmsize == 8
-    movq      m3, [pix2q + sizeq*2 + mmsize]
-    punpckhbw m4, m1
-    punpcklbw m1, m1
-    psraw     m4, 8
-    psraw     m1, 8
-    psubw     m3, m4
-    psubw     m2, m1
-    pmaddwd   m3, m3
-    pmaddwd   m2, m2
-    paddd     m0, m3
-    paddd     m0, m2
-%else
-    punpcklbw m1, m1
-    psraw     m1, 8
-    psubw     m2, m1
-    pmaddwd   m2, m2
-    paddd     m0, m2
-%endif
-    jg .loop
-    HADDD     m0, m1
-    movd     eax, m0
-    RET
-%endmacro
-
-INIT_MMX mmx
-SSD_INT8_VS_INT16
-INIT_XMM sse2
-SSD_INT8_VS_INT16
diff -uparN ffmpeg-4.1/libavcodec/x86/synth_filter.asm ffmpeg-y/libavcodec/x86/synth_filter.asm
--- ffmpeg-4.1/libavcodec/x86/synth_filter.asm	2018-07-17 17:27:41.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/synth_filter.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,246 +0,0 @@
-;******************************************************************************
-;* SSE-optimized functions for the DCA decoder
-;* Copyright (C) 2012-2014 Christophe Gisquet <christophe.gisquet@gmail.com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION .text
-
-%macro SETZERO 1
-%if cpuflag(sse2) && notcpuflag(avx)
-    pxor          %1, %1
-%else
-    xorps         %1, %1, %1
-%endif
-%endmacro
-
-%macro SHUF 3
-%if cpuflag(avx)
-    mova          %3, [%2 - 16]
-    vperm2f128    %1, %3, %3, 1
-    vshufps       %1, %1, %1, q0123
-%elif cpuflag(sse2)
-    pshufd        %1, [%2], q0123
-%else
-    mova          %1, [%2]
-    shufps        %1, %1, q0123
-%endif
-%endmacro
-
-%macro INNER_LOOP   1
-    ; reading backwards:  ptr1 = synth_buf + j + i; ptr2 = synth_buf + j - i
-    ;~ a += window[i + j]      * (-synth_buf[15 - i + j])
-    ;~ b += window[i + j + 16] * (synth_buf[i + j])
-    SHUF          m5,  ptr2 + j + (15 - 3) * 4, m6
-    mova          m6, [ptr1 + j]
-%if ARCH_X86_64
-    SHUF         m11,  ptr2 + j + (15 - 3) * 4 - mmsize, m12
-    mova         m12, [ptr1 + j + mmsize]
-%endif
-%if cpuflag(fma3)
-    fmaddps       m2, m6,  [win + %1 + j + 16 * 4], m2
-    fnmaddps      m1, m5,  [win + %1 + j], m1
-%if ARCH_X86_64
-    fmaddps       m8, m12, [win + %1 + j + mmsize + 16 * 4], m8
-    fnmaddps      m7, m11, [win + %1 + j + mmsize], m7
-%endif
-%else ; non-FMA
-    mulps         m6, m6,  [win + %1 + j + 16 * 4]
-    mulps         m5, m5,  [win + %1 + j]
-%if ARCH_X86_64
-    mulps        m12, m12, [win + %1 + j + mmsize + 16 * 4]
-    mulps        m11, m11, [win + %1 + j + mmsize]
-%endif
-    addps         m2, m2, m6
-    subps         m1, m1, m5
-%if ARCH_X86_64
-    addps         m8, m8, m12
-    subps         m7, m7, m11
-%endif
-%endif ; cpuflag(fma3)
-    ;~ c += window[i + j + 32] * (synth_buf[16 + i + j])
-    ;~ d += window[i + j + 48] * (synth_buf[31 - i + j])
-    SHUF          m6,  ptr2 + j + (31 - 3) * 4, m5
-    mova          m5, [ptr1 + j + 16 * 4]
-%if ARCH_X86_64
-    SHUF         m12,  ptr2 + j + (31 - 3) * 4 - mmsize, m11
-    mova         m11, [ptr1 + j + mmsize + 16 * 4]
-%endif
-%if cpuflag(fma3)
-    fmaddps       m3, m5,  [win + %1 + j + 32 * 4], m3
-    fmaddps       m4, m6,  [win + %1 + j + 48 * 4], m4
-%if ARCH_X86_64
-    fmaddps       m9, m11, [win + %1 + j + mmsize + 32 * 4], m9
-    fmaddps      m10, m12, [win + %1 + j + mmsize + 48 * 4], m10
-%endif
-%else ; non-FMA
-    mulps         m5, m5,  [win + %1 + j + 32 * 4]
-    mulps         m6, m6,  [win + %1 + j + 48 * 4]
-%if ARCH_X86_64
-    mulps        m11, m11, [win + %1 + j + mmsize + 32 * 4]
-    mulps        m12, m12, [win + %1 + j + mmsize + 48 * 4]
-%endif
-    addps         m3, m3, m5
-    addps         m4, m4, m6
-%if ARCH_X86_64
-    addps         m9, m9, m11
-    addps        m10, m10, m12
-%endif
-%endif ; cpuflag(fma3)
-    sub            j, 64 * 4
-%endmacro
-
-; void ff_synth_filter_inner_<opt>(float *synth_buf, float synth_buf2[32],
-;                                  const float window[512], float out[32],
-;                                  intptr_t offset, float scale)
-%macro SYNTH_FILTER 0
-cglobal synth_filter_inner, 0, 6 + 4 * ARCH_X86_64, 7 + 6 * ARCH_X86_64, \
-                              synth_buf, synth_buf2, window, out, off, scale
-%define scale m0
-%if ARCH_X86_32 || WIN64
-%if cpuflag(sse2) && notcpuflag(avx)
-    movd       scale, scalem
-    SPLATD        m0
-%else
-    VBROADCASTSS  m0, scalem
-%endif
-; Make sure offset is in a register and not on the stack
-%define OFFQ  r4q
-%else
-    SPLATD      xmm0
-%if cpuflag(avx)
-    vinsertf128   m0, m0, xmm0, 1
-%endif
-%define OFFQ  offq
-%endif
-    ; prepare inner counter limit 1
-    mov          r5q, 480
-    sub          r5q, offmp
-    and          r5q, -64
-    shl          r5q, 2
-%if ARCH_X86_32 || notcpuflag(avx)
-    mov         OFFQ, r5q
-%define i        r5q
-    mov            i, 16 * 4 - (ARCH_X86_64 + 1) * mmsize  ; main loop counter
-%else
-%define i 0
-%define OFFQ  r5q
-%endif
-
-%define buf2     synth_buf2q
-%if ARCH_X86_32
-    mov         buf2, synth_buf2mp
-%endif
-.mainloop:
-    ; m1 = a  m2 = b  m3 = c  m4 = d
-    SETZERO       m3
-    SETZERO       m4
-    mova          m1, [buf2 + i]
-    mova          m2, [buf2 + i + 16 * 4]
-%if ARCH_X86_32
-%define ptr1     r0q
-%define ptr2     r1q
-%define win      r2q
-%define j        r3q
-    mov          win, windowm
-    mov         ptr1, synth_bufm
-%if ARCH_X86_32 || notcpuflag(avx)
-    add          win, i
-    add         ptr1, i
-%endif
-%else ; ARCH_X86_64
-%define ptr1     r6q
-%define ptr2     r7q ; must be loaded
-%define win      r8q
-%define j        r9q
-    SETZERO       m9
-    SETZERO      m10
-    mova          m7, [buf2 + i + mmsize]
-    mova          m8, [buf2 + i + mmsize + 16 * 4]
-    lea          win, [windowq + i]
-    lea         ptr1, [synth_bufq + i]
-%endif
-    mov         ptr2, synth_bufmp
-    ; prepare the inner loop counter
-    mov            j, OFFQ
-%if ARCH_X86_32 || notcpuflag(avx)
-    sub         ptr2, i
-%endif
-.loop1:
-    INNER_LOOP  0
-    jge       .loop1
-
-    mov            j, 448 * 4
-    sub            j, OFFQ
-    jz          .end
-    sub         ptr1, j
-    sub         ptr2, j
-    add          win, OFFQ ; now at j-64, so define OFFSET
-    sub            j, 64 * 4
-.loop2:
-    INNER_LOOP  64 * 4
-    jge       .loop2
-
-.end:
-%if ARCH_X86_32
-    mov         buf2, synth_buf2m ; needed for next iteration anyway
-    mov         outq, outmp       ; j, which will be set again during it
-%endif
-    ;~ out[i]      = a * scale;
-    ;~ out[i + 16] = b * scale;
-    mulps         m1, m1, scale
-    mulps         m2, m2, scale
-%if ARCH_X86_64
-    mulps         m7, m7, scale
-    mulps         m8, m8, scale
-%endif
-    ;~ synth_buf2[i]      = c;
-    ;~ synth_buf2[i + 16] = d;
-    mova   [buf2 + i +  0 * 4], m3
-    mova   [buf2 + i + 16 * 4], m4
-%if ARCH_X86_64
-    mova   [buf2 + i +  0 * 4 + mmsize], m9
-    mova   [buf2 + i + 16 * 4 + mmsize], m10
-%endif
-    ;~ out[i]      = a;
-    ;~ out[i + 16] = a;
-    mova   [outq + i +  0 * 4], m1
-    mova   [outq + i + 16 * 4], m2
-%if ARCH_X86_64
-    mova   [outq + i +  0 * 4 + mmsize], m7
-    mova   [outq + i + 16 * 4 + mmsize], m8
-%endif
-%if ARCH_X86_32 || notcpuflag(avx)
-    sub            i, (ARCH_X86_64 + 1) * mmsize
-    jge    .mainloop
-%endif
-    RET
-%endmacro
-
-%if ARCH_X86_32
-INIT_XMM sse
-SYNTH_FILTER
-%endif
-INIT_XMM sse2
-SYNTH_FILTER
-INIT_YMM avx
-SYNTH_FILTER
-INIT_YMM fma3
-SYNTH_FILTER
diff -uparN ffmpeg-4.1/libavcodec/x86/takdsp.asm ffmpeg-y/libavcodec/x86/takdsp.asm
--- ffmpeg-4.1/libavcodec/x86/takdsp.asm	2018-07-17 17:27:41.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/takdsp.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,116 +0,0 @@
-;******************************************************************************
-;* TAK DSP SIMD optimizations
-;*
-;* Copyright (C) 2015 Paul B Mahol
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-
-pd_128: times 4 dd 128
-
-SECTION .text
-
-INIT_XMM sse2
-cglobal tak_decorrelate_ls, 3, 3, 2, p1, p2, length
-    shl                     lengthd, 2
-    add                         p1q, lengthq
-    add                         p2q, lengthq
-    neg                     lengthq
-.loop:
-    mova                         m0, [p1q+lengthq+mmsize*0]
-    mova                         m1, [p1q+lengthq+mmsize*1]
-    paddd                        m0, [p2q+lengthq+mmsize*0]
-    paddd                        m1, [p2q+lengthq+mmsize*1]
-    mova     [p2q+lengthq+mmsize*0], m0
-    mova     [p2q+lengthq+mmsize*1], m1
-    add                     lengthq, mmsize*2
-    jl .loop
-    REP_RET
-
-cglobal tak_decorrelate_sr, 3, 3, 2, p1, p2, length
-    shl                     lengthd, 2
-    add                         p1q, lengthq
-    add                         p2q, lengthq
-    neg                     lengthq
-
-.loop:
-    mova                         m0, [p2q+lengthq+mmsize*0]
-    mova                         m1, [p2q+lengthq+mmsize*1]
-    psubd                        m0, [p1q+lengthq+mmsize*0]
-    psubd                        m1, [p1q+lengthq+mmsize*1]
-    mova     [p1q+lengthq+mmsize*0], m0
-    mova     [p1q+lengthq+mmsize*1], m1
-    add                     lengthq, mmsize*2
-    jl .loop
-    REP_RET
-
-cglobal tak_decorrelate_sm, 3, 3, 6, p1, p2, length
-    shl                     lengthd, 2
-    add                         p1q, lengthq
-    add                         p2q, lengthq
-    neg                     lengthq
-
-.loop:
-    mova                         m0, [p1q+lengthq]
-    mova                         m1, [p2q+lengthq]
-    mova                         m3, [p1q+lengthq+mmsize]
-    mova                         m4, [p2q+lengthq+mmsize]
-    mova                         m2, m1
-    mova                         m5, m4
-    psrad                        m2, 1
-    psrad                        m5, 1
-    psubd                        m0, m2
-    psubd                        m3, m5
-    paddd                        m1, m0
-    paddd                        m4, m3
-    mova              [p1q+lengthq], m0
-    mova              [p2q+lengthq], m1
-    mova       [p1q+lengthq+mmsize], m3
-    mova       [p2q+lengthq+mmsize], m4
-    add                     lengthq, mmsize*2
-    jl .loop
-    REP_RET
-
-INIT_XMM sse4
-cglobal tak_decorrelate_sf, 3, 3, 5, p1, p2, length, dshift, dfactor
-    shl             lengthd, 2
-    add                 p1q, lengthq
-    add                 p2q, lengthq
-    neg             lengthq
-
-    movd                 m2, dshiftm
-    movd                 m3, dfactorm
-    pshufd               m3, m3, 0
-    mova                 m4, [pd_128]
-
-.loop:
-    mova                 m0, [p1q+lengthq]
-    mova                 m1, [p2q+lengthq]
-    psrad                m1, m2
-    pmulld               m1, m3
-    paddd                m1, m4
-    psrad                m1, 8
-    pslld                m1, m2
-    psubd                m1, m0
-    mova      [p1q+lengthq], m1
-    add             lengthq, mmsize
-    jl .loop
-    REP_RET
diff -uparN ffmpeg-4.1/libavcodec/x86/ttadsp.asm ffmpeg-y/libavcodec/x86/ttadsp.asm
--- ffmpeg-4.1/libavcodec/x86/ttadsp.asm	2018-11-02 02:34:25.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/ttadsp.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,119 +0,0 @@
-;******************************************************************************
-;* TTA DSP SIMD optimizations
-;*
-;* Copyright (C) 2014 James Almer
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-
-pd_n0113: dd ~0, ~1, ~1, ~3
-pd_1224:  dd 1, 2, 2, 4
-
-SECTION .text
-
-%macro TTA_FILTER 2
-INIT_XMM %1
-cglobal tta_filter_process, 5,5,%2, qm, dx, dl, error, in, shift, round
-    mova       m2, [qmq       ]
-    mova       m3, [qmq + 0x10]
-    mova       m4, [dxq       ]
-    mova       m5, [dxq + 0x10]
-
-    movd       m6, [errorq]         ; if (filter->error < 0) {
-    SPLATD     m6                   ;     for (int i = 0; i < 8; i++)
-    psignd     m0, m4, m6           ;         filter->qm[i] -= filter->dx[i];
-    psignd     m1, m5, m6           ; } else if (filter->error > 0) {
-    paddd      m2, m0               ;     for (int i = 0; i < 8; i++)
-    paddd      m3, m1               ;         filter->qm[i] += filter->dx[i];
-    mova       [qmq       ], m2     ; }
-    mova       [qmq + 0x10], m3     ;
-
-    mova       m0, [dlq       ]
-    mova       m1, [dlq + 0x10]
-
-%if cpuflag(sse4)
-    pmulld     m2, m0
-    pmulld     m3, m1
-%else
-    pshufd     m6, m0, 0xb1
-    pshufd     m7, m2, 0xb1
-    pmuludq    m6, m7
-    pshufd     m6, m6, 0xd8
-    pmuludq    m2, m0
-    pshufd     m2, m2, 0xd8
-    punpckldq  m2, m6
-
-    pshufd     m6, m1, 0xb1
-    pshufd     m7, m3, 0xb1
-    pmuludq    m6, m7
-    pshufd     m6, m6, 0xd8
-    pmuludq    m3, m1
-    pshufd     m3, m3, 0xd8
-    punpckldq  m3, m6
-%endif
-    ; Using horizontal add (phaddd) seems to be slower than shuffling stuff around
-    paddd      m2, m3               ; int sum = filter->round +
-                                    ;           filter->dl[0] * filter->qm[0] +
-    pshufd     m3, m2, 0xe          ;           filter->dl[1] * filter->qm[1] +
-    paddd      m2, m3               ;           filter->dl[2] * filter->qm[2] +
-                                    ;           filter->dl[3] * filter->qm[3] +
-    movd       m6, roundm           ;           filter->dl[4] * filter->qm[4] +
-    paddd      m6, m2               ;           filter->dl[5] * filter->qm[5] +
-    pshufd     m2, m2, 0x1          ;           filter->dl[6] * filter->qm[6] +
-    paddd      m6, m2               ;           filter->dl[7] * filter->qm[7];
-
-    palignr    m5, m4, 4            ; filter->dx[0] = filter->dx[1]; filter->dx[1] = filter->dx[2];
-                                    ; filter->dx[2] = filter->dx[3]; filter->dx[3] = filter->dx[4];
-
-    palignr    m2, m1, m0, 4        ; filter->dl[0] = filter->dl[1]; filter->dl[1] = filter->dl[2];
-                                    ; filter->dl[2] = filter->dl[3]; filter->dl[3] = filter->dl[4];
-
-    psrad      m4, m1, 30           ; filter->dx[4] = ((filter->dl[4] >> 30) | 1);
-    por        m4, [pd_1224 ]       ; filter->dx[5] = ((filter->dl[5] >> 30) | 2) & ~1;
-    pand       m4, [pd_n0113]       ; filter->dx[6] = ((filter->dl[6] >> 30) | 2) & ~1;
-                                    ; filter->dx[7] = ((filter->dl[7] >> 30) | 4) & ~3;
-
-    mova       [dlq       ], m2
-    mova       [dxq       ], m5
-    mova       [dxq + 0x10], m4
-    movd       m0, [inq]            ; filter->error = *in;
-    movd       [errorq], m0         ;
-
-    movd       m2, shiftm           ; *in += (sum >> filter->shift);
-    psrad      m6, m2               ;
-    paddd      m0, m6               ;
-    movd       [inq], m0            ;
-
-    psrldq     m1, 4                ;
-    pslldq     m0, 12               ; filter->dl[4] = -filter->dl[5];
-    pshufd     m0, m0, 0xf0         ; filter->dl[5] = -filter->dl[6];
-    psubd      m0, m1               ; filter->dl[6] = *in - filter->dl[7];
-    psrldq     m1, m0, 4            ; filter->dl[7] = *in;
-    pshufd     m1, m1, 0xf4         ; filter->dl[5] += filter->dl[6];
-    paddd      m0, m1               ; filter->dl[4] += filter->dl[5];
-    psrldq     m1, 4                ;
-    paddd      m0, m1               ;
-    mova       [dlq + 0x10], m0     ;
-    RET
-%endmacro
-
-TTA_FILTER ssse3, 8
-TTA_FILTER sse4,  7
diff -uparN ffmpeg-4.1/libavcodec/x86/ttaencdsp.asm ffmpeg-y/libavcodec/x86/ttaencdsp.asm
--- ffmpeg-4.1/libavcodec/x86/ttaencdsp.asm	2018-11-02 02:34:25.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/ttaencdsp.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,119 +0,0 @@
-;******************************************************************************
-;* TTA Encoder DSP SIMD optimizations
-;*
-;* Copyright (C) 2014-2016 James Almer
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-
-pd_n0113: dd ~0, ~1, ~1, ~3
-pd_1224:  dd 1, 2, 2, 4
-
-SECTION .text
-
-%macro TTAENC_FILTER 2
-INIT_XMM %1
-cglobal ttaenc_filter_process, 5,5,%2, qm, dx, dl, error, in, shift, round
-    mova       m2, [qmq       ]
-    mova       m3, [qmq + 0x10]
-    mova       m4, [dxq       ]
-    mova       m5, [dxq + 0x10]
-
-    movd       m6, [errorq]         ; if (filter->error < 0) {
-    SPLATD     m6                   ;     for (int i = 0; i < 8; i++)
-    psignd     m0, m4, m6           ;         filter->qm[i] -= filter->dx[i];
-    psignd     m1, m5, m6           ; } else if (filter->error > 0) {
-    paddd      m2, m0               ;     for (int i = 0; i < 8; i++)
-    paddd      m3, m1               ;         filter->qm[i] += filter->dx[i];
-    mova       [qmq       ], m2     ; }
-    mova       [qmq + 0x10], m3     ;
-
-    mova       m0, [dlq       ]
-    mova       m1, [dlq + 0x10]
-
-%if cpuflag(sse4)
-    pmulld     m2, m0
-    pmulld     m3, m1
-%else
-    pshufd     m6, m0, 0xb1
-    pshufd     m7, m2, 0xb1
-    pmuludq    m6, m7
-    pshufd     m6, m6, 0xd8
-    pmuludq    m2, m0
-    pshufd     m2, m2, 0xd8
-    punpckldq  m2, m6
-
-    pshufd     m6, m1, 0xb1
-    pshufd     m7, m3, 0xb1
-    pmuludq    m6, m7
-    pshufd     m6, m6, 0xd8
-    pmuludq    m3, m1
-    pshufd     m3, m3, 0xd8
-    punpckldq  m3, m6
-%endif
-    ; Using horizontal add (phaddd) seems to be slower than shuffling stuff around
-    paddd      m2, m3               ; int sum = filter->round +
-                                    ;           filter->dl[0] * filter->qm[0] +
-    pshufd     m3, m2, 0xe          ;           filter->dl[1] * filter->qm[1] +
-    paddd      m2, m3               ;           filter->dl[2] * filter->qm[2] +
-                                    ;           filter->dl[3] * filter->qm[3] +
-    movd       m6, roundm           ;           filter->dl[4] * filter->qm[4] +
-    paddd      m6, m2               ;           filter->dl[5] * filter->qm[5] +
-    pshufd     m2, m2, 0x1          ;           filter->dl[6] * filter->qm[6] +
-    paddd      m6, m2               ;           filter->dl[7] * filter->qm[7];
-
-    palignr    m5, m4, 4            ; filter->dx[0] = filter->dx[1]; filter->dx[1] = filter->dx[2];
-                                    ; filter->dx[2] = filter->dx[3]; filter->dx[3] = filter->dx[4];
-
-    palignr    m2, m1, m0, 4        ; filter->dl[0] = filter->dl[1]; filter->dl[1] = filter->dl[2];
-                                    ; filter->dl[2] = filter->dl[3]; filter->dl[3] = filter->dl[4];
-
-    psrad      m4, m1, 30           ; filter->dx[4] = ((filter->dl[4] >> 30) | 1);
-    por        m4, [pd_1224 ]       ; filter->dx[5] = ((filter->dl[5] >> 30) | 2) & ~1;
-    pand       m4, [pd_n0113]       ; filter->dx[6] = ((filter->dl[6] >> 30) | 2) & ~1;
-                                    ; filter->dx[7] = ((filter->dl[7] >> 30) | 4) & ~3;
-
-    mova       [dlq       ], m2
-    mova       [dxq       ], m5
-    mova       [dxq + 0x10], m4
-
-    movd       m2, shiftm           ;
-    movd       m0, [inq]            ;
-    psrad      m6, m2               ;
-    psubd      m3, m0, m6           ;
-    movd       [inq], m3            ; *in -= (sum >> filter->shift);
-    movd       [errorq], m3         ; filter->error = *in;
-
-    psrldq     m1, 4                ;
-    pslldq     m0, 12               ; filter->dl[4] = -filter->dl[5];
-    pshufd     m0, m0, 0xf0         ; filter->dl[5] = -filter->dl[6];
-    psubd      m0, m1               ; filter->dl[6] = *in - filter->dl[7];
-    psrldq     m1, m0, 4            ; filter->dl[7] = *in;
-    pshufd     m1, m1, 0xf4         ; filter->dl[5] += filter->dl[6];
-    paddd      m0, m1               ; filter->dl[4] += filter->dl[5];
-    psrldq     m1, 4                ;
-    paddd      m0, m1               ;
-    mova       [dlq + 0x10], m0     ;
-    RET
-%endmacro
-
-TTAENC_FILTER ssse3, 8
-TTAENC_FILTER sse4,  7
diff -uparN ffmpeg-4.1/libavcodec/x86/utvideodsp.asm ffmpeg-y/libavcodec/x86/utvideodsp.asm
--- ffmpeg-4.1/libavcodec/x86/utvideodsp.asm	2018-11-03 08:17:29.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/utvideodsp.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,137 +0,0 @@
-;******************************************************************************
-;* SIMD-optimized UTVideo functions
-;* Copyright (c) 2017 Paul B Mahol
-;* Copyright (c) 2017 Jokyo Images
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-
-cextern pb_80
-cextern pw_512
-cextern pw_1023
-
-SECTION .text
-
-;-------------------------------------------------------------------------------------------
-; void restore_rgb_planes(uint8_t *src_r, uint8_t *src_g, uint8_t *src_b,
-;                         ptrdiff_t linesize_r, ptrdiff_t linesize_g, ptrdiff_t linesize_b,
-;                         int width, int height)
-;-------------------------------------------------------------------------------------------
-%macro RESTORE_RGB_PLANES 0
-cglobal restore_rgb_planes, 7 + ARCH_X86_64, 7 + ARCH_X86_64 * 2, 4, src_r, src_g, src_b, linesize_r, linesize_g, linesize_b, w, h, x
-    movsxdifnidn wq, wd
-    add      src_rq, wq
-    add      src_gq, wq
-    add      src_bq, wq
-    neg          wq
-%if ARCH_X86_64 == 0
-    mov          wm, wq
-DEFINE_ARGS src_r, src_g, src_b, linesize_r, linesize_g, linesize_b, x
-%define wq r6m
-%define hd r7mp
-%endif
-    mova         m3, [pb_80]
-.nextrow:
-    mov          xq, wq
-
-    .loop:
-        mova           m0, [src_rq + xq]
-        mova           m1, [src_gq + xq]
-        mova           m2, [src_bq + xq]
-        psubb          m1, m3
-        paddb          m0, m1
-        paddb          m2, m1
-        mova  [src_rq+xq], m0
-        mova  [src_bq+xq], m2
-        add            xq, mmsize
-    jl .loop
-
-    add        src_rq, linesize_rq
-    add        src_gq, linesize_gq
-    add        src_bq, linesize_bq
-    sub        hd, 1
-    jg .nextrow
-    REP_RET
-%endmacro
-
-INIT_XMM sse2
-RESTORE_RGB_PLANES
-
-%if HAVE_AVX2_EXTERNAL
-INIT_YMM avx2
-RESTORE_RGB_PLANES
-%endif
-
-;-------------------------------------------------------------------------------------------
-; void restore_rgb_planes10(uint16_t *src_r, uint16_t *src_g, uint16_t *src_b,
-;                         ptrdiff_t linesize_r, ptrdiff_t linesize_g, ptrdiff_t linesize_b,
-;                         int width, int height)
-;-------------------------------------------------------------------------------------------
-%macro RESTORE_RGB_PLANES10 0
-cglobal restore_rgb_planes10, 7 + ARCH_X86_64, 7 + ARCH_X86_64 * 2, 5, src_r, src_g, src_b, linesize_r, linesize_g, linesize_b, w, h, x
-    shl          wd, 1
-    shl linesize_rq, 1
-    shl linesize_gq, 1
-    shl linesize_bq, 1
-    add      src_rq, wq
-    add      src_gq, wq
-    add      src_bq, wq
-    mova         m3, [pw_512]
-    mova         m4, [pw_1023]
-    neg          wq
-%if ARCH_X86_64 == 0
-    mov          wm, wq
-DEFINE_ARGS src_r, src_g, src_b, linesize_r, linesize_g, linesize_b, x
-%define wq r6m
-%define hd r7mp
-%endif
-.nextrow:
-    mov          xq, wq
-
-    .loop:
-        mova           m0, [src_rq + xq]
-        mova           m1, [src_gq + xq]
-        mova           m2, [src_bq + xq]
-        psubw          m1, m3
-        paddw          m0, m1
-        paddw          m2, m1
-        pand           m0, m4
-        pand           m2, m4
-        mova  [src_rq+xq], m0
-        mova  [src_bq+xq], m2
-        add            xq, mmsize
-    jl .loop
-
-    add        src_rq, linesize_rq
-    add        src_gq, linesize_gq
-    add        src_bq, linesize_bq
-    sub        hd, 1
-    jg .nextrow
-    REP_RET
-%endmacro
-
-INIT_XMM sse2
-RESTORE_RGB_PLANES10
-
-%if HAVE_AVX2_EXTERNAL
-INIT_YMM avx2
-RESTORE_RGB_PLANES10
-%endif
diff -uparN ffmpeg-4.1/libavcodec/x86/v210.asm ffmpeg-y/libavcodec/x86/v210.asm
--- ffmpeg-4.1/libavcodec/x86/v210.asm	2018-01-01 06:35:48.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/v210.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,90 +0,0 @@
-;******************************************************************************
-;* V210 SIMD unpack
-;* Copyright (c) 2011 Loren Merritt <lorenm@u.washington.edu>
-;* Copyright (c) 2011 Kieran Kunhya <kieran@kunhya.com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-
-v210_mask: times 4 dd 0x3ff
-v210_mult: dw 64,4,64,4,64,4,64,4
-v210_luma_shuf: db 8,9,0,1,2,3,12,13,4,5,6,7,-1,-1,-1,-1
-v210_chroma_shuf: db 0,1,8,9,6,7,-1,-1,2,3,4,5,12,13,-1,-1
-
-SECTION .text
-
-%macro v210_planar_unpack 1
-
-; v210_planar_unpack(const uint32_t *src, uint16_t *y, uint16_t *u, uint16_t *v, int width)
-cglobal v210_planar_unpack_%1, 5, 5, 7
-    movsxdifnidn r4, r4d
-    lea    r1, [r1+2*r4]
-    add    r2, r4
-    add    r3, r4
-    neg    r4
-
-    mova   m3, [v210_mult]
-    mova   m4, [v210_mask]
-    mova   m5, [v210_luma_shuf]
-    mova   m6, [v210_chroma_shuf]
-.loop:
-%ifidn %1, unaligned
-    movu   m0, [r0]
-%else
-    mova   m0, [r0]
-%endif
-
-    pmullw m1, m0, m3
-    psrld  m0, 10
-    psrlw  m1, 6  ; u0 v0 y1 y2 v1 u2 y4 y5
-    pand   m0, m4 ; y0 __ u1 __ y3 __ v2 __
-
-    shufps m2, m1, m0, 0x8d ; y1 y2 y4 y5 y0 __ y3 __
-    pshufb m2, m5 ; y0 y1 y2 y3 y4 y5 __ __
-    movu   [r1+2*r4], m2
-
-    shufps m1, m0, 0xd8 ; u0 v0 v1 u2 u1 __ v2 __
-    pshufb m1, m6 ; u0 u1 u2 __ v0 v1 v2 __
-    movq   [r2+r4], m1
-    movhps [r3+r4], m1
-
-    add r0, mmsize
-    add r4, 6
-    jl  .loop
-
-    REP_RET
-%endmacro
-
-INIT_XMM ssse3
-v210_planar_unpack unaligned
-
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-v210_planar_unpack unaligned
-%endif
-
-INIT_XMM ssse3
-v210_planar_unpack aligned
-
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-v210_planar_unpack aligned
-%endif
diff -uparN ffmpeg-4.1/libavcodec/x86/v210enc.asm ffmpeg-y/libavcodec/x86/v210enc.asm
--- ffmpeg-4.1/libavcodec/x86/v210enc.asm	2018-11-02 02:34:25.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/v210enc.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,184 +0,0 @@
-;******************************************************************************
-;* V210 SIMD pack
-;* Copyright (c) 2014 Kieran Kunhya <kierank@obe.tv>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA 32
-
-cextern pw_4
-%define v210_enc_min_10 pw_4
-v210_enc_max_10: times 16 dw 0x3fb
-
-v210_enc_luma_mult_10: times 2 dw 4,1,16,4,1,16,0,0
-v210_enc_luma_shuf_10: times 2 db -1,0,1,-1,2,3,4,5,-1,6,7,-1,8,9,10,11
-
-v210_enc_chroma_mult_10: times 2 dw 1,4,16,0,16,1,4,0
-v210_enc_chroma_shuf_10: times 2 db 0,1,8,9,-1,2,3,-1,10,11,4,5,-1,12,13,-1
-
-cextern pb_1
-%define v210_enc_min_8 pb_1
-cextern pb_FE
-%define v210_enc_max_8 pb_FE
-
-v210_enc_luma_shuf_8: times 2 db 6,-1,7,-1,8,-1,9,-1,10,-1,11,-1,-1,-1,-1,-1
-v210_enc_luma_mult_8: times 2 dw 16,4,64,16,4,64,0,0
-
-v210_enc_chroma_shuf1_8: times 2 db 0,-1,1,-1,2,-1,3,-1,8,-1,9,-1,10,-1,11,-1
-v210_enc_chroma_shuf2_8: times 2 db 3,-1,4,-1,5,-1,7,-1,11,-1,12,-1,13,-1,15,-1
-
-v210_enc_chroma_mult_8: times 2 dw 4,16,64,0,64,4,16,0
-
-SECTION .text
-
-%macro v210_planar_pack_10 0
-
-; v210_planar_pack_10(const uint16_t *y, const uint16_t *u, const uint16_t *v, uint8_t *dst, ptrdiff_t width)
-cglobal v210_planar_pack_10, 5, 5, 4+cpuflag(avx2), y, u, v, dst, width
-    lea     r0, [yq+2*widthq]
-    add     uq, widthq
-    add     vq, widthq
-    neg     widthq
-
-    mova    m2, [v210_enc_min_10]
-    mova    m3, [v210_enc_max_10]
-
-.loop:
-    movu        xm0, [yq+2*widthq]
-%if cpuflag(avx2)
-    vinserti128 m0,   m0, [yq+widthq*2+12], 1
-%endif
-    CLIPW   m0, m2, m3
-
-    movq         xm1, [uq+widthq]
-    movhps       xm1, [vq+widthq]
-%if cpuflag(avx2)
-    movq         xm4, [uq+widthq+6]
-    movhps       xm4, [vq+widthq+6]
-    vinserti128  m1,   m1, xm4, 1
-%endif
-    CLIPW   m1, m2, m3
-
-    pmullw  m0, [v210_enc_luma_mult_10]
-    pshufb  m0, [v210_enc_luma_shuf_10]
-
-    pmullw  m1, [v210_enc_chroma_mult_10]
-    pshufb  m1, [v210_enc_chroma_shuf_10]
-
-    por     m0, m1
-
-    movu    [dstq], m0
-
-    add     dstq, mmsize
-    add     widthq, (mmsize*3)/8
-    jl .loop
-
-    RET
-%endmacro
-
-%if HAVE_SSSE3_EXTERNAL
-INIT_XMM ssse3
-v210_planar_pack_10
-%endif
-
-%if HAVE_AVX2_EXTERNAL
-INIT_YMM avx2
-v210_planar_pack_10
-%endif
-
-%macro v210_planar_pack_8 0
-
-; v210_planar_pack_8(const uint8_t *y, const uint8_t *u, const uint8_t *v, uint8_t *dst, ptrdiff_t width)
-cglobal v210_planar_pack_8, 5, 5, 7, y, u, v, dst, width
-    add     yq, widthq
-    shr     widthq, 1
-    add     uq, widthq
-    add     vq, widthq
-    neg     widthq
-
-    mova    m4, [v210_enc_min_8]
-    mova    m5, [v210_enc_max_8]
-    pxor    m6, m6
-
-.loop:
-    movu        xm1, [yq+widthq*2]
-%if cpuflag(avx2)
-    vinserti128 m1,   m1, [yq+widthq*2+12], 1
-%endif
-    CLIPUB  m1, m4, m5
-
-    punpcklbw m0, m1, m6
-    ; can't unpack high bytes in the same way because we process
-    ; only six bytes at a time
-    pshufb  m1, [v210_enc_luma_shuf_8]
-
-    pmullw  m0, [v210_enc_luma_mult_8]
-    pmullw  m1, [v210_enc_luma_mult_8]
-    pshufb  m0, [v210_enc_luma_shuf_10]
-    pshufb  m1, [v210_enc_luma_shuf_10]
-
-    movq         xm3, [uq+widthq]
-    movhps       xm3, [vq+widthq]
-%if cpuflag(avx2)
-    movq         xm2, [uq+widthq+6]
-    movhps       xm2, [vq+widthq+6]
-    vinserti128  m3,   m3, xm2, 1
-%endif
-    CLIPUB  m3, m4, m5
-
-    ; shuffle and multiply to get the same packing as in 10-bit
-    pshufb  m2, m3, [v210_enc_chroma_shuf1_8]
-    pshufb  m3, [v210_enc_chroma_shuf2_8]
-
-    pmullw  m2, [v210_enc_chroma_mult_8]
-    pmullw  m3, [v210_enc_chroma_mult_8]
-    pshufb  m2, [v210_enc_chroma_shuf_10]
-    pshufb  m3, [v210_enc_chroma_shuf_10]
-
-    por     m0, m2
-    por     m1, m3
-
-    movu         [dstq],    xm0
-    movu         [dstq+16], xm1
-%if cpuflag(avx2)
-    vextracti128 [dstq+32], m0, 1
-    vextracti128 [dstq+48], m1, 1
-%endif
-
-    add     dstq, 2*mmsize
-    add     widthq, (mmsize*3)/8
-    jl .loop
-
-    RET
-%endmacro
-
-%if HAVE_SSSE3_EXTERNAL
-INIT_XMM ssse3
-v210_planar_pack_8
-%endif
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-v210_planar_pack_8
-%endif
-
-%if HAVE_AVX2_EXTERNAL
-INIT_YMM avx2
-v210_planar_pack_8
-%endif
diff -uparN ffmpeg-4.1/libavcodec/x86/vc1dsp_loopfilter.asm ffmpeg-y/libavcodec/x86/vc1dsp_loopfilter.asm
--- ffmpeg-4.1/libavcodec/x86/vc1dsp_loopfilter.asm	2018-11-02 02:34:25.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/vc1dsp_loopfilter.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,317 +0,0 @@
-;******************************************************************************
-;* VC1 loopfilter optimizations
-;* Copyright (c) 2009 David Conrad
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-cextern pw_4
-cextern pw_5
-
-SECTION .text
-
-; dst_low, dst_high (src), zero
-; zero-extends one vector from 8 to 16 bits
-%macro UNPACK_8TO16 4
-    mova      m%2, m%3
-    punpckh%1 m%3, m%4
-    punpckl%1 m%2, m%4
-%endmacro
-
-%macro STORE_4_WORDS 6
-%if cpuflag(sse4)
-    pextrw %1, %5, %6+0
-    pextrw %2, %5, %6+1
-    pextrw %3, %5, %6+2
-    pextrw %4, %5, %6+3
-%else
-    movd  %6d, %5
-%if mmsize==16
-    psrldq %5, 4
-%else
-    psrlq  %5, 32
-%endif
-    mov    %1, %6w
-    shr    %6, 16
-    mov    %2, %6w
-    movd  %6d, %5
-    mov    %3, %6w
-    shr    %6, 16
-    mov    %4, %6w
-%endif
-%endmacro
-
-; in:  p1 p0 q0 q1, clobbers p0
-; out: p1 = (2*(p1 - q1) - 5*(p0 - q0) + 4) >> 3
-%macro VC1_LOOP_FILTER_A0 4
-    psubw  %1, %4
-    psubw  %2, %3
-    paddw  %1, %1
-    pmullw %2, [pw_5]
-    psubw  %1, %2
-    paddw  %1, [pw_4]
-    psraw  %1, 3
-%endmacro
-
-; in: p0 q0 a0 a1 a2
-;     m0 m1 m7 m6 m5
-; %1: size
-; out: m0=p0' m1=q0'
-%macro VC1_FILTER 1
-    PABSW   m4, m7
-    PABSW   m3, m6
-    PABSW   m2, m5
-    mova    m6, m4
-    pminsw  m3, m2
-    pcmpgtw m6, m3  ; if (a2 < a0 || a1 < a0)
-    psubw   m3, m4
-    pmullw  m3, [pw_5]   ; 5*(a3 - a0)
-    PABSW   m2, m3
-    psraw   m2, 3   ; abs(d/8)
-    pxor    m7, m3  ; d_sign ^= a0_sign
-
-    pxor    m5, m5
-    movd    m3, r2d
-%if %1 > 4
-    punpcklbw m3, m3
-%endif
-    punpcklbw m3, m5
-    pcmpgtw m3, m4  ; if (a0 < pq)
-    pand    m6, m3
-
-    mova    m3, m0
-    psubw   m3, m1
-    PABSW   m4, m3
-    psraw   m4, 1
-    pxor    m3, m7  ; d_sign ^ clip_sign
-    psraw   m3, 15
-    pminsw  m2, m4  ; min(d, clip)
-    pcmpgtw m4, m5
-    pand    m6, m4  ; filt3 (C return value)
-
-; each set of 4 pixels is not filtered if the 3rd is not
-%if mmsize==16
-    pshuflw m4, m6, 0xaa
-%if %1 > 4
-    pshufhw m4, m4, 0xaa
-%endif
-%else
-    pshufw  m4, m6, 0xaa
-%endif
-    pandn   m3, m4
-    pand    m2, m6
-    pand    m3, m2  ; d final
-
-    psraw   m7, 15
-    pxor    m3, m7
-    psubw   m3, m7
-    psubw   m0, m3
-    paddw   m1, m3
-    packuswb m0, m0
-    packuswb m1, m1
-%endmacro
-
-; 1st param: size of filter
-; 2nd param: mov suffix equivalent to the filter size
-%macro VC1_V_LOOP_FILTER 2
-    pxor      m5, m5
-    mov%2     m6, [r4]
-    mov%2     m4, [r4+r1]
-    mov%2     m7, [r4+2*r1]
-    mov%2     m0, [r4+r3]
-    punpcklbw m6, m5
-    punpcklbw m4, m5
-    punpcklbw m7, m5
-    punpcklbw m0, m5
-
-    VC1_LOOP_FILTER_A0 m6, m4, m7, m0
-    mov%2     m1, [r0]
-    mov%2     m2, [r0+r1]
-    punpcklbw m1, m5
-    punpcklbw m2, m5
-    mova      m4, m0
-    VC1_LOOP_FILTER_A0 m7, m4, m1, m2
-    mov%2     m3, [r0+2*r1]
-    mov%2     m4, [r0+r3]
-    punpcklbw m3, m5
-    punpcklbw m4, m5
-    mova      m5, m1
-    VC1_LOOP_FILTER_A0 m5, m2, m3, m4
-
-    VC1_FILTER %1
-    mov%2 [r4+r3], m0
-    mov%2 [r0],    m1
-%endmacro
-
-; 1st param: size of filter
-;     NOTE: UNPACK_8TO16 this number of 8 bit numbers are in half a register
-; 2nd (optional) param: temp register to use for storing words
-%macro VC1_H_LOOP_FILTER 1-2
-%if %1 == 4
-    movq      m0, [r0     -4]
-    movq      m1, [r0+  r1-4]
-    movq      m2, [r0+2*r1-4]
-    movq      m3, [r0+  r3-4]
-    TRANSPOSE4x4B 0, 1, 2, 3, 4
-%else
-    movq      m0, [r0     -4]
-    movq      m4, [r0+  r1-4]
-    movq      m1, [r0+2*r1-4]
-    movq      m5, [r0+  r3-4]
-    movq      m2, [r4     -4]
-    movq      m6, [r4+  r1-4]
-    movq      m3, [r4+2*r1-4]
-    movq      m7, [r4+  r3-4]
-    punpcklbw m0, m4
-    punpcklbw m1, m5
-    punpcklbw m2, m6
-    punpcklbw m3, m7
-    TRANSPOSE4x4W 0, 1, 2, 3, 4
-%endif
-    pxor      m5, m5
-
-    UNPACK_8TO16 bw, 6, 0, 5
-    UNPACK_8TO16 bw, 7, 1, 5
-    VC1_LOOP_FILTER_A0 m6, m0, m7, m1
-    UNPACK_8TO16 bw, 4, 2, 5
-    mova    m0, m1                      ; m0 = p0
-    VC1_LOOP_FILTER_A0 m7, m1, m4, m2
-    UNPACK_8TO16 bw, 1, 3, 5
-    mova    m5, m4
-    VC1_LOOP_FILTER_A0 m5, m2, m1, m3
-    SWAP 1, 4                           ; m1 = q0
-
-    VC1_FILTER %1
-    punpcklbw m0, m1
-%if %0 > 1
-    STORE_4_WORDS [r0-1], [r0+r1-1], [r0+2*r1-1], [r0+r3-1], m0, %2
-%if %1 > 4
-    psrldq m0, 4
-    STORE_4_WORDS [r4-1], [r4+r1-1], [r4+2*r1-1], [r4+r3-1], m0, %2
-%endif
-%else
-    STORE_4_WORDS [r0-1], [r0+r1-1], [r0+2*r1-1], [r0+r3-1], m0, 0
-    STORE_4_WORDS [r4-1], [r4+r1-1], [r4+2*r1-1], [r4+r3-1], m0, 4
-%endif
-%endmacro
-
-
-%macro START_V_FILTER 0
-    mov  r4, r0
-    lea  r3, [4*r1]
-    sub  r4, r3
-    lea  r3, [r1+2*r1]
-    imul r2, 0x01010101
-%endmacro
-
-%macro START_H_FILTER 1
-    lea  r3, [r1+2*r1]
-%if %1 > 4
-    lea  r4, [r0+4*r1]
-%endif
-    imul r2, 0x01010101
-%endmacro
-
-%macro VC1_LF 0
-cglobal vc1_v_loop_filter_internal
-    VC1_V_LOOP_FILTER 4, d
-    ret
-
-cglobal vc1_h_loop_filter_internal
-    VC1_H_LOOP_FILTER 4, r4
-    ret
-
-; void ff_vc1_v_loop_filter4_mmxext(uint8_t *src, int stride, int pq)
-cglobal vc1_v_loop_filter4, 3,5,0
-    START_V_FILTER
-    call vc1_v_loop_filter_internal
-    RET
-
-; void ff_vc1_h_loop_filter4_mmxext(uint8_t *src, int stride, int pq)
-cglobal vc1_h_loop_filter4, 3,5,0
-    START_H_FILTER 4
-    call vc1_h_loop_filter_internal
-    RET
-
-; void ff_vc1_v_loop_filter8_mmxext(uint8_t *src, int stride, int pq)
-cglobal vc1_v_loop_filter8, 3,5,0
-    START_V_FILTER
-    call vc1_v_loop_filter_internal
-    add  r4, 4
-    add  r0, 4
-    call vc1_v_loop_filter_internal
-    RET
-
-; void ff_vc1_h_loop_filter8_mmxext(uint8_t *src, int stride, int pq)
-cglobal vc1_h_loop_filter8, 3,5,0
-    START_H_FILTER 4
-    call vc1_h_loop_filter_internal
-    lea  r0, [r0+4*r1]
-    call vc1_h_loop_filter_internal
-    RET
-%endmacro
-
-INIT_MMX mmxext
-VC1_LF
-
-INIT_XMM sse2
-; void ff_vc1_v_loop_filter8_sse2(uint8_t *src, int stride, int pq)
-cglobal vc1_v_loop_filter8, 3,5,8
-    START_V_FILTER
-    VC1_V_LOOP_FILTER 8, q
-    RET
-
-; void ff_vc1_h_loop_filter8_sse2(uint8_t *src, int stride, int pq)
-cglobal vc1_h_loop_filter8, 3,6,8
-    START_H_FILTER 8
-    VC1_H_LOOP_FILTER 8, r5
-    RET
-
-INIT_MMX ssse3
-; void ff_vc1_v_loop_filter4_ssse3(uint8_t *src, int stride, int pq)
-cglobal vc1_v_loop_filter4, 3,5,0
-    START_V_FILTER
-    VC1_V_LOOP_FILTER 4, d
-    RET
-
-; void ff_vc1_h_loop_filter4_ssse3(uint8_t *src, int stride, int pq)
-cglobal vc1_h_loop_filter4, 3,5,0
-    START_H_FILTER 4
-    VC1_H_LOOP_FILTER 4, r4
-    RET
-
-INIT_XMM ssse3
-; void ff_vc1_v_loop_filter8_ssse3(uint8_t *src, int stride, int pq)
-cglobal vc1_v_loop_filter8, 3,5,8
-    START_V_FILTER
-    VC1_V_LOOP_FILTER 8, q
-    RET
-
-; void ff_vc1_h_loop_filter8_ssse3(uint8_t *src, int stride, int pq)
-cglobal vc1_h_loop_filter8, 3,6,8
-    START_H_FILTER 8
-    VC1_H_LOOP_FILTER 8, r5
-    RET
-
-INIT_XMM sse4
-; void ff_vc1_h_loop_filter8_sse4(uint8_t *src, int stride, int pq)
-cglobal vc1_h_loop_filter8, 3,5,8
-    START_H_FILTER 8
-    VC1_H_LOOP_FILTER 8
-    RET
diff -uparN ffmpeg-4.1/libavcodec/x86/vc1dsp_mc.asm ffmpeg-y/libavcodec/x86/vc1dsp_mc.asm
--- ffmpeg-4.1/libavcodec/x86/vc1dsp_mc.asm	2018-11-02 02:34:25.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/vc1dsp_mc.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,292 +0,0 @@
-;******************************************************************************
-;* VC1 motion compensation optimizations
-;* Copyright (c) 2007 Christophe GISQUET <christophe.gisquet@free.fr>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-cextern pw_9
-cextern pw_128
-
-SECTION .text
-
-%if HAVE_MMX_INLINE
-
-; XXX some of these macros are not used right now, but they will in the future
-;     when more functions are ported.
-
-%macro OP_PUT 2 ; dst, src
-%endmacro
-
-%macro OP_AVG 2 ; dst, src
-    pavgb           %1, %2
-%endmacro
-
-%macro NORMALIZE_MMX 1 ; shift
-    paddw           m3, m7 ; +bias-r
-    paddw           m4, m7 ; +bias-r
-    psraw           m3, %1
-    psraw           m4, %1
-%endmacro
-
-%macro TRANSFER_DO_PACK 2 ; op, dst
-    packuswb        m3, m4
-    %1              m3, [%2]
-    mova          [%2], m3
-%endmacro
-
-%macro TRANSFER_DONT_PACK 2 ; op, dst
-    %1              m3, [%2]
-    %1              m3, [%2 + mmsize]
-    mova          [%2], m3
-    mova [mmsize + %2], m4
-%endmacro
-
-; see MSPEL_FILTER13_CORE for use as UNPACK macro
-%macro DO_UNPACK 1 ; reg
-    punpcklbw       %1, m0
-%endmacro
-%macro DONT_UNPACK 1 ; reg
-%endmacro
-
-; Compute the rounder 32-r or 8-r and unpacks it to m7
-%macro LOAD_ROUNDER_MMX 1 ; round
-    movd      m7, %1
-    punpcklwd m7, m7
-    punpckldq m7, m7
-%endmacro
-
-%macro SHIFT2_LINE 5 ; off, r0, r1, r2, r3
-    paddw          m%3, m%4
-    movh           m%2, [srcq + stride_neg2]
-    pmullw         m%3, m6
-    punpcklbw      m%2, m0
-    movh           m%5, [srcq + strideq]
-    psubw          m%3, m%2
-    punpcklbw      m%5, m0
-    paddw          m%3, m7
-    psubw          m%3, m%5
-    psraw          m%3, shift
-    movu   [dstq + %1], m%3
-    add           srcq, strideq
-%endmacro
-
-INIT_MMX mmx
-; void ff_vc1_put_ver_16b_shift2_mmx(int16_t *dst, const uint8_t *src,
-;                                    x86_reg stride, int rnd, int64_t shift)
-; Sacrificing m6 makes it possible to pipeline loads from src
-%if ARCH_X86_32
-cglobal vc1_put_ver_16b_shift2, 3,6,0, dst, src, stride
-    DECLARE_REG_TMP     3, 4, 5
-    %define rnd r3mp
-    %define shift qword r4m
-%else ; X86_64
-cglobal vc1_put_ver_16b_shift2, 4,7,0, dst, src, stride
-    DECLARE_REG_TMP     4, 5, 6
-    %define   rnd r3d
-    ; We need shift either in memory or in a mm reg as it's used in psraw
-    ; On WIN64, the arg is already on the stack
-    ; On UNIX64, m5 doesn't seem to be used
-%if WIN64
-    %define shift r4mp
-%else ; UNIX64
-    %define shift m5
-    mova shift, r4q
-%endif ; WIN64
-%endif ; X86_32
-%define stride_neg2 t0q
-%define stride_9minus4 t1q
-%define i t2q
-    mov       stride_neg2, strideq
-    neg       stride_neg2
-    add       stride_neg2, stride_neg2
-    lea    stride_9minus4, [strideq * 9 - 4]
-    mov                 i, 3
-    LOAD_ROUNDER_MMX  rnd
-    mova               m6, [pw_9]
-    pxor               m0, m0
-.loop:
-    movh               m2, [srcq]
-    add              srcq, strideq
-    movh               m3, [srcq]
-    punpcklbw          m2, m0
-    punpcklbw          m3, m0
-    SHIFT2_LINE         0, 1, 2, 3, 4
-    SHIFT2_LINE        24, 2, 3, 4, 1
-    SHIFT2_LINE        48, 3, 4, 1, 2
-    SHIFT2_LINE        72, 4, 1, 2, 3
-    SHIFT2_LINE        96, 1, 2, 3, 4
-    SHIFT2_LINE       120, 2, 3, 4, 1
-    SHIFT2_LINE       144, 3, 4, 1, 2
-    SHIFT2_LINE       168, 4, 1, 2, 3
-    sub              srcq, stride_9minus4
-    add              dstq, 8
-    dec                 i
-        jnz         .loop
-    REP_RET
-%undef rnd
-%undef shift
-%undef stride_neg2
-%undef stride_9minus4
-%undef i
-
-; void ff_vc1_*_hor_16b_shift2_mmx(uint8_t *dst, x86_reg stride,
-;                                  const int16_t *src, int rnd);
-; Data is already unpacked, so some operations can directly be made from
-; memory.
-%macro HOR_16B_SHIFT2 2 ; op, opname
-cglobal vc1_%2_hor_16b_shift2, 4, 5, 0, dst, stride, src, rnd, h
-    mov                hq, 8
-    sub              srcq, 2
-    sub              rndd, (-1+9+9-1) * 1024 ; add -1024 bias
-    LOAD_ROUNDER_MMX rndd
-    mova               m5, [pw_9]
-    mova               m6, [pw_128]
-    pxor               m0, m0
-
-.loop:
-    mova               m1, [srcq + 2 * 0]
-    mova               m2, [srcq + 2 * 0 + mmsize]
-    mova               m3, [srcq + 2 * 1]
-    mova               m4, [srcq + 2 * 1 + mmsize]
-    paddw              m3, [srcq + 2 * 2]
-    paddw              m4, [srcq + 2 * 2 + mmsize]
-    paddw              m1, [srcq + 2 * 3]
-    paddw              m2, [srcq + 2 * 3 + mmsize]
-    pmullw             m3, m5
-    pmullw             m4, m5
-    psubw              m3, m1
-    psubw              m4, m2
-    NORMALIZE_MMX      7
-    ; remove bias
-    paddw              m3, m6
-    paddw              m4, m6
-    TRANSFER_DO_PACK   %1, dstq
-    add              srcq, 24
-    add              dstq, strideq
-    dec                hq
-        jnz         .loop
-
-    RET
-%endmacro
-
-INIT_MMX mmx
-HOR_16B_SHIFT2 OP_PUT, put
-
-INIT_MMX mmxext
-HOR_16B_SHIFT2 OP_AVG, avg
-%endif ; HAVE_MMX_INLINE
-
-%macro INV_TRANS_INIT 0
-    movsxdifnidn linesizeq, linesized
-    movd       m0, blockd
-    SPLATW     m0, m0
-    pxor       m1, m1
-    psubw      m1, m0
-    packuswb   m0, m0
-    packuswb   m1, m1
-
-    DEFINE_ARGS dest, linesize, linesize3
-    lea    linesize3q, [linesizeq*3]
-%endmacro
-
-%macro INV_TRANS_PROCESS 1
-    mov%1                  m2, [destq+linesizeq*0]
-    mov%1                  m3, [destq+linesizeq*1]
-    mov%1                  m4, [destq+linesizeq*2]
-    mov%1                  m5, [destq+linesize3q]
-    paddusb                m2, m0
-    paddusb                m3, m0
-    paddusb                m4, m0
-    paddusb                m5, m0
-    psubusb                m2, m1
-    psubusb                m3, m1
-    psubusb                m4, m1
-    psubusb                m5, m1
-    mov%1 [linesizeq*0+destq], m2
-    mov%1 [linesizeq*1+destq], m3
-    mov%1 [linesizeq*2+destq], m4
-    mov%1 [linesize3q +destq], m5
-%endmacro
-
-; ff_vc1_inv_trans_?x?_dc_mmxext(uint8_t *dest, ptrdiff_t linesize, int16_t *block)
-INIT_MMX mmxext
-cglobal vc1_inv_trans_4x4_dc, 3,4,0, dest, linesize, block
-    movsx         r3d, WORD [blockq]
-    mov        blockd, r3d             ; dc
-    shl        blockd, 4               ; 16 * dc
-    lea        blockd, [blockq+r3+4]   ; 17 * dc + 4
-    sar        blockd, 3               ; >> 3
-    mov           r3d, blockd          ; dc
-    shl        blockd, 4               ; 16 * dc
-    lea        blockd, [blockq+r3+64]  ; 17 * dc + 64
-    sar        blockd, 7               ; >> 7
-
-    INV_TRANS_INIT
-
-    INV_TRANS_PROCESS h
-    RET
-
-INIT_MMX mmxext
-cglobal vc1_inv_trans_4x8_dc, 3,4,0, dest, linesize, block
-    movsx         r3d, WORD [blockq]
-    mov        blockd, r3d             ; dc
-    shl        blockd, 4               ; 16 * dc
-    lea        blockd, [blockq+r3+4]   ; 17 * dc + 4
-    sar        blockd, 3               ; >> 3
-    shl        blockd, 2               ;  4 * dc
-    lea        blockd, [blockq*3+64]   ; 12 * dc + 64
-    sar        blockd, 7               ; >> 7
-
-    INV_TRANS_INIT
-
-    INV_TRANS_PROCESS h
-    lea         destq, [destq+linesizeq*4]
-    INV_TRANS_PROCESS h
-    RET
-
-INIT_MMX mmxext
-cglobal vc1_inv_trans_8x4_dc, 3,4,0, dest, linesize, block
-    movsx      blockd, WORD [blockq]   ; dc
-    lea        blockd, [blockq*3+1]    ;  3 * dc + 1
-    sar        blockd, 1               ; >> 1
-    mov           r3d, blockd          ; dc
-    shl        blockd, 4               ; 16 * dc
-    lea        blockd, [blockq+r3+64]  ; 17 * dc + 64
-    sar        blockd, 7               ; >> 7
-
-    INV_TRANS_INIT
-
-    INV_TRANS_PROCESS a
-    RET
-
-INIT_MMX mmxext
-cglobal vc1_inv_trans_8x8_dc, 3,3,0, dest, linesize, block
-    movsx      blockd, WORD [blockq]   ; dc
-    lea        blockd, [blockq*3+1]    ;  3 * dc + 1
-    sar        blockd, 1               ; >> 1
-    lea        blockd, [blockq*3+16]   ;  3 * dc + 16
-    sar        blockd, 5               ; >> 5
-
-    INV_TRANS_INIT
-
-    INV_TRANS_PROCESS a
-    lea         destq, [destq+linesizeq*4]
-    INV_TRANS_PROCESS a
-    RET
diff -uparN ffmpeg-4.1/libavcodec/x86/videodsp.asm ffmpeg-y/libavcodec/x86/videodsp.asm
--- ffmpeg-4.1/libavcodec/x86/videodsp.asm	2018-11-02 02:34:25.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/videodsp.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,468 +0,0 @@
-;******************************************************************************
-;* Core video DSP functions
-;* Copyright (c) 2012 Ronald S. Bultje <rsbultje@gmail.com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION .text
-
-; slow vertical extension loop function. Works with variable-width, and
-; does per-line reading/writing of source data
-
-%macro V_COPY_ROW 2 ; type (top/body/bottom), h
-.%1_y_loop:                                     ; do {
-    mov              wq, r7mp                   ;   initialize w (r7mp = wmp)
-.%1_x_loop:                                     ;   do {
-    movu             m0, [srcq+wq]              ;     m0 = read($mmsize)
-    movu      [dstq+wq], m0                     ;     write(m0, $mmsize)
-    add              wq, mmsize                 ;     w -= $mmsize
-    cmp              wq, -mmsize                ;   } while (w > $mmsize);
-    jl .%1_x_loop
-    movu             m0, [srcq-mmsize]          ;     m0 = read($mmsize)
-    movu  [dstq-mmsize], m0                     ;     write(m0, $mmsize)
-%ifidn %1, body                                 ;   if ($type == body) {
-    add            srcq, src_strideq            ;     src += src_stride
-%endif                                          ;   }
-    add            dstq, dst_strideq            ;   dst += dst_stride
-    dec              %2                         ; } while (--$h);
-    jnz .%1_y_loop
-%endmacro
-
-%macro vvar_fn 0
-; .----. <- zero
-; |    |    <- top is copied from first line in body of source
-; |----| <- start_y
-; |    |    <- body is copied verbatim (line-by-line) from source
-; |----| <- end_y
-; |    |    <- bottom is copied from last line in body of source
-; '----' <- bh
-%if ARCH_X86_64
-cglobal emu_edge_vvar, 7, 8, 1, dst, dst_stride, src, src_stride, \
-                                start_y, end_y, bh, w
-%else ; x86-32
-cglobal emu_edge_vvar, 1, 6, 1, dst, src, start_y, end_y, bh, w
-%define src_strideq r3mp
-%define dst_strideq r1mp
-    mov            srcq, r2mp
-    mov        start_yq, r4mp
-    mov          end_yq, r5mp
-    mov             bhq, r6mp
-%endif
-    sub             bhq, end_yq                 ; bh    -= end_q
-    sub          end_yq, start_yq               ; end_q -= start_q
-    add            srcq, r7mp                   ; (r7mp = wmp)
-    add            dstq, r7mp                   ; (r7mp = wmp)
-    neg            r7mp                         ; (r7mp = wmp)
-    test       start_yq, start_yq               ; if (start_q) {
-    jz .body
-    V_COPY_ROW      top, start_yq               ;   v_copy_row(top, start_yq)
-.body:                                          ; }
-    V_COPY_ROW     body, end_yq                 ; v_copy_row(body, end_yq)
-    test            bhq, bhq                    ; if (bh) {
-    jz .end
-    sub            srcq, src_strideq            ;   src -= src_stride
-    V_COPY_ROW   bottom, bhq                    ;   v_copy_row(bottom, bh)
-.end:                                           ; }
-    RET
-%endmacro
-
-%if ARCH_X86_32
-INIT_MMX mmx
-vvar_fn
-%endif
-
-INIT_XMM sse
-vvar_fn
-
-%macro hvar_fn 0
-cglobal emu_edge_hvar, 5, 6, 1, dst, dst_stride, start_x, n_words, h, w
-    lea            dstq, [dstq+n_wordsq*2]
-    neg        n_wordsq
-    lea        start_xq, [start_xq+n_wordsq*2]
-.y_loop:                                        ; do {
-%if cpuflag(avx2)
-    vpbroadcastb     m0, [dstq+start_xq]
-    mov              wq, n_wordsq               ;   initialize w
-%else
-    movzx            wd, byte [dstq+start_xq]   ;   w = read(1)
-    imul             wd, 0x01010101             ;   w *= 0x01010101
-    movd             m0, wd
-    mov              wq, n_wordsq               ;   initialize w
-%if cpuflag(sse2)
-    pshufd           m0, m0, q0000              ;   splat
-%else ; mmx
-    punpckldq        m0, m0                     ;   splat
-%endif ; mmx/sse
-%endif ; avx2
-.x_loop:                                        ;   do {
-    movu    [dstq+wq*2], m0                     ;     write($reg, $mmsize)
-    add              wq, mmsize/2               ;     w -= $mmsize/2
-    cmp              wq, -(mmsize/2)            ;   } while (w > $mmsize/2)
-    jl .x_loop
-    movu  [dstq-mmsize], m0                     ;   write($reg, $mmsize)
-    add            dstq, dst_strideq            ;   dst += dst_stride
-    dec              hq                         ; } while (h--)
-    jnz .y_loop
-    RET
-%endmacro
-
-%if ARCH_X86_32
-INIT_MMX mmx
-hvar_fn
-%endif
-
-INIT_XMM sse2
-hvar_fn
-
-%if HAVE_AVX2_EXTERNAL
-INIT_XMM avx2
-hvar_fn
-%endif
-
-; macro to read/write a horizontal number of pixels (%2) to/from registers
-; on sse, - fills xmm0-15 for consecutive sets of 16 pixels
-;         - if (%2 & 8)  fills 8 bytes into xmm$next
-;         - if (%2 & 4)  fills 4 bytes into xmm$next
-;         - if (%2 & 3)  fills 1, 2 or 4 bytes in eax
-; on mmx, - fills mm0-7 for consecutive sets of 8 pixels
-;         - if (%2 & 4)  fills 4 bytes into mm$next
-;         - if (%2 & 3)  fills 1, 2 or 4 bytes in eax
-; writing data out is in the same way
-%macro READ_NUM_BYTES 2
-%assign %%off 0     ; offset in source buffer
-%assign %%mmx_idx 0 ; mmx register index
-%assign %%xmm_idx 0 ; xmm register index
-
-%rep %2/mmsize
-%if mmsize == 16
-    movu   xmm %+ %%xmm_idx, [srcq+%%off]
-%assign %%xmm_idx %%xmm_idx+1
-%else ; mmx
-    movu    mm %+ %%mmx_idx, [srcq+%%off]
-%assign %%mmx_idx %%mmx_idx+1
-%endif
-%assign %%off %%off+mmsize
-%endrep ; %2/mmsize
-
-%if mmsize == 16
-%if (%2-%%off) >= 8
-%if %2 > 16 && (%2-%%off) > 8
-    movu   xmm %+ %%xmm_idx, [srcq+%2-16]
-%assign %%xmm_idx %%xmm_idx+1
-%assign %%off %2
-%else
-    movq    mm %+ %%mmx_idx, [srcq+%%off]
-%assign %%mmx_idx %%mmx_idx+1
-%assign %%off %%off+8
-%endif
-%endif ; (%2-%%off) >= 8
-%endif
-
-%if (%2-%%off) >= 4
-%if %2 > 8 && (%2-%%off) > 4
-    movq    mm %+ %%mmx_idx, [srcq+%2-8]
-%assign %%off %2
-%else
-    movd    mm %+ %%mmx_idx, [srcq+%%off]
-%assign %%off %%off+4
-%endif
-%assign %%mmx_idx %%mmx_idx+1
-%endif ; (%2-%%off) >= 4
-
-%if (%2-%%off) >= 1
-%if %2 >= 4
-    movd mm %+ %%mmx_idx, [srcq+%2-4]
-%elif (%2-%%off) == 1
-    mov            valb, [srcq+%2-1]
-%elif (%2-%%off) == 2
-    mov            valw, [srcq+%2-2]
-%else
-    mov            valb, [srcq+%2-1]
-    ror            vald, 16
-    mov            valw, [srcq+%2-3]
-%endif
-%endif ; (%2-%%off) >= 1
-%endmacro ; READ_NUM_BYTES
-
-%macro WRITE_NUM_BYTES 2
-%assign %%off 0     ; offset in destination buffer
-%assign %%mmx_idx 0 ; mmx register index
-%assign %%xmm_idx 0 ; xmm register index
-
-%rep %2/mmsize
-%if mmsize == 16
-    movu   [dstq+%%off], xmm %+ %%xmm_idx
-%assign %%xmm_idx %%xmm_idx+1
-%else ; mmx
-    movu   [dstq+%%off], mm %+ %%mmx_idx
-%assign %%mmx_idx %%mmx_idx+1
-%endif
-%assign %%off %%off+mmsize
-%endrep ; %2/mmsize
-
-%if mmsize == 16
-%if (%2-%%off) >= 8
-%if %2 > 16 && (%2-%%off) > 8
-    movu   [dstq+%2-16], xmm %+ %%xmm_idx
-%assign %%xmm_idx %%xmm_idx+1
-%assign %%off %2
-%else
-    movq   [dstq+%%off], mm %+ %%mmx_idx
-%assign %%mmx_idx %%mmx_idx+1
-%assign %%off %%off+8
-%endif
-%endif ; (%2-%%off) >= 8
-%endif
-
-%if (%2-%%off) >= 4
-%if %2 > 8 && (%2-%%off) > 4
-    movq    [dstq+%2-8], mm %+ %%mmx_idx
-%assign %%off %2
-%else
-    movd   [dstq+%%off], mm %+ %%mmx_idx
-%assign %%off %%off+4
-%endif
-%assign %%mmx_idx %%mmx_idx+1
-%endif ; (%2-%%off) >= 4
-
-%if (%2-%%off) >= 1
-%if %2 >= 4
-    movd    [dstq+%2-4], mm %+ %%mmx_idx
-%elif (%2-%%off) == 1
-    mov     [dstq+%2-1], valb
-%elif (%2-%%off) == 2
-    mov     [dstq+%2-2], valw
-%else
-    mov     [dstq+%2-3], valw
-    ror            vald, 16
-    mov     [dstq+%2-1], valb
-%ifnidn %1, body
-    ror            vald, 16
-%endif
-%endif
-%endif ; (%2-%%off) >= 1
-%endmacro ; WRITE_NUM_BYTES
-
-; vertical top/bottom extend and body copy fast loops
-; these are function pointers to set-width line copy functions, i.e.
-; they read a fixed number of pixels into set registers, and write
-; those out into the destination buffer
-%macro VERTICAL_EXTEND 2
-%assign %%n %1
-%rep 1+%2-%1
-%if %%n <= 3
-%if ARCH_X86_64
-cglobal emu_edge_vfix %+ %%n, 6, 8, 0, dst, dst_stride, src, src_stride, \
-                                       start_y, end_y, val, bh
-    mov             bhq, r6mp                   ; r6mp = bhmp
-%else ; x86-32
-cglobal emu_edge_vfix %+ %%n, 0, 6, 0, val, dst, src, start_y, end_y, bh
-    mov            dstq, r0mp
-    mov            srcq, r2mp
-    mov        start_yq, r4mp
-    mov          end_yq, r5mp
-    mov             bhq, r6mp
-%define dst_strideq r1mp
-%define src_strideq r3mp
-%endif ; x86-64/32
-%else
-%if ARCH_X86_64
-cglobal emu_edge_vfix %+ %%n, 7, 7, 1, dst, dst_stride, src, src_stride, \
-                                       start_y, end_y, bh
-%else ; x86-32
-cglobal emu_edge_vfix %+ %%n, 1, 5, 1, dst, src, start_y, end_y, bh
-    mov            srcq, r2mp
-    mov        start_yq, r4mp
-    mov          end_yq, r5mp
-    mov             bhq, r6mp
-%define dst_strideq r1mp
-%define src_strideq r3mp
-%endif ; x86-64/32
-%endif
-    ; FIXME move this to c wrapper?
-    sub             bhq, end_yq                 ; bh    -= end_y
-    sub          end_yq, start_yq               ; end_y -= start_y
-
-    ; extend pixels above body
-    test       start_yq, start_yq               ; if (start_y) {
-    jz .body_loop
-    READ_NUM_BYTES  top, %%n                    ;   $variable_regs = read($n)
-.top_loop:                                      ;   do {
-    WRITE_NUM_BYTES top, %%n                    ;     write($variable_regs, $n)
-    add            dstq, dst_strideq            ;     dst += linesize
-    dec        start_yq                         ;   } while (--start_y)
-    jnz .top_loop                               ; }
-
-    ; copy body pixels
-.body_loop:                                     ; do {
-    READ_NUM_BYTES  body, %%n                   ;   $variable_regs = read($n)
-    WRITE_NUM_BYTES body, %%n                   ;   write($variable_regs, $n)
-    add            dstq, dst_strideq            ;   dst += dst_stride
-    add            srcq, src_strideq            ;   src += src_stride
-    dec          end_yq                         ; } while (--end_y)
-    jnz .body_loop
-
-    ; copy bottom pixels
-    test            bhq, bhq                    ; if (block_h) {
-    jz .end
-    sub            srcq, src_strideq            ;   src -= linesize
-    READ_NUM_BYTES  bottom, %%n                 ;   $variable_regs = read($n)
-.bottom_loop:                                   ;   do {
-    WRITE_NUM_BYTES bottom, %%n                 ;     write($variable_regs, $n)
-    add            dstq, dst_strideq            ;     dst += linesize
-    dec             bhq                         ;   } while (--bh)
-    jnz .bottom_loop                            ; }
-
-.end:
-    RET
-%assign %%n %%n+1
-%endrep ; 1+%2-%1
-%endmacro ; VERTICAL_EXTEND
-
-INIT_MMX mmx
-VERTICAL_EXTEND 1, 15
-%if ARCH_X86_32
-VERTICAL_EXTEND 16, 22
-%endif
-
-INIT_XMM sse
-VERTICAL_EXTEND 16, 22
-
-; left/right (horizontal) fast extend functions
-; these are essentially identical to the vertical extend ones above,
-; just left/right separated because number of pixels to extend is
-; obviously not the same on both sides.
-
-%macro READ_V_PIXEL 2
-%if cpuflag(avx2)
-    vpbroadcastb     m0, %2
-%else
-    movzx          vald, byte %2
-    imul           vald, 0x01010101
-%if %1 >= 8
-    movd             m0, vald
-%if mmsize == 16
-    pshufd           m0, m0, q0000
-%else
-    punpckldq        m0, m0
-%endif ; mmsize == 16
-%endif ; %1 > 16
-%endif ; avx2
-%endmacro ; READ_V_PIXEL
-
-%macro WRITE_V_PIXEL 2
-%assign %%off 0
-
-%if %1 >= 8
-
-%rep %1/mmsize
-    movu     [%2+%%off], m0
-%assign %%off %%off+mmsize
-%endrep ; %1/mmsize
-
-%if mmsize == 16
-%if %1-%%off >= 8
-%if %1 > 16 && %1-%%off > 8
-    movu     [%2+%1-16], m0
-%assign %%off %1
-%else
-    movq     [%2+%%off], m0
-%assign %%off %%off+8
-%endif
-%endif ; %1-%%off >= 8
-%endif ; mmsize == 16
-
-%if %1-%%off >= 4
-%if %1 > 8 && %1-%%off > 4
-    movq      [%2+%1-8], m0
-%assign %%off %1
-%else
-    movd     [%2+%%off], m0
-%assign %%off %%off+4
-%endif
-%endif ; %1-%%off >= 4
-
-%else ; %1 < 8
-
-%rep %1/4
-    mov      [%2+%%off], vald
-%assign %%off %%off+4
-%endrep ; %1/4
-
-%endif ; %1 >=/< 8
-
-%if %1-%%off == 2
-%if cpuflag(avx2)
-    movd     [%2+%%off-2], m0
-%else
-    mov      [%2+%%off], valw
-%endif ; avx2
-%endif ; (%1-%%off)/2
-%endmacro ; WRITE_V_PIXEL
-
-%macro H_EXTEND 2
-%assign %%n %1
-%rep 1+(%2-%1)/2
-%if cpuflag(avx2)
-cglobal emu_edge_hfix %+ %%n, 4, 4, 1, dst, dst_stride, start_x, bh
-%else
-cglobal emu_edge_hfix %+ %%n, 4, 5, 1, dst, dst_stride, start_x, bh, val
-%endif
-.loop_y:                                        ; do {
-    READ_V_PIXEL    %%n, [dstq+start_xq]        ;   $variable_regs = read($n)
-    WRITE_V_PIXEL   %%n, dstq                   ;   write($variable_regs, $n)
-    add            dstq, dst_strideq            ;   dst += dst_stride
-    dec             bhq                         ; } while (--bh)
-    jnz .loop_y
-    RET
-%assign %%n %%n+2
-%endrep ; 1+(%2-%1)/2
-%endmacro ; H_EXTEND
-
-INIT_MMX mmx
-H_EXTEND 2, 14
-%if ARCH_X86_32
-H_EXTEND 16, 22
-%endif
-
-INIT_XMM sse2
-H_EXTEND 16, 22
-
-%if HAVE_AVX2_EXTERNAL
-INIT_XMM avx2
-H_EXTEND 8, 22
-%endif
-
-%macro PREFETCH_FN 1
-cglobal prefetch, 3, 3, 0, buf, stride, h
-.loop:
-    %1      [bufq]
-    add      bufq, strideq
-    dec        hd
-    jg .loop
-    REP_RET
-%endmacro
-
-INIT_MMX mmxext
-PREFETCH_FN prefetcht0
-%if ARCH_X86_32
-INIT_MMX 3dnow
-PREFETCH_FN prefetch
-%endif
diff -uparN ffmpeg-4.1/libavcodec/x86/vorbisdsp.asm ffmpeg-y/libavcodec/x86/vorbisdsp.asm
--- ffmpeg-4.1/libavcodec/x86/vorbisdsp.asm	2018-11-02 02:34:25.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/vorbisdsp.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,86 +0,0 @@
-;******************************************************************************
-;* Vorbis x86 optimizations
-;* Copyright (C) 2006 Loren Merritt <lorenm@u.washington.edu>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-
-pdw_80000000: times 4 dd 0x80000000
-
-SECTION .text
-
-%if ARCH_X86_32
-INIT_MMX 3dnow
-cglobal vorbis_inverse_coupling, 3, 3, 6, mag, ang, block_size
-    pxor                     m7, m7
-    lea                    magq, [magq+block_sizeq*4]
-    lea                    angq, [angq+block_sizeq*4]
-    neg             block_sizeq
-.loop:
-    mova                     m0, [magq+block_sizeq*4]
-    mova                     m1, [angq+block_sizeq*4]
-    mova                     m2, m0
-    mova                     m3, m1
-    pfcmpge                  m2, m7     ; m <= 0.0
-    pfcmpge                  m3, m7     ; a <= 0.0
-    pslld                    m2, 31     ; keep only the sign bit
-    pxor                     m1, m2
-    mova                     m4, m3
-    pand                     m3, m1
-    pandn                    m4, m1
-    pfadd                    m3, m0     ; a = m + ((a < 0) & (a ^ sign(m)))
-    pfsub                    m0, m4     ; m = m + ((a > 0) & (a ^ sign(m)))
-    mova   [angq+block_sizeq*4], m3
-    mova   [magq+block_sizeq*4], m0
-    add             block_sizeq, 2
-    jl .loop
-    femms
-    RET
-%endif
-
-INIT_XMM sse
-cglobal vorbis_inverse_coupling, 3, 3, 6, mag, ang, block_size
-    mova                     m5, [pdw_80000000]
-    shl             block_sized, 2
-    add                    magq, block_sizeq
-    add                    angq, block_sizeq
-    neg             block_sizeq
-
-align 16
-.loop:
-    mova                     m0, [magq+block_sizeq]
-    mova                     m1, [angq+block_sizeq]
-    xorps                    m2, m2
-    xorps                    m3, m3
-    cmpleps                  m2, m0     ; m <= 0.0
-    cmpleps                  m3, m1     ; a <= 0.0
-    andps                    m2, m5     ; keep only the sign bit
-    xorps                    m1, m2
-    mova                     m4, m3
-    andps                    m3, m1
-    andnps                   m4, m1
-    addps                    m3, m0     ; a = m + ((a < 0) & (a ^ sign(m)))
-    subps                    m0, m4     ; m = m + ((a > 0) & (a ^ sign(m)))
-    mova     [angq+block_sizeq], m3
-    mova     [magq+block_sizeq], m0
-    add             block_sizeq, mmsize
-    jl .loop
-    RET
diff -uparN ffmpeg-4.1/libavcodec/x86/vp3dsp.asm ffmpeg-y/libavcodec/x86/vp3dsp.asm
--- ffmpeg-4.1/libavcodec/x86/vp3dsp.asm	2018-11-02 02:34:25.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/vp3dsp.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,742 +0,0 @@
-;******************************************************************************
-;* MMX/SSE2-optimized functions for the VP3 decoder
-;* Copyright (c) 2007 Aurelien Jacobs <aurel@gnuage.org>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-; MMX-optimized functions cribbed from the original VP3 source code.
-
-SECTION_RODATA
-
-vp3_idct_data: times 8 dw 64277
-               times 8 dw 60547
-               times 8 dw 54491
-               times 8 dw 46341
-               times 8 dw 36410
-               times 8 dw 25080
-               times 8 dw 12785
-
-pb_7:  times 8 db 0x07
-pb_1F: times 8 db 0x1f
-pb_81: times 8 db 0x81
-
-cextern pb_1
-cextern pb_3
-cextern pb_80
-cextern pb_FE
-
-cextern pw_8
-
-SECTION .text
-
-; this is off by one or two for some cases when filter_limit is greater than 63
-; in:  p0 in mm6, p1 in mm4, p2 in mm2, p3 in mm1
-; out: p1 in mm4, p2 in mm3
-%macro VP3_LOOP_FILTER 0
-    movq          m7, m6
-    pand          m6, [pb_7]    ; p0&7
-    psrlw         m7, 3
-    pand          m7, [pb_1F]   ; p0>>3
-    movq          m3, m2        ; p2
-    pxor          m2, m4
-    pand          m2, [pb_1]    ; (p2^p1)&1
-    movq          m5, m2
-    paddb         m2, m2
-    paddb         m2, m5        ; 3*(p2^p1)&1
-    paddb         m2, m6        ; extra bits lost in shifts
-    pcmpeqb       m0, m0
-    pxor          m1, m0        ; 255 - p3
-    pavgb         m1, m2        ; (256 - p3 + extrabits) >> 1
-    pxor          m0, m4        ; 255 - p1
-    pavgb         m0, m3        ; (256 + p2-p1) >> 1
-    paddb         m1, [pb_3]
-    pavgb         m1, m0        ; 128+2+(   p2-p1  - p3) >> 2
-    pavgb         m1, m0        ; 128+1+(3*(p2-p1) - p3) >> 3
-    paddusb       m7, m1        ; d+128+1
-    movq          m6, [pb_81]
-    psubusb       m6, m7
-    psubusb       m7, [pb_81]
-
-    movq          m5, [r2+516]  ; flim
-    pminub        m6, m5
-    pminub        m7, m5
-    movq          m0, m6
-    movq          m1, m7
-    paddb         m6, m6
-    paddb         m7, m7
-    pminub        m6, m5
-    pminub        m7, m5
-    psubb         m6, m0
-    psubb         m7, m1
-    paddusb       m4, m7
-    psubusb       m4, m6
-    psubusb       m3, m7
-    paddusb       m3, m6
-%endmacro
-
-%macro STORE_4_WORDS 1
-    movd         r2d, %1
-    mov  [r0     -1], r2w
-    psrlq         %1, 32
-    shr           r2, 16
-    mov  [r0+r1  -1], r2w
-    movd         r2d, %1
-    mov  [r0+r1*2-1], r2w
-    shr           r2, 16
-    mov  [r0+r3  -1], r2w
-%endmacro
-
-INIT_MMX mmxext
-cglobal vp3_v_loop_filter, 3, 4
-    mov           r3, r1
-    neg           r1
-    movq          m6, [r0+r1*2]
-    movq          m4, [r0+r1  ]
-    movq          m2, [r0     ]
-    movq          m1, [r0+r3  ]
-
-    VP3_LOOP_FILTER
-
-    movq     [r0+r1], m4
-    movq     [r0   ], m3
-    RET
-
-cglobal vp3_h_loop_filter, 3, 4
-    lea           r3, [r1*3]
-
-    movd          m6, [r0     -2]
-    movd          m4, [r0+r1  -2]
-    movd          m2, [r0+r1*2-2]
-    movd          m1, [r0+r3  -2]
-    lea           r0, [r0+r1*4  ]
-    punpcklbw     m6, [r0     -2]
-    punpcklbw     m4, [r0+r1  -2]
-    punpcklbw     m2, [r0+r1*2-2]
-    punpcklbw     m1, [r0+r3  -2]
-    sub           r0, r3
-    sub           r0, r1
-
-    TRANSPOSE4x4B  6, 4, 2, 1, 0
-    VP3_LOOP_FILTER
-    SBUTTERFLY    bw, 4, 3, 5
-
-    STORE_4_WORDS m4
-    lea           r0, [r0+r1*4  ]
-    STORE_4_WORDS m3
-    RET
-
-%macro PAVGB_NO_RND 0
-    mova   m4, m0
-    mova   m5, m2
-    pand   m4, m1
-    pand   m5, m3
-    pxor   m1, m0
-    pxor   m3, m2
-    pand   m1, m6
-    pand   m3, m6
-    psrlq  m1, 1
-    psrlq  m3, 1
-    paddb  m4, m1
-    paddb  m5, m3
-%endmacro
-
-INIT_MMX mmx
-cglobal put_vp_no_rnd_pixels8_l2, 5, 6, 0, dst, src1, src2, stride, h, stride3
-    mova   m6, [pb_FE]
-    lea    stride3q,[strideq+strideq*2]
-.loop:
-    mova   m0, [src1q]
-    mova   m1, [src2q]
-    mova   m2, [src1q+strideq]
-    mova   m3, [src2q+strideq]
-    PAVGB_NO_RND
-    mova   [dstq], m4
-    mova   [dstq+strideq], m5
-
-    mova   m0, [src1q+strideq*2]
-    mova   m1, [src2q+strideq*2]
-    mova   m2, [src1q+stride3q]
-    mova   m3, [src2q+stride3q]
-    PAVGB_NO_RND
-    mova   [dstq+strideq*2], m4
-    mova   [dstq+stride3q],  m5
-
-    lea    src1q, [src1q+strideq*4]
-    lea    src2q, [src2q+strideq*4]
-    lea    dstq,  [dstq+strideq*4]
-    sub    hd, 4
-    jnz .loop
-    RET
-
-; from original comments: The Macro does IDct on 4 1-D Dcts
-%macro BeginIDCT 0
-    movq          m2, I(3)
-    movq          m6, C(3)
-    movq          m4, m2
-    movq          m7, J(5)
-    pmulhw        m4, m6        ; r4 = c3*i3 - i3
-    movq          m1, C(5)
-    pmulhw        m6, m7        ; r6 = c3*i5 - i5
-    movq          m5, m1
-    pmulhw        m1, m2        ; r1 = c5*i3 - i3
-    movq          m3, I(1)
-    pmulhw        m5, m7        ; r5 = c5*i5 - i5
-    movq          m0, C(1)
-    paddw         m4, m2        ; r4 = c3*i3
-    paddw         m6, m7        ; r6 = c3*i5
-    paddw         m2, m1        ; r2 = c5*i3
-    movq          m1, J(7)
-    paddw         m7, m5        ; r7 = c5*i5
-    movq          m5, m0        ; r5 = c1
-    pmulhw        m0, m3        ; r0 = c1*i1 - i1
-    paddsw        m4, m7        ; r4 = C = c3*i3 + c5*i5
-    pmulhw        m5, m1        ; r5 = c1*i7 - i7
-    movq          m7, C(7)
-    psubsw        m6, m2        ; r6 = D = c3*i5 - c5*i3
-    paddw         m0, m3        ; r0 = c1*i1
-    pmulhw        m3, m7        ; r3 = c7*i1
-    movq          m2, I(2)
-    pmulhw        m7, m1        ; r7 = c7*i7
-    paddw         m5, m1        ; r5 = c1*i7
-    movq          m1, m2        ; r1 = i2
-    pmulhw        m2, C(2)      ; r2 = c2*i2 - i2
-    psubsw        m3, m5        ; r3 = B = c7*i1 - c1*i7
-    movq          m5, J(6)
-    paddsw        m0, m7        ; r0 = A = c1*i1 + c7*i7
-    movq          m7, m5        ; r7 = i6
-    psubsw        m0, m4        ; r0 = A - C
-    pmulhw        m5, C(2)      ; r5 = c2*i6 - i6
-    paddw         m2, m1        ; r2 = c2*i2
-    pmulhw        m1, C(6)      ; r1 = c6*i2
-    paddsw        m4, m4        ; r4 = C + C
-    paddsw        m4, m0        ; r4 = C. = A + C
-    psubsw        m3, m6        ; r3 = B - D
-    paddw         m5, m7        ; r5 = c2*i6
-    paddsw        m6, m6        ; r6 = D + D
-    pmulhw        m7, C(6)      ; r7 = c6*i6
-    paddsw        m6, m3        ; r6 = D. = B + D
-    movq        I(1), m4        ; save C. at I(1)
-    psubsw        m1, m5        ; r1 = H = c6*i2 - c2*i6
-    movq          m4, C(4)
-    movq          m5, m3        ; r5 = B - D
-    pmulhw        m3, m4        ; r3 = (c4 - 1) * (B - D)
-    paddsw        m7, m2        ; r3 = (c4 - 1) * (B - D)
-    movq        I(2), m6        ; save D. at I(2)
-    movq          m2, m0        ; r2 = A - C
-    movq          m6, I(0)
-    pmulhw        m0, m4        ; r0 = (c4 - 1) * (A - C)
-    paddw         m5, m3        ; r5 = B. = c4 * (B - D)
-    movq          m3, J(4)
-    psubsw        m5, m1        ; r5 = B.. = B. - H
-    paddw         m2, m0        ; r0 = A. = c4 * (A - C)
-    psubsw        m6, m3        ; r6 = i0 - i4
-    movq          m0, m6
-    pmulhw        m6, m4        ; r6 = (c4 - 1) * (i0 - i4)
-    paddsw        m3, m3        ; r3 = i4 + i4
-    paddsw        m1, m1        ; r1 = H + H
-    paddsw        m3, m0        ; r3 = i0 + i4
-    paddsw        m1, m5        ; r1 = H. = B + H
-    pmulhw        m4, m3        ; r4 = (c4 - 1) * (i0 + i4)
-    paddsw        m6, m0        ; r6 = F = c4 * (i0 - i4)
-    psubsw        m6, m2        ; r6 = F. = F - A.
-    paddsw        m2, m2        ; r2 = A. + A.
-    movq          m0, I(1)      ; r0 = C.
-    paddsw        m2, m6        ; r2 = A.. = F + A.
-    paddw         m4, m3        ; r4 = E = c4 * (i0 + i4)
-    psubsw        m2, m1        ; r2 = R2 = A.. - H.
-%endmacro
-
-; RowIDCT gets ready to transpose
-%macro RowIDCT 0
-    BeginIDCT
-    movq          m3, I(2)      ; r3 = D.
-    psubsw        m4, m7        ; r4 = E. = E - G
-    paddsw        m1, m1        ; r1 = H. + H.
-    paddsw        m7, m7        ; r7 = G + G
-    paddsw        m1, m2        ; r1 = R1 = A.. + H.
-    paddsw        m7, m4        ; r1 = R1 = A.. + H.
-    psubsw        m4, m3        ; r4 = R4 = E. - D.
-    paddsw        m3, m3
-    psubsw        m6, m5        ; r6 = R6 = F. - B..
-    paddsw        m5, m5
-    paddsw        m3, m4        ; r3 = R3 = E. + D.
-    paddsw        m5, m6        ; r5 = R5 = F. + B..
-    psubsw        m7, m0        ; r7 = R7 = G. - C.
-    paddsw        m0, m0
-    movq        I(1), m1        ; save R1
-    paddsw        m0, m7        ; r0 = R0 = G. + C.
-%endmacro
-
-; Column IDCT normalizes and stores final results
-%macro ColumnIDCT 0
-    BeginIDCT
-    paddsw        m2, OC_8      ; adjust R2 (and R1) for shift
-    paddsw        m1, m1        ; r1 = H. + H.
-    paddsw        m1, m2        ; r1 = R1 = A.. + H.
-    psraw         m2, 4         ; r2 = NR2
-    psubsw        m4, m7        ; r4 = E. = E - G
-    psraw         m1, 4         ; r1 = NR2
-    movq          m3, I(2)      ; r3 = D.
-    paddsw        m7, m7        ; r7 = G + G
-    movq        I(2), m2        ; store NR2 at I2
-    paddsw        m7, m4        ; r7 = G. = E + G
-    movq        I(1), m1        ; store NR1 at I1
-    psubsw        m4, m3        ; r4 = R4 = E. - D.
-    paddsw        m4, OC_8      ; adjust R4 (and R3) for shift
-    paddsw        m3, m3        ; r3 = D. + D.
-    paddsw        m3, m4        ; r3 = R3 = E. + D.
-    psraw         m4, 4         ; r4 = NR4
-    psubsw        m6, m5        ; r6 = R6 = F. - B..
-    psraw         m3, 4         ; r3 = NR3
-    paddsw        m6, OC_8      ; adjust R6 (and R5) for shift
-    paddsw        m5, m5        ; r5 = B.. + B..
-    paddsw        m5, m6        ; r5 = R5 = F. + B..
-    psraw         m6, 4         ; r6 = NR6
-    movq        J(4), m4        ; store NR4 at J4
-    psraw         m5, 4         ; r5 = NR5
-    movq        I(3), m3        ; store NR3 at I3
-    psubsw        m7, m0        ; r7 = R7 = G. - C.
-    paddsw        m7, OC_8      ; adjust R7 (and R0) for shift
-    paddsw        m0, m0        ; r0 = C. + C.
-    paddsw        m0, m7        ; r0 = R0 = G. + C.
-    psraw         m7, 4         ; r7 = NR7
-    movq        J(6), m6        ; store NR6 at J6
-    psraw         m0, 4         ; r0 = NR0
-    movq        J(5), m5        ; store NR5 at J5
-    movq        J(7), m7        ; store NR7 at J7
-    movq        I(0), m0        ; store NR0 at I0
-%endmacro
-
-; Following macro does two 4x4 transposes in place.
-;
-; At entry (we assume):
-;
-;   r0 = a3 a2 a1 a0
-;   I(1) = b3 b2 b1 b0
-;   r2 = c3 c2 c1 c0
-;   r3 = d3 d2 d1 d0
-;
-;   r4 = e3 e2 e1 e0
-;   r5 = f3 f2 f1 f0
-;   r6 = g3 g2 g1 g0
-;   r7 = h3 h2 h1 h0
-;
-; At exit, we have:
-;
-;   I(0) = d0 c0 b0 a0
-;   I(1) = d1 c1 b1 a1
-;   I(2) = d2 c2 b2 a2
-;   I(3) = d3 c3 b3 a3
-;
-;   J(4) = h0 g0 f0 e0
-;   J(5) = h1 g1 f1 e1
-;   J(6) = h2 g2 f2 e2
-;   J(7) = h3 g3 f3 e3
-;
-;  I(0) I(1) I(2) I(3)  is the transpose of r0 I(1) r2 r3.
-;  J(4) J(5) J(6) J(7)  is the transpose of r4 r5 r6 r7.
-;
-;  Since r1 is free at entry, we calculate the Js first.
-%macro Transpose 0
-    movq          m1, m4        ; r1 = e3 e2 e1 e0
-    punpcklwd     m4, m5        ; r4 = f1 e1 f0 e0
-    movq        I(0), m0        ; save a3 a2 a1 a0
-    punpckhwd     m1, m5        ; r1 = f3 e3 f2 e2
-    movq          m0, m6        ; r0 = g3 g2 g1 g0
-    punpcklwd     m6, m7        ; r6 = h1 g1 h0 g0
-    movq          m5, m4        ; r5 = f1 e1 f0 e0
-    punpckldq     m4, m6        ; r4 = h0 g0 f0 e0 = R4
-    punpckhdq     m5, m6        ; r5 = h1 g1 f1 e1 = R5
-    movq          m6, m1        ; r6 = f3 e3 f2 e2
-    movq        J(4), m4
-    punpckhwd     m0, m7        ; r0 = h3 g3 h2 g2
-    movq        J(5), m5
-    punpckhdq     m6, m0        ; r6 = h3 g3 f3 e3 = R7
-    movq          m4, I(0)      ; r4 = a3 a2 a1 a0
-    punpckldq     m1, m0        ; r1 = h2 g2 f2 e2 = R6
-    movq          m5, I(1)      ; r5 = b3 b2 b1 b0
-    movq          m0, m4        ; r0 = a3 a2 a1 a0
-    movq        J(7), m6
-    punpcklwd     m0, m5        ; r0 = b1 a1 b0 a0
-    movq        J(6), m1
-    punpckhwd     m4, m5        ; r4 = b3 a3 b2 a2
-    movq          m5, m2        ; r5 = c3 c2 c1 c0
-    punpcklwd     m2, m3        ; r2 = d1 c1 d0 c0
-    movq          m1, m0        ; r1 = b1 a1 b0 a0
-    punpckldq     m0, m2        ; r0 = d0 c0 b0 a0 = R0
-    punpckhdq     m1, m2        ; r1 = d1 c1 b1 a1 = R1
-    movq          m2, m4        ; r2 = b3 a3 b2 a2
-    movq        I(0), m0
-    punpckhwd     m5, m3        ; r5 = d3 c3 d2 c2
-    movq        I(1), m1
-    punpckhdq     m4, m5        ; r4 = d3 c3 b3 a3 = R3
-    punpckldq     m2, m5        ; r2 = d2 c2 b2 a2 = R2
-    movq        I(3), m4
-    movq        I(2), m2
-%endmacro
-
-%macro VP3_1D_IDCT_SSE2 0
-    movdqa        m2, I(3)      ; xmm2 = i3
-    movdqa        m6, C(3)      ; xmm6 = c3
-    movdqa        m4, m2        ; xmm4 = i3
-    movdqa        m7, I(5)      ; xmm7 = i5
-    pmulhw        m4, m6        ; xmm4 = c3 * i3 - i3
-    movdqa        m1, C(5)      ; xmm1 = c5
-    pmulhw        m6, m7        ; xmm6 = c3 * i5 - i5
-    movdqa        m5, m1        ; xmm5 = c5
-    pmulhw        m1, m2        ; xmm1 = c5 * i3 - i3
-    movdqa        m3, I(1)      ; xmm3 = i1
-    pmulhw        m5, m7        ; xmm5 = c5 * i5 - i5
-    movdqa        m0, C(1)      ; xmm0 = c1
-    paddw         m4, m2        ; xmm4 = c3 * i3
-    paddw         m6, m7        ; xmm6 = c3 * i5
-    paddw         m2, m1        ; xmm2 = c5 * i3
-    movdqa        m1, I(7)      ; xmm1 = i7
-    paddw         m7, m5        ; xmm7 = c5 * i5
-    movdqa        m5, m0        ; xmm5 = c1
-    pmulhw        m0, m3        ; xmm0 = c1 * i1 - i1
-    paddsw        m4, m7        ; xmm4 = c3 * i3 + c5 * i5 = C
-    pmulhw        m5, m1        ; xmm5 = c1 * i7 - i7
-    movdqa        m7, C(7)      ; xmm7 = c7
-    psubsw        m6, m2        ; xmm6 = c3 * i5 - c5 * i3 = D
-    paddw         m0, m3        ; xmm0 = c1 * i1
-    pmulhw        m3, m7        ; xmm3 = c7 * i1
-    movdqa        m2, I(2)      ; xmm2 = i2
-    pmulhw        m7, m1        ; xmm7 = c7 * i7
-    paddw         m5, m1        ; xmm5 = c1 * i7
-    movdqa        m1, m2        ; xmm1 = i2
-    pmulhw        m2, C(2)      ; xmm2 = i2 * c2 -i2
-    psubsw        m3, m5        ; xmm3 = c7 * i1 - c1 * i7 = B
-    movdqa        m5, I(6)      ; xmm5 = i6
-    paddsw        m0, m7        ; xmm0 = c1 * i1 + c7 * i7 = A
-    movdqa        m7, m5        ; xmm7 = i6
-    psubsw        m0, m4        ; xmm0 = A - C
-    pmulhw        m5, C(2)      ; xmm5 = c2 * i6 - i6
-    paddw         m2, m1        ; xmm2 = i2 * c2
-    pmulhw        m1, C(6)      ; xmm1 = c6 * i2
-    paddsw        m4, m4        ; xmm4 = C + C
-    paddsw        m4, m0        ; xmm4 = A + C = C.
-    psubsw        m3, m6        ; xmm3 = B - D
-    paddw         m5, m7        ; xmm5 = c2 * i6
-    paddsw        m6, m6        ; xmm6 = D + D
-    pmulhw        m7, C(6)      ; xmm7 = c6 * i6
-    paddsw        m6, m3        ; xmm6 = B + D = D.
-    movdqa      I(1), m4        ; Save C. at I(1)
-    psubsw        m1, m5        ; xmm1 = c6 * i2 - c2 * i6 = H
-    movdqa        m4, C(4)      ; xmm4 = C4
-    movdqa        m5, m3        ; xmm5 = B - D
-    pmulhw        m3, m4        ; xmm3 = ( c4 -1 ) * ( B - D )
-    paddsw        m7, m2        ; xmm7 = c2 * i2 + c6 * i6 = G
-    movdqa      I(2), m6        ; save D. at I(2)
-    movdqa        m2, m0        ; xmm2 = A - C
-    movdqa        m6, I(0)      ; xmm6 = i0
-    pmulhw        m0, m4        ; xmm0 = ( c4 - 1 ) * ( A - C ) = A.
-    paddw         m5, m3        ; xmm5 = c4 * ( B - D ) = B.
-    movdqa        m3, I(4)      ; xmm3 = i4
-    psubsw        m5, m1        ; xmm5 = B. - H = B..
-    paddw         m2, m0        ; xmm2 = c4 * ( A - C) = A.
-    psubsw        m6, m3        ; xmm6 = i0 - i4
-    movdqa        m0, m6        ; xmm0 = i0 - i4
-    pmulhw        m6, m4        ; xmm6 = (c4 - 1) * (i0 - i4) = F
-    paddsw        m3, m3        ; xmm3 = i4 + i4
-    paddsw        m1, m1        ; xmm1 = H + H
-    paddsw        m3, m0        ; xmm3 = i0 + i4
-    paddsw        m1, m5        ; xmm1 = B. + H = H.
-    pmulhw        m4, m3        ; xmm4 = ( c4 - 1 ) * ( i0 + i4 )
-    paddw         m6, m0        ; xmm6 = c4 * ( i0 - i4 )
-    psubsw        m6, m2        ; xmm6 = F - A. = F.
-    paddsw        m2, m2        ; xmm2 = A. + A.
-    movdqa        m0, I(1)      ; Load        C. from I(1)
-    paddsw        m2, m6        ; xmm2 = F + A. = A..
-    paddw         m4, m3        ; xmm4 = c4 * ( i0 + i4 ) = 3
-    psubsw        m2, m1        ; xmm2 = A.. - H. = R2
-    ADD(m2)                     ; Adjust R2 and R1 before shifting
-    paddsw        m1, m1        ; xmm1 = H. + H.
-    paddsw        m1, m2        ; xmm1 = A.. + H. = R1
-    SHIFT(m2)                   ; xmm2 = op2
-    psubsw        m4, m7        ; xmm4 = E - G = E.
-    SHIFT(m1)                   ; xmm1 = op1
-    movdqa        m3, I(2)      ; Load D. from I(2)
-    paddsw        m7, m7        ; xmm7 = G + G
-    paddsw        m7, m4        ; xmm7 = E + G = G.
-    psubsw        m4, m3        ; xmm4 = E. - D. = R4
-    ADD(m4)                     ; Adjust R4 and R3 before shifting
-    paddsw        m3, m3        ; xmm3 = D. + D.
-    paddsw        m3, m4        ; xmm3 = E. + D. = R3
-    SHIFT(m4)                   ; xmm4 = op4
-    psubsw        m6, m5        ; xmm6 = F. - B..= R6
-    SHIFT(m3)                   ; xmm3 = op3
-    ADD(m6)                     ; Adjust R6 and R5 before shifting
-    paddsw        m5, m5        ; xmm5 = B.. + B..
-    paddsw        m5, m6        ; xmm5 = F. + B.. = R5
-    SHIFT(m6)                   ; xmm6 = op6
-    SHIFT(m5)                   ; xmm5 = op5
-    psubsw        m7, m0        ; xmm7 = G. - C. = R7
-    ADD(m7)                     ; Adjust R7 and R0 before shifting
-    paddsw        m0, m0        ; xmm0 = C. + C.
-    paddsw        m0, m7        ; xmm0 = G. + C.
-    SHIFT(m7)                   ; xmm7 = op7
-    SHIFT(m0)                   ; xmm0 = op0
-%endmacro
-
-%macro PUT_BLOCK 8
-    movdqa      O(0), m%1
-    movdqa      O(1), m%2
-    movdqa      O(2), m%3
-    movdqa      O(3), m%4
-    movdqa      O(4), m%5
-    movdqa      O(5), m%6
-    movdqa      O(6), m%7
-    movdqa      O(7), m%8
-%endmacro
-
-%macro VP3_IDCT 1
-%if mmsize == 16
-%define I(x) [%1+16*x]
-%define O(x) [%1+16*x]
-%define C(x) [vp3_idct_data+16*(x-1)]
-%define SHIFT(x)
-%define ADD(x)
-        VP3_1D_IDCT_SSE2
-%if ARCH_X86_64
-        TRANSPOSE8x8W 0, 1, 2, 3, 4, 5, 6, 7, 8
-%else
-        TRANSPOSE8x8W 0, 1, 2, 3, 4, 5, 6, 7, [%1], [%1+16]
-%endif
-        PUT_BLOCK 0, 1, 2, 3, 4, 5, 6, 7
-
-%define SHIFT(x) psraw  x, 4
-%define ADD(x)   paddsw x, [pw_8]
-        VP3_1D_IDCT_SSE2
-        PUT_BLOCK 0, 1, 2, 3, 4, 5, 6, 7
-%else ; mmsize == 8
-    ; eax = quantized input
-    ; ebx = dequantizer matrix
-    ; ecx = IDCT constants
-    ;  M(I) = ecx + MaskOffset(0) + I * 8
-    ;  C(I) = ecx + CosineOffset(32) + (I-1) * 8
-    ; edx = output
-    ; r0..r7 = mm0..mm7
-%define OC_8 [pw_8]
-%define C(x) [vp3_idct_data+16*(x-1)]
-
-    ; at this point, function has completed dequantization + dezigzag +
-    ; partial transposition; now do the idct itself
-%define I(x) [%1+16*x]
-%define J(x) [%1+16*x]
-    RowIDCT
-    Transpose
-
-%define I(x) [%1+16*x+8]
-%define J(x) [%1+16*x+8]
-    RowIDCT
-    Transpose
-
-%define I(x) [%1+16* x]
-%define J(x) [%1+16*(x-4)+8]
-    ColumnIDCT
-
-%define I(x) [%1+16* x   +64]
-%define J(x) [%1+16*(x-4)+72]
-    ColumnIDCT
-%endif ; mmsize == 16/8
-%endmacro
-
-%macro vp3_idct_funcs 0
-cglobal vp3_idct_put, 3, 4, 9
-    VP3_IDCT      r2
-
-    mova          m4, [pb_80]
-    lea           r3, [r1*3]
-%assign %%i 0
-%rep 16/mmsize
-    mova          m0, [r2+mmsize*0+%%i]
-    mova          m1, [r2+mmsize*2+%%i]
-    mova          m2, [r2+mmsize*4+%%i]
-    mova          m3, [r2+mmsize*6+%%i]
-%if mmsize == 8
-    packsswb      m0, [r2+mmsize*8+%%i]
-    packsswb      m1, [r2+mmsize*10+%%i]
-    packsswb      m2, [r2+mmsize*12+%%i]
-    packsswb      m3, [r2+mmsize*14+%%i]
-%else
-    packsswb      m0, [r2+mmsize*1+%%i]
-    packsswb      m1, [r2+mmsize*3+%%i]
-    packsswb      m2, [r2+mmsize*5+%%i]
-    packsswb      m3, [r2+mmsize*7+%%i]
-%endif
-    paddb         m0, m4
-    paddb         m1, m4
-    paddb         m2, m4
-    paddb         m3, m4
-    movq   [r0     ], m0
-%if mmsize == 8
-    movq   [r0+r1  ], m1
-    movq   [r0+r1*2], m2
-    movq   [r0+r3  ], m3
-%else
-    movhps [r0+r1  ], m0
-    movq   [r0+r1*2], m1
-    movhps [r0+r3  ], m1
-%endif
-%if %%i == 0
-    lea           r0, [r0+r1*4]
-%endif
-%if mmsize == 16
-    movq   [r0     ], m2
-    movhps [r0+r1  ], m2
-    movq   [r0+r1*2], m3
-    movhps [r0+r3  ], m3
-%endif
-%assign %%i %%i+8
-%endrep
-
-    pxor          m0, m0
-%assign %%offset 0
-%rep 128/mmsize
-    mova [r2+%%offset], m0
-%assign %%offset %%offset+mmsize
-%endrep
-    RET
-
-cglobal vp3_idct_add, 3, 4, 9
-    VP3_IDCT      r2
-
-    lea           r3, [r1*3]
-    pxor          m4, m4
-%if mmsize == 16
-%assign %%i 0
-%rep 2
-    movq          m0, [r0]
-    movq          m1, [r0+r1]
-    movq          m2, [r0+r1*2]
-    movq          m3, [r0+r3]
-    punpcklbw     m0, m4
-    punpcklbw     m1, m4
-    punpcklbw     m2, m4
-    punpcklbw     m3, m4
-    paddsw        m0, [r2+ 0+%%i]
-    paddsw        m1, [r2+16+%%i]
-    paddsw        m2, [r2+32+%%i]
-    paddsw        m3, [r2+48+%%i]
-    packuswb      m0, m1
-    packuswb      m2, m3
-    movq   [r0     ], m0
-    movhps [r0+r1  ], m0
-    movq   [r0+r1*2], m2
-    movhps [r0+r3  ], m2
-%if %%i == 0
-    lea           r0, [r0+r1*4]
-%endif
-%assign %%i %%i+64
-%endrep
-%else
-%assign %%i 0
-%rep 2
-    movq          m0, [r0]
-    movq          m1, [r0+r1]
-    movq          m2, [r0+r1*2]
-    movq          m3, [r0+r3]
-    movq          m5, m0
-    movq          m6, m1
-    movq          m7, m2
-    punpcklbw     m0, m4
-    punpcklbw     m1, m4
-    punpcklbw     m2, m4
-    punpckhbw     m5, m4
-    punpckhbw     m6, m4
-    punpckhbw     m7, m4
-    paddsw        m0, [r2+ 0+%%i]
-    paddsw        m1, [r2+16+%%i]
-    paddsw        m2, [r2+32+%%i]
-    paddsw        m5, [r2+64+%%i]
-    paddsw        m6, [r2+80+%%i]
-    paddsw        m7, [r2+96+%%i]
-    packuswb      m0, m5
-    movq          m5, m3
-    punpcklbw     m3, m4
-    punpckhbw     m5, m4
-    packuswb      m1, m6
-    paddsw        m3, [r2+48+%%i]
-    paddsw        m5, [r2+112+%%i]
-    packuswb      m2, m7
-    packuswb      m3, m5
-    movq   [r0     ], m0
-    movq   [r0+r1  ], m1
-    movq   [r0+r1*2], m2
-    movq   [r0+r3  ], m3
-%if %%i == 0
-    lea           r0, [r0+r1*4]
-%endif
-%assign %%i %%i+8
-%endrep
-%endif
-%assign %%i 0
-%rep 128/mmsize
-    mova    [r2+%%i], m4
-%assign %%i %%i+mmsize
-%endrep
-    RET
-%endmacro
-
-%if ARCH_X86_32
-INIT_MMX mmx
-vp3_idct_funcs
-%endif
-
-INIT_XMM sse2
-vp3_idct_funcs
-
-%macro DC_ADD 0
-    movq          m2, [r0     ]
-    movq          m3, [r0+r1  ]
-    paddusb       m2, m0
-    movq          m4, [r0+r1*2]
-    paddusb       m3, m0
-    movq          m5, [r0+r2  ]
-    paddusb       m4, m0
-    paddusb       m5, m0
-    psubusb       m2, m1
-    psubusb       m3, m1
-    movq   [r0     ], m2
-    psubusb       m4, m1
-    movq   [r0+r1  ], m3
-    psubusb       m5, m1
-    movq   [r0+r1*2], m4
-    movq   [r0+r2  ], m5
-%endmacro
-
-INIT_MMX mmxext
-cglobal vp3_idct_dc_add, 3, 4
-    movsx         r3, word [r2]
-    mov    word [r2], 0
-    lea           r2, [r1*3]
-    add           r3, 15
-    sar           r3, 5
-    movd          m0, r3d
-    pshufw        m0, m0, 0x0
-    pxor          m1, m1
-    psubw         m1, m0
-    packuswb      m0, m0
-    packuswb      m1, m1
-    DC_ADD
-    lea           r0, [r0+r1*4]
-    DC_ADD
-    RET
diff -uparN ffmpeg-4.1/libavcodec/x86/vp6dsp.asm ffmpeg-y/libavcodec/x86/vp6dsp.asm
--- ffmpeg-4.1/libavcodec/x86/vp6dsp.asm	2018-11-02 02:34:25.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/vp6dsp.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,167 +0,0 @@
-;******************************************************************************
-;* MMX/SSE2-optimized functions for the VP6 decoder
-;* Copyright (C) 2009  Sebastien Lucas <sebastien.lucas@gmail.com>
-;* Copyright (C) 2009  Zuxy Meng <zuxy.meng@gmail.com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-cextern pw_64
-
-SECTION .text
-
-%macro DIAG4 6
-%if mmsize == 8
-    movq          m0, [%1+%2]
-    movq          m1, [%1+%3]
-    movq          m3, m0
-    movq          m4, m1
-    punpcklbw     m0, m7
-    punpcklbw     m1, m7
-    punpckhbw     m3, m7
-    punpckhbw     m4, m7
-    pmullw        m0, [rsp+8*11] ; src[x-8 ] * biweight [0]
-    pmullw        m1, [rsp+8*12] ; src[x   ] * biweight [1]
-    pmullw        m3, [rsp+8*11] ; src[x-8 ] * biweight [0]
-    pmullw        m4, [rsp+8*12] ; src[x   ] * biweight [1]
-    paddw         m0, m1
-    paddw         m3, m4
-    movq          m1, [%1+%4]
-    movq          m2, [%1+%5]
-    movq          m4, m1
-    movq          m5, m2
-    punpcklbw     m1, m7
-    punpcklbw     m2, m7
-    punpckhbw     m4, m7
-    punpckhbw     m5, m7
-    pmullw        m1, [rsp+8*13] ; src[x+8 ] * biweight [2]
-    pmullw        m2, [rsp+8*14] ; src[x+16] * biweight [3]
-    pmullw        m4, [rsp+8*13] ; src[x+8 ] * biweight [2]
-    pmullw        m5, [rsp+8*14] ; src[x+16] * biweight [3]
-    paddw         m1, m2
-    paddw         m4, m5
-    paddsw        m0, m1
-    paddsw        m3, m4
-    paddsw        m0, m6         ; Add 64
-    paddsw        m3, m6         ; Add 64
-    psraw         m0, 7
-    psraw         m3, 7
-    packuswb      m0, m3
-    movq        [%6], m0
-%else ; mmsize == 16
-    movq          m0, [%1+%2]
-    movq          m1, [%1+%3]
-    punpcklbw     m0, m7
-    punpcklbw     m1, m7
-    pmullw        m0, m4         ; src[x-8 ] * biweight [0]
-    pmullw        m1, m5         ; src[x   ] * biweight [1]
-    paddw         m0, m1
-    movq          m1, [%1+%4]
-    movq          m2, [%1+%5]
-    punpcklbw     m1, m7
-    punpcklbw     m2, m7
-    pmullw        m1, m6         ; src[x+8 ] * biweight [2]
-    pmullw        m2, m3         ; src[x+16] * biweight [3]
-    paddw         m1, m2
-    paddsw        m0, m1
-    paddsw        m0, [pw_64]    ; Add 64
-    psraw         m0, 7
-    packuswb      m0, m0
-    movq        [%6], m0
-%endif ; mmsize == 8/16
-%endmacro
-
-%macro SPLAT4REGS 0
-%if mmsize == 8
-    movq         m5, m3
-    punpcklwd    m3, m3
-    movq         m4, m3
-    punpckldq    m3, m3
-    punpckhdq    m4, m4
-    punpckhwd    m5, m5
-    movq         m2, m5
-    punpckhdq    m2, m2
-    punpckldq    m5, m5
-    movq [rsp+8*11], m3
-    movq [rsp+8*12], m4
-    movq [rsp+8*13], m5
-    movq [rsp+8*14], m2
-%else ; mmsize == 16
-    pshuflw      m4, m3, 0x0
-    pshuflw      m5, m3, 0x55
-    pshuflw      m6, m3, 0xAA
-    pshuflw      m3, m3, 0xFF
-    punpcklqdq   m4, m4
-    punpcklqdq   m5, m5
-    punpcklqdq   m6, m6
-    punpcklqdq   m3, m3
-%endif ; mmsize == 8/16
-%endmacro
-
-%macro vp6_filter_diag4 0
-; void ff_vp6_filter_diag4_<opt>(uint8_t *dst, uint8_t *src, ptrdiff_t stride,
-;                                const int16_t h_weight[4], const int16_t v_weights[4])
-cglobal vp6_filter_diag4, 5, 7, 8
-    mov          r5, rsp         ; backup stack pointer
-    and         rsp, ~(mmsize-1) ; align stack
-%if mmsize == 16
-    sub         rsp, 8*11
-%else
-    sub         rsp, 8*15
-    movq         m6, [pw_64]
-%endif
-
-    sub          r1, r2
-
-    pxor         m7, m7
-    movq         m3, [r3]
-    SPLAT4REGS
-
-    mov          r3, rsp
-    mov          r6, 11
-.nextrow:
-    DIAG4        r1, -1, 0, 1, 2, r3
-    add          r3, 8
-    add          r1, r2
-    dec          r6
-    jnz .nextrow
-
-    movq         m3, [r4]
-    SPLAT4REGS
-
-    lea          r3, [rsp+8]
-    mov          r6, 8
-.nextcol:
-    DIAG4        r3, -8, 0, 8, 16, r0
-    add          r3, 8
-    add          r0, r2
-    dec          r6
-    jnz .nextcol
-
-    mov         rsp, r5          ; restore stack pointer
-    RET
-%endmacro
-
-%if ARCH_X86_32
-INIT_MMX mmx
-vp6_filter_diag4
-%endif
-
-INIT_XMM sse2
-vp6_filter_diag4
diff -uparN ffmpeg-4.1/libavcodec/x86/vp8dsp.asm ffmpeg-y/libavcodec/x86/vp8dsp.asm
--- ffmpeg-4.1/libavcodec/x86/vp8dsp.asm	2018-11-03 08:17:29.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/vp8dsp.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,1231 +0,0 @@
-;******************************************************************************
-;* VP8 MMXEXT optimizations
-;* Copyright (c) 2010 Ronald S. Bultje <rsbultje@gmail.com>
-;* Copyright (c) 2010 Fiona Glaser <fiona@x264.com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-
-fourtap_filter_hw_m: times 4 dw  -6, 123
-                     times 4 dw  12,  -1
-                     times 4 dw  -9,  93
-                     times 4 dw  50,  -6
-                     times 4 dw  -6,  50
-                     times 4 dw  93,  -9
-                     times 4 dw  -1,  12
-                     times 4 dw 123,  -6
-
-sixtap_filter_hw_m:  times 4 dw   2, -11
-                     times 4 dw 108,  36
-                     times 4 dw  -8,   1
-                     times 4 dw   3, -16
-                     times 4 dw  77,  77
-                     times 4 dw -16,   3
-                     times 4 dw   1,  -8
-                     times 4 dw  36, 108
-                     times 4 dw -11,   2
-
-fourtap_filter_hb_m: times 8 db  -6, 123
-                     times 8 db  12,  -1
-                     times 8 db  -9,  93
-                     times 8 db  50,  -6
-                     times 8 db  -6,  50
-                     times 8 db  93,  -9
-                     times 8 db  -1,  12
-                     times 8 db 123,  -6
-
-sixtap_filter_hb_m:  times 8 db   2,   1
-                     times 8 db -11, 108
-                     times 8 db  36,  -8
-                     times 8 db   3,   3
-                     times 8 db -16,  77
-                     times 8 db  77, -16
-                     times 8 db   1,   2
-                     times 8 db  -8,  36
-                     times 8 db 108, -11
-
-fourtap_filter_v_m:  times 8 dw  -6
-                     times 8 dw 123
-                     times 8 dw  12
-                     times 8 dw  -1
-                     times 8 dw  -9
-                     times 8 dw  93
-                     times 8 dw  50
-                     times 8 dw  -6
-                     times 8 dw  -6
-                     times 8 dw  50
-                     times 8 dw  93
-                     times 8 dw  -9
-                     times 8 dw  -1
-                     times 8 dw  12
-                     times 8 dw 123
-                     times 8 dw  -6
-
-sixtap_filter_v_m:   times 8 dw   2
-                     times 8 dw -11
-                     times 8 dw 108
-                     times 8 dw  36
-                     times 8 dw  -8
-                     times 8 dw   1
-                     times 8 dw   3
-                     times 8 dw -16
-                     times 8 dw  77
-                     times 8 dw  77
-                     times 8 dw -16
-                     times 8 dw   3
-                     times 8 dw   1
-                     times 8 dw  -8
-                     times 8 dw  36
-                     times 8 dw 108
-                     times 8 dw -11
-                     times 8 dw   2
-
-bilinear_filter_vw_m: times 8 dw 1
-                      times 8 dw 2
-                      times 8 dw 3
-                      times 8 dw 4
-                      times 8 dw 5
-                      times 8 dw 6
-                      times 8 dw 7
-
-bilinear_filter_vb_m: times 8 db 7, 1
-                      times 8 db 6, 2
-                      times 8 db 5, 3
-                      times 8 db 4, 4
-                      times 8 db 3, 5
-                      times 8 db 2, 6
-                      times 8 db 1, 7
-
-%ifdef PIC
-%define fourtap_filter_hw  picregq
-%define sixtap_filter_hw   picregq
-%define fourtap_filter_hb  picregq
-%define sixtap_filter_hb   picregq
-%define fourtap_filter_v   picregq
-%define sixtap_filter_v    picregq
-%define bilinear_filter_vw picregq
-%define bilinear_filter_vb picregq
-%define npicregs 1
-%else
-%define fourtap_filter_hw  fourtap_filter_hw_m
-%define sixtap_filter_hw   sixtap_filter_hw_m
-%define fourtap_filter_hb  fourtap_filter_hb_m
-%define sixtap_filter_hb   sixtap_filter_hb_m
-%define fourtap_filter_v   fourtap_filter_v_m
-%define sixtap_filter_v    sixtap_filter_v_m
-%define bilinear_filter_vw bilinear_filter_vw_m
-%define bilinear_filter_vb bilinear_filter_vb_m
-%define npicregs 0
-%endif
-
-filter_h2_shuf:  db 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5,  6, 6,  7,  7,  8
-filter_h4_shuf:  db 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7,  8, 8,  9,  9, 10
-
-filter_h6_shuf1: db 0, 5, 1, 6, 2, 7, 3, 8, 4, 9, 5, 10, 6, 11,  7, 12
-filter_h6_shuf2: db 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6,  7, 7,  8,  8,  9
-filter_h6_shuf3: db 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8,  9, 9, 10, 10, 11
-
-pw_20091: times 4 dw 20091
-pw_17734: times 4 dw 17734
-
-cextern pw_3
-cextern pw_4
-cextern pw_64
-cextern pw_256
-
-SECTION .text
-
-;-------------------------------------------------------------------------------
-; subpel MC functions:
-;
-; void ff_put_vp8_epel<size>_h<htap>v<vtap>_<opt>(uint8_t *dst, ptrdiff_t deststride,
-;                                                 uint8_t *src, ptrdiff_t srcstride,
-;                                                 int height,   int mx, int my);
-;-------------------------------------------------------------------------------
-
-%macro FILTER_SSSE3 1
-cglobal put_vp8_epel%1_h6, 6, 6 + npicregs, 8, dst, dststride, src, srcstride, height, mx, picreg
-    lea      mxd, [mxq*3]
-    mova      m3, [filter_h6_shuf2]
-    mova      m4, [filter_h6_shuf3]
-%ifdef PIC
-    lea  picregq, [sixtap_filter_hb_m]
-%endif
-    mova      m5, [sixtap_filter_hb+mxq*8-48] ; set up 6tap filter in bytes
-    mova      m6, [sixtap_filter_hb+mxq*8-32]
-    mova      m7, [sixtap_filter_hb+mxq*8-16]
-
-.nextrow:
-    movu      m0, [srcq-2]
-    mova      m1, m0
-    mova      m2, m0
-%if mmsize == 8
-; For epel4, we need 9 bytes, but only 8 get loaded; to compensate, do the
-; shuffle with a memory operand
-    punpcklbw m0, [srcq+3]
-%else
-    pshufb    m0, [filter_h6_shuf1]
-%endif
-    pshufb    m1, m3
-    pshufb    m2, m4
-    pmaddubsw m0, m5
-    pmaddubsw m1, m6
-    pmaddubsw m2, m7
-    paddsw    m0, m1
-    paddsw    m0, m2
-    pmulhrsw  m0, [pw_256]
-    packuswb  m0, m0
-    movh  [dstq], m0        ; store
-
-    ; go to next line
-    add     dstq, dststrideq
-    add     srcq, srcstrideq
-    dec  heightd            ; next row
-    jg .nextrow
-    REP_RET
-
-cglobal put_vp8_epel%1_h4, 6, 6 + npicregs, 7, dst, dststride, src, srcstride, height, mx, picreg
-    shl      mxd, 4
-    mova      m2, [pw_256]
-    mova      m3, [filter_h2_shuf]
-    mova      m4, [filter_h4_shuf]
-%ifdef PIC
-    lea  picregq, [fourtap_filter_hb_m]
-%endif
-    mova      m5, [fourtap_filter_hb+mxq-16] ; set up 4tap filter in bytes
-    mova      m6, [fourtap_filter_hb+mxq]
-
-.nextrow:
-    movu      m0, [srcq-1]
-    mova      m1, m0
-    pshufb    m0, m3
-    pshufb    m1, m4
-    pmaddubsw m0, m5
-    pmaddubsw m1, m6
-    paddsw    m0, m1
-    pmulhrsw  m0, m2
-    packuswb  m0, m0
-    movh  [dstq], m0        ; store
-
-    ; go to next line
-    add     dstq, dststrideq
-    add     srcq, srcstrideq
-    dec  heightd            ; next row
-    jg .nextrow
-    REP_RET
-
-cglobal put_vp8_epel%1_v4, 7, 7, 8, dst, dststride, src, srcstride, height, picreg, my
-    shl      myd, 4
-%ifdef PIC
-    lea  picregq, [fourtap_filter_hb_m]
-%endif
-    mova      m5, [fourtap_filter_hb+myq-16]
-    mova      m6, [fourtap_filter_hb+myq]
-    mova      m7, [pw_256]
-
-    ; read 3 lines
-    sub     srcq, srcstrideq
-    movh      m0, [srcq]
-    movh      m1, [srcq+  srcstrideq]
-    movh      m2, [srcq+2*srcstrideq]
-    add     srcq, srcstrideq
-
-.nextrow:
-    movh      m3, [srcq+2*srcstrideq]      ; read new row
-    mova      m4, m0
-    mova      m0, m1
-    punpcklbw m4, m1
-    mova      m1, m2
-    punpcklbw m2, m3
-    pmaddubsw m4, m5
-    pmaddubsw m2, m6
-    paddsw    m4, m2
-    mova      m2, m3
-    pmulhrsw  m4, m7
-    packuswb  m4, m4
-    movh  [dstq], m4
-
-    ; go to next line
-    add      dstq, dststrideq
-    add      srcq, srcstrideq
-    dec   heightd                          ; next row
-    jg .nextrow
-    REP_RET
-
-cglobal put_vp8_epel%1_v6, 7, 7, 8, dst, dststride, src, srcstride, height, picreg, my
-    lea      myd, [myq*3]
-%ifdef PIC
-    lea  picregq, [sixtap_filter_hb_m]
-%endif
-    lea      myq, [sixtap_filter_hb+myq*8]
-
-    ; read 5 lines
-    sub     srcq, srcstrideq
-    sub     srcq, srcstrideq
-    movh      m0, [srcq]
-    movh      m1, [srcq+srcstrideq]
-    movh      m2, [srcq+srcstrideq*2]
-    lea     srcq, [srcq+srcstrideq*2]
-    add     srcq, srcstrideq
-    movh      m3, [srcq]
-    movh      m4, [srcq+srcstrideq]
-
-.nextrow:
-    movh      m5, [srcq+2*srcstrideq]      ; read new row
-    mova      m6, m0
-    punpcklbw m6, m5
-    mova      m0, m1
-    punpcklbw m1, m2
-    mova      m7, m3
-    punpcklbw m7, m4
-    pmaddubsw m6, [myq-48]
-    pmaddubsw m1, [myq-32]
-    pmaddubsw m7, [myq-16]
-    paddsw    m6, m1
-    paddsw    m6, m7
-    mova      m1, m2
-    mova      m2, m3
-    pmulhrsw  m6, [pw_256]
-    mova      m3, m4
-    packuswb  m6, m6
-    mova      m4, m5
-    movh  [dstq], m6
-
-    ; go to next line
-    add      dstq, dststrideq
-    add      srcq, srcstrideq
-    dec   heightd                          ; next row
-    jg .nextrow
-    REP_RET
-%endmacro
-
-INIT_MMX ssse3
-FILTER_SSSE3 4
-INIT_XMM ssse3
-FILTER_SSSE3 8
-
-; 4x4 block, H-only 4-tap filter
-INIT_MMX mmxext
-cglobal put_vp8_epel4_h4, 6, 6 + npicregs, 0, dst, dststride, src, srcstride, height, mx, picreg
-    shl       mxd, 4
-%ifdef PIC
-    lea   picregq, [fourtap_filter_hw_m]
-%endif
-    movq      mm4, [fourtap_filter_hw+mxq-16] ; set up 4tap filter in words
-    movq      mm5, [fourtap_filter_hw+mxq]
-    movq      mm7, [pw_64]
-    pxor      mm6, mm6
-
-.nextrow:
-    movq      mm1, [srcq-1]                ; (ABCDEFGH) load 8 horizontal pixels
-
-    ; first set of 2 pixels
-    movq      mm2, mm1                     ; byte ABCD..
-    punpcklbw mm1, mm6                     ; byte->word ABCD
-    pshufw    mm0, mm2, 9                  ; byte CDEF..
-    punpcklbw mm0, mm6                     ; byte->word CDEF
-    pshufw    mm3, mm1, 0x94               ; word ABBC
-    pshufw    mm1, mm0, 0x94               ; word CDDE
-    pmaddwd   mm3, mm4                     ; multiply 2px with F0/F1
-    movq      mm0, mm1                     ; backup for second set of pixels
-    pmaddwd   mm1, mm5                     ; multiply 2px with F2/F3
-    paddd     mm3, mm1                     ; finish 1st 2px
-
-    ; second set of 2 pixels, use backup of above
-    punpckhbw mm2, mm6                     ; byte->word EFGH
-    pmaddwd   mm0, mm4                     ; multiply backed up 2px with F0/F1
-    pshufw    mm1, mm2, 0x94               ; word EFFG
-    pmaddwd   mm1, mm5                     ; multiply 2px with F2/F3
-    paddd     mm0, mm1                     ; finish 2nd 2px
-
-    ; merge two sets of 2 pixels into one set of 4, round/clip/store
-    packssdw  mm3, mm0                     ; merge dword->word (4px)
-    paddsw    mm3, mm7                     ; rounding
-    psraw     mm3, 7
-    packuswb  mm3, mm6                     ; clip and word->bytes
-    movd   [dstq], mm3                     ; store
-
-    ; go to next line
-    add      dstq, dststrideq
-    add      srcq, srcstrideq
-    dec   heightd                          ; next row
-    jg .nextrow
-    REP_RET
-
-; 4x4 block, H-only 6-tap filter
-INIT_MMX mmxext
-cglobal put_vp8_epel4_h6, 6, 6 + npicregs, 0, dst, dststride, src, srcstride, height, mx, picreg
-    lea       mxd, [mxq*3]
-%ifdef PIC
-    lea   picregq, [sixtap_filter_hw_m]
-%endif
-    movq      mm4, [sixtap_filter_hw+mxq*8-48] ; set up 4tap filter in words
-    movq      mm5, [sixtap_filter_hw+mxq*8-32]
-    movq      mm6, [sixtap_filter_hw+mxq*8-16]
-    movq      mm7, [pw_64]
-    pxor      mm3, mm3
-
-.nextrow:
-    movq      mm1, [srcq-2]                ; (ABCDEFGH) load 8 horizontal pixels
-
-    ; first set of 2 pixels
-    movq      mm2, mm1                     ; byte ABCD..
-    punpcklbw mm1, mm3                     ; byte->word ABCD
-    pshufw    mm0, mm2, 0x9                ; byte CDEF..
-    punpckhbw mm2, mm3                     ; byte->word EFGH
-    punpcklbw mm0, mm3                     ; byte->word CDEF
-    pshufw    mm1, mm1, 0x94               ; word ABBC
-    pshufw    mm2, mm2, 0x94               ; word EFFG
-    pmaddwd   mm1, mm4                     ; multiply 2px with F0/F1
-    pshufw    mm3, mm0, 0x94               ; word CDDE
-    movq      mm0, mm3                     ; backup for second set of pixels
-    pmaddwd   mm3, mm5                     ; multiply 2px with F2/F3
-    paddd     mm1, mm3                     ; add to 1st 2px cache
-    movq      mm3, mm2                     ; backup for second set of pixels
-    pmaddwd   mm2, mm6                     ; multiply 2px with F4/F5
-    paddd     mm1, mm2                     ; finish 1st 2px
-
-    ; second set of 2 pixels, use backup of above
-    movd      mm2, [srcq+3]                ; byte FGHI (prevent overreads)
-    pmaddwd   mm0, mm4                     ; multiply 1st backed up 2px with F0/F1
-    pmaddwd   mm3, mm5                     ; multiply 2nd backed up 2px with F2/F3
-    paddd     mm0, mm3                     ; add to 2nd 2px cache
-    pxor      mm3, mm3
-    punpcklbw mm2, mm3                     ; byte->word FGHI
-    pshufw    mm2, mm2, 0xE9               ; word GHHI
-    pmaddwd   mm2, mm6                     ; multiply 2px with F4/F5
-    paddd     mm0, mm2                     ; finish 2nd 2px
-
-    ; merge two sets of 2 pixels into one set of 4, round/clip/store
-    packssdw  mm1, mm0                     ; merge dword->word (4px)
-    paddsw    mm1, mm7                     ; rounding
-    psraw     mm1, 7
-    packuswb  mm1, mm3                     ; clip and word->bytes
-    movd   [dstq], mm1                     ; store
-
-    ; go to next line
-    add      dstq, dststrideq
-    add      srcq, srcstrideq
-    dec   heightd                          ; next row
-    jg .nextrow
-    REP_RET
-
-INIT_XMM sse2
-cglobal put_vp8_epel8_h4, 6, 6 + npicregs, 10, dst, dststride, src, srcstride, height, mx, picreg
-    shl      mxd, 5
-%ifdef PIC
-    lea  picregq, [fourtap_filter_v_m]
-%endif
-    lea      mxq, [fourtap_filter_v+mxq-32]
-    pxor      m7, m7
-    mova      m4, [pw_64]
-    mova      m5, [mxq+ 0]
-    mova      m6, [mxq+16]
-%ifdef m8
-    mova      m8, [mxq+32]
-    mova      m9, [mxq+48]
-%endif
-.nextrow:
-    movq      m0, [srcq-1]
-    movq      m1, [srcq-0]
-    movq      m2, [srcq+1]
-    movq      m3, [srcq+2]
-    punpcklbw m0, m7
-    punpcklbw m1, m7
-    punpcklbw m2, m7
-    punpcklbw m3, m7
-    pmullw    m0, m5
-    pmullw    m1, m6
-%ifdef m8
-    pmullw    m2, m8
-    pmullw    m3, m9
-%else
-    pmullw    m2, [mxq+32]
-    pmullw    m3, [mxq+48]
-%endif
-    paddsw    m0, m1
-    paddsw    m2, m3
-    paddsw    m0, m2
-    paddsw    m0, m4
-    psraw     m0, 7
-    packuswb  m0, m7
-    movh  [dstq], m0        ; store
-
-    ; go to next line
-    add     dstq, dststrideq
-    add     srcq, srcstrideq
-    dec  heightd            ; next row
-    jg .nextrow
-    REP_RET
-
-INIT_XMM sse2
-cglobal put_vp8_epel8_h6, 6, 6 + npicregs, 14, dst, dststride, src, srcstride, height, mx, picreg
-    lea      mxd, [mxq*3]
-    shl      mxd, 4
-%ifdef PIC
-    lea  picregq, [sixtap_filter_v_m]
-%endif
-    lea      mxq, [sixtap_filter_v+mxq-96]
-    pxor      m7, m7
-    mova      m6, [pw_64]
-%ifdef m8
-    mova      m8, [mxq+ 0]
-    mova      m9, [mxq+16]
-    mova     m10, [mxq+32]
-    mova     m11, [mxq+48]
-    mova     m12, [mxq+64]
-    mova     m13, [mxq+80]
-%endif
-.nextrow:
-    movq      m0, [srcq-2]
-    movq      m1, [srcq-1]
-    movq      m2, [srcq-0]
-    movq      m3, [srcq+1]
-    movq      m4, [srcq+2]
-    movq      m5, [srcq+3]
-    punpcklbw m0, m7
-    punpcklbw m1, m7
-    punpcklbw m2, m7
-    punpcklbw m3, m7
-    punpcklbw m4, m7
-    punpcklbw m5, m7
-%ifdef m8
-    pmullw    m0, m8
-    pmullw    m1, m9
-    pmullw    m2, m10
-    pmullw    m3, m11
-    pmullw    m4, m12
-    pmullw    m5, m13
-%else
-    pmullw    m0, [mxq+ 0]
-    pmullw    m1, [mxq+16]
-    pmullw    m2, [mxq+32]
-    pmullw    m3, [mxq+48]
-    pmullw    m4, [mxq+64]
-    pmullw    m5, [mxq+80]
-%endif
-    paddsw    m1, m4
-    paddsw    m0, m5
-    paddsw    m1, m2
-    paddsw    m0, m3
-    paddsw    m0, m1
-    paddsw    m0, m6
-    psraw     m0, 7
-    packuswb  m0, m7
-    movh  [dstq], m0        ; store
-
-    ; go to next line
-    add     dstq, dststrideq
-    add     srcq, srcstrideq
-    dec  heightd            ; next row
-    jg .nextrow
-    REP_RET
-
-%macro FILTER_V 1
-; 4x4 block, V-only 4-tap filter
-cglobal put_vp8_epel%1_v4, 7, 7, 8, dst, dststride, src, srcstride, height, picreg, my
-    shl      myd, 5
-%ifdef PIC
-    lea  picregq, [fourtap_filter_v_m]
-%endif
-    lea      myq, [fourtap_filter_v+myq-32]
-    mova      m6, [pw_64]
-    pxor      m7, m7
-    mova      m5, [myq+48]
-
-    ; read 3 lines
-    sub     srcq, srcstrideq
-    movh      m0, [srcq]
-    movh      m1, [srcq+  srcstrideq]
-    movh      m2, [srcq+2*srcstrideq]
-    add     srcq, srcstrideq
-    punpcklbw m0, m7
-    punpcklbw m1, m7
-    punpcklbw m2, m7
-
-.nextrow:
-    ; first calculate negative taps (to prevent losing positive overflows)
-    movh      m4, [srcq+2*srcstrideq]      ; read new row
-    punpcklbw m4, m7
-    mova      m3, m4
-    pmullw    m0, [myq+0]
-    pmullw    m4, m5
-    paddsw    m4, m0
-
-    ; then calculate positive taps
-    mova      m0, m1
-    pmullw    m1, [myq+16]
-    paddsw    m4, m1
-    mova      m1, m2
-    pmullw    m2, [myq+32]
-    paddsw    m4, m2
-    mova      m2, m3
-
-    ; round/clip/store
-    paddsw    m4, m6
-    psraw     m4, 7
-    packuswb  m4, m7
-    movh  [dstq], m4
-
-    ; go to next line
-    add     dstq, dststrideq
-    add     srcq, srcstrideq
-    dec  heightd                           ; next row
-    jg .nextrow
-    REP_RET
-
-
-; 4x4 block, V-only 6-tap filter
-cglobal put_vp8_epel%1_v6, 7, 7, 8, dst, dststride, src, srcstride, height, picreg, my
-    shl      myd, 4
-    lea      myq, [myq*3]
-%ifdef PIC
-    lea  picregq, [sixtap_filter_v_m]
-%endif
-    lea      myq, [sixtap_filter_v+myq-96]
-    pxor      m7, m7
-
-    ; read 5 lines
-    sub     srcq, srcstrideq
-    sub     srcq, srcstrideq
-    movh      m0, [srcq]
-    movh      m1, [srcq+srcstrideq]
-    movh      m2, [srcq+srcstrideq*2]
-    lea     srcq, [srcq+srcstrideq*2]
-    add     srcq, srcstrideq
-    movh      m3, [srcq]
-    movh      m4, [srcq+srcstrideq]
-    punpcklbw m0, m7
-    punpcklbw m1, m7
-    punpcklbw m2, m7
-    punpcklbw m3, m7
-    punpcklbw m4, m7
-
-.nextrow:
-    ; first calculate negative taps (to prevent losing positive overflows)
-    mova      m5, m1
-    pmullw    m5, [myq+16]
-    mova      m6, m4
-    pmullw    m6, [myq+64]
-    paddsw    m6, m5
-
-    ; then calculate positive taps
-    movh      m5, [srcq+2*srcstrideq]      ; read new row
-    punpcklbw m5, m7
-    pmullw    m0, [myq+0]
-    paddsw    m6, m0
-    mova      m0, m1
-    mova      m1, m2
-    pmullw    m2, [myq+32]
-    paddsw    m6, m2
-    mova      m2, m3
-    pmullw    m3, [myq+48]
-    paddsw    m6, m3
-    mova      m3, m4
-    mova      m4, m5
-    pmullw    m5, [myq+80]
-    paddsw    m6, m5
-
-    ; round/clip/store
-    paddsw    m6, [pw_64]
-    psraw     m6, 7
-    packuswb  m6, m7
-    movh  [dstq], m6
-
-    ; go to next line
-    add     dstq, dststrideq
-    add     srcq, srcstrideq
-    dec  heightd                           ; next row
-    jg .nextrow
-    REP_RET
-%endmacro
-
-INIT_MMX mmxext
-FILTER_V 4
-INIT_XMM sse2
-FILTER_V 8
-
-%macro FILTER_BILINEAR 1
-%if cpuflag(ssse3)
-cglobal put_vp8_bilinear%1_v, 7, 7, 5, dst, dststride, src, srcstride, height, picreg, my
-    shl      myd, 4
-%ifdef PIC
-    lea  picregq, [bilinear_filter_vb_m]
-%endif
-    pxor      m4, m4
-    mova      m3, [bilinear_filter_vb+myq-16]
-.nextrow:
-    movh      m0, [srcq+srcstrideq*0]
-    movh      m1, [srcq+srcstrideq*1]
-    movh      m2, [srcq+srcstrideq*2]
-    punpcklbw m0, m1
-    punpcklbw m1, m2
-    pmaddubsw m0, m3
-    pmaddubsw m1, m3
-    psraw     m0, 2
-    psraw     m1, 2
-    pavgw     m0, m4
-    pavgw     m1, m4
-%if mmsize==8
-    packuswb  m0, m0
-    packuswb  m1, m1
-    movh   [dstq+dststrideq*0], m0
-    movh   [dstq+dststrideq*1], m1
-%else
-    packuswb  m0, m1
-    movh   [dstq+dststrideq*0], m0
-    movhps [dstq+dststrideq*1], m0
-%endif
-%else ; cpuflag(ssse3)
-cglobal put_vp8_bilinear%1_v, 7, 7, 7, dst, dststride, src, srcstride, height, picreg, my
-    shl      myd, 4
-%ifdef PIC
-    lea  picregq, [bilinear_filter_vw_m]
-%endif
-    pxor      m6, m6
-    mova      m5, [bilinear_filter_vw+myq-1*16]
-    neg      myq
-    mova      m4, [bilinear_filter_vw+myq+7*16]
-.nextrow:
-    movh      m0, [srcq+srcstrideq*0]
-    movh      m1, [srcq+srcstrideq*1]
-    movh      m3, [srcq+srcstrideq*2]
-    punpcklbw m0, m6
-    punpcklbw m1, m6
-    punpcklbw m3, m6
-    mova      m2, m1
-    pmullw    m0, m4
-    pmullw    m1, m5
-    pmullw    m2, m4
-    pmullw    m3, m5
-    paddsw    m0, m1
-    paddsw    m2, m3
-    psraw     m0, 2
-    psraw     m2, 2
-    pavgw     m0, m6
-    pavgw     m2, m6
-%if mmsize == 8
-    packuswb  m0, m0
-    packuswb  m2, m2
-    movh   [dstq+dststrideq*0], m0
-    movh   [dstq+dststrideq*1], m2
-%else
-    packuswb  m0, m2
-    movh   [dstq+dststrideq*0], m0
-    movhps [dstq+dststrideq*1], m0
-%endif
-%endif ; cpuflag(ssse3)
-
-    lea     dstq, [dstq+dststrideq*2]
-    lea     srcq, [srcq+srcstrideq*2]
-    sub  heightd, 2
-    jg .nextrow
-    REP_RET
-
-%if cpuflag(ssse3)
-cglobal put_vp8_bilinear%1_h, 6, 6 + npicregs, 5, dst, dststride, src, srcstride, height, mx, picreg
-    shl      mxd, 4
-%ifdef PIC
-    lea  picregq, [bilinear_filter_vb_m]
-%endif
-    pxor      m4, m4
-    mova      m2, [filter_h2_shuf]
-    mova      m3, [bilinear_filter_vb+mxq-16]
-.nextrow:
-    movu      m0, [srcq+srcstrideq*0]
-    movu      m1, [srcq+srcstrideq*1]
-    pshufb    m0, m2
-    pshufb    m1, m2
-    pmaddubsw m0, m3
-    pmaddubsw m1, m3
-    psraw     m0, 2
-    psraw     m1, 2
-    pavgw     m0, m4
-    pavgw     m1, m4
-%if mmsize==8
-    packuswb  m0, m0
-    packuswb  m1, m1
-    movh   [dstq+dststrideq*0], m0
-    movh   [dstq+dststrideq*1], m1
-%else
-    packuswb  m0, m1
-    movh   [dstq+dststrideq*0], m0
-    movhps [dstq+dststrideq*1], m0
-%endif
-%else ; cpuflag(ssse3)
-cglobal put_vp8_bilinear%1_h, 6, 6 + npicregs, 7, dst, dststride, src, srcstride, height, mx, picreg
-    shl      mxd, 4
-%ifdef PIC
-    lea  picregq, [bilinear_filter_vw_m]
-%endif
-    pxor      m6, m6
-    mova      m5, [bilinear_filter_vw+mxq-1*16]
-    neg      mxq
-    mova      m4, [bilinear_filter_vw+mxq+7*16]
-.nextrow:
-    movh      m0, [srcq+srcstrideq*0+0]
-    movh      m1, [srcq+srcstrideq*0+1]
-    movh      m2, [srcq+srcstrideq*1+0]
-    movh      m3, [srcq+srcstrideq*1+1]
-    punpcklbw m0, m6
-    punpcklbw m1, m6
-    punpcklbw m2, m6
-    punpcklbw m3, m6
-    pmullw    m0, m4
-    pmullw    m1, m5
-    pmullw    m2, m4
-    pmullw    m3, m5
-    paddsw    m0, m1
-    paddsw    m2, m3
-    psraw     m0, 2
-    psraw     m2, 2
-    pavgw     m0, m6
-    pavgw     m2, m6
-%if mmsize == 8
-    packuswb  m0, m0
-    packuswb  m2, m2
-    movh   [dstq+dststrideq*0], m0
-    movh   [dstq+dststrideq*1], m2
-%else
-    packuswb  m0, m2
-    movh   [dstq+dststrideq*0], m0
-    movhps [dstq+dststrideq*1], m0
-%endif
-%endif ; cpuflag(ssse3)
-
-    lea     dstq, [dstq+dststrideq*2]
-    lea     srcq, [srcq+srcstrideq*2]
-    sub  heightd, 2
-    jg .nextrow
-    REP_RET
-%endmacro
-
-INIT_MMX mmxext
-FILTER_BILINEAR 4
-INIT_XMM sse2
-FILTER_BILINEAR 8
-INIT_MMX ssse3
-FILTER_BILINEAR 4
-INIT_XMM ssse3
-FILTER_BILINEAR 8
-
-INIT_MMX mmx
-cglobal put_vp8_pixels8, 5, 5, 0, dst, dststride, src, srcstride, height
-.nextrow:
-    movq    mm0, [srcq+srcstrideq*0]
-    movq    mm1, [srcq+srcstrideq*1]
-    lea    srcq, [srcq+srcstrideq*2]
-    movq [dstq+dststrideq*0], mm0
-    movq [dstq+dststrideq*1], mm1
-    lea    dstq, [dstq+dststrideq*2]
-    sub heightd, 2
-    jg .nextrow
-    REP_RET
-
-%if ARCH_X86_32
-INIT_MMX mmx
-cglobal put_vp8_pixels16, 5, 5, 0, dst, dststride, src, srcstride, height
-.nextrow:
-    movq    mm0, [srcq+srcstrideq*0+0]
-    movq    mm1, [srcq+srcstrideq*0+8]
-    movq    mm2, [srcq+srcstrideq*1+0]
-    movq    mm3, [srcq+srcstrideq*1+8]
-    lea    srcq, [srcq+srcstrideq*2]
-    movq [dstq+dststrideq*0+0], mm0
-    movq [dstq+dststrideq*0+8], mm1
-    movq [dstq+dststrideq*1+0], mm2
-    movq [dstq+dststrideq*1+8], mm3
-    lea    dstq, [dstq+dststrideq*2]
-    sub heightd, 2
-    jg .nextrow
-    REP_RET
-%endif
-
-INIT_XMM sse
-cglobal put_vp8_pixels16, 5, 5, 2, dst, dststride, src, srcstride, height
-.nextrow:
-    movups xmm0, [srcq+srcstrideq*0]
-    movups xmm1, [srcq+srcstrideq*1]
-    lea    srcq, [srcq+srcstrideq*2]
-    movaps [dstq+dststrideq*0], xmm0
-    movaps [dstq+dststrideq*1], xmm1
-    lea    dstq, [dstq+dststrideq*2]
-    sub heightd, 2
-    jg .nextrow
-    REP_RET
-
-;-----------------------------------------------------------------------------
-; void ff_vp8_idct_dc_add_<opt>(uint8_t *dst, int16_t block[16], ptrdiff_t stride);
-;-----------------------------------------------------------------------------
-
-%macro ADD_DC 4
-    %4        m2, [dst1q+%3]
-    %4        m3, [dst1q+strideq+%3]
-    %4        m4, [dst2q+%3]
-    %4        m5, [dst2q+strideq+%3]
-    paddusb   m2, %1
-    paddusb   m3, %1
-    paddusb   m4, %1
-    paddusb   m5, %1
-    psubusb   m2, %2
-    psubusb   m3, %2
-    psubusb   m4, %2
-    psubusb   m5, %2
-    %4 [dst1q+%3], m2
-    %4 [dst1q+strideq+%3], m3
-    %4 [dst2q+%3], m4
-    %4 [dst2q+strideq+%3], m5
-%endmacro
-
-%if ARCH_X86_32
-INIT_MMX mmx
-cglobal vp8_idct_dc_add, 3, 3, 0, dst, block, stride
-    ; load data
-    movd       m0, [blockq]
-
-    ; calculate DC
-    paddw      m0, [pw_4]
-    pxor       m1, m1
-    psraw      m0, 3
-    movd [blockq], m1
-    psubw      m1, m0
-    packuswb   m0, m0
-    packuswb   m1, m1
-    punpcklbw  m0, m0
-    punpcklbw  m1, m1
-    punpcklwd  m0, m0
-    punpcklwd  m1, m1
-
-    ; add DC
-    DEFINE_ARGS dst1, dst2, stride
-    lea     dst2q, [dst1q+strideq*2]
-    ADD_DC     m0, m1, 0, movh
-    RET
-%endif
-
-%macro VP8_IDCT_DC_ADD 0
-cglobal vp8_idct_dc_add, 3, 3, 6, dst, block, stride
-    ; load data
-    movd       m0, [blockq]
-    pxor       m1, m1
-
-    ; calculate DC
-    paddw      m0, [pw_4]
-    movd [blockq], m1
-    DEFINE_ARGS dst1, dst2, stride
-    lea     dst2q, [dst1q+strideq*2]
-    movd       m2, [dst1q]
-    movd       m3, [dst1q+strideq]
-    movd       m4, [dst2q]
-    movd       m5, [dst2q+strideq]
-    psraw      m0, 3
-    pshuflw    m0, m0, 0
-    punpcklqdq m0, m0
-    punpckldq  m2, m3
-    punpckldq  m4, m5
-    punpcklbw  m2, m1
-    punpcklbw  m4, m1
-    paddw      m2, m0
-    paddw      m4, m0
-    packuswb   m2, m4
-    movd   [dst1q], m2
-%if cpuflag(sse4)
-    pextrd [dst1q+strideq], m2, 1
-    pextrd [dst2q], m2, 2
-    pextrd [dst2q+strideq], m2, 3
-%else
-    psrldq     m2, 4
-    movd [dst1q+strideq], m2
-    psrldq     m2, 4
-    movd [dst2q], m2
-    psrldq     m2, 4
-    movd [dst2q+strideq], m2
-%endif
-    RET
-%endmacro
-
-INIT_XMM sse2
-VP8_IDCT_DC_ADD
-INIT_XMM sse4
-VP8_IDCT_DC_ADD
-
-;-----------------------------------------------------------------------------
-; void ff_vp8_idct_dc_add4y_<opt>(uint8_t *dst, int16_t block[4][16], ptrdiff_t stride);
-;-----------------------------------------------------------------------------
-
-%if ARCH_X86_32
-INIT_MMX mmx
-cglobal vp8_idct_dc_add4y, 3, 3, 0, dst, block, stride
-    ; load data
-    movd      m0, [blockq+32*0] ; A
-    movd      m1, [blockq+32*2] ; C
-    punpcklwd m0, [blockq+32*1] ; A B
-    punpcklwd m1, [blockq+32*3] ; C D
-    punpckldq m0, m1        ; A B C D
-    pxor      m6, m6
-
-    ; calculate DC
-    paddw     m0, [pw_4]
-    movd [blockq+32*0], m6
-    movd [blockq+32*1], m6
-    movd [blockq+32*2], m6
-    movd [blockq+32*3], m6
-    psraw     m0, 3
-    psubw     m6, m0
-    packuswb  m0, m0
-    packuswb  m6, m6
-    punpcklbw m0, m0 ; AABBCCDD
-    punpcklbw m6, m6 ; AABBCCDD
-    movq      m1, m0
-    movq      m7, m6
-    punpcklbw m0, m0 ; AAAABBBB
-    punpckhbw m1, m1 ; CCCCDDDD
-    punpcklbw m6, m6 ; AAAABBBB
-    punpckhbw m7, m7 ; CCCCDDDD
-
-    ; add DC
-    DEFINE_ARGS dst1, dst2, stride
-    lea    dst2q, [dst1q+strideq*2]
-    ADD_DC    m0, m6, 0, mova
-    ADD_DC    m1, m7, 8, mova
-    RET
-%endif
-
-INIT_XMM sse2
-cglobal vp8_idct_dc_add4y, 3, 3, 6, dst, block, stride
-    ; load data
-    movd      m0, [blockq+32*0] ; A
-    movd      m1, [blockq+32*2] ; C
-    punpcklwd m0, [blockq+32*1] ; A B
-    punpcklwd m1, [blockq+32*3] ; C D
-    punpckldq m0, m1        ; A B C D
-    pxor      m1, m1
-
-    ; calculate DC
-    paddw     m0, [pw_4]
-    movd [blockq+32*0], m1
-    movd [blockq+32*1], m1
-    movd [blockq+32*2], m1
-    movd [blockq+32*3], m1
-    psraw     m0, 3
-    psubw     m1, m0
-    packuswb  m0, m0
-    packuswb  m1, m1
-    punpcklbw m0, m0
-    punpcklbw m1, m1
-    punpcklbw m0, m0
-    punpcklbw m1, m1
-
-    ; add DC
-    DEFINE_ARGS dst1, dst2, stride
-    lea    dst2q, [dst1q+strideq*2]
-    ADD_DC    m0, m1, 0, mova
-    RET
-
-;-----------------------------------------------------------------------------
-; void ff_vp8_idct_dc_add4uv_<opt>(uint8_t *dst, int16_t block[4][16], ptrdiff_t stride);
-;-----------------------------------------------------------------------------
-
-INIT_MMX mmx
-cglobal vp8_idct_dc_add4uv, 3, 3, 0, dst, block, stride
-    ; load data
-    movd      m0, [blockq+32*0] ; A
-    movd      m1, [blockq+32*2] ; C
-    punpcklwd m0, [blockq+32*1] ; A B
-    punpcklwd m1, [blockq+32*3] ; C D
-    punpckldq m0, m1        ; A B C D
-    pxor      m6, m6
-
-    ; calculate DC
-    paddw     m0, [pw_4]
-    movd [blockq+32*0], m6
-    movd [blockq+32*1], m6
-    movd [blockq+32*2], m6
-    movd [blockq+32*3], m6
-    psraw     m0, 3
-    psubw     m6, m0
-    packuswb  m0, m0
-    packuswb  m6, m6
-    punpcklbw m0, m0 ; AABBCCDD
-    punpcklbw m6, m6 ; AABBCCDD
-    movq      m1, m0
-    movq      m7, m6
-    punpcklbw m0, m0 ; AAAABBBB
-    punpckhbw m1, m1 ; CCCCDDDD
-    punpcklbw m6, m6 ; AAAABBBB
-    punpckhbw m7, m7 ; CCCCDDDD
-
-    ; add DC
-    DEFINE_ARGS dst1, dst2, stride
-    lea    dst2q, [dst1q+strideq*2]
-    ADD_DC    m0, m6, 0, mova
-    lea    dst1q, [dst1q+strideq*4]
-    lea    dst2q, [dst2q+strideq*4]
-    ADD_DC    m1, m7, 0, mova
-    RET
-
-;-----------------------------------------------------------------------------
-; void ff_vp8_idct_add_<opt>(uint8_t *dst, int16_t block[16], ptrdiff_t stride);
-;-----------------------------------------------------------------------------
-
-; calculate %1=mul_35468(%1)-mul_20091(%2); %2=mul_20091(%1)+mul_35468(%2)
-;           this macro assumes that m6/m7 have words for 20091/17734 loaded
-%macro VP8_MULTIPLY_SUMSUB 4
-    mova      %3, %1
-    mova      %4, %2
-    pmulhw    %3, m6 ;20091(1)
-    pmulhw    %4, m6 ;20091(2)
-    paddw     %3, %1
-    paddw     %4, %2
-    paddw     %1, %1
-    paddw     %2, %2
-    pmulhw    %1, m7 ;35468(1)
-    pmulhw    %2, m7 ;35468(2)
-    psubw     %1, %4
-    paddw     %2, %3
-%endmacro
-
-; calculate x0=%1+%3; x1=%1-%3
-;           x2=mul_35468(%2)-mul_20091(%4); x3=mul_20091(%2)+mul_35468(%4)
-;           %1=x0+x3 (tmp0); %2=x1+x2 (tmp1); %3=x1-x2 (tmp2); %4=x0-x3 (tmp3)
-;           %5/%6 are temporary registers
-;           we assume m6/m7 have constant words 20091/17734 loaded in them
-%macro VP8_IDCT_TRANSFORM4x4_1D 6
-    SUMSUB_BA         w, %3,  %1,  %5     ;t0, t1
-    VP8_MULTIPLY_SUMSUB m%2, m%4, m%5,m%6 ;t2, t3
-    SUMSUB_BA         w, %4,  %3,  %5     ;tmp0, tmp3
-    SUMSUB_BA         w, %2,  %1,  %5     ;tmp1, tmp2
-    SWAP                 %4,  %1
-    SWAP                 %4,  %3
-%endmacro
-
-%macro VP8_IDCT_ADD 0
-cglobal vp8_idct_add, 3, 3, 0, dst, block, stride
-    ; load block data
-    movq         m0, [blockq+ 0]
-    movq         m1, [blockq+ 8]
-    movq         m2, [blockq+16]
-    movq         m3, [blockq+24]
-    movq         m6, [pw_20091]
-    movq         m7, [pw_17734]
-%if cpuflag(sse)
-    xorps      xmm0, xmm0
-    movaps [blockq+ 0], xmm0
-    movaps [blockq+16], xmm0
-%else
-    pxor         m4, m4
-    movq [blockq+ 0], m4
-    movq [blockq+ 8], m4
-    movq [blockq+16], m4
-    movq [blockq+24], m4
-%endif
-
-    ; actual IDCT
-    VP8_IDCT_TRANSFORM4x4_1D 0, 1, 2, 3, 4, 5
-    TRANSPOSE4x4W            0, 1, 2, 3, 4
-    paddw        m0, [pw_4]
-    VP8_IDCT_TRANSFORM4x4_1D 0, 1, 2, 3, 4, 5
-    TRANSPOSE4x4W            0, 1, 2, 3, 4
-
-    ; store
-    pxor         m4, m4
-    DEFINE_ARGS dst1, dst2, stride
-    lea       dst2q, [dst1q+2*strideq]
-    STORE_DIFFx2 m0, m1, m6, m7, m4, 3, dst1q, strideq
-    STORE_DIFFx2 m2, m3, m6, m7, m4, 3, dst2q, strideq
-
-    RET
-%endmacro
-
-%if ARCH_X86_32
-INIT_MMX mmx
-VP8_IDCT_ADD
-%endif
-INIT_MMX sse
-VP8_IDCT_ADD
-
-;-----------------------------------------------------------------------------
-; void ff_vp8_luma_dc_wht(int16_t block[4][4][16], int16_t dc[16])
-;-----------------------------------------------------------------------------
-
-%macro SCATTER_WHT 3
-    movd dc1d, m%1
-    movd dc2d, m%2
-    mov [blockq+2*16*(0+%3)], dc1w
-    mov [blockq+2*16*(1+%3)], dc2w
-    shr  dc1d, 16
-    shr  dc2d, 16
-    psrlq m%1, 32
-    psrlq m%2, 32
-    mov [blockq+2*16*(4+%3)], dc1w
-    mov [blockq+2*16*(5+%3)], dc2w
-    movd dc1d, m%1
-    movd dc2d, m%2
-    mov [blockq+2*16*(8+%3)], dc1w
-    mov [blockq+2*16*(9+%3)], dc2w
-    shr  dc1d, 16
-    shr  dc2d, 16
-    mov [blockq+2*16*(12+%3)], dc1w
-    mov [blockq+2*16*(13+%3)], dc2w
-%endmacro
-
-%macro HADAMARD4_1D 4
-    SUMSUB_BADC w, %2, %1, %4, %3
-    SUMSUB_BADC w, %4, %2, %3, %1
-    SWAP %1, %4, %3
-%endmacro
-
-%macro VP8_DC_WHT 0
-cglobal vp8_luma_dc_wht, 2, 3, 0, block, dc1, dc2
-    movq          m0, [dc1q]
-    movq          m1, [dc1q+8]
-    movq          m2, [dc1q+16]
-    movq          m3, [dc1q+24]
-%if cpuflag(sse)
-    xorps      xmm0, xmm0
-    movaps [dc1q+ 0], xmm0
-    movaps [dc1q+16], xmm0
-%else
-    pxor         m4, m4
-    movq  [dc1q+ 0], m4
-    movq  [dc1q+ 8], m4
-    movq  [dc1q+16], m4
-    movq  [dc1q+24], m4
-%endif
-    HADAMARD4_1D  0, 1, 2, 3
-    TRANSPOSE4x4W 0, 1, 2, 3, 4
-    paddw         m0, [pw_3]
-    HADAMARD4_1D  0, 1, 2, 3
-    psraw         m0, 3
-    psraw         m1, 3
-    psraw         m2, 3
-    psraw         m3, 3
-    SCATTER_WHT   0, 1, 0
-    SCATTER_WHT   2, 3, 2
-    RET
-%endmacro
-
-%if ARCH_X86_32
-INIT_MMX mmx
-VP8_DC_WHT
-%endif
-INIT_MMX sse
-VP8_DC_WHT
diff -uparN ffmpeg-4.1/libavcodec/x86/vp8dsp_loopfilter.asm ffmpeg-y/libavcodec/x86/vp8dsp_loopfilter.asm
--- ffmpeg-4.1/libavcodec/x86/vp8dsp_loopfilter.asm	2018-11-02 02:34:25.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/vp8dsp_loopfilter.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,1584 +0,0 @@
-;******************************************************************************
-;* VP8 MMXEXT optimizations
-;* Copyright (c) 2010 Ronald S. Bultje <rsbultje@gmail.com>
-;* Copyright (c) 2010 Fiona Glaser <fiona@x264.com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-
-pw_27:    times 8 dw 27
-pw_63:    times 8 dw 63
-
-pb_4:     times 16 db 4
-pb_F8:    times 16 db 0xF8
-pb_FE:    times 16 db 0xFE
-pb_27_63: times 8 db 27, 63
-pb_18_63: times 8 db 18, 63
-pb_9_63:  times 8 db  9, 63
-
-cextern pb_1
-cextern pb_3
-cextern pw_9
-cextern pw_18
-cextern pb_80
-
-SECTION .text
-
-;-----------------------------------------------------------------------------
-; void ff_vp8_h/v_loop_filter_simple_<opt>(uint8_t *dst, ptrdiff_t stride, int flim);
-;-----------------------------------------------------------------------------
-
-; macro called with 7 mm register indexes as argument, and 4 regular registers
-;
-; first 4 mm registers will carry the transposed pixel data
-; the other three are scratchspace (one would be sufficient, but this allows
-; for more spreading/pipelining and thus faster execution on OOE CPUs)
-;
-; first two regular registers are buf+4*stride and buf+5*stride
-; third is -stride, fourth is +stride
-%macro READ_8x4_INTERLEAVED 11
-    ; interleave 8 (A-H) rows of 4 pixels each
-    movd          m%1, [%8+%10*4]   ; A0-3
-    movd          m%5, [%9+%10*4]   ; B0-3
-    movd          m%2, [%8+%10*2]   ; C0-3
-    movd          m%6, [%8+%10]     ; D0-3
-    movd          m%3, [%8]         ; E0-3
-    movd          m%7, [%9]         ; F0-3
-    movd          m%4, [%9+%11]     ; G0-3
-    punpcklbw     m%1, m%5          ; A/B interleaved
-    movd          m%5, [%9+%11*2]   ; H0-3
-    punpcklbw     m%2, m%6          ; C/D interleaved
-    punpcklbw     m%3, m%7          ; E/F interleaved
-    punpcklbw     m%4, m%5          ; G/H interleaved
-%endmacro
-
-; macro called with 7 mm register indexes as argument, and 5 regular registers
-; first 11 mean the same as READ_8x4_TRANSPOSED above
-; fifth regular register is scratchspace to reach the bottom 8 rows, it
-; will be set to second regular register + 8*stride at the end
-%macro READ_16x4_INTERLEAVED 12
-    ; transpose 16 (A-P) rows of 4 pixels each
-    lea           %12, [r0+8*r2]
-
-    ; read (and interleave) those addressable by %8 (=r0), A/C/D/E/I/K/L/M
-    movd          m%1, [%8+%10*4]   ; A0-3
-    movd          m%3, [%12+%10*4]  ; I0-3
-    movd          m%2, [%8+%10*2]   ; C0-3
-    movd          m%4, [%12+%10*2]  ; K0-3
-    movd          m%6, [%8+%10]     ; D0-3
-    movd          m%5, [%12+%10]    ; L0-3
-    movd          m%7, [%12]        ; M0-3
-    add           %12, %11
-    punpcklbw     m%1, m%3          ; A/I
-    movd          m%3, [%8]         ; E0-3
-    punpcklbw     m%2, m%4          ; C/K
-    punpcklbw     m%6, m%5          ; D/L
-    punpcklbw     m%3, m%7          ; E/M
-    punpcklbw     m%2, m%6          ; C/D/K/L interleaved
-
-    ; read (and interleave) those addressable by %9 (=r4), B/F/G/H/J/N/O/P
-    movd         m%5, [%9+%10*4]   ; B0-3
-    movd         m%4, [%12+%10*4]  ; J0-3
-    movd         m%7, [%9]         ; F0-3
-    movd         m%6, [%12]        ; N0-3
-    punpcklbw    m%5, m%4          ; B/J
-    punpcklbw    m%7, m%6          ; F/N
-    punpcklbw    m%1, m%5          ; A/B/I/J interleaved
-    punpcklbw    m%3, m%7          ; E/F/M/N interleaved
-    movd         m%4, [%9+%11]     ; G0-3
-    movd         m%6, [%12+%11]    ; O0-3
-    movd         m%5, [%9+%11*2]   ; H0-3
-    movd         m%7, [%12+%11*2]  ; P0-3
-    punpcklbw    m%4, m%6          ; G/O
-    punpcklbw    m%5, m%7          ; H/P
-    punpcklbw    m%4, m%5          ; G/H/O/P interleaved
-%endmacro
-
-; write 4 mm registers of 2 dwords each
-; first four arguments are mm register indexes containing source data
-; last four are registers containing buf+4*stride, buf+5*stride,
-; -stride and +stride
-%macro WRITE_4x2D 8
-    ; write out (2 dwords per register)
-    movd    [%5+%7*4], m%1
-    movd    [%5+%7*2], m%2
-    movd         [%5], m%3
-    movd      [%6+%8], m%4
-    punpckhdq     m%1, m%1
-    punpckhdq     m%2, m%2
-    punpckhdq     m%3, m%3
-    punpckhdq     m%4, m%4
-    movd    [%6+%7*4], m%1
-    movd      [%5+%7], m%2
-    movd         [%6], m%3
-    movd    [%6+%8*2], m%4
-%endmacro
-
-; write 4 xmm registers of 4 dwords each
-; arguments same as WRITE_2x4D, but with an extra register, so that the 5 regular
-; registers contain buf+4*stride, buf+5*stride, buf+12*stride, -stride and +stride
-; we add 1*stride to the third regular registry in the process
-; the 10th argument is 16 if it's a Y filter (i.e. all regular registers cover the
-; same memory region), or 8 if they cover two separate buffers (third one points to
-; a different memory region than the first two), allowing for more optimal code for
-; the 16-width case
-%macro WRITE_4x4D 10
-    ; write out (4 dwords per register), start with dwords zero
-    movd    [%5+%8*4], m%1
-    movd         [%5], m%2
-    movd    [%7+%8*4], m%3
-    movd         [%7], m%4
-
-    ; store dwords 1
-    psrldq        m%1, 4
-    psrldq        m%2, 4
-    psrldq        m%3, 4
-    psrldq        m%4, 4
-    movd    [%6+%8*4], m%1
-    movd         [%6], m%2
-%if %10 == 16
-    movd    [%6+%9*4], m%3
-%endif
-    movd      [%7+%9], m%4
-
-    ; write dwords 2
-    psrldq        m%1, 4
-    psrldq        m%2, 4
-%if %10 == 8
-    movd    [%5+%8*2], m%1
-    movd          %5d, m%3
-%endif
-    psrldq        m%3, 4
-    psrldq        m%4, 4
-%if %10 == 16
-    movd    [%5+%8*2], m%1
-%endif
-    movd      [%6+%9], m%2
-    movd    [%7+%8*2], m%3
-    movd    [%7+%9*2], m%4
-    add            %7, %9
-
-    ; store dwords 3
-    psrldq        m%1, 4
-    psrldq        m%2, 4
-    psrldq        m%3, 4
-    psrldq        m%4, 4
-%if %10 == 8
-    mov     [%7+%8*4], %5d
-    movd    [%6+%8*2], m%1
-%else
-    movd      [%5+%8], m%1
-%endif
-    movd    [%6+%9*2], m%2
-    movd    [%7+%8*2], m%3
-    movd    [%7+%9*2], m%4
-%endmacro
-
-; write 4 or 8 words in the mmx/xmm registers as 8 lines
-; 1 and 2 are the registers to write, this can be the same (for SSE2)
-; for pre-SSE4:
-; 3 is a general-purpose register that we will clobber
-; for SSE4:
-; 3 is a pointer to the destination's 5th line
-; 4 is a pointer to the destination's 4th line
-; 5/6 is -stride and +stride
-%macro WRITE_2x4W 6
-    movd            %3d, %1
-    punpckhdq        %1, %1
-    mov       [%4+%5*4], %3w
-    shr              %3, 16
-    add              %4, %6
-    mov       [%4+%5*4], %3w
-
-    movd            %3d, %1
-    add              %4, %5
-    mov       [%4+%5*2], %3w
-    shr              %3, 16
-    mov       [%4+%5  ], %3w
-
-    movd            %3d, %2
-    punpckhdq        %2, %2
-    mov       [%4     ], %3w
-    shr              %3, 16
-    mov       [%4+%6  ], %3w
-
-    movd            %3d, %2
-    add              %4, %6
-    mov       [%4+%6  ], %3w
-    shr              %3, 16
-    mov       [%4+%6*2], %3w
-    add              %4, %5
-%endmacro
-
-%macro WRITE_8W 5
-%if cpuflag(sse4)
-    pextrw    [%3+%4*4], %1, 0
-    pextrw    [%2+%4*4], %1, 1
-    pextrw    [%3+%4*2], %1, 2
-    pextrw    [%3+%4  ], %1, 3
-    pextrw    [%3     ], %1, 4
-    pextrw    [%2     ], %1, 5
-    pextrw    [%2+%5  ], %1, 6
-    pextrw    [%2+%5*2], %1, 7
-%else
-    movd            %2d, %1
-    psrldq           %1, 4
-    mov       [%3+%4*4], %2w
-    shr              %2, 16
-    add              %3, %5
-    mov       [%3+%4*4], %2w
-
-    movd            %2d, %1
-    psrldq           %1, 4
-    add              %3, %4
-    mov       [%3+%4*2], %2w
-    shr              %2, 16
-    mov       [%3+%4  ], %2w
-
-    movd            %2d, %1
-    psrldq           %1, 4
-    mov       [%3     ], %2w
-    shr              %2, 16
-    mov       [%3+%5  ], %2w
-
-    movd            %2d, %1
-    add              %3, %5
-    mov       [%3+%5  ], %2w
-    shr              %2, 16
-    mov       [%3+%5*2], %2w
-%endif
-%endmacro
-
-%macro SIMPLE_LOOPFILTER 2
-cglobal vp8_%1_loop_filter_simple, 3, %2, 8, dst, stride, flim, cntr
-%if mmsize == 8 ; mmx/mmxext
-    mov         cntrq, 2
-%endif
-%if cpuflag(ssse3)
-    pxor           m0, m0
-%endif
-    SPLATB_REG     m7, flim, m0     ; splat "flim" into register
-
-    ; set up indexes to address 4 rows
-%if mmsize == 8
-    DEFINE_ARGS dst1, mstride, stride, cntr, dst2
-%else
-    DEFINE_ARGS dst1, mstride, stride, dst3, dst2
-%endif
-    mov       strideq, mstrideq
-    neg      mstrideq
-%ifidn %1, h
-    lea         dst1q, [dst1q+4*strideq-2]
-%endif
-
-%if mmsize == 8 ; mmx / mmxext
-.next8px:
-%endif
-%ifidn %1, v
-    ; read 4 half/full rows of pixels
-    mova           m0, [dst1q+mstrideq*2]    ; p1
-    mova           m1, [dst1q+mstrideq]      ; p0
-    mova           m2, [dst1q]               ; q0
-    mova           m3, [dst1q+ strideq]      ; q1
-%else ; h
-    lea         dst2q, [dst1q+ strideq]
-
-%if mmsize == 8 ; mmx/mmxext
-    READ_8x4_INTERLEAVED  0, 1, 2, 3, 4, 5, 6, dst1q, dst2q, mstrideq, strideq
-%else ; sse2
-    READ_16x4_INTERLEAVED 0, 1, 2, 3, 4, 5, 6, dst1q, dst2q, mstrideq, strideq, dst3q
-%endif
-    TRANSPOSE4x4W         0, 1, 2, 3, 4
-%endif
-
-    ; simple_limit
-    mova           m5, m2           ; m5=backup of q0
-    mova           m6, m1           ; m6=backup of p0
-    psubusb        m1, m2           ; p0-q0
-    psubusb        m2, m6           ; q0-p0
-    por            m1, m2           ; FFABS(p0-q0)
-    paddusb        m1, m1           ; m1=FFABS(p0-q0)*2
-
-    mova           m4, m3
-    mova           m2, m0
-    psubusb        m3, m0           ; q1-p1
-    psubusb        m0, m4           ; p1-q1
-    por            m3, m0           ; FFABS(p1-q1)
-    mova           m0, [pb_80]
-    pxor           m2, m0
-    pxor           m4, m0
-    psubsb         m2, m4           ; m2=p1-q1 (signed) backup for below
-    pand           m3, [pb_FE]
-    psrlq          m3, 1            ; m3=FFABS(p1-q1)/2, this can be used signed
-    paddusb        m3, m1
-    psubusb        m3, m7
-    pxor           m1, m1
-    pcmpeqb        m3, m1           ; abs(p0-q0)*2+abs(p1-q1)/2<=flim mask(0xff/0x0)
-
-    ; filter_common (use m2/p1-q1, m4=q0, m6=p0, m5/q0-p0 and m3/mask)
-    mova           m4, m5
-    pxor           m5, m0
-    pxor           m0, m6
-    psubsb         m5, m0           ; q0-p0 (signed)
-    paddsb         m2, m5
-    paddsb         m2, m5
-    paddsb         m2, m5           ; a=(p1-q1) + 3*(q0-p0)
-    pand           m2, m3           ; apply filter mask (m3)
-
-    mova           m3, [pb_F8]
-    mova           m1, m2
-    paddsb         m2, [pb_4]       ; f1<<3=a+4
-    paddsb         m1, [pb_3]       ; f2<<3=a+3
-    pand           m2, m3
-    pand           m1, m3           ; cache f2<<3
-
-    pxor           m0, m0
-    pxor           m3, m3
-    pcmpgtb        m0, m2           ; which values are <0?
-    psubb          m3, m2           ; -f1<<3
-    psrlq          m2, 3            ; +f1
-    psrlq          m3, 3            ; -f1
-    pand           m3, m0
-    pandn          m0, m2
-    psubusb        m4, m0
-    paddusb        m4, m3           ; q0-f1
-
-    pxor           m0, m0
-    pxor           m3, m3
-    pcmpgtb        m0, m1           ; which values are <0?
-    psubb          m3, m1           ; -f2<<3
-    psrlq          m1, 3            ; +f2
-    psrlq          m3, 3            ; -f2
-    pand           m3, m0
-    pandn          m0, m1
-    paddusb        m6, m0
-    psubusb        m6, m3           ; p0+f2
-
-    ; store
-%ifidn %1, v
-    mova      [dst1q], m4
-    mova [dst1q+mstrideq], m6
-%else ; h
-    inc        dst1q
-    SBUTTERFLY    bw, 6, 4, 0
-
-%if mmsize == 16 ; sse2
-%if cpuflag(sse4)
-    inc         dst2q
-%endif
-    WRITE_8W       m6, dst2q, dst1q, mstrideq, strideq
-    lea         dst2q, [dst3q+mstrideq+1]
-%if cpuflag(sse4)
-    inc         dst3q
-%endif
-    WRITE_8W       m4, dst3q, dst2q, mstrideq, strideq
-%else ; mmx/mmxext
-    WRITE_2x4W     m6, m4, dst2q, dst1q, mstrideq, strideq
-%endif
-%endif
-
-%if mmsize == 8 ; mmx/mmxext
-    ; next 8 pixels
-%ifidn %1, v
-    add         dst1q, 8            ; advance 8 cols = pixels
-%else ; h
-    lea         dst1q, [dst1q+strideq*8-1]  ; advance 8 rows = lines
-%endif
-    dec         cntrq
-    jg .next8px
-    REP_RET
-%else ; sse2
-    RET
-%endif
-%endmacro
-
-%if ARCH_X86_32
-INIT_MMX mmx
-SIMPLE_LOOPFILTER v, 4
-SIMPLE_LOOPFILTER h, 5
-INIT_MMX mmxext
-SIMPLE_LOOPFILTER v, 4
-SIMPLE_LOOPFILTER h, 5
-%endif
-
-INIT_XMM sse2
-SIMPLE_LOOPFILTER v, 3
-SIMPLE_LOOPFILTER h, 5
-INIT_XMM ssse3
-SIMPLE_LOOPFILTER v, 3
-SIMPLE_LOOPFILTER h, 5
-INIT_XMM sse4
-SIMPLE_LOOPFILTER h, 5
-
-;-----------------------------------------------------------------------------
-; void ff_vp8_h/v_loop_filter<size>_inner_<opt>(uint8_t *dst, [uint8_t *v,] ptrdiff_t stride,
-;                                               int flimE, int flimI, int hev_thr);
-;-----------------------------------------------------------------------------
-
-%macro INNER_LOOPFILTER 2
-%define stack_size 0
-%ifndef m8   ; stack layout: [0]=E, [1]=I, [2]=hev_thr
-%ifidn %1, v ;               [3]=hev() result
-%define stack_size mmsize * -4
-%else ; h    ; extra storage space for transposes
-%define stack_size mmsize * -5
-%endif
-%endif
-
-%if %2 == 8 ; chroma
-cglobal vp8_%1_loop_filter8uv_inner, 6, 6, 13, stack_size, dst, dst8, stride, flimE, flimI, hevthr
-%else ; luma
-cglobal vp8_%1_loop_filter16y_inner, 5, 5, 13, stack_size, dst, stride, flimE, flimI, hevthr
-%endif
-
-%if cpuflag(ssse3)
-    pxor             m7, m7
-%endif
-
-%ifndef m8
-    ; splat function arguments
-    SPLATB_REG       m0, flimEq, m7   ; E
-    SPLATB_REG       m1, flimIq, m7   ; I
-    SPLATB_REG       m2, hevthrq, m7  ; hev_thresh
-
-%define m_flimE    [rsp]
-%define m_flimI    [rsp+mmsize]
-%define m_hevthr   [rsp+mmsize*2]
-%define m_maskres  [rsp+mmsize*3]
-%define m_p0backup [rsp+mmsize*3]
-%define m_q0backup [rsp+mmsize*4]
-
-    mova        m_flimE, m0
-    mova        m_flimI, m1
-    mova       m_hevthr, m2
-%else
-%define m_flimE    m9
-%define m_flimI    m10
-%define m_hevthr   m11
-%define m_maskres  m12
-%define m_p0backup m12
-%define m_q0backup m8
-
-    ; splat function arguments
-    SPLATB_REG  m_flimE, flimEq, m7   ; E
-    SPLATB_REG  m_flimI, flimIq, m7   ; I
-    SPLATB_REG m_hevthr, hevthrq, m7  ; hev_thresh
-%endif
-
-%if %2 == 8 ; chroma
-    DEFINE_ARGS dst1, dst8, mstride, stride, dst2
-%elif mmsize == 8
-    DEFINE_ARGS dst1, mstride, stride, dst2, cntr
-    mov           cntrq, 2
-%else
-    DEFINE_ARGS dst1, mstride, stride, dst2, dst8
-%endif
-    mov         strideq, mstrideq
-    neg        mstrideq
-%ifidn %1, h
-    lea           dst1q, [dst1q+strideq*4-4]
-%if %2 == 8 ; chroma
-    lea           dst8q, [dst8q+strideq*4-4]
-%endif
-%endif
-
-%if mmsize == 8
-.next8px:
-%endif
-    ; read
-    lea           dst2q, [dst1q+strideq]
-%ifidn %1, v
-%if %2 == 8 && mmsize == 16
-%define movrow movh
-%else
-%define movrow mova
-%endif
-    movrow           m0, [dst1q+mstrideq*4] ; p3
-    movrow           m1, [dst2q+mstrideq*4] ; p2
-    movrow           m2, [dst1q+mstrideq*2] ; p1
-    movrow           m5, [dst2q]            ; q1
-    movrow           m6, [dst2q+ strideq*1] ; q2
-    movrow           m7, [dst2q+ strideq*2] ; q3
-%if mmsize == 16 && %2 == 8
-    movhps           m0, [dst8q+mstrideq*4]
-    movhps           m2, [dst8q+mstrideq*2]
-    add           dst8q, strideq
-    movhps           m1, [dst8q+mstrideq*4]
-    movhps           m5, [dst8q]
-    movhps           m6, [dst8q+ strideq  ]
-    movhps           m7, [dst8q+ strideq*2]
-    add           dst8q, mstrideq
-%endif
-%elif mmsize == 8 ; mmx/mmxext (h)
-    ; read 8 rows of 8px each
-    movu             m0, [dst1q+mstrideq*4]
-    movu             m1, [dst2q+mstrideq*4]
-    movu             m2, [dst1q+mstrideq*2]
-    movu             m3, [dst1q+mstrideq  ]
-    movu             m4, [dst1q]
-    movu             m5, [dst2q]
-    movu             m6, [dst2q+ strideq  ]
-
-    ; 8x8 transpose
-    TRANSPOSE4x4B     0, 1, 2, 3, 7
-    mova     m_q0backup, m1
-    movu             m7, [dst2q+ strideq*2]
-    TRANSPOSE4x4B     4, 5, 6, 7, 1
-    SBUTTERFLY       dq, 0, 4, 1     ; p3/p2
-    SBUTTERFLY       dq, 2, 6, 1     ; q0/q1
-    SBUTTERFLY       dq, 3, 7, 1     ; q2/q3
-    mova             m1, m_q0backup
-    mova     m_q0backup, m2          ; store q0
-    SBUTTERFLY       dq, 1, 5, 2     ; p1/p0
-    mova     m_p0backup, m5          ; store p0
-    SWAP              1, 4
-    SWAP              2, 4
-    SWAP              6, 3
-    SWAP              5, 3
-%else ; sse2 (h)
-%if %2 == 16
-    lea           dst8q, [dst1q+ strideq*8]
-%endif
-
-    ; read 16 rows of 8px each, interleave
-    movh             m0, [dst1q+mstrideq*4]
-    movh             m1, [dst8q+mstrideq*4]
-    movh             m2, [dst1q+mstrideq*2]
-    movh             m5, [dst8q+mstrideq*2]
-    movh             m3, [dst1q+mstrideq  ]
-    movh             m6, [dst8q+mstrideq  ]
-    movh             m4, [dst1q]
-    movh             m7, [dst8q]
-    punpcklbw        m0, m1          ; A/I
-    punpcklbw        m2, m5          ; C/K
-    punpcklbw        m3, m6          ; D/L
-    punpcklbw        m4, m7          ; E/M
-
-    add           dst8q, strideq
-    movh             m1, [dst2q+mstrideq*4]
-    movh             m6, [dst8q+mstrideq*4]
-    movh             m5, [dst2q]
-    movh             m7, [dst8q]
-    punpcklbw        m1, m6          ; B/J
-    punpcklbw        m5, m7          ; F/N
-    movh             m6, [dst2q+ strideq  ]
-    movh             m7, [dst8q+ strideq  ]
-    punpcklbw        m6, m7          ; G/O
-
-    ; 8x16 transpose
-    TRANSPOSE4x4B     0, 1, 2, 3, 7
-%ifdef m8
-    SWAP              1, 8
-%else
-    mova     m_q0backup, m1
-%endif
-    movh             m7, [dst2q+ strideq*2]
-    movh             m1, [dst8q+ strideq*2]
-    punpcklbw        m7, m1          ; H/P
-    TRANSPOSE4x4B     4, 5, 6, 7, 1
-    SBUTTERFLY       dq, 0, 4, 1     ; p3/p2
-    SBUTTERFLY       dq, 2, 6, 1     ; q0/q1
-    SBUTTERFLY       dq, 3, 7, 1     ; q2/q3
-%ifdef m8
-    SWAP              1, 8
-    SWAP              2, 8
-%else
-    mova             m1, m_q0backup
-    mova     m_q0backup, m2          ; store q0
-%endif
-    SBUTTERFLY       dq, 1, 5, 2     ; p1/p0
-%ifdef m12
-    SWAP              5, 12
-%else
-    mova     m_p0backup, m5          ; store p0
-%endif
-    SWAP              1, 4
-    SWAP              2, 4
-    SWAP              6, 3
-    SWAP              5, 3
-%endif
-
-    ; normal_limit for p3-p2, p2-p1, q3-q2 and q2-q1
-    mova             m4, m1
-    SWAP              4, 1
-    psubusb          m4, m0          ; p2-p3
-    psubusb          m0, m1          ; p3-p2
-    por              m0, m4          ; abs(p3-p2)
-
-    mova             m4, m2
-    SWAP              4, 2
-    psubusb          m4, m1          ; p1-p2
-    psubusb          m1, m2          ; p2-p1
-    por              m1, m4          ; abs(p2-p1)
-
-    mova             m4, m6
-    SWAP              4, 6
-    psubusb          m4, m7          ; q2-q3
-    psubusb          m7, m6          ; q3-q2
-    por              m7, m4          ; abs(q3-q2)
-
-    mova             m4, m5
-    SWAP              4, 5
-    psubusb          m4, m6          ; q1-q2
-    psubusb          m6, m5          ; q2-q1
-    por              m6, m4          ; abs(q2-q1)
-
-%if notcpuflag(mmxext)
-    mova             m4, m_flimI
-    pxor             m3, m3
-    psubusb          m0, m4
-    psubusb          m1, m4
-    psubusb          m7, m4
-    psubusb          m6, m4
-    pcmpeqb          m0, m3          ; abs(p3-p2) <= I
-    pcmpeqb          m1, m3          ; abs(p2-p1) <= I
-    pcmpeqb          m7, m3          ; abs(q3-q2) <= I
-    pcmpeqb          m6, m3          ; abs(q2-q1) <= I
-    pand             m0, m1
-    pand             m7, m6
-    pand             m0, m7
-%else ; mmxext/sse2
-    pmaxub           m0, m1
-    pmaxub           m6, m7
-    pmaxub           m0, m6
-%endif
-
-    ; normal_limit and high_edge_variance for p1-p0, q1-q0
-    SWAP              7, 3           ; now m7 is zero
-%ifidn %1, v
-    movrow           m3, [dst1q+mstrideq  ] ; p0
-%if mmsize == 16 && %2 == 8
-    movhps           m3, [dst8q+mstrideq  ]
-%endif
-%elifdef m12
-    SWAP              3, 12
-%else
-    mova             m3, m_p0backup
-%endif
-
-    mova             m1, m2
-    SWAP              1, 2
-    mova             m6, m3
-    SWAP              3, 6
-    psubusb          m1, m3          ; p1-p0
-    psubusb          m6, m2          ; p0-p1
-    por              m1, m6          ; abs(p1-p0)
-%if notcpuflag(mmxext)
-    mova             m6, m1
-    psubusb          m1, m4
-    psubusb          m6, m_hevthr
-    pcmpeqb          m1, m7          ; abs(p1-p0) <= I
-    pcmpeqb          m6, m7          ; abs(p1-p0) <= hev_thresh
-    pand             m0, m1
-    mova      m_maskres, m6
-%else ; mmxext/sse2
-    pmaxub           m0, m1          ; max_I
-    SWAP              1, 4           ; max_hev_thresh
-%endif
-
-    SWAP              6, 4           ; now m6 is I
-%ifidn %1, v
-    movrow           m4, [dst1q]     ; q0
-%if mmsize == 16 && %2 == 8
-    movhps           m4, [dst8q]
-%endif
-%elifdef m8
-    SWAP              4, 8
-%else
-    mova             m4, m_q0backup
-%endif
-    mova             m1, m4
-    SWAP              1, 4
-    mova             m7, m5
-    SWAP              7, 5
-    psubusb          m1, m5          ; q0-q1
-    psubusb          m7, m4          ; q1-q0
-    por              m1, m7          ; abs(q1-q0)
-%if notcpuflag(mmxext)
-    mova             m7, m1
-    psubusb          m1, m6
-    psubusb          m7, m_hevthr
-    pxor             m6, m6
-    pcmpeqb          m1, m6          ; abs(q1-q0) <= I
-    pcmpeqb          m7, m6          ; abs(q1-q0) <= hev_thresh
-    mova             m6, m_maskres
-    pand             m0, m1          ; abs([pq][321]-[pq][210]) <= I
-    pand             m6, m7
-%else ; mmxext/sse2
-    pxor             m7, m7
-    pmaxub           m0, m1
-    pmaxub           m6, m1
-    psubusb          m0, m_flimI
-    psubusb          m6, m_hevthr
-    pcmpeqb          m0, m7          ; max(abs(..)) <= I
-    pcmpeqb          m6, m7          ; !(max(abs..) > thresh)
-%endif
-%ifdef m12
-    SWAP              6, 12
-%else
-    mova      m_maskres, m6          ; !(abs(p1-p0) > hev_t || abs(q1-q0) > hev_t)
-%endif
-
-    ; simple_limit
-    mova             m1, m3
-    SWAP              1, 3
-    mova             m6, m4          ; keep copies of p0/q0 around for later use
-    SWAP              6, 4
-    psubusb          m1, m4          ; p0-q0
-    psubusb          m6, m3          ; q0-p0
-    por              m1, m6          ; abs(q0-p0)
-    paddusb          m1, m1          ; m1=2*abs(q0-p0)
-
-    mova             m7, m2
-    SWAP              7, 2
-    mova             m6, m5
-    SWAP              6, 5
-    psubusb          m7, m5          ; p1-q1
-    psubusb          m6, m2          ; q1-p1
-    por              m7, m6          ; abs(q1-p1)
-    pxor             m6, m6
-    pand             m7, [pb_FE]
-    psrlq            m7, 1           ; abs(q1-p1)/2
-    paddusb          m7, m1          ; abs(q0-p0)*2+abs(q1-p1)/2
-    psubusb          m7, m_flimE
-    pcmpeqb          m7, m6          ; abs(q0-p0)*2+abs(q1-p1)/2 <= E
-    pand             m0, m7          ; normal_limit result
-
-    ; filter_common; at this point, m2-m5=p1-q1 and m0 is filter_mask
-%ifdef m8 ; x86-64 && sse2
-    mova             m8, [pb_80]
-%define m_pb_80 m8
-%else ; x86-32 or mmx/mmxext
-%define m_pb_80 [pb_80]
-%endif
-    mova             m1, m4
-    mova             m7, m3
-    pxor             m1, m_pb_80
-    pxor             m7, m_pb_80
-    psubsb           m1, m7          ; (signed) q0-p0
-    mova             m6, m2
-    mova             m7, m5
-    pxor             m6, m_pb_80
-    pxor             m7, m_pb_80
-    psubsb           m6, m7          ; (signed) p1-q1
-    mova             m7, m_maskres
-    pandn            m7, m6
-    paddsb           m7, m1
-    paddsb           m7, m1
-    paddsb           m7, m1          ; 3*(q0-p0)+is4tap?(p1-q1)
-
-    pand             m7, m0
-    mova             m1, [pb_F8]
-    mova             m6, m7
-    paddsb           m7, [pb_3]
-    paddsb           m6, [pb_4]
-    pand             m7, m1
-    pand             m6, m1
-
-    pxor             m1, m1
-    pxor             m0, m0
-    pcmpgtb          m1, m7
-    psubb            m0, m7
-    psrlq            m7, 3           ; +f2
-    psrlq            m0, 3           ; -f2
-    pand             m0, m1
-    pandn            m1, m7
-    psubusb          m3, m0
-    paddusb          m3, m1          ; p0+f2
-
-    pxor             m1, m1
-    pxor             m0, m0
-    pcmpgtb          m0, m6
-    psubb            m1, m6
-    psrlq            m6, 3           ; +f1
-    psrlq            m1, 3           ; -f1
-    pand             m1, m0
-    pandn            m0, m6
-    psubusb          m4, m0
-    paddusb          m4, m1          ; q0-f1
-
-%ifdef m12
-    SWAP              6, 12
-%else
-    mova             m6, m_maskres
-%endif
-%if notcpuflag(mmxext)
-    mova             m7, [pb_1]
-%else ; mmxext/sse2
-    pxor             m7, m7
-%endif
-    pand             m0, m6
-    pand             m1, m6
-%if notcpuflag(mmxext)
-    paddusb          m0, m7
-    pand             m1, [pb_FE]
-    pandn            m7, m0
-    psrlq            m1, 1
-    psrlq            m7, 1
-    SWAP              0, 7
-%else ; mmxext/sse2
-    psubusb          m1, [pb_1]
-    pavgb            m0, m7          ; a
-    pavgb            m1, m7          ; -a
-%endif
-    psubusb          m5, m0
-    psubusb          m2, m1
-    paddusb          m5, m1          ; q1-a
-    paddusb          m2, m0          ; p1+a
-
-    ; store
-%ifidn %1, v
-    movrow [dst1q+mstrideq*2], m2
-    movrow [dst1q+mstrideq  ], m3
-    movrow      [dst1q], m4
-    movrow [dst1q+ strideq  ], m5
-%if mmsize == 16 && %2 == 8
-    movhps [dst8q+mstrideq*2], m2
-    movhps [dst8q+mstrideq  ], m3
-    movhps      [dst8q], m4
-    movhps [dst8q+ strideq  ], m5
-%endif
-%else ; h
-    add           dst1q, 2
-    add           dst2q, 2
-
-    ; 4x8/16 transpose
-    TRANSPOSE4x4B     2, 3, 4, 5, 6
-
-%if mmsize == 8 ; mmx/mmxext (h)
-    WRITE_4x2D        2, 3, 4, 5, dst1q, dst2q, mstrideq, strideq
-%else ; sse2 (h)
-    lea           dst8q, [dst8q+mstrideq  +2]
-    WRITE_4x4D        2, 3, 4, 5, dst1q, dst2q, dst8q, mstrideq, strideq, %2
-%endif
-%endif
-
-%if mmsize == 8
-%if %2 == 8 ; chroma
-%ifidn %1, h
-    sub           dst1q, 2
-%endif
-    cmp           dst1q, dst8q
-    mov           dst1q, dst8q
-    jnz .next8px
-%else
-%ifidn %1, h
-    lea           dst1q, [dst1q+ strideq*8-2]
-%else ; v
-    add           dst1q, 8
-%endif
-    dec           cntrq
-    jg .next8px
-%endif
-    REP_RET
-%else ; mmsize == 16
-    RET
-%endif
-%endmacro
-
-%if ARCH_X86_32
-INIT_MMX mmx
-INNER_LOOPFILTER v, 16
-INNER_LOOPFILTER h, 16
-INNER_LOOPFILTER v,  8
-INNER_LOOPFILTER h,  8
-
-INIT_MMX mmxext
-INNER_LOOPFILTER v, 16
-INNER_LOOPFILTER h, 16
-INNER_LOOPFILTER v,  8
-INNER_LOOPFILTER h,  8
-%endif
-
-INIT_XMM sse2
-INNER_LOOPFILTER v, 16
-INNER_LOOPFILTER h, 16
-INNER_LOOPFILTER v,  8
-INNER_LOOPFILTER h,  8
-
-INIT_XMM ssse3
-INNER_LOOPFILTER v, 16
-INNER_LOOPFILTER h, 16
-INNER_LOOPFILTER v,  8
-INNER_LOOPFILTER h,  8
-
-;-----------------------------------------------------------------------------
-; void ff_vp8_h/v_loop_filter<size>_mbedge_<opt>(uint8_t *dst, [uint8_t *v,] ptrdiff_t stride,
-;                                                int flimE, int flimI, int hev_thr);
-;-----------------------------------------------------------------------------
-
-%macro MBEDGE_LOOPFILTER 2
-%define stack_size 0
-%ifndef m8       ; stack layout: [0]=E, [1]=I, [2]=hev_thr
-%if mmsize == 16 ;               [3]=hev() result
-                 ;               [4]=filter tmp result
-                 ;               [5]/[6] = p2/q2 backup
-                 ;               [7]=lim_res sign result
-%define stack_size mmsize * -7
-%else ; 8        ; extra storage space for transposes
-%define stack_size mmsize * -8
-%endif
-%endif
-
-%if %2 == 8 ; chroma
-cglobal vp8_%1_loop_filter8uv_mbedge, 6, 6, 15, stack_size, dst1, dst8, stride, flimE, flimI, hevthr
-%else ; luma
-cglobal vp8_%1_loop_filter16y_mbedge, 5, 5, 15, stack_size, dst1, stride, flimE, flimI, hevthr
-%endif
-
-%if cpuflag(ssse3)
-    pxor             m7, m7
-%endif
-
-%ifndef m8
-    ; splat function arguments
-    SPLATB_REG       m0, flimEq, m7   ; E
-    SPLATB_REG       m1, flimIq, m7   ; I
-    SPLATB_REG       m2, hevthrq, m7  ; hev_thresh
-
-%define m_flimE    [rsp]
-%define m_flimI    [rsp+mmsize]
-%define m_hevthr   [rsp+mmsize*2]
-%define m_maskres  [rsp+mmsize*3]
-%define m_limres   [rsp+mmsize*4]
-%define m_p0backup [rsp+mmsize*3]
-%define m_q0backup [rsp+mmsize*4]
-%define m_p2backup [rsp+mmsize*5]
-%define m_q2backup [rsp+mmsize*6]
-%if mmsize == 16
-%define m_limsign  [rsp]
-%else
-%define m_limsign  [rsp+mmsize*7]
-%endif
-
-    mova        m_flimE, m0
-    mova        m_flimI, m1
-    mova       m_hevthr, m2
-%else ; sse2 on x86-64
-%define m_flimE    m9
-%define m_flimI    m10
-%define m_hevthr   m11
-%define m_maskres  m12
-%define m_limres   m8
-%define m_p0backup m12
-%define m_q0backup m8
-%define m_p2backup m13
-%define m_q2backup m14
-%define m_limsign  m9
-
-    ; splat function arguments
-    SPLATB_REG  m_flimE, flimEq, m7   ; E
-    SPLATB_REG  m_flimI, flimIq, m7   ; I
-    SPLATB_REG m_hevthr, hevthrq, m7  ; hev_thresh
-%endif
-
-%if %2 == 8 ; chroma
-    DEFINE_ARGS dst1, dst8, mstride, stride, dst2
-%elif mmsize == 8
-    DEFINE_ARGS dst1, mstride, stride, dst2, cntr
-    mov           cntrq, 2
-%else
-    DEFINE_ARGS dst1, mstride, stride, dst2, dst8
-%endif
-    mov         strideq, mstrideq
-    neg        mstrideq
-%ifidn %1, h
-    lea           dst1q, [dst1q+strideq*4-4]
-%if %2 == 8 ; chroma
-    lea           dst8q, [dst8q+strideq*4-4]
-%endif
-%endif
-
-%if mmsize == 8
-.next8px:
-%endif
-    ; read
-    lea           dst2q, [dst1q+ strideq  ]
-%ifidn %1, v
-%if %2 == 8 && mmsize == 16
-%define movrow movh
-%else
-%define movrow mova
-%endif
-    movrow           m0, [dst1q+mstrideq*4] ; p3
-    movrow           m1, [dst2q+mstrideq*4] ; p2
-    movrow           m2, [dst1q+mstrideq*2] ; p1
-    movrow           m5, [dst2q]            ; q1
-    movrow           m6, [dst2q+ strideq  ] ; q2
-    movrow           m7, [dst2q+ strideq*2] ; q3
-%if mmsize == 16 && %2 == 8
-    movhps           m0, [dst8q+mstrideq*4]
-    movhps           m2, [dst8q+mstrideq*2]
-    add           dst8q, strideq
-    movhps           m1, [dst8q+mstrideq*4]
-    movhps           m5, [dst8q]
-    movhps           m6, [dst8q+ strideq  ]
-    movhps           m7, [dst8q+ strideq*2]
-    add           dst8q, mstrideq
-%endif
-%elif mmsize == 8 ; mmx/mmxext (h)
-    ; read 8 rows of 8px each
-    movu             m0, [dst1q+mstrideq*4]
-    movu             m1, [dst2q+mstrideq*4]
-    movu             m2, [dst1q+mstrideq*2]
-    movu             m3, [dst1q+mstrideq  ]
-    movu             m4, [dst1q]
-    movu             m5, [dst2q]
-    movu             m6, [dst2q+ strideq  ]
-
-    ; 8x8 transpose
-    TRANSPOSE4x4B     0, 1, 2, 3, 7
-    mova     m_q0backup, m1
-    movu             m7, [dst2q+ strideq*2]
-    TRANSPOSE4x4B     4, 5, 6, 7, 1
-    SBUTTERFLY       dq, 0, 4, 1     ; p3/p2
-    SBUTTERFLY       dq, 2, 6, 1     ; q0/q1
-    SBUTTERFLY       dq, 3, 7, 1     ; q2/q3
-    mova             m1, m_q0backup
-    mova     m_q0backup, m2          ; store q0
-    SBUTTERFLY       dq, 1, 5, 2     ; p1/p0
-    mova     m_p0backup, m5          ; store p0
-    SWAP              1, 4
-    SWAP              2, 4
-    SWAP              6, 3
-    SWAP              5, 3
-%else ; sse2 (h)
-%if %2 == 16
-    lea           dst8q, [dst1q+ strideq*8  ]
-%endif
-
-    ; read 16 rows of 8px each, interleave
-    movh             m0, [dst1q+mstrideq*4]
-    movh             m1, [dst8q+mstrideq*4]
-    movh             m2, [dst1q+mstrideq*2]
-    movh             m5, [dst8q+mstrideq*2]
-    movh             m3, [dst1q+mstrideq  ]
-    movh             m6, [dst8q+mstrideq  ]
-    movh             m4, [dst1q]
-    movh             m7, [dst8q]
-    punpcklbw        m0, m1          ; A/I
-    punpcklbw        m2, m5          ; C/K
-    punpcklbw        m3, m6          ; D/L
-    punpcklbw        m4, m7          ; E/M
-
-    add           dst8q, strideq
-    movh             m1, [dst2q+mstrideq*4]
-    movh             m6, [dst8q+mstrideq*4]
-    movh             m5, [dst2q]
-    movh             m7, [dst8q]
-    punpcklbw        m1, m6          ; B/J
-    punpcklbw        m5, m7          ; F/N
-    movh             m6, [dst2q+ strideq  ]
-    movh             m7, [dst8q+ strideq  ]
-    punpcklbw        m6, m7          ; G/O
-
-    ; 8x16 transpose
-    TRANSPOSE4x4B     0, 1, 2, 3, 7
-%ifdef m8
-    SWAP              1, 8
-%else
-    mova     m_q0backup, m1
-%endif
-    movh             m7, [dst2q+ strideq*2]
-    movh             m1, [dst8q+ strideq*2]
-    punpcklbw        m7, m1          ; H/P
-    TRANSPOSE4x4B     4, 5, 6, 7, 1
-    SBUTTERFLY       dq, 0, 4, 1     ; p3/p2
-    SBUTTERFLY       dq, 2, 6, 1     ; q0/q1
-    SBUTTERFLY       dq, 3, 7, 1     ; q2/q3
-%ifdef m8
-    SWAP              1, 8
-    SWAP              2, 8
-%else
-    mova             m1, m_q0backup
-    mova     m_q0backup, m2          ; store q0
-%endif
-    SBUTTERFLY       dq, 1, 5, 2     ; p1/p0
-%ifdef m12
-    SWAP              5, 12
-%else
-    mova     m_p0backup, m5          ; store p0
-%endif
-    SWAP              1, 4
-    SWAP              2, 4
-    SWAP              6, 3
-    SWAP              5, 3
-%endif
-
-    ; normal_limit for p3-p2, p2-p1, q3-q2 and q2-q1
-    mova             m4, m1
-    SWAP              4, 1
-    psubusb          m4, m0          ; p2-p3
-    psubusb          m0, m1          ; p3-p2
-    por              m0, m4          ; abs(p3-p2)
-
-    mova             m4, m2
-    SWAP              4, 2
-    psubusb          m4, m1          ; p1-p2
-    mova     m_p2backup, m1
-    psubusb          m1, m2          ; p2-p1
-    por              m1, m4          ; abs(p2-p1)
-
-    mova             m4, m6
-    SWAP              4, 6
-    psubusb          m4, m7          ; q2-q3
-    psubusb          m7, m6          ; q3-q2
-    por              m7, m4          ; abs(q3-q2)
-
-    mova             m4, m5
-    SWAP              4, 5
-    psubusb          m4, m6          ; q1-q2
-    mova     m_q2backup, m6
-    psubusb          m6, m5          ; q2-q1
-    por              m6, m4          ; abs(q2-q1)
-
-%if notcpuflag(mmxext)
-    mova             m4, m_flimI
-    pxor             m3, m3
-    psubusb          m0, m4
-    psubusb          m1, m4
-    psubusb          m7, m4
-    psubusb          m6, m4
-    pcmpeqb          m0, m3          ; abs(p3-p2) <= I
-    pcmpeqb          m1, m3          ; abs(p2-p1) <= I
-    pcmpeqb          m7, m3          ; abs(q3-q2) <= I
-    pcmpeqb          m6, m3          ; abs(q2-q1) <= I
-    pand             m0, m1
-    pand             m7, m6
-    pand             m0, m7
-%else ; mmxext/sse2
-    pmaxub           m0, m1
-    pmaxub           m6, m7
-    pmaxub           m0, m6
-%endif
-
-    ; normal_limit and high_edge_variance for p1-p0, q1-q0
-    SWAP              7, 3           ; now m7 is zero
-%ifidn %1, v
-    movrow           m3, [dst1q+mstrideq  ] ; p0
-%if mmsize == 16 && %2 == 8
-    movhps           m3, [dst8q+mstrideq  ]
-%endif
-%elifdef m12
-    SWAP              3, 12
-%else
-    mova             m3, m_p0backup
-%endif
-
-    mova             m1, m2
-    SWAP              1, 2
-    mova             m6, m3
-    SWAP              3, 6
-    psubusb          m1, m3          ; p1-p0
-    psubusb          m6, m2          ; p0-p1
-    por              m1, m6          ; abs(p1-p0)
-%if notcpuflag(mmxext)
-    mova             m6, m1
-    psubusb          m1, m4
-    psubusb          m6, m_hevthr
-    pcmpeqb          m1, m7          ; abs(p1-p0) <= I
-    pcmpeqb          m6, m7          ; abs(p1-p0) <= hev_thresh
-    pand             m0, m1
-    mova      m_maskres, m6
-%else ; mmxext/sse2
-    pmaxub           m0, m1          ; max_I
-    SWAP              1, 4           ; max_hev_thresh
-%endif
-
-    SWAP              6, 4           ; now m6 is I
-%ifidn %1, v
-    movrow           m4, [dst1q]     ; q0
-%if mmsize == 16 && %2 == 8
-    movhps           m4, [dst8q]
-%endif
-%elifdef m8
-    SWAP              4, 8
-%else
-    mova             m4, m_q0backup
-%endif
-    mova             m1, m4
-    SWAP              1, 4
-    mova             m7, m5
-    SWAP              7, 5
-    psubusb          m1, m5          ; q0-q1
-    psubusb          m7, m4          ; q1-q0
-    por              m1, m7          ; abs(q1-q0)
-%if notcpuflag(mmxext)
-    mova             m7, m1
-    psubusb          m1, m6
-    psubusb          m7, m_hevthr
-    pxor             m6, m6
-    pcmpeqb          m1, m6          ; abs(q1-q0) <= I
-    pcmpeqb          m7, m6          ; abs(q1-q0) <= hev_thresh
-    mova             m6, m_maskres
-    pand             m0, m1          ; abs([pq][321]-[pq][210]) <= I
-    pand             m6, m7
-%else ; mmxext/sse2
-    pxor             m7, m7
-    pmaxub           m0, m1
-    pmaxub           m6, m1
-    psubusb          m0, m_flimI
-    psubusb          m6, m_hevthr
-    pcmpeqb          m0, m7          ; max(abs(..)) <= I
-    pcmpeqb          m6, m7          ; !(max(abs..) > thresh)
-%endif
-%ifdef m12
-    SWAP              6, 12
-%else
-    mova      m_maskres, m6          ; !(abs(p1-p0) > hev_t || abs(q1-q0) > hev_t)
-%endif
-
-    ; simple_limit
-    mova             m1, m3
-    SWAP              1, 3
-    mova             m6, m4          ; keep copies of p0/q0 around for later use
-    SWAP              6, 4
-    psubusb          m1, m4          ; p0-q0
-    psubusb          m6, m3          ; q0-p0
-    por              m1, m6          ; abs(q0-p0)
-    paddusb          m1, m1          ; m1=2*abs(q0-p0)
-
-    mova             m7, m2
-    SWAP              7, 2
-    mova             m6, m5
-    SWAP              6, 5
-    psubusb          m7, m5          ; p1-q1
-    psubusb          m6, m2          ; q1-p1
-    por              m7, m6          ; abs(q1-p1)
-    pxor             m6, m6
-    pand             m7, [pb_FE]
-    psrlq            m7, 1           ; abs(q1-p1)/2
-    paddusb          m7, m1          ; abs(q0-p0)*2+abs(q1-p1)/2
-    psubusb          m7, m_flimE
-    pcmpeqb          m7, m6          ; abs(q0-p0)*2+abs(q1-p1)/2 <= E
-    pand             m0, m7          ; normal_limit result
-
-    ; filter_common; at this point, m2-m5=p1-q1 and m0 is filter_mask
-%ifdef m8 ; x86-64 && sse2
-    mova             m8, [pb_80]
-%define m_pb_80 m8
-%else ; x86-32 or mmx/mmxext
-%define m_pb_80 [pb_80]
-%endif
-    mova             m1, m4
-    mova             m7, m3
-    pxor             m1, m_pb_80
-    pxor             m7, m_pb_80
-    psubsb           m1, m7          ; (signed) q0-p0
-    mova             m6, m2
-    mova             m7, m5
-    pxor             m6, m_pb_80
-    pxor             m7, m_pb_80
-    psubsb           m6, m7          ; (signed) p1-q1
-    mova             m7, m_maskres
-    paddsb           m6, m1
-    paddsb           m6, m1
-    paddsb           m6, m1
-    pand             m6, m0
-%ifdef m8
-    mova       m_limres, m6          ; 3*(qp-p0)+(p1-q1) masked for filter_mbedge
-    pand       m_limres, m7
-%else
-    mova             m0, m6
-    pand             m0, m7
-    mova       m_limres, m0
-%endif
-    pandn            m7, m6          ; 3*(q0-p0)+(p1-q1) masked for filter_common
-
-    mova             m1, [pb_F8]
-    mova             m6, m7
-    paddsb           m7, [pb_3]
-    paddsb           m6, [pb_4]
-    pand             m7, m1
-    pand             m6, m1
-
-    pxor             m1, m1
-    pxor             m0, m0
-    pcmpgtb          m1, m7
-    psubb            m0, m7
-    psrlq            m7, 3           ; +f2
-    psrlq            m0, 3           ; -f2
-    pand             m0, m1
-    pandn            m1, m7
-    psubusb          m3, m0
-    paddusb          m3, m1          ; p0+f2
-
-    pxor             m1, m1
-    pxor             m0, m0
-    pcmpgtb          m0, m6
-    psubb            m1, m6
-    psrlq            m6, 3           ; +f1
-    psrlq            m1, 3           ; -f1
-    pand             m1, m0
-    pandn            m0, m6
-    psubusb          m4, m0
-    paddusb          m4, m1          ; q0-f1
-
-    ; filter_mbedge (m2-m5 = p1-q1; lim_res carries w)
-%if cpuflag(ssse3)
-    mova             m7, [pb_1]
-%else
-    mova             m7, [pw_63]
-%endif
-%ifdef m8
-    SWAP              1, 8
-%else
-    mova             m1, m_limres
-%endif
-    pxor             m0, m0
-    mova             m6, m1
-    pcmpgtb          m0, m1         ; which are negative
-%if cpuflag(ssse3)
-    punpcklbw        m6, m7         ; interleave with "1" for rounding
-    punpckhbw        m1, m7
-%else
-    punpcklbw        m6, m0         ; signed byte->word
-    punpckhbw        m1, m0
-%endif
-    mova      m_limsign, m0
-%if cpuflag(ssse3)
-    mova             m7, [pb_27_63]
-%ifndef m8
-    mova       m_limres, m1
-%endif
-%ifdef m10
-    SWAP              0, 10         ; don't lose lim_sign copy
-%endif
-    mova             m0, m7
-    pmaddubsw        m7, m6
-    SWAP              6, 7
-    pmaddubsw        m0, m1
-    SWAP              1, 0
-%ifdef m10
-    SWAP              0, 10
-%else
-    mova             m0, m_limsign
-%endif
-%else
-    mova      m_maskres, m6         ; backup for later in filter
-    mova       m_limres, m1
-    pmullw          m6, [pw_27]
-    pmullw          m1, [pw_27]
-    paddw           m6, m7
-    paddw           m1, m7
-%endif
-    psraw           m6, 7
-    psraw           m1, 7
-    packsswb        m6, m1          ; a0
-    pxor            m1, m1
-    psubb           m1, m6
-    pand            m1, m0          ; -a0
-    pandn           m0, m6          ; +a0
-%if cpuflag(ssse3)
-    mova            m6, [pb_18_63]  ; pipelining
-%endif
-    psubusb         m3, m1
-    paddusb         m4, m1
-    paddusb         m3, m0          ; p0+a0
-    psubusb         m4, m0          ; q0-a0
-
-%if cpuflag(ssse3)
-    SWAP             6, 7
-%ifdef m10
-    SWAP             1, 10
-%else
-    mova            m1, m_limres
-%endif
-    mova            m0, m7
-    pmaddubsw       m7, m6
-    SWAP             6, 7
-    pmaddubsw       m0, m1
-    SWAP             1, 0
-%ifdef m10
-    SWAP             0, 10
-%endif
-    mova            m0, m_limsign
-%else
-    mova            m6, m_maskres
-    mova            m1, m_limres
-    pmullw          m6, [pw_18]
-    pmullw          m1, [pw_18]
-    paddw           m6, m7
-    paddw           m1, m7
-%endif
-    mova            m0, m_limsign
-    psraw           m6, 7
-    psraw           m1, 7
-    packsswb        m6, m1          ; a1
-    pxor            m1, m1
-    psubb           m1, m6
-    pand            m1, m0          ; -a1
-    pandn           m0, m6          ; +a1
-%if cpuflag(ssse3)
-    mova            m6, [pb_9_63]
-%endif
-    psubusb         m2, m1
-    paddusb         m5, m1
-    paddusb         m2, m0          ; p1+a1
-    psubusb         m5, m0          ; q1-a1
-
-%if cpuflag(ssse3)
-    SWAP             6, 7
-%ifdef m10
-    SWAP             1, 10
-%else
-    mova            m1, m_limres
-%endif
-    mova            m0, m7
-    pmaddubsw       m7, m6
-    SWAP             6, 7
-    pmaddubsw       m0, m1
-    SWAP             1, 0
-%else
-%ifdef m8
-    SWAP             6, 12
-    SWAP             1, 8
-%else
-    mova            m6, m_maskres
-    mova            m1, m_limres
-%endif
-    pmullw          m6, [pw_9]
-    pmullw          m1, [pw_9]
-    paddw           m6, m7
-    paddw           m1, m7
-%endif
-%ifdef m9
-    SWAP             7, 9
-%else
-    mova            m7, m_limsign
-%endif
-    psraw           m6, 7
-    psraw           m1, 7
-    packsswb        m6, m1          ; a1
-    pxor            m0, m0
-    psubb           m0, m6
-    pand            m0, m7          ; -a1
-    pandn           m7, m6          ; +a1
-%ifdef m8
-    SWAP             1, 13
-    SWAP             6, 14
-%else
-    mova            m1, m_p2backup
-    mova            m6, m_q2backup
-%endif
-    psubusb         m1, m0
-    paddusb         m6, m0
-    paddusb         m1, m7          ; p1+a1
-    psubusb         m6, m7          ; q1-a1
-
-    ; store
-%ifidn %1, v
-    movrow [dst2q+mstrideq*4], m1
-    movrow [dst1q+mstrideq*2], m2
-    movrow [dst1q+mstrideq  ], m3
-    movrow     [dst1q], m4
-    movrow     [dst2q], m5
-    movrow [dst2q+ strideq  ], m6
-%if mmsize == 16 && %2 == 8
-    add           dst8q, mstrideq
-    movhps [dst8q+mstrideq*2], m1
-    movhps [dst8q+mstrideq  ], m2
-    movhps     [dst8q], m3
-    add          dst8q, strideq
-    movhps     [dst8q], m4
-    movhps [dst8q+ strideq  ], m5
-    movhps [dst8q+ strideq*2], m6
-%endif
-%else ; h
-    inc          dst1q
-    inc          dst2q
-
-    ; 4x8/16 transpose
-    TRANSPOSE4x4B    1, 2, 3, 4, 0
-    SBUTTERFLY      bw, 5, 6, 0
-
-%if mmsize == 8 ; mmx/mmxext (h)
-    WRITE_4x2D       1, 2, 3, 4, dst1q, dst2q, mstrideq, strideq
-    add          dst1q, 4
-    WRITE_2x4W      m5, m6, dst2q, dst1q, mstrideq, strideq
-%else ; sse2 (h)
-    lea          dst8q, [dst8q+mstrideq+1]
-    WRITE_4x4D       1, 2, 3, 4, dst1q, dst2q, dst8q, mstrideq, strideq, %2
-    lea          dst1q, [dst2q+mstrideq+4]
-    lea          dst8q, [dst8q+mstrideq+4]
-%if cpuflag(sse4)
-    add          dst2q, 4
-%endif
-    WRITE_8W        m5, dst2q, dst1q,  mstrideq, strideq
-%if cpuflag(sse4)
-    lea          dst2q, [dst8q+ strideq  ]
-%endif
-    WRITE_8W        m6, dst2q, dst8q, mstrideq, strideq
-%endif
-%endif
-
-%if mmsize == 8
-%if %2 == 8 ; chroma
-%ifidn %1, h
-    sub          dst1q, 5
-%endif
-    cmp          dst1q, dst8q
-    mov          dst1q, dst8q
-    jnz .next8px
-%else
-%ifidn %1, h
-    lea          dst1q, [dst1q+ strideq*8-5]
-%else ; v
-    add          dst1q, 8
-%endif
-    dec          cntrq
-    jg .next8px
-%endif
-    REP_RET
-%else ; mmsize == 16
-    RET
-%endif
-%endmacro
-
-%if ARCH_X86_32
-INIT_MMX mmx
-MBEDGE_LOOPFILTER v, 16
-MBEDGE_LOOPFILTER h, 16
-MBEDGE_LOOPFILTER v,  8
-MBEDGE_LOOPFILTER h,  8
-
-INIT_MMX mmxext
-MBEDGE_LOOPFILTER v, 16
-MBEDGE_LOOPFILTER h, 16
-MBEDGE_LOOPFILTER v,  8
-MBEDGE_LOOPFILTER h,  8
-%endif
-
-INIT_XMM sse2
-MBEDGE_LOOPFILTER v, 16
-MBEDGE_LOOPFILTER h, 16
-MBEDGE_LOOPFILTER v,  8
-MBEDGE_LOOPFILTER h,  8
-
-INIT_XMM ssse3
-MBEDGE_LOOPFILTER v, 16
-MBEDGE_LOOPFILTER h, 16
-MBEDGE_LOOPFILTER v,  8
-MBEDGE_LOOPFILTER h,  8
-
-INIT_XMM sse4
-MBEDGE_LOOPFILTER h, 16
-MBEDGE_LOOPFILTER h,  8
diff -uparN ffmpeg-4.1/libavcodec/x86/vp9intrapred_16bpp.asm ffmpeg-y/libavcodec/x86/vp9intrapred_16bpp.asm
--- ffmpeg-4.1/libavcodec/x86/vp9intrapred_16bpp.asm	2018-11-02 02:34:25.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/vp9intrapred_16bpp.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,2392 +0,0 @@
-;******************************************************************************
-;* VP9 Intra prediction SIMD optimizations
-;*
-;* Copyright (c) 2015 Ronald S. Bultje <rsbultje gmail com>
-;* Copyright (c) 2015 Henrik Gramner <henrik gramner com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA 32
-
-pd_2: times 8 dd 2
-pd_4: times 8 dd 4
-pd_8: times 8 dd 8
-
-pb_2to15_14_15: db 2,3,4,5,6,7,8,9,10,11,12,13,14,15,14,15
-pb_4_5_8to13_8x0: db 4,5,8,9,10,11,12,13,0,0,0,0,0,0,0,0
-pb_0to7_67x4: db 0,1,2,3,4,5,6,7,6,7,6,7,6,7,6,7
-
-cextern pw_1
-cextern pw_1023
-cextern pw_4095
-cextern pd_16
-cextern pd_32
-cextern pd_65535;
-
-; FIXME most top-only functions (ddl, vl, v, dc_top) can be modified to take
-; only 3 registers on x86-32, which would make it one cycle faster, but that
-; would make the code quite a bit uglier...
-
-SECTION .text
-
-%macro SCRATCH 3-4
-%if ARCH_X86_64
-    SWAP                %1, %2
-%if %0 == 4
-%define reg_%4 m%2
-%endif
-%else
-    mova              [%3], m%1
-%if %0 == 4
-%define reg_%4 [%3]
-%endif
-%endif
-%endmacro
-
-%macro UNSCRATCH 3-4
-%if ARCH_X86_64
-    SWAP                %1, %2
-%else
-    mova               m%1, [%3]
-%endif
-%if %0 == 4
-%undef reg_%4
-%endif
-%endmacro
-
-%macro PRELOAD 2-3
-%if ARCH_X86_64
-    mova               m%1, [%2]
-%if %0 == 3
-%define reg_%3 m%1
-%endif
-%elif %0 == 3
-%define reg_%3 [%2]
-%endif
-%endmacro
-
-INIT_MMX mmx
-cglobal vp9_ipred_v_4x4_16, 2, 4, 1, dst, stride, l, a
-    movifnidn               aq, amp
-    mova                    m0, [aq]
-    DEFINE_ARGS dst, stride, stride3
-    lea               stride3q, [strideq*3]
-    mova      [dstq+strideq*0], m0
-    mova      [dstq+strideq*1], m0
-    mova      [dstq+strideq*2], m0
-    mova      [dstq+stride3q ], m0
-    RET
-
-INIT_XMM sse
-cglobal vp9_ipred_v_8x8_16, 2, 4, 1, dst, stride, l, a
-    movifnidn               aq, amp
-    mova                    m0, [aq]
-    DEFINE_ARGS dst, stride, stride3
-    lea               stride3q, [strideq*3]
-    mova      [dstq+strideq*0], m0
-    mova      [dstq+strideq*1], m0
-    mova      [dstq+strideq*2], m0
-    mova      [dstq+stride3q ], m0
-    lea                   dstq, [dstq+strideq*4]
-    mova      [dstq+strideq*0], m0
-    mova      [dstq+strideq*1], m0
-    mova      [dstq+strideq*2], m0
-    mova      [dstq+stride3q ], m0
-    RET
-
-INIT_XMM sse
-cglobal vp9_ipred_v_16x16_16, 2, 4, 2, dst, stride, l, a
-    movifnidn               aq, amp
-    mova                    m0, [aq]
-    mova                    m1, [aq+mmsize]
-    DEFINE_ARGS dst, stride, stride3, cnt
-    lea               stride3q, [strideq*3]
-    mov                   cntd, 4
-.loop:
-    mova   [dstq+strideq*0+ 0], m0
-    mova   [dstq+strideq*0+16], m1
-    mova   [dstq+strideq*1+ 0], m0
-    mova   [dstq+strideq*1+16], m1
-    mova   [dstq+strideq*2+ 0], m0
-    mova   [dstq+strideq*2+16], m1
-    mova   [dstq+stride3q + 0], m0
-    mova   [dstq+stride3q +16], m1
-    lea                   dstq, [dstq+strideq*4]
-    dec               cntd
-    jg .loop
-    RET
-
-INIT_XMM sse
-cglobal vp9_ipred_v_32x32_16, 2, 4, 4, dst, stride, l, a
-    movifnidn               aq, amp
-    mova                    m0, [aq+mmsize*0]
-    mova                    m1, [aq+mmsize*1]
-    mova                    m2, [aq+mmsize*2]
-    mova                    m3, [aq+mmsize*3]
-    DEFINE_ARGS dst, stride, cnt
-    mov                   cntd, 16
-.loop:
-    mova   [dstq+strideq*0+ 0], m0
-    mova   [dstq+strideq*0+16], m1
-    mova   [dstq+strideq*0+32], m2
-    mova   [dstq+strideq*0+48], m3
-    mova   [dstq+strideq*1+ 0], m0
-    mova   [dstq+strideq*1+16], m1
-    mova   [dstq+strideq*1+32], m2
-    mova   [dstq+strideq*1+48], m3
-    lea                   dstq, [dstq+strideq*2]
-    dec               cntd
-    jg .loop
-    RET
-
-INIT_MMX mmxext
-cglobal vp9_ipred_h_4x4_16, 3, 3, 4, dst, stride, l, a
-    mova                    m3, [lq]
-    DEFINE_ARGS dst, stride, stride3
-    lea               stride3q, [strideq*3]
-    pshufw                  m0, m3, q3333
-    pshufw                  m1, m3, q2222
-    pshufw                  m2, m3, q1111
-    pshufw                  m3, m3, q0000
-    mova      [dstq+strideq*0], m0
-    mova      [dstq+strideq*1], m1
-    mova      [dstq+strideq*2], m2
-    mova      [dstq+stride3q ], m3
-    RET
-
-INIT_XMM sse2
-cglobal vp9_ipred_h_8x8_16, 3, 3, 4, dst, stride, l, a
-    mova                    m2, [lq]
-    DEFINE_ARGS dst, stride, stride3
-    lea               stride3q, [strideq*3]
-    punpckhwd               m3, m2, m2
-    pshufd                  m0, m3, q3333
-    pshufd                  m1, m3, q2222
-    mova      [dstq+strideq*0], m0
-    mova      [dstq+strideq*1], m1
-    pshufd                  m0, m3, q1111
-    pshufd                  m1, m3, q0000
-    mova      [dstq+strideq*2], m0
-    mova      [dstq+stride3q ], m1
-    lea                   dstq, [dstq+strideq*4]
-    punpcklwd               m2, m2
-    pshufd                  m0, m2, q3333
-    pshufd                  m1, m2, q2222
-    mova      [dstq+strideq*0], m0
-    mova      [dstq+strideq*1], m1
-    pshufd                  m0, m2, q1111
-    pshufd                  m1, m2, q0000
-    mova      [dstq+strideq*2], m0
-    mova      [dstq+stride3q ], m1
-    RET
-
-INIT_XMM sse2
-cglobal vp9_ipred_h_16x16_16, 3, 5, 4, dst, stride, l, stride3, cnt
-    mov                   cntd, 3
-    lea               stride3q, [strideq*3]
-.loop:
-    movh                    m3, [lq+cntq*8]
-    punpcklwd               m3, m3
-    pshufd                  m0, m3, q3333
-    pshufd                  m1, m3, q2222
-    pshufd                  m2, m3, q1111
-    pshufd                  m3, m3, q0000
-    mova    [dstq+strideq*0+ 0], m0
-    mova    [dstq+strideq*0+16], m0
-    mova    [dstq+strideq*1+ 0], m1
-    mova    [dstq+strideq*1+16], m1
-    mova    [dstq+strideq*2+ 0], m2
-    mova    [dstq+strideq*2+16], m2
-    mova    [dstq+stride3q + 0], m3
-    mova    [dstq+stride3q +16], m3
-    lea                   dstq, [dstq+strideq*4]
-    dec                   cntd
-    jge .loop
-    RET
-
-INIT_XMM sse2
-cglobal vp9_ipred_h_32x32_16, 3, 5, 4, dst, stride, l, stride3, cnt
-    mov                   cntd, 7
-    lea               stride3q, [strideq*3]
-.loop:
-    movh                    m3, [lq+cntq*8]
-    punpcklwd               m3, m3
-    pshufd                  m0, m3, q3333
-    pshufd                  m1, m3, q2222
-    pshufd                  m2, m3, q1111
-    pshufd                  m3, m3, q0000
-    mova   [dstq+strideq*0+ 0], m0
-    mova   [dstq+strideq*0+16], m0
-    mova   [dstq+strideq*0+32], m0
-    mova   [dstq+strideq*0+48], m0
-    mova   [dstq+strideq*1+ 0], m1
-    mova   [dstq+strideq*1+16], m1
-    mova   [dstq+strideq*1+32], m1
-    mova   [dstq+strideq*1+48], m1
-    mova   [dstq+strideq*2+ 0], m2
-    mova   [dstq+strideq*2+16], m2
-    mova   [dstq+strideq*2+32], m2
-    mova   [dstq+strideq*2+48], m2
-    mova   [dstq+stride3q + 0], m3
-    mova   [dstq+stride3q +16], m3
-    mova   [dstq+stride3q +32], m3
-    mova   [dstq+stride3q +48], m3
-    lea                   dstq, [dstq+strideq*4]
-    dec                   cntd
-    jge .loop
-    RET
-
-INIT_MMX mmxext
-cglobal vp9_ipred_dc_4x4_16, 4, 4, 2, dst, stride, l, a
-    mova                    m0, [lq]
-    paddw                   m0, [aq]
-    DEFINE_ARGS dst, stride, stride3
-    lea               stride3q, [strideq*3]
-    pmaddwd                 m0, [pw_1]
-    pshufw                  m1, m0, q3232
-    paddd                   m0, [pd_4]
-    paddd                   m0, m1
-    psrad                   m0, 3
-    pshufw                  m0, m0, q0000
-    mova      [dstq+strideq*0], m0
-    mova      [dstq+strideq*1], m0
-    mova      [dstq+strideq*2], m0
-    mova      [dstq+stride3q ], m0
-    RET
-
-INIT_XMM sse2
-cglobal vp9_ipred_dc_8x8_16, 4, 4, 2, dst, stride, l, a
-    mova                    m0, [lq]
-    paddw                   m0, [aq]
-    DEFINE_ARGS dst, stride, stride3
-    lea               stride3q, [strideq*3]
-    pmaddwd                 m0, [pw_1]
-    pshufd                  m1, m0, q3232
-    paddd                   m0, m1
-    pshufd                  m1, m0, q1111
-    paddd                   m0, [pd_8]
-    paddd                   m0, m1
-    psrad                   m0, 4
-    pshuflw                 m0, m0, q0000
-    punpcklqdq              m0, m0
-    mova      [dstq+strideq*0], m0
-    mova      [dstq+strideq*1], m0
-    mova      [dstq+strideq*2], m0
-    mova      [dstq+stride3q ], m0
-    lea                   dstq, [dstq+strideq*4]
-    mova      [dstq+strideq*0], m0
-    mova      [dstq+strideq*1], m0
-    mova      [dstq+strideq*2], m0
-    mova      [dstq+stride3q ], m0
-    RET
-
-INIT_XMM sse2
-cglobal vp9_ipred_dc_16x16_16, 4, 4, 2, dst, stride, l, a
-    mova                    m0, [lq]
-    paddw                   m0, [lq+mmsize]
-    paddw                   m0, [aq]
-    paddw                   m0, [aq+mmsize]
-    DEFINE_ARGS dst, stride, stride3, cnt
-    lea               stride3q, [strideq*3]
-    mov                   cntd, 4
-    pmaddwd                 m0, [pw_1]
-    pshufd                  m1, m0, q3232
-    paddd                   m0, m1
-    pshufd                  m1, m0, q1111
-    paddd                   m0, [pd_16]
-    paddd                   m0, m1
-    psrad                   m0, 5
-    pshuflw                 m0, m0, q0000
-    punpcklqdq              m0, m0
-.loop:
-    mova   [dstq+strideq*0+ 0], m0
-    mova   [dstq+strideq*0+16], m0
-    mova   [dstq+strideq*1+ 0], m0
-    mova   [dstq+strideq*1+16], m0
-    mova   [dstq+strideq*2+ 0], m0
-    mova   [dstq+strideq*2+16], m0
-    mova   [dstq+stride3q + 0], m0
-    mova   [dstq+stride3q +16], m0
-    lea                   dstq, [dstq+strideq*4]
-    dec                   cntd
-    jg .loop
-    RET
-
-INIT_XMM sse2
-cglobal vp9_ipred_dc_32x32_16, 4, 4, 2, dst, stride, l, a
-    mova                    m0, [lq+mmsize*0]
-    paddw                   m0, [lq+mmsize*1]
-    paddw                   m0, [lq+mmsize*2]
-    paddw                   m0, [lq+mmsize*3]
-    paddw                   m0, [aq+mmsize*0]
-    paddw                   m0, [aq+mmsize*1]
-    paddw                   m0, [aq+mmsize*2]
-    paddw                   m0, [aq+mmsize*3]
-    DEFINE_ARGS dst, stride, stride3, cnt
-    lea               stride3q, [strideq*3]
-    mov                   cntd, 16
-    pmaddwd                 m0, [pw_1]
-    pshufd                  m1, m0, q3232
-    paddd                   m0, m1
-    pshufd                  m1, m0, q1111
-    paddd                   m0, [pd_32]
-    paddd                   m0, m1
-    psrad                   m0, 6
-    pshuflw                 m0, m0, q0000
-    punpcklqdq              m0, m0
-.loop:
-    mova   [dstq+strideq*0+ 0], m0
-    mova   [dstq+strideq*0+16], m0
-    mova   [dstq+strideq*0+32], m0
-    mova   [dstq+strideq*0+48], m0
-    mova   [dstq+strideq*1+ 0], m0
-    mova   [dstq+strideq*1+16], m0
-    mova   [dstq+strideq*1+32], m0
-    mova   [dstq+strideq*1+48], m0
-    lea                   dstq, [dstq+strideq*2]
-    dec                   cntd
-    jg .loop
-    RET
-
-%macro DC_1D_FNS 2
-INIT_MMX mmxext
-cglobal vp9_ipred_dc_%1_4x4_16, 4, 4, 2, dst, stride, l, a
-    mova                    m0, [%2]
-    DEFINE_ARGS dst, stride, stride3
-    lea               stride3q, [strideq*3]
-    pmaddwd                 m0, [pw_1]
-    pshufw                  m1, m0, q3232
-    paddd                   m0, [pd_2]
-    paddd                   m0, m1
-    psrad                   m0, 2
-    pshufw                  m0, m0, q0000
-    mova      [dstq+strideq*0], m0
-    mova      [dstq+strideq*1], m0
-    mova      [dstq+strideq*2], m0
-    mova      [dstq+stride3q ], m0
-    RET
-
-INIT_XMM sse2
-cglobal vp9_ipred_dc_%1_8x8_16, 4, 4, 2, dst, stride, l, a
-    mova                    m0, [%2]
-    DEFINE_ARGS dst, stride, stride3
-    lea               stride3q, [strideq*3]
-    pmaddwd                 m0, [pw_1]
-    pshufd                  m1, m0, q3232
-    paddd                   m0, m1
-    pshufd                  m1, m0, q1111
-    paddd                   m0, [pd_4]
-    paddd                   m0, m1
-    psrad                   m0, 3
-    pshuflw                 m0, m0, q0000
-    punpcklqdq              m0, m0
-    mova      [dstq+strideq*0], m0
-    mova      [dstq+strideq*1], m0
-    mova      [dstq+strideq*2], m0
-    mova      [dstq+stride3q ], m0
-    lea                   dstq, [dstq+strideq*4]
-    mova      [dstq+strideq*0], m0
-    mova      [dstq+strideq*1], m0
-    mova      [dstq+strideq*2], m0
-    mova      [dstq+stride3q ], m0
-    RET
-
-INIT_XMM sse2
-cglobal vp9_ipred_dc_%1_16x16_16, 4, 4, 2, dst, stride, l, a
-    mova                    m0, [%2]
-    paddw                   m0, [%2+mmsize]
-    DEFINE_ARGS dst, stride, stride3, cnt
-    lea               stride3q, [strideq*3]
-    mov                   cntd, 4
-    pmaddwd                 m0, [pw_1]
-    pshufd                  m1, m0, q3232
-    paddd                   m0, m1
-    pshufd                  m1, m0, q1111
-    paddd                   m0, [pd_8]
-    paddd                   m0, m1
-    psrad                   m0, 4
-    pshuflw                 m0, m0, q0000
-    punpcklqdq              m0, m0
-.loop:
-    mova   [dstq+strideq*0+ 0], m0
-    mova   [dstq+strideq*0+16], m0
-    mova   [dstq+strideq*1+ 0], m0
-    mova   [dstq+strideq*1+16], m0
-    mova   [dstq+strideq*2+ 0], m0
-    mova   [dstq+strideq*2+16], m0
-    mova   [dstq+stride3q + 0], m0
-    mova   [dstq+stride3q +16], m0
-    lea                   dstq, [dstq+strideq*4]
-    dec                   cntd
-    jg .loop
-    RET
-
-INIT_XMM sse2
-cglobal vp9_ipred_dc_%1_32x32_16, 4, 4, 2, dst, stride, l, a
-    mova                    m0, [%2+mmsize*0]
-    paddw                   m0, [%2+mmsize*1]
-    paddw                   m0, [%2+mmsize*2]
-    paddw                   m0, [%2+mmsize*3]
-    DEFINE_ARGS dst, stride, cnt
-    mov                   cntd, 16
-    pmaddwd                 m0, [pw_1]
-    pshufd                  m1, m0, q3232
-    paddd                   m0, m1
-    pshufd                  m1, m0, q1111
-    paddd                   m0, [pd_16]
-    paddd                   m0, m1
-    psrad                   m0, 5
-    pshuflw                 m0, m0, q0000
-    punpcklqdq              m0, m0
-.loop:
-    mova   [dstq+strideq*0+ 0], m0
-    mova   [dstq+strideq*0+16], m0
-    mova   [dstq+strideq*0+32], m0
-    mova   [dstq+strideq*0+48], m0
-    mova   [dstq+strideq*1+ 0], m0
-    mova   [dstq+strideq*1+16], m0
-    mova   [dstq+strideq*1+32], m0
-    mova   [dstq+strideq*1+48], m0
-    lea                   dstq, [dstq+strideq*2]
-    dec                   cntd
-    jg .loop
-    RET
-%endmacro
-
-DC_1D_FNS top,  aq
-DC_1D_FNS left, lq
-
-INIT_MMX mmxext
-cglobal vp9_ipred_tm_4x4_10, 4, 4, 6, dst, stride, l, a
-    mova                    m5, [pw_1023]
-.body:
-    mova                    m4, [aq]
-    mova                    m3, [lq]
-    movd                    m0, [aq-4]
-    pshufw                  m0, m0, q1111
-    psubw                   m4, m0
-    DEFINE_ARGS dst, stride, stride3
-    lea               stride3q, [strideq*3]
-    pshufw                  m0, m3, q3333
-    pshufw                  m1, m3, q2222
-    pshufw                  m2, m3, q1111
-    pshufw                  m3, m3, q0000
-    paddw                   m0, m4
-    paddw                   m1, m4
-    paddw                   m2, m4
-    paddw                   m3, m4
-    pxor                    m4, m4
-    pmaxsw                  m0, m4
-    pmaxsw                  m1, m4
-    pmaxsw                  m2, m4
-    pmaxsw                  m3, m4
-    pminsw                  m0, m5
-    pminsw                  m1, m5
-    pminsw                  m2, m5
-    pminsw                  m3, m5
-    mova      [dstq+strideq*0], m0
-    mova      [dstq+strideq*1], m1
-    mova      [dstq+strideq*2], m2
-    mova      [dstq+stride3q ], m3
-    RET
-
-cglobal vp9_ipred_tm_4x4_12, 4, 4, 6, dst, stride, l, a
-    mova                    m5, [pw_4095]
-    jmp mangle(private_prefix %+ _ %+ vp9_ipred_tm_4x4_10 %+ SUFFIX).body
-
-INIT_XMM sse2
-cglobal vp9_ipred_tm_8x8_10, 4, 5, 7, dst, stride, l, a
-    mova                    m4, [pw_1023]
-.body:
-    pxor                    m6, m6
-    mova                    m5, [aq]
-    movd                    m0, [aq-4]
-    pshuflw                 m0, m0, q1111
-    punpcklqdq              m0, m0
-    psubw                   m5, m0
-    DEFINE_ARGS dst, stride, l, stride3, cnt
-    lea               stride3q, [strideq*3]
-    mov                   cntd, 1
-.loop:
-    movh                    m3, [lq+cntq*8]
-    punpcklwd               m3, m3
-    pshufd                  m0, m3, q3333
-    pshufd                  m1, m3, q2222
-    pshufd                  m2, m3, q1111
-    pshufd                  m3, m3, q0000
-    paddw                   m0, m5
-    paddw                   m1, m5
-    paddw                   m2, m5
-    paddw                   m3, m5
-    pmaxsw                  m0, m6
-    pmaxsw                  m1, m6
-    pmaxsw                  m2, m6
-    pmaxsw                  m3, m6
-    pminsw                  m0, m4
-    pminsw                  m1, m4
-    pminsw                  m2, m4
-    pminsw                  m3, m4
-    mova      [dstq+strideq*0], m0
-    mova      [dstq+strideq*1], m1
-    mova      [dstq+strideq*2], m2
-    mova      [dstq+stride3q ], m3
-    lea                   dstq, [dstq+strideq*4]
-    dec                   cntd
-    jge .loop
-    RET
-
-cglobal vp9_ipred_tm_8x8_12, 4, 5, 7, dst, stride, l, a
-    mova                    m4, [pw_4095]
-    jmp mangle(private_prefix %+ _ %+ vp9_ipred_tm_8x8_10 %+ SUFFIX).body
-
-INIT_XMM sse2
-cglobal vp9_ipred_tm_16x16_10, 4, 4, 8, dst, stride, l, a
-    mova                    m7, [pw_1023]
-.body:
-    pxor                    m6, m6
-    mova                    m4, [aq]
-    mova                    m5, [aq+mmsize]
-    movd                    m0, [aq-4]
-    pshuflw                 m0, m0, q1111
-    punpcklqdq              m0, m0
-    psubw                   m4, m0
-    psubw                   m5, m0
-    DEFINE_ARGS dst, stride, l, cnt
-    mov                   cntd, 7
-.loop:
-    movd                    m3, [lq+cntq*4]
-    punpcklwd               m3, m3
-    pshufd                  m2, m3, q1111
-    pshufd                  m3, m3, q0000
-    paddw                   m0, m2, m4
-    paddw                   m2, m5
-    paddw                   m1, m3, m4
-    paddw                   m3, m5
-    pmaxsw                  m0, m6
-    pmaxsw                  m2, m6
-    pmaxsw                  m1, m6
-    pmaxsw                  m3, m6
-    pminsw                  m0, m7
-    pminsw                  m2, m7
-    pminsw                  m1, m7
-    pminsw                  m3, m7
-    mova   [dstq+strideq*0+ 0], m0
-    mova   [dstq+strideq*0+16], m2
-    mova   [dstq+strideq*1+ 0], m1
-    mova   [dstq+strideq*1+16], m3
-    lea                   dstq, [dstq+strideq*2]
-    dec                   cntd
-    jge .loop
-    RET
-
-cglobal vp9_ipred_tm_16x16_12, 4, 4, 8, dst, stride, l, a
-    mova                    m7, [pw_4095]
-    jmp mangle(private_prefix %+ _ %+ vp9_ipred_tm_16x16_10 %+ SUFFIX).body
-
-INIT_XMM sse2
-cglobal vp9_ipred_tm_32x32_10, 4, 4, 10, 32 * -ARCH_X86_32, dst, stride, l, a
-    mova                    m0, [pw_1023]
-.body:
-    pxor                    m1, m1
-%if ARCH_X86_64
-    SWAP                     0, 8
-    SWAP                     1, 9
-%define reg_min m9
-%define reg_max m8
-%else
-    mova              [rsp+ 0], m0
-    mova              [rsp+16], m1
-%define reg_min [rsp+16]
-%define reg_max [rsp+ 0]
-%endif
-
-    mova                    m4, [aq+mmsize*0]
-    mova                    m5, [aq+mmsize*1]
-    mova                    m6, [aq+mmsize*2]
-    mova                    m7, [aq+mmsize*3]
-    movd                    m0, [aq-4]
-    pshuflw                 m0, m0, q1111
-    punpcklqdq              m0, m0
-    psubw                   m4, m0
-    psubw                   m5, m0
-    psubw                   m6, m0
-    psubw                   m7, m0
-    DEFINE_ARGS dst, stride, l, cnt
-    mov                   cntd, 31
-.loop:
-    pinsrw                  m3, [lq+cntq*2], 0
-    punpcklwd               m3, m3
-    pshufd                  m3, m3, q0000
-    paddw                   m0, m3, m4
-    paddw                   m1, m3, m5
-    paddw                   m2, m3, m6
-    paddw                   m3, m7
-    pmaxsw                  m0, reg_min
-    pmaxsw                  m1, reg_min
-    pmaxsw                  m2, reg_min
-    pmaxsw                  m3, reg_min
-    pminsw                  m0, reg_max
-    pminsw                  m1, reg_max
-    pminsw                  m2, reg_max
-    pminsw                  m3, reg_max
-    mova   [dstq+strideq*0+ 0], m0
-    mova   [dstq+strideq*0+16], m1
-    mova   [dstq+strideq*0+32], m2
-    mova   [dstq+strideq*0+48], m3
-    add                   dstq, strideq
-    dec                   cntd
-    jge .loop
-    RET
-
-cglobal vp9_ipred_tm_32x32_12, 4, 4, 10, 32 * -ARCH_X86_32, dst, stride, l, a
-    mova                    m0, [pw_4095]
-    jmp mangle(private_prefix %+ _ %+ vp9_ipred_tm_32x32_10 %+ SUFFIX).body
-
-; Directional intra predicion functions
-;
-; in the functions below, 'abcdefgh' refers to above data (sometimes simply
-; abbreviated as a[N-M]). 'stuvwxyz' refers to left data (sometimes simply
-; abbreviated as l[N-M]). * is top-left data. ABCDEFG or A[N-M] is filtered
-; above data, STUVWXYZ or L[N-M] is filtered left data, and # is filtered
-; top-left data.
-
-; left=(left+2*center+right+2)>>2
-%macro LOWPASS 3 ; left [dst], center, right
-    paddw                  m%1, m%3
-    psraw                  m%1, 1
-    pavgw                  m%1, m%2
-%endmacro
-
-; abcdefgh (src) -> bcdefghh (dst)
-; dst/src can be the same register
-%macro SHIFT_RIGHT 2-3 [pb_2to15_14_15] ; dst, src, [ssse3_shift_reg]
-%if cpuflag(ssse3)
-    pshufb                  %1, %2, %3              ; abcdefgh -> bcdefghh
-%else
-    psrldq                  %1, %2, 2               ; abcdefgh -> bcdefgh.
-    pshufhw                 %1, %1, q2210           ; bcdefgh. -> bcdefghh
-%endif
-%endmacro
-
-; abcdefgh (src) -> bcdefghh (dst1) and cdefghhh (dst2)
-%macro SHIFT_RIGHTx2 3-4 [pb_2to15_14_15] ; dst1, dst2, src, [ssse3_shift_reg]
-%if cpuflag(ssse3)
-    pshufb                  %1, %3, %4              ; abcdefgh -> bcdefghh
-    pshufb                  %2, %1, %4              ; bcdefghh -> cdefghhh
-%else
-    psrldq                  %1, %3, 2               ; abcdefgh -> bcdefgh.
-    psrldq                  %2, %3, 4               ; abcdefgh -> cdefgh..
-    pshufhw                 %1, %1, q2210           ; bcdefgh. -> bcdefghh
-    pshufhw                 %2, %2, q1110           ; cdefgh.. -> cdefghhh
-%endif
-%endmacro
-
-%macro DL_FUNCS 0
-cglobal vp9_ipred_dl_4x4_16, 2, 4, 3, dst, stride, l, a
-    movifnidn               aq, amp
-    movu                    m1, [aq]                ; abcdefgh
-    pshufhw                 m0, m1, q3310           ; abcdefhh
-    SHIFT_RIGHT             m1, m1                  ; bcdefghh
-    psrldq                  m2, m1, 2               ; cdefghh.
-    LOWPASS                  0,  1,  2              ; BCDEFGh.
-    pshufd                  m1, m0, q3321           ; DEFGh...
-    movh      [dstq+strideq*0], m0
-    movh      [dstq+strideq*2], m1
-    add                   dstq, strideq
-    psrldq                  m0, 2                   ; CDEFGh..
-    psrldq                  m1, 2                   ; EFGh....
-    movh      [dstq+strideq*0], m0
-    movh      [dstq+strideq*2], m1
-    RET
-
-cglobal vp9_ipred_dl_8x8_16, 2, 4, 5, dst, stride, l, a
-    movifnidn               aq, amp
-    mova                    m0, [aq]                ; abcdefgh
-%if cpuflag(ssse3)
-    mova                    m4, [pb_2to15_14_15]
-%endif
-    SHIFT_RIGHTx2           m1, m2, m0, m4          ; bcdefghh/cdefghhh
-    LOWPASS                  0,  1,  2              ; BCDEFGHh
-    shufps                  m1, m0, m2, q3332       ; FGHhhhhh
-    shufps                  m3, m0, m1, q2121       ; DEFGHhhh
-    DEFINE_ARGS dst, stride, stride5
-    lea               stride5q, [strideq*5]
-
-    mova      [dstq+strideq*0], m0
-    mova      [dstq+strideq*4], m1
-    SHIFT_RIGHT             m0, m0, m4              ; CDEFGHhh
-    pshuflw                 m1, m1, q3321           ; GHhhhhhh
-    pshufd                  m2, m0, q3321           ; EFGHhhhh
-    mova      [dstq+strideq*1], m0
-    mova      [dstq+stride5q ], m1
-    lea                   dstq, [dstq+strideq*2]
-    pshuflw                 m1, m1, q3321           ; Hhhhhhhh
-    mova      [dstq+strideq*0], m3
-    mova      [dstq+strideq*4], m1
-    pshuflw                 m1, m1, q3321           ; hhhhhhhh
-    mova      [dstq+strideq*1], m2
-    mova      [dstq+stride5q ], m1
-    RET
-
-cglobal vp9_ipred_dl_16x16_16, 2, 4, 5, dst, stride, l, a
-    movifnidn               aq, amp
-    mova                    m0, [aq]                ; abcdefgh
-    mova                    m3, [aq+mmsize]         ; ijklmnop
-    PALIGNR                 m1, m3, m0, 2, m4       ; bcdefghi
-    PALIGNR                 m2, m3, m0, 4, m4       ; cdefghij
-    LOWPASS                  0,  1,  2              ; BCDEFGHI
-%if cpuflag(ssse3)
-    mova                    m4, [pb_2to15_14_15]
-%endif
-    SHIFT_RIGHTx2           m2, m1, m3, m4          ; jklmnopp/klmnoppp
-    LOWPASS                  1,  2,  3              ; JKLMNOPp
-    pshufd                  m2, m2, q3333           ; pppppppp
-    DEFINE_ARGS dst, stride, cnt
-    mov                   cntd, 8
-
-.loop:
-    mova   [dstq+strideq*0+ 0], m0
-    mova   [dstq+strideq*0+16], m1
-    mova   [dstq+strideq*8+ 0], m1
-    mova   [dstq+strideq*8+16], m2
-    add                   dstq, strideq
-%if cpuflag(avx)
-    vpalignr                m0, m1, m0, 2
-%else
-    PALIGNR                 m3, m1, m0, 2, m4
-    mova                    m0, m3
-%endif
-    SHIFT_RIGHT             m1, m1, m4
-    dec                   cntd
-    jg .loop
-    RET
-
-cglobal vp9_ipred_dl_32x32_16, 2, 5, 7, dst, stride, l, a
-    movifnidn               aq, amp
-    mova                    m0, [aq+mmsize*0]       ; abcdefgh
-    mova                    m1, [aq+mmsize*1]       ; ijklmnop
-    mova                    m2, [aq+mmsize*2]       ; qrstuvwx
-    mova                    m3, [aq+mmsize*3]       ; yz012345
-    PALIGNR                 m4, m1, m0, 2, m6
-    PALIGNR                 m5, m1, m0, 4, m6
-    LOWPASS                  0,  4,  5              ; BCDEFGHI
-    PALIGNR                 m4, m2, m1, 2, m6
-    PALIGNR                 m5, m2, m1, 4, m6
-    LOWPASS                  1,  4,  5              ; JKLMNOPQ
-    PALIGNR                 m4, m3, m2, 2, m6
-    PALIGNR                 m5, m3, m2, 4, m6
-    LOWPASS                  2,  4,  5              ; RSTUVWXY
-%if cpuflag(ssse3)
-    mova                    m6, [pb_2to15_14_15]
-%endif
-    SHIFT_RIGHTx2           m4, m5, m3, m6
-    LOWPASS                  3,  4,  5              ; Z0123455
-    pshufd                  m4, m4, q3333           ; 55555555
-    DEFINE_ARGS dst, stride, stride8, stride24, cnt
-    mov                   cntd, 8
-    lea               stride8q, [strideq*8]
-    lea              stride24q, [stride8q*3]
-
-.loop:
-    mova  [dstq+stride8q*0+ 0], m0
-    mova  [dstq+stride8q*0+16], m1
-    mova  [dstq+stride8q*0+32], m2
-    mova  [dstq+stride8q*0+48], m3
-    mova  [dstq+stride8q*1+ 0], m1
-    mova  [dstq+stride8q*1+16], m2
-    mova  [dstq+stride8q*1+32], m3
-    mova  [dstq+stride8q*1+48], m4
-    mova  [dstq+stride8q*2+ 0], m2
-    mova  [dstq+stride8q*2+16], m3
-    mova  [dstq+stride8q*2+32], m4
-    mova  [dstq+stride8q*2+48], m4
-    mova  [dstq+stride24q + 0], m3
-    mova  [dstq+stride24q +16], m4
-    mova  [dstq+stride24q +32], m4
-    mova  [dstq+stride24q +48], m4
-    add                   dstq, strideq
-%if cpuflag(avx)
-    vpalignr                m0, m1, m0, 2
-    vpalignr                m1, m2, m1, 2
-    vpalignr                m2, m3, m2, 2
-%else
-    PALIGNR                 m5, m1, m0, 2, m6
-    mova                    m0, m5
-    PALIGNR                 m5, m2, m1, 2, m6
-    mova                    m1, m5
-    PALIGNR                 m5, m3, m2, 2, m6
-    mova                    m2, m5
-%endif
-    SHIFT_RIGHT             m3, m3, m6
-    dec                   cntd
-    jg .loop
-    RET
-%endmacro
-
-INIT_XMM sse2
-DL_FUNCS
-INIT_XMM ssse3
-DL_FUNCS
-INIT_XMM avx
-DL_FUNCS
-
-%if HAVE_AVX2_EXTERNAL
-INIT_YMM avx2
-cglobal vp9_ipred_dl_16x16_16, 2, 4, 5, dst, stride, l, a
-    movifnidn               aq, amp
-    mova                    m0, [aq]                   ; abcdefghijklmnop
-    vpbroadcastw           xm1, [aq+30]                ; pppppppp
-    vperm2i128              m2, m0, m1, q0201          ; ijklmnoppppppppp
-    vpalignr                m3, m2, m0, 2              ; bcdefghijklmnopp
-    vpalignr                m4, m2, m0, 4              ; cdefghijklmnoppp
-    LOWPASS                  0,  3,  4                 ; BCDEFGHIJKLMNOPp
-    vperm2i128              m2, m0, m1, q0201          ; JKLMNOPppppppppp
-    DEFINE_ARGS dst, stride, stride3, cnt
-    mov                   cntd, 2
-    lea               stride3q, [strideq*3]
-
-.loop:
-    mova      [dstq+strideq*0], m0
-    vpalignr                m3, m2, m0, 2
-    vpalignr                m4, m2, m0, 4
-    mova      [dstq+strideq*1], m3
-    mova      [dstq+strideq*2], m4
-    vpalignr                m3, m2, m0, 6
-    vpalignr                m4, m2, m0, 8
-    mova      [dstq+stride3q ], m3
-    lea                   dstq, [dstq+strideq*4]
-    mova      [dstq+strideq*0], m4
-    vpalignr                m3, m2, m0, 10
-    vpalignr                m4, m2, m0, 12
-    mova      [dstq+strideq*1], m3
-    mova      [dstq+strideq*2], m4
-    vpalignr                m3, m2, m0, 14
-    mova      [dstq+stride3q ], m3
-    lea                   dstq, [dstq+strideq*4]
-    mova                    m0, m2
-    vperm2i128              m2, m2, m2, q0101          ; pppppppppppppppp
-    dec                   cntd
-    jg .loop
-    RET
-
-cglobal vp9_ipred_dl_32x32_16, 2, 6, 7, dst, stride, l, a
-    movifnidn               aq, amp
-    mova                    m0, [aq+mmsize*0+ 0]       ; abcdefghijklmnop
-    mova                    m1, [aq+mmsize*1+ 0]       ; qrstuvwxyz012345
-    vpbroadcastw           xm4, [aq+mmsize*1+30]       ; 55555555
-    vperm2i128              m5, m0, m1, q0201          ; ijklmnopqrstuvwx
-    vpalignr                m2, m5, m0, 2              ; bcdefghijklmnopq
-    vpalignr                m3, m5, m0, 4              ; cdefghijklmnopqr
-    LOWPASS                  0,  2,  3                 ; BCDEFGHIJKLMNOPQ
-    vperm2i128              m5, m1, m4, q0201          ; yz01234555555555
-    vpalignr                m2, m5, m1, 2              ; rstuvwxyz0123455
-    vpalignr                m3, m5, m1, 4              ; stuvwxyz01234555
-    LOWPASS                  1,  2,  3                 ; RSTUVWXYZ......5
-    vperm2i128              m2, m1, m4, q0201          ; Z......555555555
-    vperm2i128              m5, m0, m1, q0201          ; JKLMNOPQRSTUVWXY
-    DEFINE_ARGS dst, stride, stride3, cnt
-    lea               stride3q, [strideq*3]
-    mov                   cntd, 4
-
-.loop:
-    mova   [dstq+strideq*0 + 0], m0
-    mova   [dstq+strideq*0 +32], m1
-    vpalignr                 m3, m5, m0, 2
-    vpalignr                 m4, m2, m1, 2
-    mova   [dstq+strideq*1 + 0], m3
-    mova   [dstq+strideq*1 +32], m4
-    vpalignr                 m3, m5, m0, 4
-    vpalignr                 m4, m2, m1, 4
-    mova   [dstq+strideq*2 + 0], m3
-    mova   [dstq+strideq*2 +32], m4
-    vpalignr                 m3, m5, m0, 6
-    vpalignr                 m4, m2, m1, 6
-    mova   [dstq+stride3q*1+ 0], m3
-    mova   [dstq+stride3q*1+32], m4
-    lea                    dstq, [dstq+strideq*4]
-    vpalignr                 m3, m5, m0, 8
-    vpalignr                 m4, m2, m1, 8
-    mova   [dstq+strideq*0 + 0], m3
-    mova   [dstq+strideq*0 +32], m4
-    vpalignr                 m3, m5, m0, 10
-    vpalignr                 m4, m2, m1, 10
-    mova   [dstq+strideq*1 + 0], m3
-    mova   [dstq+strideq*1 +32], m4
-    vpalignr                 m3, m5, m0, 12
-    vpalignr                 m4, m2, m1, 12
-    mova   [dstq+strideq*2+ 0], m3
-    mova   [dstq+strideq*2+32], m4
-    vpalignr                 m3, m5, m0, 14
-    vpalignr                 m4, m2, m1, 14
-    mova   [dstq+stride3q+  0], m3
-    mova   [dstq+stride3q+ 32], m4
-    vpalignr                 m3, m5, m0, 16
-    vpalignr                 m4, m2, m1, 16
-    vperm2i128               m5, m3, m4, q0201
-    vperm2i128               m2, m4, m4, q0101
-    mova                     m0, m3
-    mova                     m1, m4
-    lea                    dstq, [dstq+strideq*4]
-    dec                    cntd
-    jg .loop
-    RET
-%endif
-
-%macro DR_FUNCS 1 ; stack_mem_for_32x32_32bit_function
-cglobal vp9_ipred_dr_4x4_16, 4, 4, 3, dst, stride, l, a
-    movh                    m0, [lq]                ; wxyz....
-    movhps                  m0, [aq-2]              ; wxyz*abc
-    movd                    m1, [aq+6]              ; d.......
-    PALIGNR                 m1, m0, 2, m2           ; xyz*abcd
-    psrldq                  m2, m1, 2               ; yz*abcd.
-    LOWPASS                  0, 1, 2                ; XYZ#ABC.
-    DEFINE_ARGS dst, stride, stride3
-    lea               stride3q, [strideq*3]
-
-    movh      [dstq+stride3q ], m0
-    psrldq                  m0, 2                   ; YZ#ABC..
-    movh      [dstq+strideq*2], m0
-    psrldq                  m0, 2                   ; Z#ABC...
-    movh      [dstq+strideq*1], m0
-    psrldq                  m0, 2                   ; #ABC....
-    movh      [dstq+strideq*0], m0
-    RET
-
-cglobal vp9_ipred_dr_8x8_16, 4, 4, 5, dst, stride, l, a
-    mova                    m0, [lq]                ; stuvwxyz
-    movu                    m1, [aq-2]              ; *abcdefg
-    mova                    m2, [aq]                ; abcdefgh
-    psrldq                  m3, m2, 2               ; bcdefgh.
-    LOWPASS                  3,  2, 1               ; ABCDEFG.
-    PALIGNR                 m1, m0, 2, m4           ; tuvwxyz*
-    PALIGNR                 m2, m1, 2, m4           ; uvwxyz*a
-    LOWPASS                  2,  1, 0               ; TUVWXYZ#
-    DEFINE_ARGS dst, stride, dst4, stride3
-    lea               stride3q, [strideq*3]
-    lea                  dst4q, [dstq+strideq*4]
-
-    movhps [dstq +stride3q +0], m2
-    movh   [dstq+ stride3q +8], m3
-    mova   [dst4q+stride3q +0], m2
-    PALIGNR                 m1, m3, m2, 2, m0
-    psrldq                  m3, 2
-    movhps [dstq +strideq*2+0], m1
-    movh   [dstq+ strideq*2+8], m3
-    mova   [dst4q+strideq*2+0], m1
-    PALIGNR                 m2, m3, m1, 2, m0
-    psrldq                  m3, 2
-    movhps [dstq +strideq*1+0], m2
-    movh   [dstq+ strideq*1+8], m3
-    mova   [dst4q+strideq*1+0], m2
-    PALIGNR                 m1, m3, m2, 2, m0
-    psrldq                  m3, 2
-    movhps [dstq +strideq*0+0], m1
-    movh   [dstq+ strideq*0+8], m3
-    mova   [dst4q+strideq*0+0], m1
-    RET
-
-cglobal vp9_ipred_dr_16x16_16, 4, 4, 7, dst, stride, l, a
-    mova                    m0, [lq]                ; klmnopqr
-    mova                    m1, [lq+mmsize]         ; stuvwxyz
-    movu                    m2, [aq-2]              ; *abcdefg
-    movu                    m3, [aq+mmsize-2]       ; hijklmno
-    mova                    m4, [aq]                ; abcdefgh
-    mova                    m5, [aq+mmsize]         ; ijklmnop
-    psrldq                  m6, m5, 2               ; jklmnop.
-    LOWPASS                  6,  5, 3               ; IJKLMNO.
-    PALIGNR                 m5, m4, 2, m3           ; bcdefghi
-    LOWPASS                  5,  4, 2               ; ABCDEFGH
-    PALIGNR                 m2, m1, 2, m3           ; tuvwxyz*
-    PALIGNR                 m4, m2, 2, m3           ; uvwxyz*a
-    LOWPASS                  4,  2, 1               ; TUVWXYZ#
-    PALIGNR                 m1, m0, 2, m3           ; lmnopqrs
-    PALIGNR                 m2, m1, 2, m3           ; mnopqrst
-    LOWPASS                  2, 1, 0                ; LMNOPQRS
-    DEFINE_ARGS dst, stride, dst8, cnt
-    lea                  dst8q, [dstq+strideq*8]
-    mov                   cntd, 8
-
-.loop:
-    sub                  dst8q, strideq
-    mova  [dst8q+strideq*0+ 0], m4
-    mova  [dst8q+strideq*0+16], m5
-    mova  [dst8q+strideq*8+ 0], m2
-    mova  [dst8q+strideq*8+16], m4
-%if cpuflag(avx)
-    vpalignr                m2, m4, m2, 2
-    vpalignr                m4, m5, m4, 2
-    vpalignr                m5, m6, m5, 2
-%else
-    PALIGNR                 m0, m4, m2, 2, m1
-    mova                    m2, m0
-    PALIGNR                 m0, m5, m4, 2, m1
-    mova                    m4, m0
-    PALIGNR                 m0, m6, m5, 2, m1
-    mova                    m5, m0
-%endif
-    psrldq                  m6, 2
-    dec                   cntd
-    jg .loop
-    RET
-
-cglobal vp9_ipred_dr_32x32_16, 4, 5, 10 + notcpuflag(ssse3), \
-                               %1 * ARCH_X86_32 * -mmsize, dst, stride, l, a
-    mova                    m0, [aq+mmsize*3]       ; a[24-31]
-    movu                    m1, [aq+mmsize*3-2]     ; a[23-30]
-    psrldq                  m2, m0, 2               ; a[25-31].
-    LOWPASS                  2,  0, 1               ; A[24-30].
-    mova                    m1, [aq+mmsize*2]       ; a[16-23]
-    movu                    m3, [aq+mmsize*2-2]     ; a[15-22]
-    PALIGNR                 m0, m1, 2, m4           ; a[17-24]
-    LOWPASS                  0,  1, 3               ; A[16-23]
-    mova                    m3, [aq+mmsize*1]       ; a[8-15]
-    movu                    m4, [aq+mmsize*1-2]     ; a[7-14]
-    PALIGNR                 m1, m3, 2, m5           ; a[9-16]
-    LOWPASS                  1,  3, 4               ; A[8-15]
-    mova                    m4, [aq+mmsize*0]       ; a[0-7]
-    movu                    m5, [aq+mmsize*0-2]     ; *a[0-6]
-    PALIGNR                 m3, m4, 2, m6           ; a[1-8]
-    LOWPASS                  3,  4, 5               ; A[0-7]
-    SCRATCH                  1,  8, rsp+0*mmsize
-    SCRATCH                  3,  9, rsp+1*mmsize
-%if notcpuflag(ssse3)
-    SCRATCH                  0, 10, rsp+2*mmsize
-%endif
-    mova                    m6, [lq+mmsize*3]       ; l[24-31]
-    PALIGNR                 m5, m6, 2, m0           ; l[25-31]*
-    PALIGNR                 m4, m5, 2, m0           ; l[26-31]*a
-    LOWPASS                  4,  5, 6               ; L[25-31]#
-    mova                    m7, [lq+mmsize*2]       ; l[16-23]
-    PALIGNR                 m6, m7, 2, m0           ; l[17-24]
-    PALIGNR                 m5, m6, 2, m0           ; l[18-25]
-    LOWPASS                  5,  6, 7               ; L[17-24]
-    mova                    m1, [lq+mmsize*1]       ; l[8-15]
-    PALIGNR                 m7, m1, 2, m0           ; l[9-16]
-    PALIGNR                 m6, m7, 2, m0           ; l[10-17]
-    LOWPASS                  6,  7, 1               ; L[9-16]
-    mova                    m3, [lq+mmsize*0]       ; l[0-7]
-    PALIGNR                 m1, m3, 2, m0           ; l[1-8]
-    PALIGNR                 m7, m1, 2, m0           ; l[2-9]
-    LOWPASS                  7,  1, 3               ; L[1-8]
-%if cpuflag(ssse3)
-%if cpuflag(avx)
-    UNSCRATCH                1,  8, rsp+0*mmsize
-%endif
-    UNSCRATCH                3,  9, rsp+1*mmsize
-%else
-    UNSCRATCH                0, 10, rsp+2*mmsize
-%endif
-    DEFINE_ARGS dst8, stride, stride8, stride24, cnt
-    lea               stride8q, [strideq*8]
-    lea              stride24q, [stride8q*3]
-    lea                  dst8q, [dst8q+strideq*8]
-    mov                   cntd, 8
-
-.loop:
-    sub                  dst8q, strideq
-%if notcpuflag(avx)
-    UNSCRATCH                1,  8, rsp+0*mmsize
-%if notcpuflag(ssse3)
-    UNSCRATCH                3,  9, rsp+1*mmsize
-%endif
-%endif
-    mova [dst8q+stride8q*0+ 0], m4
-    mova [dst8q+stride8q*0+16], m3
-    mova [dst8q+stride8q*0+32], m1
-    mova [dst8q+stride8q*0+48], m0
-    mova [dst8q+stride8q*1+ 0], m5
-    mova [dst8q+stride8q*1+16], m4
-    mova [dst8q+stride8q*1+32], m3
-    mova [dst8q+stride8q*1+48], m1
-    mova [dst8q+stride8q*2+ 0], m6
-    mova [dst8q+stride8q*2+16], m5
-    mova [dst8q+stride8q*2+32], m4
-    mova [dst8q+stride8q*2+48], m3
-    mova [dst8q+stride24q + 0], m7
-    mova [dst8q+stride24q +16], m6
-    mova [dst8q+stride24q +32], m5
-    mova [dst8q+stride24q +48], m4
-%if cpuflag(avx)
-    vpalignr                m7, m6, m7, 2
-    vpalignr                m6, m5, m6, 2
-    vpalignr                m5, m4, m5, 2
-    vpalignr                m4, m3, m4, 2
-    vpalignr                m3, m1, m3, 2
-    vpalignr                m1, m0, m1, 2
-    vpalignr                m0, m2, m0, 2
-%else
-    SCRATCH                  2,  8, rsp+0*mmsize
-%if notcpuflag(ssse3)
-    SCRATCH                  0,  9, rsp+1*mmsize
-%endif
-    PALIGNR                 m2, m6, m7, 2, m0
-    mova                    m7, m2
-    PALIGNR                 m2, m5, m6, 2, m0
-    mova                    m6, m2
-    PALIGNR                 m2, m4, m5, 2, m0
-    mova                    m5, m2
-    PALIGNR                 m2, m3, m4, 2, m0
-    mova                    m4, m2
-    PALIGNR                 m2, m1, m3, 2, m0
-    mova                    m3, m2
-%if notcpuflag(ssse3)
-    UNSCRATCH                0,  9, rsp+1*mmsize
-    SCRATCH                  3,  9, rsp+1*mmsize
-%endif
-    PALIGNR                 m2, m0, m1, 2, m3
-    mova                    m1, m2
-    UNSCRATCH                2,  8, rsp+0*mmsize
-    SCRATCH                  1,  8, rsp+0*mmsize
-    PALIGNR                 m1, m2, m0, 2, m3
-    mova                    m0, m1
-%endif
-    psrldq                  m2, 2
-    dec                   cntd
-    jg .loop
-    RET
-%endmacro
-
-INIT_XMM sse2
-DR_FUNCS 3
-INIT_XMM ssse3
-DR_FUNCS 2
-INIT_XMM avx
-DR_FUNCS 2
-
-%if HAVE_AVX2_EXTERNAL
-INIT_YMM avx2
-cglobal vp9_ipred_dr_16x16_16, 4, 5, 6, dst, stride, l, a
-    mova                    m0, [lq]                   ; klmnopqrstuvwxyz
-    movu                    m1, [aq-2]                 ; *abcdefghijklmno
-    mova                    m2, [aq]                   ; abcdefghijklmnop
-    vperm2i128              m4, m2, m2, q2001          ; ijklmnop........
-    vpalignr                m5, m4, m2, 2              ; bcdefghijklmnop.
-    vperm2i128              m3, m0, m1, q0201          ; stuvwxyz*abcdefg
-    LOWPASS                  1,  2,  5                 ; ABCDEFGHIJKLMNO.
-    vpalignr                m4, m3, m0, 2              ; lmnopqrstuvwxyz*
-    vpalignr                m5, m3, m0, 4              ; mnopqrstuvwxyz*a
-    LOWPASS                  0,  4,  5                 ; LMNOPQRSTUVWXYZ#
-    vperm2i128              m5, m0, m1, q0201          ; TUVWXYZ#ABCDEFGH
-    DEFINE_ARGS dst, stride, stride3, stride5, dst3
-    lea                  dst3q, [dstq+strideq*4]
-    lea               stride3q, [strideq*3]
-    lea               stride5q, [stride3q+strideq*2]
-
-    vpalignr                m3, m5, m0, 2
-    vpalignr                m4, m1, m5, 2
-    mova    [dst3q+stride5q*2], m3                     ; 14
-    mova    [ dstq+stride3q*2], m4                     ; 6
-    vpalignr                m3, m5, m0, 4
-    vpalignr                m4, m1, m5, 4
-    sub                  dst3q, strideq
-    mova    [dst3q+stride5q*2], m3                     ; 13
-    mova    [dst3q+strideq*2 ], m4                     ; 5
-    mova    [dst3q+stride3q*4], m0                     ; 15
-    vpalignr                m3, m5, m0, 6
-    vpalignr                m4, m1, m5, 6
-    mova     [dstq+stride3q*4], m3                     ; 12
-    mova     [dst3q+strideq*1], m4                     ; 4
-    vpalignr                m3, m5, m0, 8
-    vpalignr                m4, m1, m5, 8
-    mova     [dst3q+strideq*8], m3                     ; 11
-    mova     [dst3q+strideq*0], m4                     ; 3
-    vpalignr                m3, m5, m0, 10
-    vpalignr                m4, m1, m5, 10
-    mova     [dstq+stride5q*2], m3                     ; 10
-    mova     [dstq+strideq*2 ], m4                     ; 2
-    vpalignr                m3, m5, m0, 12
-    vpalignr                m4, m1, m5, 12
-    mova    [dst3q+stride3q*2], m3                     ; 9
-    mova     [dstq+strideq*1 ], m4                     ; 1
-    vpalignr                m3, m5, m0, 14
-    vpalignr                m4, m1, m5, 14
-    mova      [dstq+strideq*8], m3                     ; 8
-    mova      [dstq+strideq*0], m4                     ; 0
-    mova     [dst3q+strideq*4], m5                     ; 7
-    RET
-
-%if ARCH_X86_64
-cglobal vp9_ipred_dr_32x32_16, 4, 7, 10, dst, stride, l, a
-    mova                    m0, [lq+mmsize*0+0]        ; l[0-15]
-    mova                    m1, [lq+mmsize*1+0]        ; l[16-31]
-    movu                    m2, [aq+mmsize*0-2]        ; *abcdefghijklmno
-    mova                    m3, [aq+mmsize*0+0]        ; abcdefghijklmnop
-    mova                    m4, [aq+mmsize*1+0]        ; qrstuvwxyz012345
-    vperm2i128              m5, m0, m1, q0201          ; lmnopqrstuvwxyz0
-    vpalignr                m6, m5, m0, 2              ; mnopqrstuvwxyz01
-    vpalignr                m7, m5, m0, 4              ; nopqrstuvwxyz012
-    LOWPASS                  0,  6,  7                 ; L[0-15]
-    vperm2i128              m7, m1, m2, q0201          ; stuvwxyz*abcdefg
-    vpalignr                m5, m7, m1, 2              ; lmnopqrstuvwxyz*
-    vpalignr                m6, m7, m1, 4              ; mnopqrstuvwxyz*a
-    LOWPASS                  1,  5,  6                 ; L[16-31]#
-    vperm2i128              m5, m3, m4, q0201          ; ijklmnopqrstuvwx
-    vpalignr                m6, m5, m3, 2              ; bcdefghijklmnopq
-    LOWPASS                  2,  3,  6                 ; A[0-15]
-    movu                    m3, [aq+mmsize*1-2]        ; pqrstuvwxyz01234
-    vperm2i128              m6, m4, m4, q2001          ; yz012345........
-    vpalignr                m7, m6, m4, 2              ; rstuvwxyz012345.
-    LOWPASS                  3,  4,  7                 ; A[16-31].
-    vperm2i128              m4, m1, m2, q0201          ; TUVWXYZ#ABCDEFGH
-    vperm2i128              m5, m0, m1, q0201          ; L[7-15]L[16-23]
-    vperm2i128              m8, m2, m3, q0201          ; IJKLMNOPQRSTUVWX
-    DEFINE_ARGS dst8, stride, stride3, stride7, stride5, dst24, cnt
-    lea               stride3q, [strideq*3]
-    lea               stride5q, [stride3q+strideq*2]
-    lea               stride7q, [strideq*4+stride3q]
-    lea                 dst24q, [dst8q+stride3q*8]
-    lea                  dst8q, [dst8q+strideq*8]
-    mov                   cntd, 2
-
-.loop:
-    mova  [dst24q+stride7q+0 ], m0                     ; 31 23 15 7
-    mova  [dst24q+stride7q+32], m1
-    mova    [dst8q+stride7q+0], m1
-    mova   [dst8q+stride7q+32], m2
-    vpalignr                m6, m4, m1, 2
-    vpalignr                m7, m5, m0, 2
-    vpalignr                m9, m8, m2, 2
-    mova [dst24q+stride3q*2+0], m7                     ; 30 22 14 6
-    mova [dst24q+stride3q*2+32], m6
-    mova  [dst8q+stride3q*2+0], m6
-    mova [dst8q+stride3q*2+32], m9
-    vpalignr                m6, m4, m1, 4
-    vpalignr                m7, m5, m0, 4
-    vpalignr                m9, m8, m2, 4
-    mova   [dst24q+stride5q+0], m7                     ; 29 21 13 5
-    mova  [dst24q+stride5q+32], m6
-    mova    [dst8q+stride5q+0], m6
-    mova   [dst8q+stride5q+32], m9
-    vpalignr                m6, m4, m1, 6
-    vpalignr                m7, m5, m0, 6
-    vpalignr                m9, m8, m2, 6
-    mova [dst24q+strideq*4+0 ], m7                     ; 28 20 12 4
-    mova [dst24q+strideq*4+32], m6
-    mova   [dst8q+strideq*4+0], m6
-    mova  [dst8q+strideq*4+32], m9
-    vpalignr                m6, m4, m1, 8
-    vpalignr                m7, m5, m0, 8
-    vpalignr                m9, m8, m2, 8
-    mova  [dst24q+stride3q+0 ], m7                     ; 27 19 11 3
-    mova  [dst24q+stride3q+32], m6
-    mova    [dst8q+stride3q+0], m6
-    mova   [dst8q+stride3q+32], m9
-    vpalignr                m6, m4, m1, 10
-    vpalignr                m7, m5, m0, 10
-    vpalignr                m9, m8, m2, 10
-    mova [dst24q+strideq*2+0 ], m7                     ; 26 18 10 2
-    mova [dst24q+strideq*2+32], m6
-    mova   [dst8q+strideq*2+0], m6
-    mova  [dst8q+strideq*2+32], m9
-    vpalignr                m6, m4, m1, 12
-    vpalignr                m7, m5, m0, 12
-    vpalignr                m9, m8, m2, 12
-    mova   [dst24q+strideq+0 ], m7                     ; 25 17 9 1
-    mova   [dst24q+strideq+32], m6
-    mova     [dst8q+strideq+0], m6
-    mova    [dst8q+strideq+32], m9
-    vpalignr                m6, m4, m1, 14
-    vpalignr                m7, m5, m0, 14
-    vpalignr                m9, m8, m2, 14
-    mova [dst24q+strideq*0+0 ], m7                     ; 24 16 8 0
-    mova [dst24q+strideq*0+32], m6
-    mova   [dst8q+strideq*0+0], m6
-    mova  [dst8q+strideq*0+32], m9
-    mova                    m0, m5
-    mova                    m5, m1
-    mova                    m1, m4
-    mova                    m4, m2
-    mova                    m2, m8
-    mova                    m8, m3
-    sub                 dst24q, stride7q
-    sub                 dst24q, strideq
-    sub                  dst8q, stride7q
-    sub                  dst8q, strideq
-    dec                   cntd
-    jg .loop
-    RET
-%endif
-%endif
-
-%macro VL_FUNCS 1 ; stack_mem_for_32x32_32bit_function
-cglobal vp9_ipred_vl_4x4_16, 2, 4, 3, dst, stride, l, a
-    movifnidn               aq, amp
-    movu                    m0, [aq]                ; abcdefgh
-    psrldq                  m1, m0, 2               ; bcdefgh.
-    psrldq                  m2, m0, 4               ; cdefgh..
-    LOWPASS                  2,  1, 0               ; BCDEFGH.
-    pavgw                   m1, m0                  ; ABCDEFG.
-    DEFINE_ARGS dst, stride, stride3
-    lea               stride3q, [strideq*3]
-
-    movh      [dstq+strideq*0], m1
-    movh      [dstq+strideq*1], m2
-    psrldq                  m1, 2
-    psrldq                  m2, 2
-    movh      [dstq+strideq*2], m1
-    movh      [dstq+stride3q ], m2
-    RET
-
-cglobal vp9_ipred_vl_8x8_16, 2, 4, 4, dst, stride, l, a
-    movifnidn               aq, amp
-    mova                    m0, [aq]                ; abcdefgh
-%if cpuflag(ssse3)
-    mova                    m3, [pb_2to15_14_15]
-%endif
-    SHIFT_RIGHTx2           m1, m2, m0, m3          ; bcdefghh/cdefghhh
-    LOWPASS                  2,  1, 0               ; BCDEFGHh
-    pavgw                   m1, m0                  ; ABCDEFGh
-    DEFINE_ARGS dst, stride, stride3
-    lea               stride3q, [strideq*3]
-
-    mova      [dstq+strideq*0], m1
-    mova      [dstq+strideq*1], m2
-    SHIFT_RIGHT             m1, m1, m3
-    SHIFT_RIGHT             m2, m2, m3
-    mova      [dstq+strideq*2], m1
-    mova      [dstq+stride3q ], m2
-    lea                   dstq, [dstq+strideq*4]
-    SHIFT_RIGHT             m1, m1, m3
-    SHIFT_RIGHT             m2, m2, m3
-    mova      [dstq+strideq*0], m1
-    mova      [dstq+strideq*1], m2
-    SHIFT_RIGHT             m1, m1, m3
-    SHIFT_RIGHT             m2, m2, m3
-    mova      [dstq+strideq*2], m1
-    mova      [dstq+stride3q ], m2
-    RET
-
-cglobal vp9_ipred_vl_16x16_16, 2, 4, 6, dst, stride, l, a
-    movifnidn               aq, amp
-    mova                    m0, [aq]
-    mova                    m1, [aq+mmsize]
-    PALIGNR                 m2, m1, m0, 2, m3
-    PALIGNR                 m3, m1, m0, 4, m4
-    LOWPASS                  3,  2,  0
-    pavgw                   m2, m0
-%if cpuflag(ssse3)
-    mova                    m4, [pb_2to15_14_15]
-%endif
-    SHIFT_RIGHTx2           m5, m0, m1, m4
-    LOWPASS                  0,  5,  1
-    pavgw                   m1, m5
-    DEFINE_ARGS dst, stride, cnt
-    mov                   cntd, 8
-
-.loop:
-    mova   [dstq+strideq*0+ 0], m2
-    mova   [dstq+strideq*0+16], m1
-    mova   [dstq+strideq*1+ 0], m3
-    mova   [dstq+strideq*1+16], m0
-    lea                   dstq, [dstq+strideq*2]
-%if cpuflag(avx)
-    vpalignr                m2, m1, m2, 2
-    vpalignr                m3, m0, m3, 2
-%else
-    PALIGNR                 m5, m1, m2, 2, m4
-    mova                    m2, m5
-    PALIGNR                 m5, m0, m3, 2, m4
-    mova                    m3, m5
-%endif
-    SHIFT_RIGHT             m1, m1, m4
-    SHIFT_RIGHT             m0, m0, m4
-    dec                   cntd
-    jg .loop
-    RET
-
-cglobal vp9_ipred_vl_32x32_16, 2, 5, 11, %1 * mmsize * ARCH_X86_32, dst, stride, l, a
-    movifnidn               aq, amp
-    mova                    m0, [aq+mmsize*0]
-    mova                    m1, [aq+mmsize*1]
-    mova                    m2, [aq+mmsize*2]
-    PALIGNR                 m6, m1, m0, 2, m5
-    PALIGNR                 m7, m1, m0, 4, m5
-    LOWPASS                  7,  6,  0
-    pavgw                   m6, m0
-    SCRATCH                  6,  8, rsp+0*mmsize
-    PALIGNR                 m4, m2, m1, 2, m0
-    PALIGNR                 m5, m2, m1, 4, m0
-    LOWPASS                  5,  4,  1
-    pavgw                   m4, m1
-    mova                    m0, [aq+mmsize*3]
-    PALIGNR                 m1, m0, m2, 2, m6
-    PALIGNR                 m3, m0, m2, 4, m6
-    LOWPASS                  3,  1,  2
-    pavgw                   m2, m1
-%if cpuflag(ssse3)
-    PRELOAD                 10, pb_2to15_14_15, shuf
-%endif
-    SHIFT_RIGHTx2           m6, m1, m0, reg_shuf
-    LOWPASS                  1,  6,  0
-    pavgw                   m0, m6
-%if ARCH_X86_64
-    pshufd                  m9, m6, q3333
-%endif
-%if cpuflag(avx)
-    UNSCRATCH                6,  8, rsp+0*mmsize
-%endif
-    DEFINE_ARGS dst, stride, cnt, stride16, stride17
-    mov              stride16q, strideq
-    mov                   cntd, 8
-    shl              stride16q, 4
-    lea              stride17q, [stride16q+strideq]
-
-    ; FIXME m8 is unused for avx, so we could save one register here for win64
-.loop:
-%if notcpuflag(avx)
-    UNSCRATCH                6,  8, rsp+0*mmsize
-%endif
-    mova   [dstq+strideq*0+ 0], m6
-    mova   [dstq+strideq*0+16], m4
-    mova   [dstq+strideq*0+32], m2
-    mova   [dstq+strideq*0+48], m0
-    mova   [dstq+strideq*1+ 0], m7
-    mova   [dstq+strideq*1+16], m5
-    mova   [dstq+strideq*1+32], m3
-    mova   [dstq+strideq*1+48], m1
-    mova   [dstq+stride16q+ 0], m4
-    mova   [dstq+stride16q+16], m2
-    mova   [dstq+stride16q+32], m0
-%if ARCH_X86_64
-    mova   [dstq+stride16q+48], m9
-%endif
-    mova   [dstq+stride17q+ 0], m5
-    mova   [dstq+stride17q+16], m3
-    mova   [dstq+stride17q+32], m1
-%if ARCH_X86_64
-    mova   [dstq+stride17q+48], m9
-%endif
-    lea                   dstq, [dstq+strideq*2]
-%if cpuflag(avx)
-    vpalignr                m6, m4, m6, 2
-    vpalignr                m4, m2, m4, 2
-    vpalignr                m2, m0, m2, 2
-    vpalignr                m7, m5, m7, 2
-    vpalignr                m5, m3, m5, 2
-    vpalignr                m3, m1, m3, 2
-%else
-    SCRATCH                  3,  8, rsp+0*mmsize
-%if notcpuflag(ssse3)
-    SCRATCH                  1, 10, rsp+1*mmsize
-%endif
-    PALIGNR                 m3, m4, m6, 2, m1
-    mova                    m6, m3
-    PALIGNR                 m3, m2, m4, 2, m1
-    mova                    m4, m3
-    PALIGNR                 m3, m0, m2, 2, m1
-    mova                    m2, m3
-    PALIGNR                 m3, m5, m7, 2, m1
-    mova                    m7, m3
-    UNSCRATCH                3,  8, rsp+0*mmsize
-    SCRATCH                  6,  8, rsp+0*mmsize
-%if notcpuflag(ssse3)
-    UNSCRATCH                1, 10, rsp+1*mmsize
-    SCRATCH                  7, 10, rsp+1*mmsize
-%endif
-    PALIGNR                 m6, m3, m5, 2, m7
-    mova                    m5, m6
-    PALIGNR                 m6, m1, m3, 2, m7
-    mova                    m3, m6
-%if notcpuflag(ssse3)
-    UNSCRATCH                7, 10, rsp+1*mmsize
-%endif
-%endif
-    SHIFT_RIGHT             m1, m1, reg_shuf
-    SHIFT_RIGHT             m0, m0, reg_shuf
-    dec                   cntd
-    jg .loop
-
-%if ARCH_X86_32
-    DEFINE_ARGS dst, stride, stride3
-    lea               stride3q, [strideq*3]
-%assign %%n 0
-%rep 4
-    mova   [dstq+strideq*0+48], m0
-    mova   [dstq+strideq*1+48], m0
-    mova   [dstq+strideq*2+48], m0
-    mova   [dstq+stride3q +48], m0
-%if %%n < 3
-    lea                   dstq, [dstq+strideq*4]
-%endif
-%assign %%n (%%n+1)
-%endrep
-%endif
-    RET
-%endmacro
-
-INIT_XMM sse2
-VL_FUNCS 2
-INIT_XMM ssse3
-VL_FUNCS 1
-INIT_XMM avx
-VL_FUNCS 1
-
-%macro VR_FUNCS 0
-cglobal vp9_ipred_vr_4x4_16, 4, 4, 3, dst, stride, l, a
-    movu                    m0, [aq-2]
-    movhps                  m1, [lq]
-    PALIGNR                 m0, m1, 10, m2          ; xyz*abcd
-    pslldq                  m1, m0, 2               ; .xyz*abc
-    pslldq                  m2, m0, 4               ; ..xyz*ab
-    LOWPASS                  2,  1, 0               ; ..YZ#ABC
-    pavgw                   m1, m0                  ; ....#ABC
-    DEFINE_ARGS dst, stride, stride3
-    lea               stride3q, [strideq*3]
-
-    movhps    [dstq+strideq*0], m1
-    movhps    [dstq+strideq*1], m2
-    shufps                  m0, m2, m1, q3210
-%if cpuflag(ssse3)
-    pshufb                  m2, [pb_4_5_8to13_8x0]
-%else
-    pshuflw                 m2, m2, q2222
-    psrldq                  m2, 6
-%endif
-    psrldq                  m0, 6
-    movh      [dstq+strideq*2], m0
-    movh      [dstq+stride3q ], m2
-    RET
-
-cglobal vp9_ipred_vr_8x8_16, 4, 4, 5, dst, stride, l, a
-    movu                    m1, [aq-2]              ; *abcdefg
-    movu                    m2, [lq]                ; stuvwxyz
-    mova                    m0, [aq]                ; abcdefgh
-    PALIGNR                 m3, m1, m2, 14, m4      ; z*abcdef
-    LOWPASS                  3,  1,  0
-    pavgw                   m0, m1
-    PALIGNR                 m1, m2,  2, m4          ; tuvwxyz*
-    pslldq                  m4, m2,  2              ; .stuvwxy
-    LOWPASS                  4,  2,  1
-    DEFINE_ARGS dst, stride, stride3
-    lea               stride3q, [strideq*3]
-
-    mova      [dstq+strideq*0], m0
-    mova      [dstq+strideq*1], m3
-    PALIGNR                 m0, m4, 14, m1
-    pslldq                  m4, 2
-    PALIGNR                 m3, m4, 14, m1
-    pslldq                  m4, 2
-    mova      [dstq+strideq*2], m0
-    mova      [dstq+stride3q ], m3
-    lea                   dstq, [dstq+strideq*4]
-    PALIGNR                 m0, m4, 14, m1
-    pslldq                  m4, 2
-    PALIGNR                 m3, m4, 14, m1
-    pslldq                  m4, 2
-    mova      [dstq+strideq*0], m0
-    mova      [dstq+strideq*1], m3
-    PALIGNR                 m0, m4, 14, m1
-    pslldq                  m4, 2
-    PALIGNR                 m3, m4, 14, m4
-    mova      [dstq+strideq*2], m0
-    mova      [dstq+stride3q ], m3
-    RET
-
-cglobal vp9_ipred_vr_16x16_16, 4, 4, 8, dst, stride, l, a
-    movu                    m1, [aq-2]              ; *abcdefg
-    movu                    m2, [aq+mmsize-2]       ; hijklmno
-    mova                    m3, [aq]                ; abcdefgh
-    mova                    m4, [aq+mmsize]         ; ijklmnop
-    mova                    m5, [lq+mmsize]         ; stuvwxyz
-    PALIGNR                 m0, m1, m5, 14, m6      ; z*abcdef
-    movu                    m6, [aq+mmsize-4]       ; ghijklmn
-    LOWPASS                  6,  2,  4
-    pavgw                   m2, m4
-    LOWPASS                  0,  1,  3
-    pavgw                   m3, m1
-    PALIGNR                 m1, m5,  2, m7          ; tuvwxyz*
-    movu                    m7, [lq+mmsize-2]       ; rstuvwxy
-    LOWPASS                  1,  5,  7
-    movu                    m5, [lq+2]              ; lmnopqrs
-    pslldq                  m4, m5,  2              ; .lmnopqr
-    pslldq                  m7, m5,  4              ; ..lmnopq
-    LOWPASS                  5,  4,  7
-    psrld                   m4, m1, 16
-    psrld                   m7, m5, 16
-    pand                    m1, [pd_65535]
-    pand                    m5, [pd_65535]
-    packssdw                m7, m4
-    packssdw                m5, m1
-    DEFINE_ARGS dst, stride, cnt
-    mov                   cntd, 8
-
-.loop:
-    mova   [dstq+strideq*0+ 0], m3
-    mova   [dstq+strideq*0+16], m2
-    mova   [dstq+strideq*1+ 0], m0
-    mova   [dstq+strideq*1+16], m6
-    lea                   dstq, [dstq+strideq*2]
-    PALIGNR                 m2, m3, 14, m4
-    PALIGNR                 m3, m7, 14, m4
-    pslldq                  m7, 2
-    PALIGNR                 m6, m0, 14, m4
-    PALIGNR                 m0, m5, 14, m4
-    pslldq                  m5, 2
-    dec                   cntd
-    jg .loop
-    RET
-
-cglobal vp9_ipred_vr_32x32_16, 4, 5, 14, 6 * mmsize * ARCH_X86_32, dst, stride, l, a
-    movu                    m0, [aq+mmsize*0-2]     ; *a[0-6]
-    movu                    m1, [aq+mmsize*1-2]     ; a[7-14]
-    movu                    m2, [aq+mmsize*2-2]     ; a[15-22]
-    movu                    m3, [aq+mmsize*3-2]     ; a[23-30]
-    mova                    m4, [aq+mmsize*3+0]     ; a[24-31]
-    movu                    m5, [aq+mmsize*3-4]     ; a[22-29]
-    LOWPASS                  5,  3,  4              ; A[23-30]
-    SCRATCH                  5,  8, rsp+0*mmsize
-    pavgw                   m3, m4
-    mova                    m4, [aq+mmsize*2+0]     ; a[16-23]
-    movu                    m6, [aq+mmsize*2-4]     ; a[14-21]
-    LOWPASS                  6,  2,  4              ; A[15-22]
-    SCRATCH                  6,  9, rsp+1*mmsize
-    pavgw                   m2, m4
-    mova                    m4, [aq+mmsize*1+0]     ; a[8-15]
-    movu                    m7, [aq+mmsize*1-4]     ; a[6-13]
-    LOWPASS                  7,  1,  4              ; A[7-14]
-    SCRATCH                  7, 10, rsp+2*mmsize
-    pavgw                   m1, m4
-    mova                    m4, [aq+mmsize*0+0]     ; a[0-7]
-    mova                    m5, [lq+mmsize*3+0]     ; l[24-31]
-    PALIGNR                 m6, m0, m5, 14, m7      ; l[31]*a[0-5]
-    LOWPASS                  6,  0,  4              ; #A[0-6]
-    SCRATCH                  6, 11, rsp+3*mmsize
-    pavgw                   m4, m0
-    PALIGNR                 m0, m5,  2, m7          ; l[25-31]*
-    movu                    m7, [lq+mmsize*3-2]     ; l[23-30]
-    LOWPASS                  0,  5,  7              ; L[24-31]
-    movu                    m5, [lq+mmsize*2-2]     ; l[15-22]
-    mova                    m7, [lq+mmsize*2+0]     ; l[16-23]
-    movu                    m6, [lq+mmsize*2+2]     ; l[17-24]
-    LOWPASS                  5,  7,  6              ; L[16-23]
-    psrld                   m7, m0, 16
-    psrld                   m6, m5, 16
-    pand                    m0, [pd_65535]
-    pand                    m5, [pd_65535]
-    packssdw                m6, m7
-    packssdw                m5, m0
-    SCRATCH                  5, 12, rsp+4*mmsize
-    SCRATCH                  6, 13, rsp+5*mmsize
-    movu                    m6, [lq+mmsize*1-2]     ; l[7-14]
-    mova                    m0, [lq+mmsize*1+0]     ; l[8-15]
-    movu                    m5, [lq+mmsize*1+2]     ; l[9-16]
-    LOWPASS                  6,  0,  5              ; L[8-15]
-    movu                    m0, [lq+mmsize*0+2]     ; l[1-8]
-    pslldq                  m5, m0,  2              ; .l[1-7]
-    pslldq                  m7, m0,  4              ; ..l[1-6]
-    LOWPASS                  0,  5,  7
-    psrld                   m5, m6, 16
-    psrld                   m7, m0, 16
-    pand                    m6, [pd_65535]
-    pand                    m0, [pd_65535]
-    packssdw                m7, m5
-    packssdw                m0, m6
-    UNSCRATCH                6, 13, rsp+5*mmsize
-    DEFINE_ARGS dst, stride, stride16, cnt, stride17
-    mov              stride16q, strideq
-    mov                   cntd, 8
-    shl              stride16q, 4
-%if ARCH_X86_64
-    lea              stride17q, [stride16q+strideq]
-%endif
-
-.loop:
-    mova   [dstq+strideq*0+ 0], m4
-    mova   [dstq+strideq*0+16], m1
-    mova   [dstq+strideq*0+32], m2
-    mova   [dstq+strideq*0+48], m3
-%if ARCH_X86_64
-    mova   [dstq+strideq*1+ 0], m11
-    mova   [dstq+strideq*1+16], m10
-    mova   [dstq+strideq*1+32], m9
-    mova   [dstq+strideq*1+48], m8
-%endif
-    mova   [dstq+stride16q+ 0], m6
-    mova   [dstq+stride16q+16], m4
-    mova   [dstq+stride16q+32], m1
-    mova   [dstq+stride16q+48], m2
-%if ARCH_X86_64
-    mova   [dstq+stride17q+ 0], m12
-    mova   [dstq+stride17q+16], m11
-    mova   [dstq+stride17q+32], m10
-    mova   [dstq+stride17q+48], m9
-%endif
-    lea                   dstq, [dstq+strideq*2]
-    PALIGNR                 m3, m2,  14, m5
-    PALIGNR                 m2, m1,  14, m5
-    PALIGNR                 m1, m4,  14, m5
-    PALIGNR                 m4, m6,  14, m5
-    PALIGNR                 m6, m7,  14, m5
-    pslldq                  m7, 2
-%if ARCH_X86_64
-    PALIGNR                 m8, m9,  14, m5
-    PALIGNR                 m9, m10, 14, m5
-    PALIGNR                m10, m11, 14, m5
-    PALIGNR                m11, m12, 14, m5
-    PALIGNR                m12, m0,  14, m5
-    pslldq                  m0, 2
-%endif
-    dec                   cntd
-    jg .loop
-
-%if ARCH_X86_32
-    UNSCRATCH                5, 12, rsp+4*mmsize
-    UNSCRATCH                4, 11, rsp+3*mmsize
-    UNSCRATCH                3, 10, rsp+2*mmsize
-    UNSCRATCH                2,  9, rsp+1*mmsize
-    UNSCRATCH                1,  8, rsp+0*mmsize
-    mov                   dstq, dstm
-    mov                   cntd, 8
-    add                   dstq, strideq
-.loop2:
-    mova   [dstq+strideq*0+ 0], m4
-    mova   [dstq+strideq*0+16], m3
-    mova   [dstq+strideq*0+32], m2
-    mova   [dstq+strideq*0+48], m1
-    mova   [dstq+stride16q+ 0], m5
-    mova   [dstq+stride16q+16], m4
-    mova   [dstq+stride16q+32], m3
-    mova   [dstq+stride16q+48], m2
-    lea                   dstq, [dstq+strideq*2]
-    PALIGNR                 m1, m2,  14, m6
-    PALIGNR                 m2, m3,  14, m6
-    PALIGNR                 m3, m4,  14, m6
-    PALIGNR                 m4, m5,  14, m6
-    PALIGNR                 m5, m0,  14, m6
-    pslldq                  m0, 2
-    dec                   cntd
-    jg .loop2
-%endif
-    RET
-%endmacro
-
-INIT_XMM sse2
-VR_FUNCS
-INIT_XMM ssse3
-VR_FUNCS
-INIT_XMM avx
-VR_FUNCS
-
-%macro HU_FUNCS 1 ; stack_mem_for_32x32_32bit_function
-cglobal vp9_ipred_hu_4x4_16, 3, 3, 3, dst, stride, l, a
-    movh                    m0, [lq]                ; abcd
-%if cpuflag(ssse3)
-    pshufb                  m0, [pb_0to7_67x4]      ; abcddddd
-%else
-    punpcklqdq              m0, m0
-    pshufhw                 m0, m0, q3333           ; abcddddd
-%endif
-    psrldq                  m1, m0,  2              ; bcddddd.
-    psrldq                  m2, m0,  4              ; cddddd..
-    LOWPASS                  2,  1,  0              ; BCDddd..
-    pavgw                   m1, m0                  ; abcddddd
-    SBUTTERFLY          wd,  1,  2,  0              ; aBbCcDdd, dddddddd
-    PALIGNR                 m2, m1,  4, m0          ; bCcDdddd
-    DEFINE_ARGS dst, stride, stride3
-    lea               stride3q, [strideq*3]
-
-    movh      [dstq+strideq*0], m1                  ; aBbC
-    movh      [dstq+strideq*1], m2                  ; bCcD
-    movhps    [dstq+strideq*2], m1                  ; cDdd
-    movhps    [dstq+stride3q ], m2                  ; dddd
-    RET
-
-cglobal vp9_ipred_hu_8x8_16, 3, 3, 4, dst, stride, l, a
-    mova                    m0, [lq]
-%if cpuflag(ssse3)
-    mova                    m3, [pb_2to15_14_15]
-%endif
-    SHIFT_RIGHTx2           m1, m2, m0, m3
-    LOWPASS                  2,  1,  0
-    pavgw                   m1, m0
-    SBUTTERFLY          wd,  1,  2,  0
-    shufps                  m0, m1, m2, q1032
-    pshufd                  m3, m2, q3332
-    DEFINE_ARGS dst, stride, stride3
-    lea               stride3q, [strideq*3]
-
-    mova     [dstq+strideq *0], m1
-    mova     [dstq+strideq *2], m0
-    mova     [dstq+strideq *4], m2
-    mova     [dstq+stride3q*2], m3
-    add                   dstq, strideq
-%if cpuflag(avx)
-    vpalignr                m1, m2, m1, 4
-%else
-    PALIGNR                 m0, m2, m1, 4, m3
-    mova                    m1, m0
-%endif
-    pshufd                  m2, m2, q3321
-    shufps                  m0, m1, m2, q1032
-    pshufd                  m3, m2, q3332
-    mova     [dstq+strideq *0], m1
-    mova     [dstq+strideq *2], m0
-    mova     [dstq+strideq *4], m2
-    mova     [dstq+stride3q*2], m3
-    RET
-
-cglobal vp9_ipred_hu_16x16_16, 3, 4, 6 + notcpuflag(ssse3), dst, stride, l, a
-    mova                    m0, [lq]
-    mova                    m3, [lq+mmsize]
-    movu                    m1, [lq+2]
-    movu                    m2, [lq+4]
-    LOWPASS                  2,  1,  0
-    pavgw                   m1, m0
-    SBUTTERFLY           wd, 1,  2,  0
-%if cpuflag(ssse3)
-    mova                    m5, [pb_2to15_14_15]
-%endif
-    SHIFT_RIGHTx2           m0, m4, m3, m5
-    LOWPASS                  4,  0,  3
-    pavgw                   m3, m0
-    SBUTTERFLY           wd, 3,  4,  5
-    pshufd                  m0, m0, q3333
-    DEFINE_ARGS dst, stride, stride3, cnt
-    lea               stride3q, [strideq*3]
-    mov                   cntd, 4
-
-.loop:
-    mova  [dstq+strideq *0+ 0], m1
-    mova  [dstq+strideq *0+16], m2
-    mova  [dstq+strideq *4+ 0], m2
-    mova  [dstq+strideq *4+16], m3
-    mova  [dstq+strideq *8+ 0], m3
-    mova  [dstq+strideq *8+16], m4
-    mova  [dstq+stride3q*4+ 0], m4
-    mova  [dstq+stride3q*4+16], m0
-    add                   dstq, strideq
-%if cpuflag(avx)
-    vpalignr                m1, m2, m1, 4
-    vpalignr                m2, m3, m2, 4
-    vpalignr                m3, m4, m3, 4
-    vpalignr                m4, m0, m4, 4
-%else
-    PALIGNR                 m5, m2, m1, 4, m6
-    mova                    m1, m5
-    PALIGNR                 m5, m3, m2, 4, m6
-    mova                    m2, m5
-    PALIGNR                 m5, m4, m3, 4, m6
-    mova                    m3, m5
-    PALIGNR                 m5, m0, m4, 4, m6
-    mova                    m4, m5
-%endif
-    dec                   cntd
-    jg .loop
-    RET
-
-cglobal vp9_ipred_hu_32x32_16, 3, 7, 10 + notcpuflag(ssse3), \
-                               %1 * -mmsize * ARCH_X86_32, dst, stride, l, a
-    mova                    m2, [lq+mmsize*0+0]
-    movu                    m1, [lq+mmsize*0+2]
-    movu                    m0, [lq+mmsize*0+4]
-    LOWPASS                  0,  1,  2
-    pavgw                   m1, m2
-    SBUTTERFLY           wd, 1,  0,  2
-    SCRATCH                  1,  8, rsp+0*mmsize
-    mova                    m4, [lq+mmsize*1+0]
-    movu                    m3, [lq+mmsize*1+2]
-    movu                    m2, [lq+mmsize*1+4]
-    LOWPASS                  2,  3,  4
-    pavgw                   m3, m4
-    SBUTTERFLY           wd, 3,  2,  4
-    mova                    m6, [lq+mmsize*2+0]
-    movu                    m5, [lq+mmsize*2+2]
-    movu                    m4, [lq+mmsize*2+4]
-    LOWPASS                  4,  5,  6
-    pavgw                   m5, m6
-    SBUTTERFLY           wd, 5,  4,  6
-    mova                    m7, [lq+mmsize*3+0]
-    SCRATCH                  0,  9, rsp+1*mmsize
-%if cpuflag(ssse3)
-    mova                    m0, [pb_2to15_14_15]
-%endif
-    SHIFT_RIGHTx2           m1, m6, m7, m0
-    LOWPASS                  6,  1,  7
-    pavgw                   m7, m1
-    SBUTTERFLY           wd, 7,  6,  0
-    pshufd                  m1, m1, q3333
-    UNSCRATCH                0,  9, rsp+1*mmsize
-    DEFINE_ARGS dst, stride, cnt, stride3, stride4, stride20, stride28
-    lea               stride3q, [strideq*3]
-    lea               stride4q, [strideq*4]
-    lea              stride28q, [stride4q*8]
-    lea              stride20q, [stride4q*5]
-    sub              stride28q, stride4q
-    mov                   cntd, 4
-
-.loop:
-%if ARCH_X86_64
-    SWAP                     1,  8
-%else
-    mova        [rsp+1*mmsize], m1
-    mova                    m1, [rsp+0*mmsize]
-%endif
-    mova  [dstq+strideq *0+ 0], m1
-    mova  [dstq+strideq *0+16], m0
-    mova  [dstq+strideq *0+32], m3
-    mova  [dstq+strideq *0+48], m2
-    mova  [dstq+stride4q*1+ 0], m0
-    mova  [dstq+stride4q*1+16], m3
-    mova  [dstq+stride4q*1+32], m2
-    mova  [dstq+stride4q*1+48], m5
-    mova  [dstq+stride4q*2+ 0], m3
-    mova  [dstq+stride4q*2+16], m2
-    mova  [dstq+stride4q*2+32], m5
-    mova  [dstq+stride4q*2+48], m4
-%if cpuflag(avx)
-    vpalignr                m1, m0, m1, 4
-    vpalignr                m0, m3, m0, 4
-    vpalignr                m3, m2, m3, 4
-%else
-    SCRATCH                  6,  9, rsp+2*mmsize
-%if notcpuflag(ssse3)
-    SCRATCH                  7, 10, rsp+3*mmsize
-%endif
-    PALIGNR                 m6, m0, m1, 4, m7
-    mova                    m1, m6
-    PALIGNR                 m6, m3, m0, 4, m7
-    mova                    m0, m6
-    PALIGNR                 m6, m2, m3, 4, m7
-    mova                    m3, m6
-    UNSCRATCH                6,  9, rsp+2*mmsize
-    SCRATCH                  0,  9, rsp+2*mmsize
-%if notcpuflag(ssse3)
-    UNSCRATCH                7, 10, rsp+3*mmsize
-    SCRATCH                  3, 10, rsp+3*mmsize
-%endif
-%endif
-%if ARCH_X86_64
-    SWAP                     1,  8
-%else
-    mova        [rsp+0*mmsize], m1
-    mova                    m1, [rsp+1*mmsize]
-%endif
-    mova  [dstq+stride3q*4+ 0], m2
-    mova  [dstq+stride3q*4+16], m5
-    mova  [dstq+stride3q*4+32], m4
-    mova  [dstq+stride3q*4+48], m7
-    mova  [dstq+stride4q*4+ 0], m5
-    mova  [dstq+stride4q*4+16], m4
-    mova  [dstq+stride4q*4+32], m7
-    mova  [dstq+stride4q*4+48], m6
-    mova  [dstq+stride20q + 0], m4
-    mova  [dstq+stride20q +16], m7
-    mova  [dstq+stride20q +32], m6
-    mova  [dstq+stride20q +48], m1
-    mova  [dstq+stride3q*8+ 0], m7
-    mova  [dstq+stride3q*8+16], m6
-    mova  [dstq+stride3q*8+32], m1
-    mova  [dstq+stride3q*8+48], m1
-    mova  [dstq+stride28q + 0], m6
-    mova  [dstq+stride28q +16], m1
-    mova  [dstq+stride28q +32], m1
-    mova  [dstq+stride28q +48], m1
-%if cpuflag(avx)
-    vpalignr                m2, m5, m2, 4
-    vpalignr                m5, m4, m5, 4
-    vpalignr                m4, m7, m4, 4
-    vpalignr                m7, m6, m7, 4
-    vpalignr                m6, m1, m6, 4
-%else
-    PALIGNR                 m0, m5, m2, 4, m3
-    mova                    m2, m0
-    PALIGNR                 m0, m4, m5, 4, m3
-    mova                    m5, m0
-    PALIGNR                 m0, m7, m4, 4, m3
-    mova                    m4, m0
-    PALIGNR                 m0, m6, m7, 4, m3
-    mova                    m7, m0
-    PALIGNR                 m0, m1, m6, 4, m3
-    mova                    m6, m0
-    UNSCRATCH                0,  9, rsp+2*mmsize
-%if notcpuflag(ssse3)
-    UNSCRATCH                3, 10, rsp+3*mmsize
-%endif
-%endif
-    add                   dstq, strideq
-    dec                   cntd
-    jg .loop
-    RET
-%endmacro
-
-INIT_XMM sse2
-HU_FUNCS 4
-INIT_XMM ssse3
-HU_FUNCS 3
-INIT_XMM avx
-HU_FUNCS 2
-
-%macro HD_FUNCS 0
-cglobal vp9_ipred_hd_4x4_16, 4, 4, 4, dst, stride, l, a
-    movh                    m0, [lq]
-    movhps                  m0, [aq-2]
-    psrldq                  m1, m0, 2
-    psrldq                  m2, m0, 4
-    LOWPASS                  2,  1,  0
-    pavgw                   m1, m0
-    punpcklwd               m1, m2
-    DEFINE_ARGS dst, stride, stride3
-    lea               stride3q, [strideq*3]
-
-    movh      [dstq+stride3q ], m1
-    movhps    [dstq+strideq*1], m1
-    movhlps                 m2, m2
-    PALIGNR                 m2, m1, 4, m0
-    movh      [dstq+strideq*2], m2
-    movhps    [dstq+strideq*0], m2
-    RET
-
-cglobal vp9_ipred_hd_8x8_16, 4, 4, 5, dst, stride, l, a
-    mova                    m0, [lq]
-    movu                    m1, [aq-2]
-    PALIGNR                 m2, m1, m0, 2, m3
-    PALIGNR                 m3, m1, m0, 4, m4
-    LOWPASS                  3,  2,  0
-    pavgw                   m2, m0
-    SBUTTERFLY           wd, 2,  3,  0
-    psrldq                  m0, m1,  2
-    psrldq                  m4, m1,  4
-    LOWPASS                  1,  0,  4
-    DEFINE_ARGS dst8, mstride, cnt
-    lea                  dst8q, [dst8q+mstrideq*8]
-    neg               mstrideq
-    mov                   cntd, 4
-
-.loop:
-    add                  dst8q, mstrideq
-    mova    [dst8q+mstrideq*0], m2
-    mova    [dst8q+mstrideq*4], m3
-%if cpuflag(avx)
-    vpalignr                m2, m3, m2, 4
-    vpalignr                m3, m1, m3, 4
-%else
-    PALIGNR                 m0, m3, m2, 4, m4
-    mova                    m2, m0
-    PALIGNR                 m0, m1, m3, 4, m4
-    mova                    m3, m0
-%endif
-    psrldq                  m1, 4
-    dec                   cntd
-    jg .loop
-    RET
-
-cglobal vp9_ipred_hd_16x16_16, 4, 4, 8, dst, stride, l, a
-    mova                    m2, [lq]
-    movu                    m1, [lq+2]
-    movu                    m0, [lq+4]
-    LOWPASS                  0,  1,  2
-    pavgw                   m1, m2
-    mova                    m4, [lq+mmsize]
-    movu                    m5, [aq-2]
-    PALIGNR                 m3, m5, m4, 2, m6
-    PALIGNR                 m2, m5, m4, 4, m6
-    LOWPASS                  2,  3,  4
-    pavgw                   m3, m4
-    SBUTTERFLY           wd, 1,  0,  4
-    SBUTTERFLY           wd, 3,  2,  4
-    mova                    m6, [aq]
-    movu                    m4, [aq+2]
-    LOWPASS                  4,  6,  5
-    movu                    m5, [aq+mmsize-2]
-    psrldq                  m6, m5,  2
-    psrldq                  m7, m5,  4
-    LOWPASS                  5,  6,  7
-    DEFINE_ARGS dst, mstride, mstride3, cnt
-    lea                   dstq, [dstq+mstrideq*8]
-    lea                   dstq, [dstq+mstrideq*8]
-    neg               mstrideq
-    lea              mstride3q, [mstrideq*3]
-    mov                   cntd, 4
-
-.loop:
-    add                  dstq, mstrideq
-    mova [dstq+mstride3q*4+ 0], m2
-    mova [dstq+mstride3q*4+16], m4
-    mova [dstq+mstrideq *8+ 0], m3
-    mova [dstq+mstrideq *8+16], m2
-    mova [dstq+mstrideq *4+ 0], m0
-    mova [dstq+mstrideq *4+16], m3
-    mova [dstq+mstrideq *0+ 0], m1
-    mova [dstq+mstrideq *0+16], m0
-%if cpuflag(avx)
-    vpalignr                m1, m0, m1, 4
-    vpalignr                m0, m3, m0, 4
-    vpalignr                m3, m2, m3, 4
-    vpalignr                m2, m4, m2, 4
-    vpalignr                m4, m5, m4, 4
-%else
-    PALIGNR                 m6, m0, m1, 4, m7
-    mova                    m1, m6
-    PALIGNR                 m6, m3, m0, 4, m7
-    mova                    m0, m6
-    PALIGNR                 m6, m2, m3, 4, m7
-    mova                    m3, m6
-    PALIGNR                 m6, m4, m2, 4, m7
-    mova                    m2, m6
-    PALIGNR                 m6, m5, m4, 4, m7
-    mova                    m4, m6
-%endif
-    psrldq                  m5, 4
-    dec                   cntd
-    jg .loop
-    RET
-
-cglobal vp9_ipred_hd_32x32_16, 4, 4 + 3 * ARCH_X86_64, 14, \
-                               10 * -mmsize * ARCH_X86_32, dst, stride, l, a
-    mova                    m2, [lq+mmsize*0+0]
-    movu                    m1, [lq+mmsize*0+2]
-    movu                    m0, [lq+mmsize*0+4]
-    LOWPASS                  0,  1,  2
-    pavgw                   m1, m2
-    SBUTTERFLY           wd, 1,  0,  2
-    mova                    m4, [lq+mmsize*1+0]
-    movu                    m3, [lq+mmsize*1+2]
-    movu                    m2, [lq+mmsize*1+4]
-    LOWPASS                  2,  3,  4
-    pavgw                   m3, m4
-    SBUTTERFLY           wd, 3,  2,  4
-    SCRATCH                  0,  8, rsp+0*mmsize
-    SCRATCH                  1,  9, rsp+1*mmsize
-    SCRATCH                  2, 10, rsp+2*mmsize
-    SCRATCH                  3, 11, rsp+3*mmsize
-    mova                    m6, [lq+mmsize*2+0]
-    movu                    m5, [lq+mmsize*2+2]
-    movu                    m4, [lq+mmsize*2+4]
-    LOWPASS                  4,  5,  6
-    pavgw                   m5, m6
-    SBUTTERFLY           wd, 5,  4,  6
-    mova                    m0, [lq+mmsize*3+0]
-    movu                    m1, [aq+mmsize*0-2]
-    PALIGNR                 m7, m1, m0, 2, m2
-    PALIGNR                 m6, m1, m0, 4, m2
-    LOWPASS                  6,  7,  0
-    pavgw                   m7, m0
-    SBUTTERFLY           wd, 7,  6,  0
-    mova                    m2, [aq+mmsize*0+0]
-    movu                    m0, [aq+mmsize*0+2]
-    LOWPASS                  0,  2,  1
-    movu                    m1, [aq+mmsize*1-2]
-    mova                    m2, [aq+mmsize*1+0]
-    movu                    m3, [aq+mmsize*1+2]
-    LOWPASS                  1,  2,  3
-    SCRATCH                  6, 12, rsp+6*mmsize
-    SCRATCH                  7, 13, rsp+7*mmsize
-    movu                    m2, [aq+mmsize*2-2]
-    mova                    m3, [aq+mmsize*2+0]
-    movu                    m6, [aq+mmsize*2+2]
-    LOWPASS                  2,  3,  6
-    movu                    m3, [aq+mmsize*3-2]
-    psrldq                  m6, m3,  2
-    psrldq                  m7, m3,  4
-    LOWPASS                  3,  6,  7
-    UNSCRATCH                6, 12, rsp+6*mmsize
-    UNSCRATCH                7, 13, rsp+7*mmsize
-%if ARCH_X86_32
-    mova        [rsp+4*mmsize], m4
-    mova        [rsp+5*mmsize], m5
-    ; we already backed up m6/m7 earlier on x86-32 in SCRATCH, so we don't need
-    ; to do it again here
-%endif
-    DEFINE_ARGS dst, stride, cnt, stride3, stride4, stride20, stride28
-    mov                   cntd, 4
-    lea               stride3q, [strideq*3]
-%if ARCH_X86_64
-    lea               stride4q, [strideq*4]
-    lea              stride28q, [stride4q*8]
-    lea              stride20q, [stride4q*5]
-    sub              stride28q, stride4q
-%endif
-    add                   dstq, stride3q
-
-    ; x86-32 doesn't have enough registers, so on that platform, we split
-    ; the loop in 2... Otherwise you spend most of the loop (un)scratching
-.loop:
-%if ARCH_X86_64
-    mova  [dstq+stride28q + 0], m9
-    mova  [dstq+stride28q +16], m8
-    mova  [dstq+stride28q +32], m11
-    mova  [dstq+stride28q +48], m10
-    mova  [dstq+stride3q*8+ 0], m8
-    mova  [dstq+stride3q*8+16], m11
-    mova  [dstq+stride3q*8+32], m10
-    mova  [dstq+stride3q*8+48], m5
-    mova  [dstq+stride20q + 0], m11
-    mova  [dstq+stride20q +16], m10
-    mova  [dstq+stride20q +32], m5
-    mova  [dstq+stride20q +48], m4
-    mova  [dstq+stride4q*4+ 0], m10
-    mova  [dstq+stride4q*4+16], m5
-    mova  [dstq+stride4q*4+32], m4
-    mova  [dstq+stride4q*4+48], m7
-%endif
-    mova  [dstq+stride3q*4+ 0], m5
-    mova  [dstq+stride3q*4+16], m4
-    mova  [dstq+stride3q*4+32], m7
-    mova  [dstq+stride3q*4+48], m6
-    mova  [dstq+strideq* 8+ 0], m4
-    mova  [dstq+strideq* 8+16], m7
-    mova  [dstq+strideq* 8+32], m6
-    mova  [dstq+strideq* 8+48], m0
-    mova  [dstq+strideq* 4+ 0], m7
-    mova  [dstq+strideq* 4+16], m6
-    mova  [dstq+strideq* 4+32], m0
-    mova  [dstq+strideq* 4+48], m1
-    mova  [dstq+strideq* 0+ 0], m6
-    mova  [dstq+strideq* 0+16], m0
-    mova  [dstq+strideq* 0+32], m1
-    mova  [dstq+strideq* 0+48], m2
-    sub                   dstq, strideq
-%if cpuflag(avx)
-%if ARCH_X86_64
-    vpalignr                m9, m8,  m9,  4
-    vpalignr                m8, m11, m8,  4
-    vpalignr               m11, m10, m11, 4
-    vpalignr               m10, m5,  m10, 4
-%endif
-    vpalignr                m5, m4,  m5,  4
-    vpalignr                m4, m7,  m4,  4
-    vpalignr                m7, m6,  m7,  4
-    vpalignr                m6, m0,  m6,  4
-    vpalignr                m0, m1,  m0,  4
-    vpalignr                m1, m2,  m1,  4
-    vpalignr                m2, m3,  m2,  4
-%else
-%if ARCH_X86_64
-    PALIGNR                m12, m8,  m9,  4, m13
-    mova                    m9, m12
-    PALIGNR                m12, m11, m8,  4, m13
-    mova                    m8, m12
-    PALIGNR                m12, m10, m11, 4, m13
-    mova                   m11, m12
-    PALIGNR                m12, m5,  m10, 4, m13
-    mova                   m10, m12
-%endif
-    SCRATCH                  3, 12, rsp+8*mmsize, sh
-%if notcpuflag(ssse3)
-    SCRATCH                  2, 13, rsp+9*mmsize
-%endif
-    PALIGNR                 m3, m4,  m5,  4, m2
-    mova                    m5, m3
-    PALIGNR                 m3, m7,  m4,  4, m2
-    mova                    m4, m3
-    PALIGNR                 m3, m6,  m7,  4, m2
-    mova                    m7, m3
-    PALIGNR                 m3, m0,  m6,  4, m2
-    mova                    m6, m3
-    PALIGNR                 m3, m1,  m0,  4, m2
-    mova                    m0, m3
-%if notcpuflag(ssse3)
-    UNSCRATCH                2, 13, rsp+9*mmsize
-    SCRATCH                  0, 13, rsp+9*mmsize
-%endif
-    PALIGNR                 m3, m2,  m1,  4, m0
-    mova                    m1, m3
-    PALIGNR                 m3, reg_sh,  m2,  4, m0
-    mova                    m2, m3
-%if notcpuflag(ssse3)
-    UNSCRATCH                0, 13, rsp+9*mmsize
-%endif
-    UNSCRATCH                3, 12, rsp+8*mmsize, sh
-%endif
-    psrldq                  m3, 4
-    dec                   cntd
-    jg .loop
-
-%if ARCH_X86_32
-    UNSCRATCH                0,  8, rsp+0*mmsize
-    UNSCRATCH                1,  9, rsp+1*mmsize
-    UNSCRATCH                2, 10, rsp+2*mmsize
-    UNSCRATCH                3, 11, rsp+3*mmsize
-    mova                    m4, [rsp+4*mmsize]
-    mova                    m5, [rsp+5*mmsize]
-    mova                    m6, [rsp+6*mmsize]
-    mova                    m7, [rsp+7*mmsize]
-    DEFINE_ARGS dst, stride, stride5, stride3
-    lea               stride5q, [strideq*5]
-    lea                   dstq, [dstq+stride5q*4]
-    DEFINE_ARGS dst, stride, cnt, stride3
-    mov                   cntd, 4
-.loop_2:
-    mova  [dstq+stride3q*4+ 0], m1
-    mova  [dstq+stride3q*4+16], m0
-    mova  [dstq+stride3q*4+32], m3
-    mova  [dstq+stride3q*4+48], m2
-    mova  [dstq+strideq* 8+ 0], m0
-    mova  [dstq+strideq* 8+16], m3
-    mova  [dstq+strideq* 8+32], m2
-    mova  [dstq+strideq* 8+48], m5
-    mova  [dstq+strideq* 4+ 0], m3
-    mova  [dstq+strideq* 4+16], m2
-    mova  [dstq+strideq* 4+32], m5
-    mova  [dstq+strideq* 4+48], m4
-    mova  [dstq+strideq* 0+ 0], m2
-    mova  [dstq+strideq* 0+16], m5
-    mova  [dstq+strideq* 0+32], m4
-    mova  [dstq+strideq* 0+48], m7
-    sub                   dstq, strideq
-%if cpuflag(avx)
-    vpalignr                m1, m0,  m1,  4
-    vpalignr                m0, m3,  m0,  4
-    vpalignr                m3, m2,  m3,  4
-    vpalignr                m2, m5,  m2,  4
-    vpalignr                m5, m4,  m5,  4
-    vpalignr                m4, m7,  m4,  4
-    vpalignr                m7, m6,  m7,  4
-%else
-    SCRATCH                  6, 12, rsp+8*mmsize, sh
-%if notcpuflag(ssse3)
-    SCRATCH                  7, 13, rsp+9*mmsize
-%endif
-    PALIGNR                 m6, m0,  m1,  4, m7
-    mova                    m1, m6
-    PALIGNR                 m6, m3,  m0,  4, m7
-    mova                    m0, m6
-    PALIGNR                 m6, m2,  m3,  4, m7
-    mova                    m3, m6
-    PALIGNR                 m6, m5,  m2,  4, m7
-    mova                    m2, m6
-    PALIGNR                 m6, m4,  m5,  4, m7
-    mova                    m5, m6
-%if notcpuflag(ssse3)
-    UNSCRATCH                7, 13, rsp+9*mmsize
-    SCRATCH                  5, 13, rsp+9*mmsize
-%endif
-    PALIGNR                 m6, m7,  m4,  4, m5
-    mova                    m4, m6
-    PALIGNR                 m6, reg_sh,  m7,  4, m5
-    mova                    m7, m6
-%if notcpuflag(ssse3)
-    UNSCRATCH                5, 13, rsp+9*mmsize
-%endif
-    UNSCRATCH                6, 12, rsp+8*mmsize, sh
-%endif
-    psrldq                  m6, 4
-    dec                   cntd
-    jg .loop_2
-%endif
-    RET
-%endmacro
-
-INIT_XMM sse2
-HD_FUNCS
-INIT_XMM ssse3
-HD_FUNCS
-INIT_XMM avx
-HD_FUNCS
diff -uparN ffmpeg-4.1/libavcodec/x86/vp9intrapred.asm ffmpeg-y/libavcodec/x86/vp9intrapred.asm
--- ffmpeg-4.1/libavcodec/x86/vp9intrapred.asm	2018-01-01 06:35:48.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/vp9intrapred.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,2044 +0,0 @@
-;******************************************************************************
-;* VP9 Intra prediction SIMD optimizations
-;*
-;* Copyright (c) 2013 Ronald S. Bultje <rsbultje gmail com>
-;*
-;* Parts based on:
-;* H.264 intra prediction asm optimizations
-;* Copyright (c) 2010 Fiona Glaser
-;* Copyright (c) 2010 Holger Lubitz
-;* Copyright (c) 2010 Loren Merritt
-;* Copyright (c) 2010 Ronald S. Bultje
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA 32
-
-pw_m256: times 16 dw -256
-pw_m255: times 16 dw -255
-pw_4096: times 8 dw 4096
-
-pb_4x3_4x2_4x1_4x0: times 4 db 3
-                    times 4 db 2
-                    times 4 db 1
-                    times 4 db 0
-pb_8x1_8x0:   times 8 db 1
-              times 8 db 0
-pb_8x3_8x2:   times 8 db 3
-              times 8 db 2
-pb_0to5_2x7:  db 0, 1, 2, 3, 4, 5, 7, 7
-              times 8 db -1
-pb_0to6_9x7:  db 0, 1, 2, 3, 4, 5, 6
-              times 9 db 7
-pb_1to6_10x7: db 1, 2, 3, 4, 5, 6
-              times 10 db 7
-pb_2to6_3x7:
-pb_2to6_11x7: db 2, 3, 4, 5, 6
-              times 11 db 7
-pb_1toE_2xF:  db 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 15
-pb_2toE_3xF:  db 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 15, 15
-pb_13456_3xm1: db 1, 3, 4, 5, 6
-               times 3 db -1
-pb_6012_4xm1: db 6, 0, 1, 2
-              times 4 db -1
-pb_6xm1_246_8toE: times 6 db -1
-                  db 2, 4, 6, 8, 9, 10, 11, 12, 13, 14
-pb_6xm1_BDF_0to6: times 6 db -1
-                  db 11, 13, 15, 0, 1, 2, 3, 4, 5, 6
-pb_02468ACE_13579BDF: db 0, 2, 4, 6, 8, 10, 12, 14, 1, 3, 5, 7, 9, 11, 13, 15
-
-pb_15x0_1xm1: times 15 db 0
-              db -1
-pb_0to2_5x3: db 0, 1, 2
-             times 5 db 3
-pb_6xm1_2x0: times 6 db -1
-             times 2 db 0
-pb_6x0_2xm1: times 6 db 0
-             times 2 db -1
-
-cextern pb_1
-cextern pb_2
-cextern pb_3
-cextern pb_15
-cextern pw_2
-cextern pw_4
-cextern pw_8
-cextern pw_16
-cextern pw_32
-cextern pw_255
-cextern pw_512
-cextern pw_1024
-cextern pw_2048
-cextern pw_8192
-
-SECTION .text
-
-; dc_NxN(uint8_t *dst, ptrdiff_t stride, const uint8_t *l, const uint8_t *a)
-
-%macro DC_4to8_FUNCS 0
-cglobal vp9_ipred_dc_4x4, 4, 4, 0, dst, stride, l, a
-    movd                    m0, [lq]
-    punpckldq               m0, [aq]
-    pxor                    m1, m1
-    psadbw                  m0, m1
-%if cpuflag(ssse3)
-    pmulhrsw                m0, [pw_4096]
-    pshufb                  m0, m1
-%else
-    paddw                   m0, [pw_4]
-    psraw                   m0, 3
-    punpcklbw               m0, m0
-    pshufw                  m0, m0, q0000
-%endif
-    movd      [dstq+strideq*0], m0
-    movd      [dstq+strideq*1], m0
-    lea                   dstq, [dstq+strideq*2]
-    movd      [dstq+strideq*0], m0
-    movd      [dstq+strideq*1], m0
-    RET
-
-cglobal vp9_ipred_dc_8x8, 4, 4, 0, dst, stride, l, a
-    movq                    m0, [lq]
-    movq                    m1, [aq]
-    DEFINE_ARGS dst, stride, stride3
-    lea               stride3q, [strideq*3]
-    pxor                    m2, m2
-    psadbw                  m0, m2
-    psadbw                  m1, m2
-    paddw                   m0, m1
-%if cpuflag(ssse3)
-    pmulhrsw                m0, [pw_2048]
-    pshufb                  m0, m2
-%else
-    paddw                   m0, [pw_8]
-    psraw                   m0, 4
-    punpcklbw               m0, m0
-    pshufw                  m0, m0, q0000
-%endif
-    movq      [dstq+strideq*0], m0
-    movq      [dstq+strideq*1], m0
-    movq      [dstq+strideq*2], m0
-    movq      [dstq+stride3q ], m0
-    lea                   dstq, [dstq+strideq*4]
-    movq      [dstq+strideq*0], m0
-    movq      [dstq+strideq*1], m0
-    movq      [dstq+strideq*2], m0
-    movq      [dstq+stride3q ], m0
-    RET
-%endmacro
-
-INIT_MMX mmxext
-DC_4to8_FUNCS
-INIT_MMX ssse3
-DC_4to8_FUNCS
-
-%macro DC_16to32_FUNCS 0
-cglobal vp9_ipred_dc_16x16, 4, 4, 3, dst, stride, l, a
-    mova                    m0, [lq]
-    mova                    m1, [aq]
-    DEFINE_ARGS dst, stride, stride3, cnt
-    lea               stride3q, [strideq*3]
-    pxor                    m2, m2
-    psadbw                  m0, m2
-    psadbw                  m1, m2
-    paddw                   m0, m1
-    movhlps                 m1, m0
-    paddw                   m0, m1
-%if cpuflag(ssse3)
-    pmulhrsw                m0, [pw_1024]
-    pshufb                  m0, m2
-%else
-    paddw                   m0, [pw_16]
-    psraw                   m0, 5
-    punpcklbw               m0, m0
-    pshuflw                 m0, m0, q0000
-    punpcklqdq              m0, m0
-%endif
-    mov                   cntd, 4
-.loop:
-    mova      [dstq+strideq*0], m0
-    mova      [dstq+strideq*1], m0
-    mova      [dstq+strideq*2], m0
-    mova      [dstq+stride3q ], m0
-    lea                   dstq, [dstq+strideq*4]
-    dec                   cntd
-    jg .loop
-    RET
-
-cglobal vp9_ipred_dc_32x32, 4, 4, 5, dst, stride, l, a
-    mova                    m0, [lq]
-    mova                    m1, [lq+16]
-    mova                    m2, [aq]
-    mova                    m3, [aq+16]
-    DEFINE_ARGS dst, stride, stride3, cnt
-    lea               stride3q, [strideq*3]
-    pxor                    m4, m4
-    psadbw                  m0, m4
-    psadbw                  m1, m4
-    psadbw                  m2, m4
-    psadbw                  m3, m4
-    paddw                   m0, m1
-    paddw                   m2, m3
-    paddw                   m0, m2
-    movhlps                 m1, m0
-    paddw                   m0, m1
-%if cpuflag(ssse3)
-    pmulhrsw                m0, [pw_512]
-    pshufb                  m0, m4
-%else
-    paddw                   m0, [pw_32]
-    psraw                   m0, 6
-    punpcklbw               m0, m0
-    pshuflw                 m0, m0, q0000
-    punpcklqdq              m0, m0
-%endif
-    mov                   cntd, 8
-.loop:
-    mova   [dstq+strideq*0+ 0], m0
-    mova   [dstq+strideq*0+16], m0
-    mova   [dstq+strideq*1+ 0], m0
-    mova   [dstq+strideq*1+16], m0
-    mova   [dstq+strideq*2+ 0], m0
-    mova   [dstq+strideq*2+16], m0
-    mova   [dstq+stride3q + 0], m0
-    mova   [dstq+stride3q +16], m0
-    lea                   dstq, [dstq+strideq*4]
-    dec                   cntd
-    jg .loop
-    RET
-%endmacro
-
-INIT_XMM sse2
-DC_16to32_FUNCS
-INIT_XMM ssse3
-DC_16to32_FUNCS
-
-%if HAVE_AVX2_EXTERNAL
-INIT_YMM avx2
-cglobal vp9_ipred_dc_32x32, 4, 4, 3, dst, stride, l, a
-    mova                    m0, [lq]
-    mova                    m1, [aq]
-    DEFINE_ARGS dst, stride, stride3, cnt
-    lea               stride3q, [strideq*3]
-    pxor                    m2, m2
-    psadbw                  m0, m2
-    psadbw                  m1, m2
-    paddw                   m0, m1
-    vextracti128           xm1, m0, 1
-    paddw                  xm0, xm1
-    movhlps                xm1, xm0
-    paddw                  xm0, xm1
-    pmulhrsw               xm0, [pw_512]
-    vpbroadcastb            m0, xm0
-    mov                   cntd, 4
-.loop:
-    mova      [dstq+strideq*0], m0
-    mova      [dstq+strideq*1], m0
-    mova      [dstq+strideq*2], m0
-    mova      [dstq+stride3q ], m0
-    lea                   dstq, [dstq+strideq*4]
-    mova      [dstq+strideq*0], m0
-    mova      [dstq+strideq*1], m0
-    mova      [dstq+strideq*2], m0
-    mova      [dstq+stride3q ], m0
-    lea                   dstq, [dstq+strideq*4]
-    dec                   cntd
-    jg .loop
-    RET
-%endif
-
-; dc_top/left_NxN(uint8_t *dst, ptrdiff_t stride, const uint8_t *l, const uint8_t *a)
-
-%macro DC_1D_4to8_FUNCS 2 ; dir (top or left), arg (a or l)
-cglobal vp9_ipred_dc_%1_4x4, 4, 4, 0, dst, stride, l, a
-    movd                    m0, [%2q]
-    pxor                    m1, m1
-    psadbw                  m0, m1
-%if cpuflag(ssse3)
-    pmulhrsw                m0, [pw_8192]
-    pshufb                  m0, m1
-%else
-    paddw                   m0, [pw_2]
-    psraw                   m0, 2
-    punpcklbw               m0, m0
-    pshufw                  m0, m0, q0000
-%endif
-    movd      [dstq+strideq*0], m0
-    movd      [dstq+strideq*1], m0
-    lea                   dstq, [dstq+strideq*2]
-    movd      [dstq+strideq*0], m0
-    movd      [dstq+strideq*1], m0
-    RET
-
-cglobal vp9_ipred_dc_%1_8x8, 4, 4, 0, dst, stride, l, a
-    movq                    m0, [%2q]
-    DEFINE_ARGS dst, stride, stride3
-    lea               stride3q, [strideq*3]
-    pxor                    m1, m1
-    psadbw                  m0, m1
-%if cpuflag(ssse3)
-    pmulhrsw                m0, [pw_4096]
-    pshufb                  m0, m1
-%else
-    paddw                   m0, [pw_4]
-    psraw                   m0, 3
-    punpcklbw               m0, m0
-    pshufw                  m0, m0, q0000
-%endif
-    movq      [dstq+strideq*0], m0
-    movq      [dstq+strideq*1], m0
-    movq      [dstq+strideq*2], m0
-    movq      [dstq+stride3q ], m0
-    lea                   dstq, [dstq+strideq*4]
-    movq      [dstq+strideq*0], m0
-    movq      [dstq+strideq*1], m0
-    movq      [dstq+strideq*2], m0
-    movq      [dstq+stride3q ], m0
-    RET
-%endmacro
-
-INIT_MMX mmxext
-DC_1D_4to8_FUNCS top,  a
-DC_1D_4to8_FUNCS left, l
-INIT_MMX ssse3
-DC_1D_4to8_FUNCS top,  a
-DC_1D_4to8_FUNCS left, l
-
-%macro DC_1D_16to32_FUNCS 2; dir (top or left), arg (a or l)
-cglobal vp9_ipred_dc_%1_16x16, 4, 4, 3, dst, stride, l, a
-    mova                    m0, [%2q]
-    DEFINE_ARGS dst, stride, stride3, cnt
-    lea               stride3q, [strideq*3]
-    pxor                    m2, m2
-    psadbw                  m0, m2
-    movhlps                 m1, m0
-    paddw                   m0, m1
-%if cpuflag(ssse3)
-    pmulhrsw                m0, [pw_2048]
-    pshufb                  m0, m2
-%else
-    paddw                   m0, [pw_8]
-    psraw                   m0, 4
-    punpcklbw               m0, m0
-    pshuflw                 m0, m0, q0000
-    punpcklqdq              m0, m0
-%endif
-    mov                   cntd, 4
-.loop:
-    mova      [dstq+strideq*0], m0
-    mova      [dstq+strideq*1], m0
-    mova      [dstq+strideq*2], m0
-    mova      [dstq+stride3q ], m0
-    lea                   dstq, [dstq+strideq*4]
-    dec                   cntd
-    jg .loop
-    RET
-
-cglobal vp9_ipred_dc_%1_32x32, 4, 4, 3, dst, stride, l, a
-    mova                    m0, [%2q]
-    mova                    m1, [%2q+16]
-    DEFINE_ARGS dst, stride, stride3, cnt
-    lea               stride3q, [strideq*3]
-    pxor                    m2, m2
-    psadbw                  m0, m2
-    psadbw                  m1, m2
-    paddw                   m0, m1
-    movhlps                 m1, m0
-    paddw                   m0, m1
-%if cpuflag(ssse3)
-    pmulhrsw                m0, [pw_1024]
-    pshufb                  m0, m2
-%else
-    paddw                   m0, [pw_16]
-    psraw                   m0, 5
-    punpcklbw               m0, m0
-    pshuflw                 m0, m0, q0000
-    punpcklqdq              m0, m0
-%endif
-    mov                   cntd, 8
-.loop:
-    mova   [dstq+strideq*0+ 0], m0
-    mova   [dstq+strideq*0+16], m0
-    mova   [dstq+strideq*1+ 0], m0
-    mova   [dstq+strideq*1+16], m0
-    mova   [dstq+strideq*2+ 0], m0
-    mova   [dstq+strideq*2+16], m0
-    mova   [dstq+stride3q + 0], m0
-    mova   [dstq+stride3q +16], m0
-    lea                   dstq, [dstq+strideq*4]
-    dec                   cntd
-    jg .loop
-    RET
-%endmacro
-
-INIT_XMM sse2
-DC_1D_16to32_FUNCS top,  a
-DC_1D_16to32_FUNCS left, l
-INIT_XMM ssse3
-DC_1D_16to32_FUNCS top,  a
-DC_1D_16to32_FUNCS left, l
-
-%macro DC_1D_AVX2_FUNCS 2 ; dir (top or left), arg (a or l)
-%if HAVE_AVX2_EXTERNAL
-cglobal vp9_ipred_dc_%1_32x32, 4, 4, 3, dst, stride, l, a
-    mova                    m0, [%2q]
-    DEFINE_ARGS dst, stride, stride3, cnt
-    lea               stride3q, [strideq*3]
-    pxor                    m2, m2
-    psadbw                  m0, m2
-    vextracti128           xm1, m0, 1
-    paddw                  xm0, xm1
-    movhlps                xm1, xm0
-    paddw                  xm0, xm1
-    pmulhrsw               xm0, [pw_1024]
-    vpbroadcastb            m0, xm0
-    mov                   cntd, 4
-.loop:
-    mova      [dstq+strideq*0], m0
-    mova      [dstq+strideq*1], m0
-    mova      [dstq+strideq*2], m0
-    mova      [dstq+stride3q ], m0
-    lea                   dstq, [dstq+strideq*4]
-    mova      [dstq+strideq*0], m0
-    mova      [dstq+strideq*1], m0
-    mova      [dstq+strideq*2], m0
-    mova      [dstq+stride3q ], m0
-    lea                   dstq, [dstq+strideq*4]
-    dec                   cntd
-    jg .loop
-    RET
-%endif
-%endmacro
-
-INIT_YMM avx2
-DC_1D_AVX2_FUNCS top,  a
-DC_1D_AVX2_FUNCS left, l
-
-; v
-
-INIT_MMX mmx
-cglobal vp9_ipred_v_8x8, 4, 4, 0, dst, stride, l, a
-    movq                    m0, [aq]
-    DEFINE_ARGS dst, stride, stride3
-    lea               stride3q, [strideq*3]
-    movq      [dstq+strideq*0], m0
-    movq      [dstq+strideq*1], m0
-    movq      [dstq+strideq*2], m0
-    movq      [dstq+stride3q ], m0
-    lea                   dstq, [dstq+strideq*4]
-    movq      [dstq+strideq*0], m0
-    movq      [dstq+strideq*1], m0
-    movq      [dstq+strideq*2], m0
-    movq      [dstq+stride3q ], m0
-    RET
-
-INIT_XMM sse
-cglobal vp9_ipred_v_16x16, 4, 4, 1, dst, stride, l, a
-    mova                    m0, [aq]
-    DEFINE_ARGS dst, stride, stride3, cnt
-    lea               stride3q, [strideq*3]
-    mov                   cntd, 4
-.loop:
-    mova      [dstq+strideq*0], m0
-    mova      [dstq+strideq*1], m0
-    mova      [dstq+strideq*2], m0
-    mova      [dstq+stride3q ], m0
-    lea                   dstq, [dstq+strideq*4]
-    dec                   cntd
-    jg .loop
-    RET
-
-INIT_XMM sse
-cglobal vp9_ipred_v_32x32, 4, 4, 2, dst, stride, l, a
-    mova                    m0, [aq]
-    mova                    m1, [aq+16]
-    DEFINE_ARGS dst, stride, stride3, cnt
-    lea               stride3q, [strideq*3]
-    mov                   cntd, 8
-.loop:
-    mova   [dstq+strideq*0+ 0], m0
-    mova   [dstq+strideq*0+16], m1
-    mova   [dstq+strideq*1+ 0], m0
-    mova   [dstq+strideq*1+16], m1
-    mova   [dstq+strideq*2+ 0], m0
-    mova   [dstq+strideq*2+16], m1
-    mova   [dstq+stride3q + 0], m0
-    mova   [dstq+stride3q +16], m1
-    lea                   dstq, [dstq+strideq*4]
-    dec                   cntd
-    jg .loop
-    RET
-
-INIT_YMM avx
-cglobal vp9_ipred_v_32x32, 4, 4, 1, dst, stride, l, a
-    mova                    m0, [aq]
-    DEFINE_ARGS dst, stride, stride3, cnt
-    lea               stride3q, [strideq*3]
-    mov                   cntd, 4
-.loop:
-    mova      [dstq+strideq*0], m0
-    mova      [dstq+strideq*1], m0
-    mova      [dstq+strideq*2], m0
-    mova      [dstq+stride3q ], m0
-    lea                   dstq, [dstq+strideq*4]
-    mova      [dstq+strideq*0], m0
-    mova      [dstq+strideq*1], m0
-    mova      [dstq+strideq*2], m0
-    mova      [dstq+stride3q ], m0
-    lea                   dstq, [dstq+strideq*4]
-    dec                   cntd
-    jg .loop
-    RET
-
-; h
-
-%macro H_XMM_FUNCS 2
-%if notcpuflag(avx)
-cglobal vp9_ipred_h_4x4, 3, 4, 1, dst, stride, l, stride3
-    movd                    m0, [lq]
-%if cpuflag(ssse3)
-    pshufb                  m0, [pb_4x3_4x2_4x1_4x0]
-%else
-    punpcklbw               m0, m0
-    pshuflw                 m0, m0, q0123
-    punpcklwd               m0, m0
-%endif
-    lea               stride3q, [strideq*3]
-    movd      [dstq+strideq*0], m0
-    psrldq                  m0, 4
-    movd      [dstq+strideq*1], m0
-    psrldq                  m0, 4
-    movd      [dstq+strideq*2], m0
-    psrldq                  m0, 4
-    movd      [dstq+stride3q ], m0
-    RET
-%endif
-
-cglobal vp9_ipred_h_8x8, 3, 5, %1, dst, stride, l, stride3, cnt
-%if cpuflag(ssse3)
-    mova                    m2, [pb_8x1_8x0]
-    mova                    m3, [pb_8x3_8x2]
-%endif
-    lea               stride3q, [strideq*3]
-    mov                   cntq, 1
-.loop:
-    movd                    m0, [lq+cntq*4]
-%if cpuflag(ssse3)
-    pshufb                  m1, m0, m3
-    pshufb                  m0, m2
-%else
-    punpcklbw               m0, m0
-    punpcklwd               m0, m0
-    pshufd                  m1, m0, q2233
-    pshufd                  m0, m0, q0011
-%endif
-    movq      [dstq+strideq*0], m1
-    movhps    [dstq+strideq*1], m1
-    movq      [dstq+strideq*2], m0
-    movhps    [dstq+stride3q ], m0
-    lea                   dstq, [dstq+strideq*4]
-    dec                   cntq
-    jge .loop
-    RET
-
-cglobal vp9_ipred_h_16x16, 3, 5, %2, dst, stride, l, stride3, cnt
-%if cpuflag(ssse3)
-    mova                    m5, [pb_1]
-    mova                    m6, [pb_2]
-    mova                    m7, [pb_3]
-    pxor                    m4, m4
-%endif
-    lea               stride3q, [strideq*3]
-    mov                   cntq, 3
-.loop:
-    movd                    m3, [lq+cntq*4]
-%if cpuflag(ssse3)
-    pshufb                  m0, m3, m7
-    pshufb                  m1, m3, m6
-%else
-    punpcklbw               m3, m3
-    punpcklwd               m3, m3
-    pshufd                  m0, m3, q3333
-    pshufd                  m1, m3, q2222
-%endif
-    mova      [dstq+strideq*0], m0
-    mova      [dstq+strideq*1], m1
-%if cpuflag(ssse3)
-    pshufb                  m2, m3, m5
-    pshufb                  m3, m4
-%else
-    pshufd                  m2, m3, q1111
-    pshufd                  m3, m3, q0000
-%endif
-    mova      [dstq+strideq*2], m2
-    mova      [dstq+stride3q ], m3
-    lea                   dstq, [dstq+strideq*4]
-    dec                   cntq
-    jge .loop
-    RET
-
-cglobal vp9_ipred_h_32x32, 3, 5, %2, dst, stride, l, stride3, cnt
-%if cpuflag(ssse3)
-    mova                    m5, [pb_1]
-    mova                    m6, [pb_2]
-    mova                    m7, [pb_3]
-    pxor                    m4, m4
-%endif
-    lea               stride3q, [strideq*3]
-    mov                   cntq, 7
-.loop:
-    movd                    m3, [lq+cntq*4]
-%if cpuflag(ssse3)
-    pshufb                  m0, m3, m7
-    pshufb                  m1, m3, m6
-%else
-    punpcklbw               m3, m3
-    punpcklwd               m3, m3
-    pshufd                  m0, m3, q3333
-    pshufd                  m1, m3, q2222
-%endif
-    mova   [dstq+strideq*0+ 0], m0
-    mova   [dstq+strideq*0+16], m0
-    mova   [dstq+strideq*1+ 0], m1
-    mova   [dstq+strideq*1+16], m1
-%if cpuflag(ssse3)
-    pshufb                  m2, m3, m5
-    pshufb                  m3, m4
-%else
-    pshufd                  m2, m3, q1111
-    pshufd                  m3, m3, q0000
-%endif
-    mova   [dstq+strideq*2+ 0], m2
-    mova   [dstq+strideq*2+16], m2
-    mova   [dstq+stride3q + 0], m3
-    mova   [dstq+stride3q +16], m3
-    lea                   dstq, [dstq+strideq*4]
-    dec                   cntq
-    jge .loop
-    RET
-%endmacro
-
-INIT_XMM sse2
-H_XMM_FUNCS 2, 4
-INIT_XMM ssse3
-H_XMM_FUNCS 4, 8
-INIT_XMM avx
-H_XMM_FUNCS 4, 8
-
-%if HAVE_AVX2_EXTERNAL
-INIT_YMM avx2
-cglobal vp9_ipred_h_32x32, 3, 5, 8, dst, stride, l, stride3, cnt
-    mova                    m5, [pb_1]
-    mova                    m6, [pb_2]
-    mova                    m7, [pb_3]
-    pxor                    m4, m4
-    lea               stride3q, [strideq*3]
-    mov                   cntq, 7
-.loop:
-    movd                   xm3, [lq+cntq*4]
-    vinserti128             m3, m3, xm3, 1
-    pshufb                  m0, m3, m7
-    pshufb                  m1, m3, m6
-    mova      [dstq+strideq*0], m0
-    mova      [dstq+strideq*1], m1
-    pshufb                  m2, m3, m5
-    pshufb                  m3, m4
-    mova      [dstq+strideq*2], m2
-    mova      [dstq+stride3q ], m3
-    lea                   dstq, [dstq+strideq*4]
-    dec                   cntq
-    jge .loop
-    RET
-%endif
-
-; tm
-
-%macro TM_MMX_FUNCS 0
-cglobal vp9_ipred_tm_4x4, 4, 4, 0, dst, stride, l, a
-    pxor                    m1, m1
-    movd                    m0, [aq]
-    pinsrw                  m2, [aq-1], 0
-    punpcklbw               m0, m1
-    DEFINE_ARGS dst, stride, l, cnt
-%if cpuflag(ssse3)
-    mova                    m3, [pw_m256]
-    mova                    m1, [pw_m255]
-    pshufb                  m2, m3
-%else
-    punpcklbw               m2, m1
-    pshufw                  m2, m2, q0000
-%endif
-    psubw                   m0, m2
-    mov                   cntq, 1
-.loop:
-    pinsrw                  m2, [lq+cntq*2], 0
-%if cpuflag(ssse3)
-    pshufb                  m4, m2, m1
-    pshufb                  m2, m3
-%else
-    punpcklbw               m2, m1
-    pshufw                  m4, m2, q1111
-    pshufw                  m2, m2, q0000
-%endif
-    paddw                   m4, m0
-    paddw                   m2, m0
-    packuswb                m4, m4
-    packuswb                m2, m2
-    movd      [dstq+strideq*0], m4
-    movd      [dstq+strideq*1], m2
-    lea                   dstq, [dstq+strideq*2]
-    dec                   cntq
-    jge .loop
-    RET
-%endmacro
-
-INIT_MMX mmxext
-TM_MMX_FUNCS
-INIT_MMX ssse3
-TM_MMX_FUNCS
-
-%macro TM_XMM_FUNCS 0
-cglobal vp9_ipred_tm_8x8, 4, 4, 5, dst, stride, l, a
-    pxor                    m1, m1
-    movh                    m0, [aq]
-    pinsrw                  m2, [aq-1], 0
-    punpcklbw               m0, m1
-    DEFINE_ARGS dst, stride, l, cnt
-%if cpuflag(ssse3)
-    mova                    m3, [pw_m256]
-    mova                    m1, [pw_m255]
-    pshufb                  m2, m3
-%else
-    punpcklbw               m2, m1
-    punpcklwd               m2, m2
-    pshufd                  m2, m2, q0000
-%endif
-    psubw                   m0, m2
-    mov                   cntq, 3
-.loop:
-    pinsrw                  m2, [lq+cntq*2], 0
-%if cpuflag(ssse3)
-    pshufb                  m4, m2, m1
-    pshufb                  m2, m3
-%else
-    punpcklbw               m2, m1
-    punpcklwd               m2, m2
-    pshufd                  m4, m2, q1111
-    pshufd                  m2, m2, q0000
-%endif
-    paddw                   m4, m0
-    paddw                   m2, m0
-    packuswb                m4, m2
-    movh      [dstq+strideq*0], m4
-    movhps    [dstq+strideq*1], m4
-    lea                   dstq, [dstq+strideq*2]
-    dec                   cntq
-    jge .loop
-    RET
-
-cglobal vp9_ipred_tm_16x16, 4, 4, 8, dst, stride, l, a
-    pxor                    m3, m3
-    mova                    m0, [aq]
-    pinsrw                  m2, [aq-1], 0
-    punpckhbw               m1, m0, m3
-    punpcklbw               m0, m3
-    DEFINE_ARGS dst, stride, l, cnt
-%if cpuflag(ssse3)
-    mova                    m4, [pw_m256]
-    mova                    m3, [pw_m255]
-    pshufb                  m2, m4
-%else
-    punpcklbw               m2, m3
-    punpcklwd               m2, m2
-    pshufd                  m2, m2, q0000
-%endif
-    psubw                   m1, m2
-    psubw                   m0, m2
-    mov                   cntq, 7
-.loop:
-    pinsrw                  m7, [lq+cntq*2], 0
-%if cpuflag(ssse3)
-    pshufb                  m5, m7, m3
-    pshufb                  m7, m4
-%else
-    punpcklbw               m7, m3
-    punpcklwd               m7, m7
-    pshufd                  m5, m7, q1111
-    pshufd                  m7, m7, q0000
-%endif
-    paddw                   m2, m5, m0
-    paddw                   m5, m1
-    paddw                   m6, m7, m0
-    paddw                   m7, m1
-    packuswb                m2, m5
-    packuswb                m6, m7
-    mova      [dstq+strideq*0], m2
-    mova      [dstq+strideq*1], m6
-    lea                   dstq, [dstq+strideq*2]
-    dec                   cntq
-    jge .loop
-    RET
-
-%if ARCH_X86_64
-%define mem 0
-%else
-%define mem 64
-%endif
-cglobal vp9_ipred_tm_32x32, 4, 4, 14, mem, dst, stride, l, a
-    pxor                    m5, m5
-    pinsrw                  m4, [aq-1], 0
-    mova                    m0, [aq]
-    mova                    m2, [aq+16]
-    DEFINE_ARGS dst, stride, l, cnt
-%if cpuflag(ssse3)
-%if ARCH_X86_64
-    mova                   m12, [pw_m256]
-    mova                   m13, [pw_m255]
-%define pw_m256_reg m12
-%define pw_m255_reg m13
-%else
-%define pw_m256_reg [pw_m256]
-%define pw_m255_reg [pw_m255]
-%endif
-    pshufb                  m4, pw_m256_reg
-%else
-    punpcklbw               m4, m5
-    punpcklwd               m4, m4
-    pshufd                  m4, m4, q0000
-%endif
-    punpckhbw               m1, m0,  m5
-    punpckhbw               m3, m2,  m5
-    punpcklbw               m0, m5
-    punpcklbw               m2, m5
-    psubw                   m1, m4
-    psubw                   m0, m4
-    psubw                   m3, m4
-    psubw                   m2, m4
-%if ARCH_X86_64
-    SWAP                     0, 8
-    SWAP                     1, 9
-    SWAP                     2, 10
-    SWAP                     3, 11
-%else
-    mova            [rsp+0*16], m0
-    mova            [rsp+1*16], m1
-    mova            [rsp+2*16], m2
-    mova            [rsp+3*16], m3
-%endif
-    mov                   cntq, 15
-.loop:
-    pinsrw                  m3, [lq+cntq*2], 0
-%if cpuflag(ssse3)
-    pshufb                  m7, m3, pw_m255_reg
-    pshufb                  m3, pw_m256_reg
-%else
-    pxor                    m7, m7
-    punpcklbw               m3, m7
-    punpcklwd               m3, m3
-    pshufd                  m7, m3, q1111
-    pshufd                  m3, m3, q0000
-%endif
-%if ARCH_X86_64
-    paddw                   m4, m7, m8
-    paddw                   m5, m7, m9
-    paddw                   m6, m7, m10
-    paddw                   m7, m11
-    paddw                   m0, m3, m8
-    paddw                   m1, m3, m9
-    paddw                   m2, m3, m10
-    paddw                   m3, m11
-%else
-    paddw                   m4, m7, [rsp+0*16]
-    paddw                   m5, m7, [rsp+1*16]
-    paddw                   m6, m7, [rsp+2*16]
-    paddw                   m7, [rsp+3*16]
-    paddw                   m0, m3, [rsp+0*16]
-    paddw                   m1, m3, [rsp+1*16]
-    paddw                   m2, m3, [rsp+2*16]
-    paddw                   m3, [rsp+3*16]
-%endif
-    packuswb                m4, m5
-    packuswb                m6, m7
-    packuswb                m0, m1
-    packuswb                m2, m3
-    mova   [dstq+strideq*0+ 0], m4
-    mova   [dstq+strideq*0+16], m6
-    mova   [dstq+strideq*1+ 0], m0
-    mova   [dstq+strideq*1+16], m2
-    lea                   dstq, [dstq+strideq*2]
-    dec                   cntq
-    jge .loop
-    RET
-%undef pw_m256_reg
-%undef pw_m255_reg
-%undef mem
-%endmacro
-
-INIT_XMM sse2
-TM_XMM_FUNCS
-INIT_XMM ssse3
-TM_XMM_FUNCS
-INIT_XMM avx
-TM_XMM_FUNCS
-
-%if HAVE_AVX2_EXTERNAL
-INIT_YMM avx2
-cglobal vp9_ipred_tm_32x32, 4, 4, 8, dst, stride, l, a
-    pxor                    m3, m3
-    pinsrw                 xm2, [aq-1], 0
-    vinserti128             m2, m2, xm2, 1
-    mova                    m0, [aq]
-    DEFINE_ARGS dst, stride, l, cnt
-    mova                    m4, [pw_m256]
-    mova                    m5, [pw_m255]
-    pshufb                  m2, m4
-    punpckhbw               m1, m0, m3
-    punpcklbw               m0, m3
-    psubw                   m1, m2
-    psubw                   m0, m2
-    mov                   cntq, 15
-.loop:
-    pinsrw                 xm7, [lq+cntq*2], 0
-    vinserti128             m7, m7, xm7, 1
-    pshufb                  m3, m7, m5
-    pshufb                  m7, m4
-    paddw                   m2, m3, m0
-    paddw                   m3, m1
-    paddw                   m6, m7, m0
-    paddw                   m7, m1
-    packuswb                m2, m3
-    packuswb                m6, m7
-    mova      [dstq+strideq*0], m2
-    mova      [dstq+strideq*1], m6
-    lea                   dstq, [dstq+strideq*2]
-    dec                   cntq
-    jge .loop
-    RET
-%endif
-
-; dl
-
-%macro LOWPASS 4 ; left [dst], center, right, tmp
-    pxor                   m%4, m%1, m%3
-    pand                   m%4, [pb_1]
-    pavgb                  m%1, m%3
-    psubusb                m%1, m%4
-    pavgb                  m%1, m%2
-%endmacro
-
-%macro DL_MMX_FUNCS 0
-cglobal vp9_ipred_dl_4x4, 4, 4, 0, dst, stride, l, a
-    movq                    m1, [aq]
-%if cpuflag(ssse3)
-    pshufb                  m0, m1, [pb_0to5_2x7]
-    pshufb                  m2, m1, [pb_2to6_3x7]
-%else
-    punpckhbw               m3, m1, m1              ; 44556677
-    pand                    m0, m1, [pb_6xm1_2x0]   ; 012345__
-    pand                    m3, [pb_6x0_2xm1]       ; ______77
-    psrlq                   m2, m1, 16              ; 234567__
-    por                     m0, m3                  ; 01234577
-    por                     m2, m3                  ; 23456777
-%endif
-    psrlq                   m1, 8
-    LOWPASS                  0, 1, 2, 3
-
-    pshufw                  m1, m0, q3321
-    movd      [dstq+strideq*0], m0
-    movd      [dstq+strideq*2], m1
-    psrlq                   m0, 8
-    psrlq                   m1, 8
-    add                   dstq, strideq
-    movd      [dstq+strideq*0], m0
-    movd      [dstq+strideq*2], m1
-    RET
-%endmacro
-
-INIT_MMX mmxext
-DL_MMX_FUNCS
-INIT_MMX ssse3
-DL_MMX_FUNCS
-
-%macro DL_XMM_FUNCS 0
-cglobal vp9_ipred_dl_8x8, 4, 4, 4, dst, stride, stride5, a
-    movq                    m0, [aq]
-    lea               stride5q, [strideq*5]
-%if cpuflag(ssse3)
-    pshufb                  m1, m0, [pb_1to6_10x7]
-%else
-    punpcklbw               m1, m0, m0              ; 0011223344556677
-    punpckhwd               m1, m1                  ; 4x4,4x5,4x6,4x7
-%endif
-    shufps                  m0, m1, q3310
-%if notcpuflag(ssse3)
-    psrldq                  m1, m0, 1
-    shufps                  m1, m0, q3210
-%endif
-    psrldq                  m2, m1, 1
-    LOWPASS                  0, 1, 2, 3
-
-    pshufd                  m1, m0, q3321
-    movq      [dstq+strideq*0], m0
-    movq      [dstq+strideq*4], m1
-    psrldq                  m0, 1
-    psrldq                  m1, 1
-    movq      [dstq+strideq*1], m0
-    movq      [dstq+stride5q ], m1
-    lea                   dstq, [dstq+strideq*2]
-    psrldq                  m0, 1
-    psrldq                  m1, 1
-    movq      [dstq+strideq*0], m0
-    movq      [dstq+strideq*4], m1
-    psrldq                  m0, 1
-    psrldq                  m1, 1
-    movq      [dstq+strideq*1], m0
-    movq      [dstq+stride5q ], m1
-    RET
-
-cglobal vp9_ipred_dl_16x16, 4, 4, 6, dst, stride, l, a
-    mova                    m0, [aq]
-%if cpuflag(ssse3)
-    mova                    m5, [pb_1toE_2xF]
-    pshufb                  m1, m0, m5
-    pshufb                  m2, m1, m5
-    pshufb                  m4, m0, [pb_15]
-%else
-    pand                    m5, m0, [pb_15x0_1xm1]      ; _______________F
-    psrldq                  m1, m0, 1                   ; 123456789ABCDEF_
-    por                     m1, m5                      ; 123456789ABCDEFF
-    psrldq                  m2, m1, 1                   ; 23456789ABCDEFF_
-    por                     m2, m5                      ; 23456789ABCDEFFF
-    pshufhw                 m4, m1, q3333               ; xxxxxxxxFFFFFFFF
-%endif
-    LOWPASS                  0, 1, 2, 3
-    DEFINE_ARGS dst, stride, cnt, stride9
-    lea               stride9q, [strideq+strideq*8]
-    mov                   cntd, 4
-
-.loop:
-    movhlps                 m4, m0
-    mova      [dstq+strideq*0], m0
-%if cpuflag(ssse3)
-    pshufb                  m0, m5
-%else
-    psrldq                  m0, 1
-    por                     m0, m5
-%endif
-    mova      [dstq+strideq*8], m4
-    movhlps                 m4, m0
-    mova      [dstq+strideq*1], m0
-%if cpuflag(ssse3)
-    pshufb                  m0, m5
-%else
-    psrldq                  m0, 1
-    por                     m0, m5
-%endif
-    mova      [dstq+stride9q ], m4
-    lea                   dstq, [dstq+strideq*2]
-    dec                   cntd
-    jg .loop
-    RET
-
-cglobal vp9_ipred_dl_32x32, 4, 5, 8, dst, stride, cnt, a, dst16
-    mova                    m0, [aq]
-    mova                    m1, [aq+16]
-    PALIGNR                 m2, m1, m0, 1, m4
-    PALIGNR                 m3, m1, m0, 2, m4
-    LOWPASS                  0, 2, 3, 4
-%if cpuflag(ssse3)
-    mova                    m5, [pb_1toE_2xF]
-    pshufb                  m2, m1, m5
-    pshufb                  m3, m2, m5
-    pshufb                  m6, m1, [pb_15]
-    mova                    m7, m6
-%else
-    pand                    m5, m1, [pb_15x0_1xm1]      ; _______________F
-    psrldq                  m2, m1, 1                   ; 123456789ABCDEF_
-    por                     m2, m5                      ; 123456789ABCDEFF
-    psrldq                  m3, m2, 1                   ; 23456789ABCDEFF_
-    por                     m3, m5                      ; 23456789ABCDEFFF
-    pshufhw                 m7, m2, q3333               ; xxxxxxxxFFFFFFFF
-    pshufd                  m6, m7, q3333
-%endif
-    LOWPASS                  1, 2, 3, 4
-    lea                 dst16q, [dstq  +strideq*8]
-    mov                   cntd, 8
-    lea                 dst16q, [dst16q+strideq*8]
-.loop:
-    movhlps                 m7, m1
-    mova [dstq  +strideq*0+ 0], m0
-    mova [dstq  +strideq*0+16], m1
-    movhps [dstq+strideq*8+ 0], m0
-    movq [dstq  +strideq*8+ 8], m1
-    mova [dstq  +strideq*8+16], m7
-    mova [dst16q+strideq*0+ 0], m1
-    mova [dst16q+strideq*0+16], m6
-    mova [dst16q+strideq*8+ 0], m7
-    mova [dst16q+strideq*8+16], m6
-%if cpuflag(avx)
-    vpalignr                m0, m1, m0, 1
-    pshufb                  m1, m5
-%elif cpuflag(ssse3)
-    palignr                 m2, m1, m0, 1
-    pshufb                  m1, m5
-    mova                    m0, m2
-%else
-    mova                    m4, m1
-    psrldq                  m0, 1
-    pslldq                  m4, 15
-    psrldq                  m1, 1
-    por                     m0, m4
-    por                     m1, m5
-%endif
-    add                   dstq, strideq
-    add                 dst16q, strideq
-    dec                   cntd
-    jg .loop
-    RET
-%endmacro
-
-INIT_XMM sse2
-DL_XMM_FUNCS
-INIT_XMM ssse3
-DL_XMM_FUNCS
-INIT_XMM avx
-DL_XMM_FUNCS
-
-; dr
-
-%macro DR_MMX_FUNCS 0
-cglobal vp9_ipred_dr_4x4, 4, 4, 0, dst, stride, l, a
-    movd                    m0, [lq]
-    punpckldq               m0, [aq-1]
-    movd                    m1, [aq+3]
-    DEFINE_ARGS dst, stride, stride3
-    lea               stride3q, [strideq*3]
-    PALIGNR                 m1, m0, 1, m3
-    psrlq                   m2, m1, 8
-    LOWPASS                  0, 1, 2, 3
-
-    movd      [dstq+stride3q ], m0
-    psrlq                   m0, 8
-    movd      [dstq+strideq*2], m0
-    psrlq                   m0, 8
-    movd      [dstq+strideq*1], m0
-    psrlq                   m0, 8
-    movd      [dstq+strideq*0], m0
-    RET
-%endmacro
-
-INIT_MMX mmxext
-DR_MMX_FUNCS
-INIT_MMX ssse3
-DR_MMX_FUNCS
-
-%macro DR_XMM_FUNCS 0
-cglobal vp9_ipred_dr_8x8, 4, 4, 4, dst, stride, l, a
-    movq                    m1, [lq]
-    movhps                  m1, [aq-1]
-    movd                    m2, [aq+7]
-    DEFINE_ARGS dst, stride, stride3
-    lea               stride3q, [strideq*3]
-    pslldq                  m0, m1, 1
-    PALIGNR                 m2, m1, 1, m3
-    LOWPASS                  0, 1, 2, 3
-
-    movhps    [dstq+strideq*0], m0
-    pslldq                  m0, 1
-    movhps    [dstq+strideq*1], m0
-    pslldq                  m0, 1
-    movhps    [dstq+strideq*2], m0
-    pslldq                  m0, 1
-    movhps    [dstq+stride3q ], m0
-    pslldq                  m0, 1
-    lea                   dstq, [dstq+strideq*4]
-    movhps    [dstq+strideq*0], m0
-    pslldq                  m0, 1
-    movhps    [dstq+strideq*1], m0
-    pslldq                  m0, 1
-    movhps    [dstq+strideq*2], m0
-    pslldq                  m0, 1
-    movhps    [dstq+stride3q ], m0
-    RET
-
-cglobal vp9_ipred_dr_16x16, 4, 4, 6, dst, stride, l, a
-    mova                    m1, [lq]
-    movu                    m2, [aq-1]
-    movd                    m4, [aq+15]
-    DEFINE_ARGS dst, stride, stride9, cnt
-    lea               stride9q, [strideq *3]
-    mov                   cntd, 4
-    lea               stride9q, [stride9q*3]
-    PALIGNR                 m4, m2, 1, m5
-    PALIGNR                 m3, m2, m1, 15, m5
-    LOWPASS                  3,  2, 4, 5
-    pslldq                  m0, m1, 1
-    PALIGNR                 m2, m1, 1, m4
-    LOWPASS                  0,  1, 2, 4
-
-.loop:
-    mova    [dstq+strideq*0  ], m3
-    movhps  [dstq+strideq*8+0], m0
-    movq    [dstq+strideq*8+8], m3
-    PALIGNR                 m3, m0, 15, m1
-    pslldq                  m0, 1
-    mova    [dstq+strideq*1  ], m3
-    movhps  [dstq+stride9q +0], m0
-    movq    [dstq+stride9q +8], m3
-    PALIGNR                 m3, m0, 15, m1
-    pslldq                  m0, 1
-    lea                   dstq, [dstq+strideq*2]
-    dec                   cntd
-    jg .loop
-    RET
-
-cglobal vp9_ipred_dr_32x32, 4, 4, 8, dst, stride, l, a
-    mova                    m1, [lq]
-    mova                    m2, [lq+16]
-    movu                    m3, [aq-1]
-    movu                    m4, [aq+15]
-    movd                    m5, [aq+31]
-    DEFINE_ARGS dst, stride, stride8, cnt
-    lea               stride8q, [strideq*8]
-    PALIGNR                 m5, m4, 1, m7
-    PALIGNR                 m6, m4, m3, 15, m7
-    LOWPASS                  5,  4,  6,  7
-    PALIGNR                 m4, m3, 1, m7
-    PALIGNR                 m6, m3, m2, 15, m7
-    LOWPASS                  4,  3,  6,  7
-    PALIGNR                 m3, m2, 1, m7
-    PALIGNR                 m6, m2, m1, 15, m7
-    LOWPASS                  3,  2,  6,  7
-    PALIGNR                 m2, m1, 1, m6
-    pslldq                  m0, m1, 1
-    LOWPASS                  2,  1,  0,  6
-    mov                   cntd, 16
-
-    ; out=m2/m3/m4/m5
-.loop:
-    mova  [dstq+stride8q*0+ 0], m4
-    mova  [dstq+stride8q*0+16], m5
-    mova  [dstq+stride8q*2+ 0], m3
-    mova  [dstq+stride8q*2+16], m4
-    PALIGNR                 m5, m4, 15, m6
-    PALIGNR                 m4, m3, 15, m6
-    PALIGNR                 m3, m2, 15, m6
-    pslldq                  m2, 1
-    add                   dstq, strideq
-    dec                   cntd
-    jg .loop
-    RET
-%endmacro
-
-INIT_XMM sse2
-DR_XMM_FUNCS
-INIT_XMM ssse3
-DR_XMM_FUNCS
-INIT_XMM avx
-DR_XMM_FUNCS
-
-; vl
-
-INIT_MMX mmxext
-cglobal vp9_ipred_vl_4x4, 4, 4, 0, dst, stride, l, a
-    movq                    m0, [aq]
-    psrlq                   m1, m0, 8
-    psrlq                   m2, m1, 8
-    LOWPASS                  2,  1, 0, 3
-    pavgb                   m1, m0
-    movd      [dstq+strideq*0], m1
-    movd      [dstq+strideq*1], m2
-    lea                   dstq, [dstq+strideq*2]
-    psrlq                   m1, 8
-    psrlq                   m2, 8
-    movd      [dstq+strideq*0], m1
-    movd      [dstq+strideq*1], m2
-    RET
-
-%macro VL_XMM_FUNCS 0
-cglobal vp9_ipred_vl_8x8, 4, 4, 4, dst, stride, l, a
-    movq                    m0, [aq]
-%if cpuflag(ssse3)
-    pshufb                  m0, [pb_0to6_9x7]
-%else
-    punpcklbw               m1, m0, m0
-    punpckhwd               m1, m1
-    shufps                  m0, m1, q3310
-%endif
-    DEFINE_ARGS dst, stride, stride3
-    lea               stride3q, [strideq*3]
-    psrldq                  m1, m0, 1
-    psrldq                  m2, m0, 2
-    LOWPASS                  2,  1,  0,  3
-    pavgb                   m1, m0
-
-    movq      [dstq+strideq*0], m1
-    movq      [dstq+strideq*1], m2
-    psrldq                  m1, 1
-    psrldq                  m2, 1
-    movq      [dstq+strideq*2], m1
-    movq      [dstq+stride3q ], m2
-    lea                   dstq, [dstq+strideq*4]
-    psrldq                  m1, 1
-    psrldq                  m2, 1
-    movq      [dstq+strideq*0], m1
-    movq      [dstq+strideq*1], m2
-    psrldq                  m1, 1
-    psrldq                  m2, 1
-    movq      [dstq+strideq*2], m1
-    movq      [dstq+stride3q ], m2
-    RET
-
-cglobal vp9_ipred_vl_16x16, 4, 4, 5, dst, stride, l, a
-    mova                    m0, [aq]
-    DEFINE_ARGS dst, stride, stride3, cnt
-    lea               stride3q, [strideq*3]
-%if cpuflag(ssse3)
-    mova                    m4, [pb_1toE_2xF]
-    pshufb                  m1, m0, m4
-    pshufb                  m2, m1, m4
-%else
-    pand                    m4, m0, [pb_15x0_1xm1]  ; _______________F
-    psrldq                  m1, m0, 1               ; 123456789ABCDEF_
-    por                     m1, m4                  ; 123456789ABCDEFF
-    psrldq                  m2, m1, 1               ; 23456789ABCDEFF_
-    por                     m2, m4                  ; 23456789ABCDEFFF
-%endif
-    LOWPASS                  2,  1,  0, 3
-    pavgb                   m1, m0
-    mov                   cntd, 4
-.loop:
-    mova      [dstq+strideq*0], m1
-    mova      [dstq+strideq*1], m2
-%if cpuflag(ssse3)
-    pshufb                  m1, m4
-    pshufb                  m2, m4
-%else
-    psrldq                  m1, 1
-    psrldq                  m2, 1
-    por                     m1, m4
-    por                     m2, m4
-%endif
-    mova      [dstq+strideq*2], m1
-    mova      [dstq+stride3q ], m2
-%if cpuflag(ssse3)
-    pshufb                  m1, m4
-    pshufb                  m2, m4
-%else
-    psrldq                  m1, 1
-    psrldq                  m2, 1
-    por                     m1, m4
-    por                     m2, m4
-%endif
-    lea                   dstq, [dstq+strideq*4]
-    dec                   cntd
-    jg .loop
-    RET
-
-cglobal vp9_ipred_vl_32x32, 4, 4, 7, dst, stride, l, a
-    mova                    m0, [aq]
-    mova                    m5, [aq+16]
-    DEFINE_ARGS dst, stride, dst16, cnt
-    PALIGNR                 m2, m5, m0, 1, m4
-    PALIGNR                 m3, m5, m0, 2, m4
-    lea                 dst16q, [dstq  +strideq*8]
-    LOWPASS                  3,  2,  0, 6
-    pavgb                   m2, m0
-%if cpuflag(ssse3)
-    mova                    m4, [pb_1toE_2xF]
-    pshufb                  m0, m5, m4
-    pshufb                  m1, m0, m4
-%else
-    pand                    m4, m5, [pb_15x0_1xm1]  ; _______________F
-    psrldq                  m0, m5, 1               ; 123456789ABCDEF_
-    por                     m0, m4                  ; 123456789ABCDEFF
-    psrldq                  m1, m0, 1               ; 23456789ABCDEFF_
-    por                     m1, m4                  ; 23456789ABCDEFFF
-%endif
-    lea                 dst16q, [dst16q+strideq*8]
-    LOWPASS                  1,  0,  5, 6
-    pavgb                   m0, m5
-%if cpuflag(ssse3)
-    pshufb                  m5, [pb_15]
-%else
-    punpckhbw               m5, m4, m4
-    pshufhw                 m5, m5, q3333
-    punpckhqdq              m5, m5
-%endif
-    mov                   cntd, 8
-
-.loop:
-%macro %%write 3
-    mova    [dstq+stride%1+ 0], %2
-    mova    [dstq+stride%1+16], %3
-    movhps  [dst16q+stride%1 ], %2
-    movu  [dst16q+stride%1+ 8], %3
-    movq  [dst16q+stride%1+24], m5
-%if cpuflag(avx)
-    palignr                 %2, %3, %2, 1
-    pshufb                  %3, m4
-%elif cpuflag(ssse3)
-    palignr                 m6, %3, %2, 1
-    pshufb                  %3, m4
-    mova                    %2, m6
-%else
-    pslldq                  m6, %3, 15
-    psrldq                  %3, 1
-    psrldq                  %2, 1
-    por                     %3, m4
-    por                     %2, m6
-%endif
-%endmacro
-
-    %%write                q*0, m2, m0
-    %%write                q*1, m3, m1
-    lea                   dstq, [dstq  +strideq*2]
-    lea                 dst16q, [dst16q+strideq*2]
-    dec                   cntd
-    jg .loop
-    RET
-%endmacro
-
-INIT_XMM sse2
-VL_XMM_FUNCS
-INIT_XMM ssse3
-VL_XMM_FUNCS
-INIT_XMM avx
-VL_XMM_FUNCS
-
-; vr
-
-%macro VR_MMX_FUNCS 0
-cglobal vp9_ipred_vr_4x4, 4, 4, 0, dst, stride, l, a
-    movq                    m1, [aq-1]
-    punpckldq               m2, [lq]
-    movd                    m0, [aq]
-    DEFINE_ARGS dst, stride, stride3
-    lea               stride3q, [strideq*3]
-    pavgb                   m0, m1
-    PALIGNR                 m1, m2, 5, m3
-    psrlq                   m2, m1, 8
-    psllq                   m3, m1, 8
-    LOWPASS                  2,  1, 3, 4
-
-    ; ABCD <- for the following predictor:
-    ; EFGH
-    ; IABC  | m0 contains ABCDxxxx
-    ; JEFG  | m2 contains xJIEFGHx
-
-%if cpuflag(ssse3)
-    punpckldq               m0, m2
-    pshufb                  m2, [pb_13456_3xm1]
-    movd      [dstq+strideq*0], m0
-    pshufb                  m0, [pb_6012_4xm1]
-    movd      [dstq+stride3q ], m2
-    psrlq                   m2, 8
-    movd      [dstq+strideq*2], m0
-    movd      [dstq+strideq*1], m2
-%else
-    psllq                   m1, m2, 40
-    psrlq                   m2, 24
-    movd      [dstq+strideq*0], m0
-    movd      [dstq+strideq*1], m2
-    PALIGNR                 m0, m1, 7, m3
-    psllq                   m1, 8
-    PALIGNR                 m2, m1, 7, m3
-    movd      [dstq+strideq*2], m0
-    movd      [dstq+stride3q ], m2
-%endif
-    RET
-%endmacro
-
-INIT_MMX mmxext
-VR_MMX_FUNCS
-INIT_MMX ssse3
-VR_MMX_FUNCS
-
-%macro VR_XMM_FUNCS 1 ; n_xmm_regs for 16x16
-cglobal vp9_ipred_vr_8x8, 4, 4, 5, dst, stride, l, a
-    movu                    m1, [aq-1]
-    movhps                  m2, [lq]
-    movq                    m0, [aq]
-    DEFINE_ARGS dst, stride, stride3
-    lea               stride3q, [strideq*3]
-    pavgb                   m0, m1
-    PALIGNR                 m1, m2, 9, m3
-    pslldq                  m2, m1, 1
-    pslldq                  m3, m1, 2
-    LOWPASS                  1,  2, 3, 4
-
-    ; ABCDEFGH <- for the following predictor:
-    ; IJKLMNOP
-    ; QABCDEFG  | m0 contains ABCDEFGHxxxxxxxx
-    ; RIJKLMNO  | m1 contains xxVUTSRQIJKLMNOP
-    ; SQABCDEF
-    ; TRIJKLMN
-    ; USQABCDE
-    ; VTRIJKLM
-
-%if cpuflag(ssse3)
-    punpcklqdq              m0, m1 ; ABCDEFGHxxVUTSRQ
-%endif
-    movq      [dstq+strideq*0], m0
-    movhps    [dstq+strideq*1], m1
-%if cpuflag(ssse3)
-    pshufb                  m0, [pb_6xm1_BDF_0to6]  ; xxxxxxUSQABCDEFG
-    pshufb                  m1, [pb_6xm1_246_8toE]  ; xxxxxxVTRIJKLMNO
-%else
-    psrlw                   m2, m1, 8               ; x_U_S_Q_xxxxxxxx
-    pand                    m3, m1, [pw_255]        ; x_V_T_R_xxxxxxxx
-    packuswb                m3, m2                  ; xVTRxxxxxUSQxxxx
-    pslldq                  m3, 4                   ; xxxxxVTRxxxxxUSQ
-    PALIGNR                 m0, m3, 7, m4           ; xxxxxxUSQABCDEFG
-    psrldq                  m1, 8
-    pslldq                  m3, 8
-    PALIGNR                 m1, m3, 7, m4           ; xxxxxxVTRIJKLMNO
-%endif
-    movhps    [dstq+strideq*2], m0
-    movhps    [dstq+stride3q ], m1
-    lea                   dstq, [dstq+strideq*4]
-    pslldq                  m0, 1
-    pslldq                  m1, 1
-    movhps    [dstq+strideq*0], m0
-    movhps    [dstq+strideq*1], m1
-    pslldq                  m0, 1
-    pslldq                  m1, 1
-    movhps    [dstq+strideq*2], m0
-    movhps    [dstq+stride3q ], m1
-    RET
-
-cglobal vp9_ipred_vr_16x16, 4, 4, %1, dst, stride, l, a
-    mova                    m0, [aq]
-    movu                    m1, [aq-1]
-    mova                    m2, [lq]
-    DEFINE_ARGS dst, stride, stride3, cnt
-    lea               stride3q, [strideq*3]
-    PALIGNR                 m3, m1, m2, 15, m6
-    LOWPASS                  3,  1,  0,  4
-    pavgb                   m0, m1
-    PALIGNR                 m1, m2,  1, m6
-    pslldq                  m4, m2,  1
-    LOWPASS                  1,  2,  4,  5
-%if cpuflag(ssse3)
-    pshufb                  m1, [pb_02468ACE_13579BDF]
-%else
-    psrlw                   m5, m1, 8
-    pand                    m1, [pw_255]
-    packuswb                m1, m5
-%endif
-    mov                   cntd, 4
-
-.loop:
-    movlhps                 m2, m1
-    mova      [dstq+strideq*0], m0
-    mova      [dstq+strideq*1], m3
-    PALIGNR                 m4, m0, m1, 15, m6
-    PALIGNR                 m5, m3, m2, 15, m6
-    mova      [dstq+strideq*2], m4
-    mova      [dstq+stride3q ], m5
-    lea                   dstq, [dstq+strideq*4]
-    PALIGNR                 m0, m1, 14, m6
-    PALIGNR                 m3, m2, 14, m6
-    pslldq                  m1, 2
-    dec                   cntd
-    jg .loop
-    RET
-
-cglobal vp9_ipred_vr_32x32, 4, 4, 9, dst, stride, l, a
-    mova                    m0, [aq]
-    mova                    m2, [aq+16]
-    movu                    m1, [aq-1]
-    PALIGNR                 m3, m2, m0, 15, m6
-    PALIGNR                 m4, m2, m0, 14, m6
-    LOWPASS                  4,  3,  2,  5
-    pavgb                   m3, m2
-    mova                    m2, [lq+16]
-    PALIGNR                 m5, m1, m2, 15, m6
-    LOWPASS                  5,  1,  0,  6
-    pavgb                   m0, m1
-    mova                    m6, [lq]
-%if ARCH_X86_64
-    SWAP                     0, 8
-%else
-    mova                [dstq], m0
-%endif
-    PALIGNR                 m1, m2,  1, m0
-    PALIGNR                 m7, m2, m6, 15, m0
-    LOWPASS                  1,  2,  7,  0
-    PALIGNR                 m2, m6,  1, m0
-    pslldq                  m7, m6,  1
-    LOWPASS                  2,  6,  7,  0
-%if cpuflag(ssse3)
-    pshufb                  m1, [pb_02468ACE_13579BDF]
-    pshufb                  m2, [pb_02468ACE_13579BDF]
-%else
-    psrlw                   m0, m1, 8
-    psrlw                   m6, m2, 8
-    pand                    m1, [pw_255]
-    pand                    m2, [pw_255]
-    packuswb                m1, m0
-    packuswb                m2, m6
-%endif
-    DEFINE_ARGS dst, stride, dst16, cnt
-    lea                 dst16q, [dstq  +strideq*8]
-    lea                 dst16q, [dst16q+strideq*8]
-    SBUTTERFLY             qdq,  2,  1,  6
-%if ARCH_X86_64
-    SWAP                     0, 8
-%else
-    mova                    m0, [dstq]
-%endif
-    mov                   cntd, 8
-
-.loop:
-    ; even lines (0, 2, 4, ...): m1 | m0, m3
-    ;  odd lines (1, 3, 5, ...): m2 | m5, m4
-%macro %%write 4
-    mova    [dstq+stride%1+ 0], %3
-    mova    [dstq+stride%1+16], %4
-    movhps  [dst16q+stride%1 ], %2
-    movu  [dst16q+stride%1+ 8], %3
-    movq  [dst16q+stride%1+24], %4
-    PALIGNR                 %4, %3, 15, m6
-    PALIGNR                 %3, %2, 15, m6
-    pslldq                  %2,  1
-%endmacro
-
-    %%write                q*0, m1, m0, m3
-    %%write                q*1, m2, m5, m4
-    lea                   dstq, [dstq  +strideq*2]
-    lea                 dst16q, [dst16q+strideq*2]
-    dec                   cntd
-    jg .loop
-    RET
-%endmacro
-
-INIT_XMM sse2
-VR_XMM_FUNCS 7
-INIT_XMM ssse3
-VR_XMM_FUNCS 6
-INIT_XMM avx
-VR_XMM_FUNCS 6
-
-; hd
-
-INIT_MMX mmxext
-cglobal vp9_ipred_hd_4x4, 4, 4, 0, dst, stride, l, a
-    movd                    m0, [lq]
-    punpckldq               m0, [aq-1]
-    DEFINE_ARGS dst, stride, stride3
-    lea               stride3q, [strideq*3]
-    psrlq                   m1, m0, 8
-    psrlq                   m2, m1, 8
-    LOWPASS                  2,  1, 0,  3
-    pavgb                   m1, m0
-
-    ; DHIJ <- for the following predictor:
-    ; CGDH
-    ; BFCG  | m1 contains ABCDxxxx
-    ; AEBF  | m2 contains EFGHIJxx
-
-    punpcklbw               m1, m2
-    punpckhdq               m0, m1, m2
-
-    ; m1 contains AEBFCGDH
-    ; m0 contains CGDHIJxx
-
-    movd      [dstq+stride3q ], m1
-    movd      [dstq+strideq*1], m0
-    psrlq                   m1, 16
-    psrlq                   m0, 16
-    movd      [dstq+strideq*2], m1
-    movd      [dstq+strideq*0], m0
-    RET
-
-%macro HD_XMM_FUNCS 0
-cglobal vp9_ipred_hd_8x8, 4, 4, 5, dst, stride, l, a
-    movq                    m0, [lq]
-    movhps                  m0, [aq-1]
-    DEFINE_ARGS dst, stride, stride3, dst4
-    lea               stride3q, [strideq*3]
-    lea                  dst4q, [dstq+strideq*4]
-    psrldq                  m1, m0, 1
-    psrldq                  m2, m1, 1
-    LOWPASS                  2,  1,  0,  3
-    pavgb                   m1, m0
-
-    ; HPQRSTUV <- for the following predictor
-    ; GOHPQRST
-    ; FNGOHPQR  | m1 contains ABCDEFGHxxxxxxxx
-    ; EMFNGOHP  | m2 contains IJKLMNOPQRSTUVxx
-    ; DLEMFNGO
-    ; CKDLEMFN
-    ; BJCKDLEM
-    ; AIBJCKDL
-
-    punpcklbw               m1, m2
-    movhlps                 m2, m2
-
-    ; m1 contains AIBJCKDLEMFNGOHP
-    ; m2 contains QRSTUVxxxxxxxxxx
-
-    movhps   [dstq +stride3q ], m1
-    movq     [dst4q+stride3q ], m1
-    PALIGNR                 m3, m2, m1, 2, m4
-    movhps   [dstq +strideq*2], m3
-    movq     [dst4q+strideq*2], m3
-    PALIGNR                 m3, m2, m1, 4, m4
-    movhps   [dstq +strideq*1], m3
-    movq     [dst4q+strideq*1], m3
-    PALIGNR                 m2, m1, 6, m4
-    movhps   [dstq +strideq*0], m2
-    movq     [dst4q+strideq*0], m2
-    RET
-
-cglobal vp9_ipred_hd_16x16, 4, 6, 7, dst, stride, l, a
-    mova                    m0, [lq]
-    movu                    m3, [aq-1]
-    DEFINE_ARGS dst, stride, stride4, dst4, dst8, dst12
-    lea               stride4q, [strideq*4]
-    lea                  dst4q, [dstq +stride4q]
-    lea                  dst8q, [dst4q+stride4q]
-    lea                 dst12q, [dst8q+stride4q]
-    psrldq                  m4, m3,  1
-    psrldq                  m5, m3,  2
-    LOWPASS                  5,  4,  3,  6
-    PALIGNR                 m1, m3, m0,  1, m6
-    PALIGNR                 m2, m3, m0,  2, m6
-    LOWPASS                  2,  1,  0,  6
-    pavgb                   m1, m0
-    SBUTTERFLY              bw,  1,  2,  6
-
-    ; I PROBABLY INVERTED L0 ad L16 here
-    ; m1, m2, m5
-.loop:
-    sub               stride4q, strideq
-    movhps [dstq +stride4q +0], m2
-    movq   [dstq +stride4q +8], m5
-    mova   [dst4q+stride4q   ], m2
-    movhps [dst8q+stride4q +0], m1
-    movq   [dst8q+stride4q +8], m2
-    mova  [dst12q+stride4q   ], m1
-%if cpuflag(avx)
-    palignr                 m1, m2, m1, 2
-    palignr                 m2, m5, m2, 2
-%elif cpuflag(ssse3)
-    palignr                 m3, m2, m1, 2
-    palignr                 m0, m5, m2, 2
-    mova                    m1, m3
-    mova                    m2, m0
-%else
-    ; slightly modified version of PALIGNR
-    mova                    m6, m2
-    mova                    m4, m5
-    pslldq                  m6, 14
-    pslldq                  m4, 14
-    psrldq                  m1, 2
-    psrldq                  m2, 2
-    por                     m1, m6
-    por                     m2, m4
-%endif
-    psrldq                  m5, 2
-    jg .loop
-    RET
-
-cglobal vp9_ipred_hd_32x32, 4, 6, 8, dst, stride, l, a
-    mova                    m0, [lq]
-    mova                    m1, [lq+16]
-    movu                    m2, [aq-1]
-    movu                    m3, [aq+15]
-    DEFINE_ARGS dst, stride, stride8, dst8, dst16, dst24
-    lea               stride8q, [strideq*8]
-    lea                  dst8q, [dstq  +stride8q]
-    lea                 dst16q, [dst8q +stride8q]
-    lea                 dst24q, [dst16q+stride8q]
-    psrldq                  m4, m3,  1
-    psrldq                  m5, m3,  2
-    LOWPASS                  5,  4,  3,  6
-    PALIGNR                 m4, m3, m2,  2, m6
-    PALIGNR                 m3, m2,  1, m6
-    LOWPASS                  4,  3,  2,  6
-    PALIGNR                 m3, m2, m1,  2, m6
-    PALIGNR                 m2, m1,  1, m6
-    LOWPASS                  3,  2,  1,  6
-    pavgb                   m2, m1
-    PALIGNR                 m6, m1, m0,  1, m7
-    PALIGNR                 m1, m0,  2, m7
-    LOWPASS                  1,  6,  0,  7
-    pavgb                   m0, m6
-    SBUTTERFLY              bw,  2,  3,  6
-    SBUTTERFLY              bw,  0,  1,  6
-
-    ; m0, m1, m2, m3, m4, m5
-.loop:
-    sub               stride8q, strideq
-    mova  [dstq  +stride8q+ 0], m3
-    mova  [dstq  +stride8q+16], m4
-    mova  [dst8q +stride8q+ 0], m2
-    mova  [dst8q +stride8q+16], m3
-    mova  [dst16q+stride8q+ 0], m1
-    mova  [dst16q+stride8q+16], m2
-    mova  [dst24q+stride8q+ 0], m0
-    mova  [dst24q+stride8q+16], m1
-%if cpuflag(avx)
-    palignr                 m0, m1, m0, 2
-    palignr                 m1, m2, m1, 2
-    palignr                 m2, m3, m2, 2
-    palignr                 m3, m4, m3, 2
-    palignr                 m4, m5, m4, 2
-    psrldq                  m5, 2
-%elif cpuflag(ssse3)
-    psrldq                  m6, m5, 2
-    palignr                 m5, m4, 2
-    palignr                 m4, m3, 2
-    palignr                 m3, m2, 2
-    palignr                 m2, m1, 2
-    palignr                 m1, m0, 2
-    mova                    m0, m1
-    mova                    m1, m2
-    mova                    m2, m3
-    mova                    m3, m4
-    mova                    m4, m5
-    mova                    m5, m6
-%else
-    ; sort of a half-integrated version of PALIGNR
-    pslldq                  m7, m4, 14
-    pslldq                  m6, m5, 14
-    psrldq                  m4, 2
-    psrldq                  m5, 2
-    por                     m4, m6
-    pslldq                  m6, m3, 14
-    psrldq                  m3, 2
-    por                     m3, m7
-    pslldq                  m7, m2, 14
-    psrldq                  m2, 2
-    por                     m2, m6
-    pslldq                  m6, m1, 14
-    psrldq                  m1, 2
-    por                     m1, m7
-    psrldq                  m0, 2
-    por                     m0, m6
-%endif
-    jg .loop
-    RET
-%endmacro
-
-INIT_XMM sse2
-HD_XMM_FUNCS
-INIT_XMM ssse3
-HD_XMM_FUNCS
-INIT_XMM avx
-HD_XMM_FUNCS
-
-%macro HU_MMX_FUNCS 0
-cglobal vp9_ipred_hu_4x4, 3, 3, 0, dst, stride, l
-    movd                    m0, [lq]
-%if cpuflag(ssse3)
-    pshufb                  m0, [pb_0to2_5x3]
-%else
-    punpcklbw               m1, m0, m0          ; 00112233
-    pshufw                  m1, m1, q3333       ; 33333333
-    punpckldq               m0, m1              ; 01233333
-%endif
-    psrlq                   m1, m0, 8
-    psrlq                   m2, m1, 8
-    LOWPASS                  2,  1, 0, 3
-    pavgb                   m1, m0
-    DEFINE_ARGS dst, stride, stride3
-    lea               stride3q, [strideq*3]
-    SBUTTERFLY              bw,  1, 2, 0
-    PALIGNR                 m2, m1, 2, m0
-    movd      [dstq+strideq*0], m1
-    movd      [dstq+strideq*1], m2
-    punpckhdq               m1, m1
-    punpckhdq               m2, m2
-    movd      [dstq+strideq*2], m1
-    movd      [dstq+stride3q ], m2
-    RET
-%endmacro
-
-INIT_MMX mmxext
-HU_MMX_FUNCS
-INIT_MMX ssse3
-HU_MMX_FUNCS
-
-%macro HU_XMM_FUNCS 1 ; n_xmm_regs in hu_32x32
-cglobal vp9_ipred_hu_8x8, 3, 4, 4, dst, stride, l
-    movq                    m0, [lq]
-%if cpuflag(ssse3)
-    pshufb                  m0, [pb_0to6_9x7]
-%else
-    punpcklbw               m1, m0, m0          ; 0011223344556677
-    punpckhwd               m1, m1              ; 4444555566667777
-    shufps                  m0, m1, q3310       ; 0123456777777777
-%endif
-    psrldq                  m1, m0, 1
-    psrldq                  m2, m1, 1
-    LOWPASS                  2,  1, 0, 3
-    pavgb                   m1, m0
-    DEFINE_ARGS dst, stride, stride3, dst4
-    lea               stride3q, [strideq*3]
-    lea                  dst4q, [dstq+strideq*4]
-    SBUTTERFLY              bw,  1, 2, 0
-    movq     [dstq +strideq*0], m1
-    movhps   [dst4q+strideq*0], m1
-    PALIGNR                 m0, m2, m1, 2, m3
-    movq     [dstq +strideq*1], m0
-    movhps   [dst4q+strideq*1], m0
-    PALIGNR                 m0, m2, m1, 4, m3
-    movq     [dstq +strideq*2], m0
-    movhps   [dst4q+strideq*2], m0
-    PALIGNR                 m2, m1, 6, m3
-    movq     [dstq +stride3q ], m2
-    movhps   [dst4q+stride3q ], m2
-    RET
-
-cglobal vp9_ipred_hu_16x16, 3, 4, 5, dst, stride, l
-    mova                    m0, [lq]
-%if cpuflag(ssse3)
-    mova                    m3, [pb_2toE_3xF]
-    pshufb                  m1, m0, [pb_1toE_2xF]
-    pshufb                  m2, m0, m3
-%else
-    pand                    m3, m0, [pb_15x0_1xm1]
-    psrldq                  m1, m0, 1
-    por                     m1, m3
-    punpckhbw               m3, m3
-    psrldq                  m2, m0, 2
-    por                     m2, m3
-%endif
-    LOWPASS                  2,  1,  0,  4
-    pavgb                   m1, m0
-    DEFINE_ARGS dst, stride, stride9, cnt
-    lea                stride9q, [strideq*8+strideq]
-    mov                   cntd,  4
-    SBUTTERFLY              bw,  1,  2,  0
-
-.loop:
-    mova      [dstq+strideq*0], m1
-    mova      [dstq+strideq*8], m2
-    PALIGNR                 m0, m2, m1, 2, m4
-%if cpuflag(ssse3)
-    pshufb                  m2, m3
-%else
-    psrldq                  m2, 2
-    por                     m2, m3
-%endif
-    mova      [dstq+strideq*1], m0
-    mova      [dstq+stride9q ], m2
-    PALIGNR                 m1, m2, m0, 2, m4
-%if cpuflag(ssse3)
-    pshufb                  m2, m3
-%else
-    psrldq                  m2, 2
-    por                     m2, m3
-%endif
-    lea                   dstq, [dstq+strideq*2]
-    dec                   cntd
-    jg .loop
-    RET
-
-cglobal vp9_ipred_hu_32x32, 3, 7, %1, dst, stride, l
-    mova                    m1, [lq]
-    mova                    m0, [lq+16]
-    PALIGNR                 m2, m0, m1,  1, m5
-    PALIGNR                 m3, m0, m1,  2, m5
-    LOWPASS                  3,  2,  1,  5
-    pavgb                   m2, m1
-%if cpuflag(ssse3)
-    mova                    m4, [pb_2toE_3xF]
-    pshufb                  m5, m0, [pb_1toE_2xF]
-    pshufb                  m1, m0, m4
-%else
-    pand                    m4, m0, [pb_15x0_1xm1]
-    psrldq                  m5, m0, 1
-    por                     m5, m4
-    punpckhbw               m4, m4
-    psrldq                  m1, m0, 2
-    por                     m1, m4
-%endif
-    LOWPASS                  1,  5,  0,  6
-    pavgb                   m0, m5
-    DEFINE_ARGS dst, stride, cnt, stride0, dst8, dst16, dst24
-    mov                   cntd,  8
-    xor               stride0q, stride0q
-    lea                  dst8q, [dstq  +strideq*8]
-    lea                 dst16q, [dst8q +strideq*8]
-    lea                 dst24q, [dst16q+strideq*8]
-    SBUTTERFLY              bw,  0,  1,  5
-    SBUTTERFLY              bw,  2,  3,  5
-%if cpuflag(ssse3)
-    pshufb                  m6, m1, [pb_15]
-%else
-    pshufhw                 m6, m4, q3333
-    punpckhqdq              m6, m6
-%endif
-
-.loop:
-    mova  [dstq  +stride0q+ 0], m2
-    mova  [dstq  +stride0q+16], m3
-    mova  [dst8q +stride0q+ 0], m3
-    mova  [dst8q +stride0q+16], m0
-    mova  [dst16q+stride0q+ 0], m0
-    mova  [dst16q+stride0q+16], m1
-    mova  [dst24q+stride0q+ 0], m1
-    mova  [dst24q+stride0q+16], m6
-%if cpuflag(avx)
-    palignr                 m2, m3, m2, 2
-    palignr                 m3, m0, m3, 2
-    palignr                 m0, m1, m0, 2
-    pshufb                  m1, m4
-%elif cpuflag(ssse3)
-    pshufb                  m5, m1, m4
-    palignr                 m1, m0, 2
-    palignr                 m0, m3, 2
-    palignr                 m3, m2, 2
-    mova                    m2, m3
-    mova                    m3, m0
-    mova                    m0, m1
-    mova                    m1, m5
-%else
-    ; half-integrated version of PALIGNR
-    pslldq                  m5, m1, 14
-    pslldq                  m7, m0, 14
-    psrldq                  m1, 2
-    psrldq                  m0, 2
-    por                     m1, m4
-    por                     m0, m5
-    pslldq                  m5, m3, 14
-    psrldq                  m3, 2
-    por                     m3, m7
-    psrldq                  m2, 2
-    por                     m2, m5
-%endif
-    add               stride0q, strideq
-    dec                   cntd
-    jg .loop
-    RET
-%endmacro
-
-INIT_XMM sse2
-HU_XMM_FUNCS 8
-INIT_XMM ssse3
-HU_XMM_FUNCS 7
-INIT_XMM avx
-HU_XMM_FUNCS 7
-
-; FIXME 127, 128, 129 ?
diff -uparN ffmpeg-4.1/libavcodec/x86/vp9itxfm_16bpp.asm ffmpeg-y/libavcodec/x86/vp9itxfm_16bpp.asm
--- ffmpeg-4.1/libavcodec/x86/vp9itxfm_16bpp.asm	2018-07-17 17:27:41.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/vp9itxfm_16bpp.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,2044 +0,0 @@
-;******************************************************************************
-;* VP9 inverse transform x86 SIMD optimizations
-;*
-;* Copyright (C) 2015 Ronald S. Bultje <rsbultje gmail com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-%include "vp9itxfm_template.asm"
-
-SECTION_RODATA
-
-cextern pw_8
-cextern pw_1023
-cextern pw_2048
-cextern pw_4095
-cextern pw_m1
-cextern pd_1
-cextern pd_16
-cextern pd_32
-cextern pd_8192
-
-pd_8: times 4 dd 8
-pd_3fff: times 4 dd 0x3fff
-
-cextern pw_11585x2
-
-cextern pw_5283_13377
-cextern pw_9929_13377
-cextern pw_15212_m13377
-cextern pw_15212_9929
-cextern pw_m5283_m15212
-cextern pw_13377x2
-cextern pw_m13377_13377
-cextern pw_13377_0
-
-pw_9929_m5283: times 4 dw 9929, -5283
-
-%macro COEF_PAIR 2-3
-cextern pw_m%1_%2
-cextern pw_%2_%1
-%if %0 == 3
-cextern pw_m%1_m%2
-%if %1 != %2
-cextern pw_m%2_%1
-cextern pw_%1_%2
-%endif
-%endif
-%endmacro
-
-COEF_PAIR  2404, 16207
-COEF_PAIR  3196, 16069, 1
-COEF_PAIR  4756, 15679
-COEF_PAIR  5520, 15426
-COEF_PAIR  6270, 15137, 1
-COEF_PAIR  8423, 14053
-COEF_PAIR 10394, 12665
-COEF_PAIR 11003, 12140
-COEF_PAIR 11585, 11585, 1
-COEF_PAIR 13160,  9760
-COEF_PAIR 13623,  9102, 1
-COEF_PAIR 14449,  7723
-COEF_PAIR 14811,  7005
-COEF_PAIR 15893,  3981
-COEF_PAIR 16305,  1606
-COEF_PAIR 16364,   804
-
-default_8x8:
-times 12 db 1
-times 52 db 2
-row_8x8:
-times 18 db 1
-times 46 db 2
-col_8x8:
-times 6 db 1
-times 58 db 2
-default_16x16:
-times 10 db 1
-times 28 db 2
-times 51 db 3
-times 167 db 4
-row_16x16:
-times 21 db 1
-times 45 db 2
-times 60 db 3
-times 130 db 4
-col_16x16:
-times 5 db 1
-times 12 db 2
-times 25 db 3
-times 214 db 4
-default_32x32:
-times 9 db 1
-times 25 db 2
-times 36 db 3
-times 65 db 4
-times 105 db 5
-times 96 db 6
-times 112 db 7
-times 576 db 8
-
-SECTION .text
-
-%macro VP9_STORE_2X 6-7 dstq ; reg1, reg2, tmp1, tmp2, min, max, dst
-    mova               m%3, [%7]
-    mova               m%4, [%7+strideq]
-    paddw              m%3, m%1
-    paddw              m%4, m%2
-    pmaxsw             m%3, m%5
-    pmaxsw             m%4, m%5
-    pminsw             m%3, m%6
-    pminsw             m%4, m%6
-    mova              [%7], m%3
-    mova      [%7+strideq], m%4
-%endmacro
-
-%macro ZERO_BLOCK 4 ; mem, stride, nnzcpl, zero_reg
-%assign %%y 0
-%rep %3
-%assign %%x 0
-%rep %3*4/mmsize
-    mova      [%1+%%y+%%x], %4
-%assign %%x (%%x+mmsize)
-%endrep
-%assign %%y (%%y+%2)
-%endrep
-%endmacro
-
-; the input coefficients are scaled up by 2 bit (which we downscale immediately
-; in the iwht), and is otherwise orthonormally increased by 1 bit per iwht_1d.
-; therefore, a diff of 10-12+sign bit will fit in 12-14+sign bit after scaling,
-; i.e. everything can be done in 15+1bpp words. Since the quant fractional bits
-; add 2 bits, we need to scale before converting to word in 12bpp, since the
-; input will be 16+sign bit which doesn't fit in 15+sign words, but in 10bpp
-; we can scale after converting to words (which is half the instructions),
-; since the input is only 14+sign bit, which fits in 15+sign words directly.
-
-%macro IWHT4_FN 2 ; bpp, max
-cglobal vp9_iwht_iwht_4x4_add_%1, 3, 3, 8, dst, stride, block, eob
-    mova                m7, [pw_%2]
-    mova                m0, [blockq+0*16+0]
-    mova                m1, [blockq+1*16+0]
-%if %1 >= 12
-    mova                m4, [blockq+0*16+8]
-    mova                m5, [blockq+1*16+8]
-    psrad               m0, 2
-    psrad               m1, 2
-    psrad               m4, 2
-    psrad               m5, 2
-    packssdw            m0, m4
-    packssdw            m1, m5
-%else
-    packssdw            m0, [blockq+0*16+8]
-    packssdw            m1, [blockq+1*16+8]
-    psraw               m0, 2
-    psraw               m1, 2
-%endif
-    mova                m2, [blockq+2*16+0]
-    mova                m3, [blockq+3*16+0]
-%if %1 >= 12
-    mova                m4, [blockq+2*16+8]
-    mova                m5, [blockq+3*16+8]
-    psrad               m2, 2
-    psrad               m3, 2
-    psrad               m4, 2
-    psrad               m5, 2
-    packssdw            m2, m4
-    packssdw            m3, m5
-%else
-    packssdw            m2, [blockq+2*16+8]
-    packssdw            m3, [blockq+3*16+8]
-    psraw               m2, 2
-    psraw               m3, 2
-%endif
-
-    VP9_IWHT4_1D
-    TRANSPOSE4x4W        0, 1, 2, 3, 4
-    VP9_IWHT4_1D
-
-    pxor                m6, m6
-    VP9_STORE_2X         0, 1, 4, 5, 6, 7
-    lea               dstq, [dstq+strideq*2]
-    VP9_STORE_2X         2, 3, 4, 5, 6, 7
-    ZERO_BLOCK      blockq, 16, 4, m6
-    RET
-%endmacro
-
-INIT_MMX mmxext
-IWHT4_FN 10, 1023
-INIT_MMX mmxext
-IWHT4_FN 12, 4095
-
-%macro VP9_IDCT4_WRITEOUT 0
-%if cpuflag(ssse3)
-    mova                m5, [pw_2048]
-    pmulhrsw            m0, m5
-    pmulhrsw            m1, m5
-    pmulhrsw            m2, m5
-    pmulhrsw            m3, m5
-%else
-    mova                m5, [pw_8]
-    paddw               m0, m5
-    paddw               m1, m5
-    paddw               m2, m5
-    paddw               m3, m5
-    psraw               m0, 4
-    psraw               m1, 4
-    psraw               m2, 4
-    psraw               m3, 4
-%endif
-    mova                m5, [pw_1023]
-    VP9_STORE_2X         0,  1,  6,  7,  4,  5
-    lea               dstq, [dstq+2*strideq]
-    VP9_STORE_2X         2,  3,  6,  7,  4,  5
-%endmacro
-
-%macro DC_ONLY 2 ; shift, zero
-    mov              coefd, dword [blockq]
-    movd          [blockq], %2
-    imul             coefd, 11585
-    add              coefd, 8192
-    sar              coefd, 14
-    imul             coefd, 11585
-    add              coefd, ((1 << (%1 - 1)) << 14) + 8192
-    sar              coefd, 14 + %1
-%endmacro
-
-; 4x4 coefficients are 5+depth+sign bits, so for 10bpp, everything still fits
-; in 15+1 words without additional effort, since the coefficients are 15bpp.
-
-%macro IDCT4_10_FN 0
-cglobal vp9_idct_idct_4x4_add_10, 4, 4, 8, dst, stride, block, eob
-    cmp               eobd, 1
-    jg .idctfull
-
-    ; dc-only
-    pxor                m4, m4
-%if cpuflag(ssse3)
-    movd                m0, [blockq]
-    movd          [blockq], m4
-    mova                m5, [pw_11585x2]
-    pmulhrsw            m0, m5
-    pmulhrsw            m0, m5
-%else
-    DEFINE_ARGS dst, stride, block, coef
-    DC_ONLY              4, m4
-    movd                m0, coefd
-%endif
-    pshufw              m0, m0, 0
-    mova                m5, [pw_1023]
-%if cpuflag(ssse3)
-    pmulhrsw            m0, [pw_2048]       ; (x*2048 + (1<<14))>>15 <=> (x+8)>>4
-%endif
-    VP9_STORE_2X         0,  0,  6,  7,  4,  5
-    lea               dstq, [dstq+2*strideq]
-    VP9_STORE_2X         0,  0,  6,  7,  4,  5
-    RET
-
-.idctfull:
-    mova                m0, [blockq+0*16+0]
-    mova                m1, [blockq+1*16+0]
-    packssdw            m0, [blockq+0*16+8]
-    packssdw            m1, [blockq+1*16+8]
-    mova                m2, [blockq+2*16+0]
-    mova                m3, [blockq+3*16+0]
-    packssdw            m2, [blockq+2*16+8]
-    packssdw            m3, [blockq+3*16+8]
-
-%if cpuflag(ssse3)
-    mova                m6, [pw_11585x2]
-%endif
-    mova                m7, [pd_8192]       ; rounding
-    VP9_IDCT4_1D
-    TRANSPOSE4x4W  0, 1, 2, 3, 4
-    VP9_IDCT4_1D
-
-    pxor                m4, m4
-    ZERO_BLOCK      blockq, 16, 4, m4
-    VP9_IDCT4_WRITEOUT
-    RET
-%endmacro
-
-INIT_MMX mmxext
-IDCT4_10_FN
-INIT_MMX ssse3
-IDCT4_10_FN
-
-%macro IADST4_FN 4
-cglobal vp9_%1_%3_4x4_add_10, 3, 3, 0, dst, stride, block, eob
-%if WIN64 && notcpuflag(ssse3)
-    WIN64_SPILL_XMM 8
-%endif
-    movdqa            xmm5, [pd_8192]
-    mova                m0, [blockq+0*16+0]
-    mova                m1, [blockq+1*16+0]
-    packssdw            m0, [blockq+0*16+8]
-    packssdw            m1, [blockq+1*16+8]
-    mova                m2, [blockq+2*16+0]
-    mova                m3, [blockq+3*16+0]
-    packssdw            m2, [blockq+2*16+8]
-    packssdw            m3, [blockq+3*16+8]
-
-%if cpuflag(ssse3)
-    mova                m6, [pw_11585x2]
-%endif
-%ifnidn %1%3, iadstiadst
-    movdq2q             m7, xmm5
-%endif
-    VP9_%2_1D
-    TRANSPOSE4x4W  0, 1, 2, 3, 4
-    VP9_%4_1D
-
-    pxor                m4, m4
-    ZERO_BLOCK      blockq, 16, 4, m4
-    VP9_IDCT4_WRITEOUT
-    RET
-%endmacro
-
-INIT_MMX sse2
-IADST4_FN idct,  IDCT4,  iadst, IADST4
-IADST4_FN iadst, IADST4, idct,  IDCT4
-IADST4_FN iadst, IADST4, iadst, IADST4
-
-INIT_MMX ssse3
-IADST4_FN idct,  IDCT4,  iadst, IADST4
-IADST4_FN iadst, IADST4, idct,  IDCT4
-IADST4_FN iadst, IADST4, iadst, IADST4
-
-; inputs and outputs are dwords, coefficients are words
-;
-; dst1 = src1 * coef1 + src2 * coef2 + rnd >> 14
-; dst2 = src1 * coef2 - src2 * coef1 + rnd >> 14
-%macro SUMSUB_MUL 6-8 [pd_8192], [pd_3fff] ; src/dst 1-2, tmp1-2, coef1-2, rnd, mask
-    pand               m%3, m%1, %8
-    pand               m%4, m%2, %8
-    psrad              m%1, 14
-    psrad              m%2, 14
-    packssdw           m%4, m%2
-    packssdw           m%3, m%1
-    punpckhwd          m%2, m%4, m%3
-    punpcklwd          m%4, m%3
-    pmaddwd            m%3, m%4, [pw_%6_%5]
-    pmaddwd            m%1, m%2, [pw_%6_%5]
-    pmaddwd            m%4, [pw_m%5_%6]
-    pmaddwd            m%2, [pw_m%5_%6]
-    paddd              m%3, %7
-    paddd              m%4, %7
-    psrad              m%3, 14
-    psrad              m%4, 14
-    paddd              m%1, m%3
-    paddd              m%2, m%4
-%endmacro
-
-%macro IDCT4_12BPP_1D 0-8 [pd_8192], [pd_3fff], 0, 1, 2, 3, 4, 5 ; rnd, mask, in/out0-3, tmp0-1
-    SUMSUB_MUL          %3, %5, %7, %8, 11585, 11585, %1, %2
-    SUMSUB_MUL          %4, %6, %7, %8, 15137,  6270, %1, %2
-    SUMSUB_BA        d, %4, %3, %7
-    SUMSUB_BA        d, %6, %5, %7
-    SWAP                %4, %6, %3
-%endmacro
-
-%macro STORE_4x4 6 ; tmp1-2, reg1-2, min, max
-    movh               m%1, [dstq+strideq*0]
-    movh               m%2, [dstq+strideq*2]
-    movhps             m%1, [dstq+strideq*1]
-    movhps             m%2, [dstq+stride3q ]
-    paddw              m%1, m%3
-    paddw              m%2, m%4
-    pmaxsw             m%1, %5
-    pmaxsw             m%2, %5
-    pminsw             m%1, %6
-    pminsw             m%2, %6
-    movh   [dstq+strideq*0], m%1
-    movhps [dstq+strideq*1], m%1
-    movh   [dstq+strideq*2], m%2
-    movhps [dstq+stride3q ], m%2
-%endmacro
-
-%macro ROUND_AND_STORE_4x4 8 ; reg1-4, min, max, rnd, shift
-    paddd              m%1, %7
-    paddd              m%2, %7
-    paddd              m%3, %7
-    paddd              m%4, %7
-    psrad              m%1, %8
-    psrad              m%2, %8
-    psrad              m%3, %8
-    psrad              m%4, %8
-    packssdw           m%1, m%2
-    packssdw           m%3, m%4
-    STORE_4x4           %2, %4, %1, %3, %5, %6
-%endmacro
-
-INIT_XMM sse2
-cglobal vp9_idct_idct_4x4_add_12, 4, 4, 8, dst, stride, block, eob
-    cmp               eobd, 1
-    jg .idctfull
-
-    ; dc-only - this is special, since for 4x4 12bpp, the max coef size is
-    ; 17+sign bpp. Since the multiply is with 11585, which is 14bpp, the
-    ; result of each multiply is 31+sign bit, i.e. it _exactly_ fits in a
-    ; dword. After the final shift (4), the result is 13+sign bits, so we
-    ; don't need any additional processing to fit it in a word
-    DEFINE_ARGS dst, stride, block, coef
-    pxor                m4, m4
-    DC_ONLY              4, m4
-    movd                m0, coefd
-    pshuflw             m0, m0, q0000
-    punpcklqdq          m0, m0
-    mova                m5, [pw_4095]
-    DEFINE_ARGS dst, stride, stride3
-    lea           stride3q, [strideq*3]
-    STORE_4x4            1, 3, 0, 0, m4, m5
-    RET
-
-.idctfull:
-    DEFINE_ARGS dst, stride, block, eob
-    mova                m0, [blockq+0*16]
-    mova                m1, [blockq+1*16]
-    mova                m2, [blockq+2*16]
-    mova                m3, [blockq+3*16]
-    mova                m6, [pd_8192]
-    mova                m7, [pd_3fff]
-
-    IDCT4_12BPP_1D      m6, m7
-    TRANSPOSE4x4D        0, 1, 2, 3, 4
-    IDCT4_12BPP_1D      m6, m7
-
-    pxor                m4, m4
-    ZERO_BLOCK      blockq, 16, 4, m4
-
-    ; writeout
-    DEFINE_ARGS dst, stride, stride3
-    lea           stride3q, [strideq*3]
-    mova                m5, [pw_4095]
-    mova                m6, [pd_8]
-    ROUND_AND_STORE_4x4  0, 1, 2, 3, m4, m5, m6, 4
-    RET
-
-%macro SCRATCH 3-4
-%if ARCH_X86_64
-    SWAP                %1, %2
-%if %0 == 4
-%define reg_%4 m%2
-%endif
-%else
-    mova              [%3], m%1
-%if %0 == 4
-%define reg_%4 [%3]
-%endif
-%endif
-%endmacro
-
-%macro UNSCRATCH 3-4
-%if ARCH_X86_64
-    SWAP                %1, %2
-%else
-    mova               m%1, [%3]
-%endif
-%if %0 == 4
-%undef reg_%4
-%endif
-%endmacro
-
-%macro PRELOAD 2-3
-%if ARCH_X86_64
-    mova               m%1, [%2]
-%if %0 == 3
-%define reg_%3 m%1
-%endif
-%elif %0 == 3
-%define reg_%3 [%2]
-%endif
-%endmacro
-
-; out0 =  5283 * in0 + 13377 + in1 + 15212 * in2 +  9929 * in3 + rnd >> 14
-; out1 =  9929 * in0 + 13377 * in1 -  5283 * in2 - 15282 * in3 + rnd >> 14
-; out2 = 13377 * in0               - 13377 * in2 + 13377 * in3 + rnd >> 14
-; out3 = 15212 * in0 - 13377 * in1 +  9929 * in2 -  5283 * in3 + rnd >> 14
-%macro IADST4_12BPP_1D 0-2 [pd_8192], [pd_3fff] ; rnd, mask
-    pand                m4, m0, %2
-    pand                m5, m1, %2
-    psrad               m0, 14
-    psrad               m1, 14
-    packssdw            m5, m1
-    packssdw            m4, m0
-    punpckhwd           m1, m4, m5
-    punpcklwd           m4, m5
-    pand                m5, m2, %2
-    pand                m6, m3, %2
-    psrad               m2, 14
-    psrad               m3, 14
-    packssdw            m6, m3
-    packssdw            m5, m2
-    punpckhwd           m3, m5, m6
-    punpcklwd           m5, m6
-    SCRATCH              1,  8, rsp+0*mmsize, a
-    SCRATCH              5,  9, rsp+1*mmsize, b
-
-    ; m1/3 have the high bits of 0,1,2,3
-    ; m4/5 have the low bits of 0,1,2,3
-    ; m0/2/6/7 are free
-
-    mova                m2, [pw_15212_9929]
-    mova                m0, [pw_5283_13377]
-    pmaddwd             m7, m2, reg_b
-    pmaddwd             m6, m4, m0
-    pmaddwd             m2, m3
-    pmaddwd             m0, reg_a
-    paddd               m6, m7
-    paddd               m0, m2
-    mova                m1, [pw_m13377_13377]
-    mova                m5, [pw_13377_0]
-    pmaddwd             m7, m1, reg_b
-    pmaddwd             m2, m4, m5
-    pmaddwd             m1, m3
-    pmaddwd             m5, reg_a
-    paddd               m2, m7
-    paddd               m1, m5
-    paddd               m6, %1
-    paddd               m2, %1
-    psrad               m6, 14
-    psrad               m2, 14
-    paddd               m0, m6                      ; t0
-    paddd               m2, m1                      ; t2
-
-    mova                m7, [pw_m5283_m15212]
-    mova                m5, [pw_9929_13377]
-    pmaddwd             m1, m7, reg_b
-    pmaddwd             m6, m4, m5
-    pmaddwd             m7, m3
-    pmaddwd             m5, reg_a
-    paddd               m6, m1
-    paddd               m7, m5
-    UNSCRATCH            5,  9, rsp+1*mmsize, b
-    pmaddwd             m5, [pw_9929_m5283]
-    pmaddwd             m4, [pw_15212_m13377]
-    pmaddwd             m3, [pw_9929_m5283]
-    UNSCRATCH            1,  8, rsp+0*mmsize, a
-    pmaddwd             m1, [pw_15212_m13377]
-    paddd               m4, m5
-    paddd               m3, m1
-    paddd               m6, %1
-    paddd               m4, %1
-    psrad               m6, 14
-    psrad               m4, 14
-    paddd               m7, m6                      ; t1
-    paddd               m3, m4                      ; t3
-
-    SWAP                 1, 7
-%endmacro
-
-%macro IADST4_12BPP_FN 4
-cglobal vp9_%1_%3_4x4_add_12, 3, 3, 12, 2 * ARCH_X86_32 * mmsize, dst, stride, block, eob
-    mova                m0, [blockq+0*16]
-    mova                m1, [blockq+1*16]
-    mova                m2, [blockq+2*16]
-    mova                m3, [blockq+3*16]
-
-    PRELOAD             10, pd_8192, rnd
-    PRELOAD             11, pd_3fff, mask
-    %2_12BPP_1D    reg_rnd, reg_mask
-    TRANSPOSE4x4D        0, 1, 2, 3, 4
-    %4_12BPP_1D    reg_rnd, reg_mask
-
-    pxor                m4, m4
-    ZERO_BLOCK      blockq, 16, 4, m4
-
-    ; writeout
-    DEFINE_ARGS dst, stride, stride3
-    lea           stride3q, [strideq*3]
-    mova                m5, [pw_4095]
-    mova                m6, [pd_8]
-    ROUND_AND_STORE_4x4  0, 1, 2, 3, m4, m5, m6, 4
-    RET
-%endmacro
-
-INIT_XMM sse2
-IADST4_12BPP_FN idct,  IDCT4,  iadst, IADST4
-IADST4_12BPP_FN iadst, IADST4, idct,  IDCT4
-IADST4_12BPP_FN iadst, IADST4, iadst, IADST4
-
-; the following line has not been executed at the end of this macro:
-; UNSCRATCH            6, 8, rsp+%3*mmsize
-%macro IDCT8_1D 1-5 [pd_8192], [pd_3fff], 2 * mmsize, 17 ; src, rnd, mask, src_stride, stack_offset
-    mova                m0, [%1+0*%4]
-    mova                m2, [%1+2*%4]
-    mova                m4, [%1+4*%4]
-    mova                m6, [%1+6*%4]
-    IDCT4_12BPP_1D      %2, %3, 0, 2, 4, 6, 1, 3            ; m0/2/4/6 have t0/1/2/3
-    SCRATCH              4, 8, rsp+(%5+0)*mmsize
-    SCRATCH              6, 9, rsp+(%5+1)*mmsize
-    mova                m1, [%1+1*%4]
-    mova                m3, [%1+3*%4]
-    mova                m5, [%1+5*%4]
-    mova                m7, [%1+7*%4]
-    SUMSUB_MUL           1, 7, 4, 6, 16069,  3196, %2, %3   ; m1=t7a, m7=t4a
-    SUMSUB_MUL           5, 3, 4, 6,  9102, 13623, %2, %3   ; m5=t6a, m3=t5a
-    SUMSUB_BA         d, 3, 7, 4                            ; m3=t4, m7=t5a
-    SUMSUB_BA         d, 5, 1, 4                            ; m5=t7, m1=t6a
-    SUMSUB_MUL           1, 7, 4, 6, 11585, 11585, %2, %3   ; m1=t6, m7=t5
-    SUMSUB_BA         d, 5, 0, 4                            ; m5=out0, m0=out7
-    SUMSUB_BA         d, 1, 2, 4                            ; m1=out1, m2=out6
-    UNSCRATCH            4, 8, rsp+(%5+0)*mmsize
-    UNSCRATCH            6, 9, rsp+(%5+1)*mmsize
-    SCRATCH              2, 8, rsp+(%5+0)*mmsize
-    SUMSUB_BA         d, 7, 4, 2                            ; m7=out2, m4=out5
-    SUMSUB_BA         d, 3, 6, 2                            ; m3=out3, m6=out4
-    SWAP                 0, 5, 4, 6, 2, 7
-%endmacro
-
-%macro STORE_2x8 5-7 dstq, strideq ; tmp1-2, reg, min, max
-    mova               m%1, [%6+%7*0]
-    mova               m%2, [%6+%7*1]
-    paddw              m%1, m%3
-    paddw              m%2, m%3
-    pmaxsw             m%1, %4
-    pmaxsw             m%2, %4
-    pminsw             m%1, %5
-    pminsw             m%2, %5
-    mova         [%6+%7*0], m%1
-    mova         [%6+%7*1], m%2
-%endmacro
-
-; FIXME we can use the intermediate storage (rsp[0-15]) on x86-32 for temp
-; storage also instead of allocating two more stack spaces. This doesn't
-; matter much but it's something...
-INIT_XMM sse2
-cglobal vp9_idct_idct_8x8_add_10, 4, 6 + ARCH_X86_64, 14, \
-                                  16 * mmsize + 3 * ARCH_X86_32 * mmsize, \
-                                  dst, stride, block, eob
-    mova                m0, [pw_1023]
-    cmp               eobd, 1
-    jg .idctfull
-
-    ; dc-only - the 10bit version can be done entirely in 32bit, since the max
-    ; coef values are 16+sign bit, and the coef is 14bit, so 30+sign easily
-    ; fits in 32bit
-    DEFINE_ARGS dst, stride, block, coef
-    pxor                m2, m2
-    DC_ONLY              5, m2
-    movd                m1, coefd
-    pshuflw             m1, m1, q0000
-    punpcklqdq          m1, m1
-    DEFINE_ARGS dst, stride, cnt
-    mov               cntd, 4
-.loop_dc:
-    STORE_2x8            3, 4, 1, m2, m0
-    lea               dstq, [dstq+strideq*2]
-    dec               cntd
-    jg .loop_dc
-    RET
-
-.idctfull:
-    SCRATCH              0, 12, rsp+16*mmsize, max
-    DEFINE_ARGS dst, stride, block, cnt, ptr, skip, dstbak
-%if ARCH_X86_64
-    mov            dstbakq, dstq
-    movsxd            cntq, cntd
-%endif
-%ifdef PIC
-    lea               ptrq, [default_8x8]
-    movzx             cntd, byte [ptrq+cntq-1]
-%else
-    movzx             cntd, byte [default_8x8+cntq-1]
-%endif
-    mov              skipd, 2
-    sub              skipd, cntd
-    mov               ptrq, rsp
-    PRELOAD             10, pd_8192, rnd
-    PRELOAD             11, pd_3fff, mask
-    PRELOAD             13, pd_16, srnd
-.loop_1:
-    IDCT8_1D        blockq, reg_rnd, reg_mask
-
-    TRANSPOSE4x4D        0, 1, 2, 3, 6
-    mova  [ptrq+ 0*mmsize], m0
-    mova  [ptrq+ 2*mmsize], m1
-    mova  [ptrq+ 4*mmsize], m2
-    mova  [ptrq+ 6*mmsize], m3
-    UNSCRATCH            6, 8, rsp+17*mmsize
-    TRANSPOSE4x4D        4, 5, 6, 7, 0
-    mova  [ptrq+ 1*mmsize], m4
-    mova  [ptrq+ 3*mmsize], m5
-    mova  [ptrq+ 5*mmsize], m6
-    mova  [ptrq+ 7*mmsize], m7
-    add               ptrq, 8 * mmsize
-    add             blockq, mmsize
-    dec               cntd
-    jg .loop_1
-
-    ; zero-pad the remainder (skipped cols)
-    test             skipd, skipd
-    jz .end
-    add              skipd, skipd
-    lea             blockq, [blockq+skipq*(mmsize/2)]
-    pxor                m0, m0
-.loop_z:
-    mova   [ptrq+mmsize*0], m0
-    mova   [ptrq+mmsize*1], m0
-    mova   [ptrq+mmsize*2], m0
-    mova   [ptrq+mmsize*3], m0
-    add               ptrq, 4 * mmsize
-    dec              skipd
-    jg .loop_z
-.end:
-
-    DEFINE_ARGS dst, stride, block, cnt, ptr, stride3, dstbak
-    lea           stride3q, [strideq*3]
-    mov               cntd, 2
-    mov               ptrq, rsp
-.loop_2:
-    IDCT8_1D          ptrq, reg_rnd, reg_mask
-
-    pxor                m6, m6
-    ROUND_AND_STORE_4x4  0, 1, 2, 3, m6, reg_max, reg_srnd, 5
-    lea               dstq, [dstq+strideq*4]
-    UNSCRATCH            0, 8, rsp+17*mmsize
-    UNSCRATCH            1, 12, rsp+16*mmsize, max
-    UNSCRATCH            2, 13, pd_16, srnd
-    ROUND_AND_STORE_4x4  4, 5, 0, 7, m6, m1, m2, 5
-    add               ptrq, 16
-%if ARCH_X86_64
-    lea               dstq, [dstbakq+8]
-%else
-    mov               dstq, dstm
-    add               dstq, 8
-%endif
-    dec               cntd
-    jg .loop_2
-
-    ; m6 is still zero
-    ZERO_BLOCK blockq-2*mmsize, 32, 8, m6
-    RET
-
-%macro DC_ONLY_64BIT 2 ; shift, zero
-%if ARCH_X86_64
-    movsxd           coefq, dword [blockq]
-    movd          [blockq], %2
-    imul             coefq, 11585
-    add              coefq, 8192
-    sar              coefq, 14
-    imul             coefq, 11585
-    add              coefq, ((1 << (%1 - 1)) << 14) + 8192
-    sar              coefq, 14 + %1
-%else
-    mov              coefd, dword [blockq]
-    movd          [blockq], %2
-    DEFINE_ARGS dst, stride, cnt, coef, coefl
-    mov               cntd, 2
-.loop_dc_calc:
-    mov             coefld, coefd
-    sar              coefd, 14
-    and             coefld, 0x3fff
-    imul             coefd, 11585
-    imul            coefld, 11585
-    add             coefld, 8192
-    sar             coefld, 14
-    add              coefd, coefld
-    dec               cntd
-    jg .loop_dc_calc
-    add              coefd, 1 << (%1 - 1)
-    sar              coefd, %1
-%endif
-%endmacro
-
-INIT_XMM sse2
-cglobal vp9_idct_idct_8x8_add_12, 4, 6 + ARCH_X86_64, 14, \
-                                  16 * mmsize + 3 * ARCH_X86_32 * mmsize, \
-                                  dst, stride, block, eob
-    mova                m0, [pw_4095]
-    cmp               eobd, 1
-    jg mangle(private_prefix %+ _ %+ vp9_idct_idct_8x8_add_10 %+ SUFFIX).idctfull
-
-    ; dc-only - unfortunately, this one can overflow, since coefs are 18+sign
-    ; bpp, and 18+14+sign does not fit in 32bit, so we do 2-stage multiplies
-    DEFINE_ARGS dst, stride, block, coef, coefl
-    pxor                m2, m2
-    DC_ONLY_64BIT        5, m2
-    movd                m1, coefd
-    pshuflw             m1, m1, q0000
-    punpcklqdq          m1, m1
-    DEFINE_ARGS dst, stride, cnt
-    mov               cntd, 4
-.loop_dc:
-    STORE_2x8            3, 4, 1, m2, m0
-    lea               dstq, [dstq+strideq*2]
-    dec               cntd
-    jg .loop_dc
-    RET
-
-; inputs and outputs are dwords, coefficients are words
-;
-; dst1[hi]:dst3[lo] = src1 * coef1 + src2 * coef2
-; dst2[hi]:dst4[lo] = src1 * coef2 - src2 * coef1
-%macro SUMSUB_MUL_D 6-7 [pd_3fff] ; src/dst 1-2, dst3-4, coef1-2, mask
-    pand               m%3, m%1, %7
-    pand               m%4, m%2, %7
-    psrad              m%1, 14
-    psrad              m%2, 14
-    packssdw           m%4, m%2
-    packssdw           m%3, m%1
-    punpckhwd          m%2, m%4, m%3
-    punpcklwd          m%4, m%3
-    pmaddwd            m%3, m%4, [pw_%6_%5]
-    pmaddwd            m%1, m%2, [pw_%6_%5]
-    pmaddwd            m%4, [pw_m%5_%6]
-    pmaddwd            m%2, [pw_m%5_%6]
-%endmacro
-
-; dst1 = src2[hi]:src4[lo] + src1[hi]:src3[lo] + rnd >> 14
-; dst2 = src2[hi]:src4[lo] - src1[hi]:src3[lo] + rnd >> 14
-%macro SUMSUB_PACK_D 5-6 [pd_8192] ; src/dst 1-2, src3-4, tmp, rnd
-    SUMSUB_BA        d, %1, %2, %5
-    SUMSUB_BA        d, %3, %4, %5
-    paddd              m%3, %6
-    paddd              m%4, %6
-    psrad              m%3, 14
-    psrad              m%4, 14
-    paddd              m%1, m%3
-    paddd              m%2, m%4
-%endmacro
-
-%macro NEGD 1
-%if cpuflag(ssse3)
-    psignd              %1, [pw_m1]
-%else
-    pxor                %1, [pw_m1]
-    paddd               %1, [pd_1]
-%endif
-%endmacro
-
-; the following line has not been executed at the end of this macro:
-; UNSCRATCH            6, 8, rsp+17*mmsize
-%macro IADST8_1D 1-3 [pd_8192], [pd_3fff] ; src, rnd, mask
-    mova                m0, [%1+ 0*mmsize]
-    mova                m3, [%1+ 6*mmsize]
-    mova                m4, [%1+ 8*mmsize]
-    mova                m7, [%1+14*mmsize]
-    SUMSUB_MUL_D         7, 0, 1, 2, 16305,  1606, %3   ; m7/1=t0a, m0/2=t1a
-    SUMSUB_MUL_D         3, 4, 5, 6, 10394, 12665, %3   ; m3/5=t4a, m4/6=t5a
-    SCRATCH              0, 8, rsp+17*mmsize
-    SUMSUB_PACK_D        3, 7, 5, 1, 0, %2              ; m3=t0, m7=t4
-    UNSCRATCH            0, 8, rsp+17*mmsize
-    SUMSUB_PACK_D        4, 0, 6, 2, 1, %2              ; m4=t1, m0=t5
-
-    SCRATCH              3, 8, rsp+17*mmsize
-    SCRATCH              4, 9, rsp+18*mmsize
-    SCRATCH              7, 10, rsp+19*mmsize
-    SCRATCH              0, 11, rsp+20*mmsize
-
-    mova                m1, [%1+ 2*mmsize]
-    mova                m2, [%1+ 4*mmsize]
-    mova                m5, [%1+10*mmsize]
-    mova                m6, [%1+12*mmsize]
-    SUMSUB_MUL_D         5, 2, 3, 4, 14449,  7723, %3   ; m5/8=t2a, m2/9=t3a
-    SUMSUB_MUL_D         1, 6, 7, 0,  4756, 15679, %3   ; m1/10=t6a, m6/11=t7a
-    SCRATCH              2, 12, rsp+21*mmsize
-    SUMSUB_PACK_D        1, 5, 7, 3, 2, %2              ; m1=t2, m5=t6
-    UNSCRATCH            2, 12, rsp+21*mmsize
-    SUMSUB_PACK_D        6, 2, 0, 4, 3, %2              ; m6=t3, m2=t7
-
-    UNSCRATCH            7, 10, rsp+19*mmsize
-    UNSCRATCH            0, 11, rsp+20*mmsize
-    SCRATCH              1, 10, rsp+19*mmsize
-    SCRATCH              6, 11, rsp+20*mmsize
-
-    SUMSUB_MUL_D         7, 0, 3, 4, 15137,  6270, %3   ; m7/8=t4a, m0/9=t5a
-    SUMSUB_MUL_D         2, 5, 1, 6,  6270, 15137, %3   ; m2/10=t7a, m5/11=t6a
-    SCRATCH              2, 12, rsp+21*mmsize
-    SUMSUB_PACK_D        5, 7, 6, 3, 2, %2              ; m5=-out1, m7=t6
-    UNSCRATCH            2, 12, rsp+21*mmsize
-    NEGD                m5                              ; m5=out1
-    SUMSUB_PACK_D        2, 0, 1, 4, 3, %2              ; m2=out6, m0=t7
-    SUMSUB_MUL           7, 0, 3, 4, 11585, 11585, %2, %3   ; m7=out2, m0=-out5
-    NEGD                m0                              ; m0=out5
-
-    UNSCRATCH            3, 8, rsp+17*mmsize
-    UNSCRATCH            4, 9, rsp+18*mmsize
-    UNSCRATCH            1, 10, rsp+19*mmsize
-    UNSCRATCH            6, 11, rsp+20*mmsize
-    SCRATCH              2, 8, rsp+17*mmsize
-    SCRATCH              0, 9, rsp+18*mmsize
-
-    SUMSUB_BA         d, 1, 3,  2                       ; m1=out0, m3=t2
-    SUMSUB_BA         d, 6, 4,  2                       ; m6=-out7, m4=t3
-    NEGD                m6                              ; m6=out7
-    SUMSUB_MUL           3, 4,  2,  0, 11585, 11585, %2, %3 ; m3=-out3, m4=out4
-    NEGD                m3                              ; m3=out3
-
-    UNSCRATCH            0, 9, rsp+18*mmsize
-
-    SWAP                 0, 1, 5
-    SWAP                 2, 7, 6
-%endmacro
-
-%macro IADST8_FN 5
-cglobal vp9_%1_%3_8x8_add_10, 4, 6 + ARCH_X86_64, 16, \
-                              16 * mmsize + ARCH_X86_32 * 6 * mmsize, \
-                              dst, stride, block, eob
-    mova                m0, [pw_1023]
-
-.body:
-    SCRATCH              0, 13, rsp+16*mmsize, max
-    DEFINE_ARGS dst, stride, block, cnt, ptr, skip, dstbak
-%if ARCH_X86_64
-    mov            dstbakq, dstq
-    movsxd            cntq, cntd
-%endif
-%ifdef PIC
-    lea               ptrq, [%5_8x8]
-    movzx             cntd, byte [ptrq+cntq-1]
-%else
-    movzx             cntd, byte [%5_8x8+cntq-1]
-%endif
-    mov              skipd, 2
-    sub              skipd, cntd
-    mov               ptrq, rsp
-    PRELOAD             14, pd_8192, rnd
-    PRELOAD             15, pd_3fff, mask
-.loop_1:
-    %2_1D           blockq, reg_rnd, reg_mask
-
-    TRANSPOSE4x4D        0, 1, 2, 3, 6
-    mova  [ptrq+ 0*mmsize], m0
-    mova  [ptrq+ 2*mmsize], m1
-    mova  [ptrq+ 4*mmsize], m2
-    mova  [ptrq+ 6*mmsize], m3
-    UNSCRATCH            6, 8, rsp+17*mmsize
-    TRANSPOSE4x4D        4, 5, 6, 7, 0
-    mova  [ptrq+ 1*mmsize], m4
-    mova  [ptrq+ 3*mmsize], m5
-    mova  [ptrq+ 5*mmsize], m6
-    mova  [ptrq+ 7*mmsize], m7
-    add               ptrq, 8 * mmsize
-    add             blockq, mmsize
-    dec               cntd
-    jg .loop_1
-
-    ; zero-pad the remainder (skipped cols)
-    test             skipd, skipd
-    jz .end
-    add              skipd, skipd
-    lea             blockq, [blockq+skipq*(mmsize/2)]
-    pxor                m0, m0
-.loop_z:
-    mova   [ptrq+mmsize*0], m0
-    mova   [ptrq+mmsize*1], m0
-    mova   [ptrq+mmsize*2], m0
-    mova   [ptrq+mmsize*3], m0
-    add               ptrq, 4 * mmsize
-    dec              skipd
-    jg .loop_z
-.end:
-
-    DEFINE_ARGS dst, stride, block, cnt, ptr, stride3, dstbak
-    lea           stride3q, [strideq*3]
-    mov               cntd, 2
-    mov               ptrq, rsp
-.loop_2:
-    %4_1D             ptrq, reg_rnd, reg_mask
-
-    pxor                m6, m6
-    PRELOAD              9, pd_16, srnd
-    ROUND_AND_STORE_4x4  0, 1, 2, 3, m6, reg_max, reg_srnd, 5
-    lea               dstq, [dstq+strideq*4]
-    UNSCRATCH            0, 8, rsp+17*mmsize
-    UNSCRATCH            1, 13, rsp+16*mmsize, max
-    UNSCRATCH            2, 9, pd_16, srnd
-    ROUND_AND_STORE_4x4  4, 5, 0, 7, m6, m1, m2, 5
-    add               ptrq, 16
-%if ARCH_X86_64
-    lea               dstq, [dstbakq+8]
-%else
-    mov               dstq, dstm
-    add               dstq, 8
-%endif
-    dec               cntd
-    jg .loop_2
-
-    ; m6 is still zero
-    ZERO_BLOCK blockq-2*mmsize, 32, 8, m6
-    RET
-
-cglobal vp9_%1_%3_8x8_add_12, 4, 6 + ARCH_X86_64, 16, \
-                              16 * mmsize + ARCH_X86_32 * 6 * mmsize, \
-                              dst, stride, block, eob
-    mova                m0, [pw_4095]
-    jmp mangle(private_prefix %+ _ %+ vp9_%1_%3_8x8_add_10 %+ SUFFIX).body
-%endmacro
-
-INIT_XMM sse2
-IADST8_FN idct,  IDCT8,  iadst, IADST8, row
-IADST8_FN iadst, IADST8, idct,  IDCT8,  col
-IADST8_FN iadst, IADST8, iadst, IADST8, default
-
-%macro IDCT16_1D 1-4 4 * mmsize, 65, 67 ; src, src_stride, stack_offset, mm32bit_stack_offset
-    IDCT8_1D            %1, [pd_8192], [pd_3fff], %2 * 2, %4    ; m0-3=t0-3a, m4-5/m8|r67/m7=t4-7
-    ; SCRATCH            6, 8, rsp+(%4+0)*mmsize    ; t6
-    SCRATCH              0, 15, rsp+(%4+7)*mmsize   ; t0a
-    SCRATCH              1, 14, rsp+(%4+6)*mmsize   ; t1a
-    SCRATCH              2, 13, rsp+(%4+5)*mmsize   ; t2a
-    SCRATCH              3, 12, rsp+(%4+4)*mmsize   ; t3a
-    SCRATCH              4, 11, rsp+(%4+3)*mmsize   ; t4
-    mova [rsp+(%3+0)*mmsize], m5                    ; t5
-    mova [rsp+(%3+1)*mmsize], m7                    ; t7
-
-    mova                m0, [%1+ 1*%2]              ; in1
-    mova                m3, [%1+ 7*%2]              ; in7
-    mova                m4, [%1+ 9*%2]              ; in9
-    mova                m7, [%1+15*%2]              ; in15
-
-    SUMSUB_MUL           0, 7, 1, 2, 16305,  1606   ; m0=t15a, m7=t8a
-    SUMSUB_MUL           4, 3, 1, 2, 10394, 12665   ; m4=t14a, m3=t9a
-    SUMSUB_BA         d, 3, 7, 1                    ; m3=t8, m7=t9
-    SUMSUB_BA         d, 4, 0, 1                    ; m4=t15,m0=t14
-    SUMSUB_MUL           0, 7, 1, 2, 15137,  6270   ; m0=t14a, m7=t9a
-
-    mova                m1, [%1+ 3*%2]              ; in3
-    mova                m2, [%1+ 5*%2]              ; in5
-    mova                m5, [%1+11*%2]              ; in11
-    mova                m6, [%1+13*%2]              ; in13
-
-    SCRATCH              0,  9, rsp+(%4+1)*mmsize
-    SCRATCH              7, 10, rsp+(%4+2)*mmsize
-
-    SUMSUB_MUL           2, 5, 0, 7, 14449,  7723   ; m2=t13a, m5=t10a
-    SUMSUB_MUL           6, 1, 0, 7,  4756, 15679   ; m6=t12a, m1=t11a
-    SUMSUB_BA         d, 5, 1, 0                    ; m5=t11,m1=t10
-    SUMSUB_BA         d, 2, 6, 0                    ; m2=t12,m6=t13
-    NEGD                m1                          ; m1=-t10
-    SUMSUB_MUL           1, 6, 0, 7, 15137,  6270   ; m1=t13a, m6=t10a
-
-    UNSCRATCH            7, 10, rsp+(%4+2)*mmsize
-    SUMSUB_BA         d, 5, 3, 0                    ; m5=t8a, m3=t11a
-    SUMSUB_BA         d, 6, 7, 0                    ; m6=t9,  m7=t10
-    SUMSUB_BA         d, 2, 4, 0                    ; m2=t15a,m4=t12a
-    SCRATCH              5, 10, rsp+(%4+2)*mmsize
-    SUMSUB_MUL           4, 3, 0, 5, 11585, 11585   ; m4=t12, m3=t11
-    UNSCRATCH            0, 9, rsp+(%4+1)*mmsize
-    SUMSUB_BA         d, 1, 0, 5                    ; m1=t14, m0=t13
-    SCRATCH              6, 9, rsp+(%4+1)*mmsize
-    SUMSUB_MUL           0, 7, 6, 5, 11585, 11585   ; m0=t13a,m7=t10a
-
-    ; order: 15|r74,14|r73,13|r72,12|r71,11|r70,r65,8|r67,r66,10|r69,9|r68,7,3,4,0,1,2
-    ; free: 6,5
-
-    UNSCRATCH            5, 15, rsp+(%4+7)*mmsize
-    SUMSUB_BA         d, 2, 5, 6                    ; m2=out0, m5=out15
-    SCRATCH              5, 15, rsp+(%4+7)*mmsize
-    UNSCRATCH            5, 14, rsp+(%4+6)*mmsize
-    SUMSUB_BA         d, 1, 5, 6                    ; m1=out1, m5=out14
-    SCRATCH              5, 14, rsp+(%4+6)*mmsize
-    UNSCRATCH            5, 13, rsp+(%4+5)*mmsize
-    SUMSUB_BA         d, 0, 5, 6                    ; m0=out2, m5=out13
-    SCRATCH              5, 13, rsp+(%4+5)*mmsize
-    UNSCRATCH            5, 12, rsp+(%4+4)*mmsize
-    SUMSUB_BA         d, 4, 5, 6                    ; m4=out3, m5=out12
-    SCRATCH              5, 12, rsp+(%4+4)*mmsize
-    UNSCRATCH            5, 11, rsp+(%4+3)*mmsize
-    SUMSUB_BA         d, 3, 5, 6                    ; m3=out4, m5=out11
-    SCRATCH              4, 11, rsp+(%4+3)*mmsize
-    mova                m4, [rsp+(%3+0)*mmsize]
-    SUMSUB_BA         d, 7, 4, 6                    ; m7=out5, m4=out10
-    mova [rsp+(%3+0)*mmsize], m5
-    UNSCRATCH            5, 8, rsp+(%4+0)*mmsize
-    UNSCRATCH            6, 9, rsp+(%4+1)*mmsize
-    SCRATCH              2, 8, rsp+(%4+0)*mmsize
-    SCRATCH              1, 9, rsp+(%4+1)*mmsize
-    UNSCRATCH            1, 10, rsp+(%4+2)*mmsize
-    SCRATCH              0, 10, rsp+(%4+2)*mmsize
-    mova                m0, [rsp+(%3+1)*mmsize]
-    SUMSUB_BA         d, 6, 5, 2                    ; m6=out6, m5=out9
-    SUMSUB_BA         d, 1, 0, 2                    ; m1=out7, m0=out8
-
-    SWAP                 0, 3, 1, 7, 2, 6, 4
-
-    ; output order: 8-11|r67-70=out0-3
-    ;               0-6,r65=out4-11
-    ;               12-15|r71-74=out12-15
-%endmacro
-
-INIT_XMM sse2
-cglobal vp9_idct_idct_16x16_add_10, 4, 6 + ARCH_X86_64, 16, \
-                                    67 * mmsize + ARCH_X86_32 * 8 * mmsize, \
-                                    dst, stride, block, eob
-    mova                m0, [pw_1023]
-    cmp               eobd, 1
-    jg .idctfull
-
-    ; dc-only - the 10bit version can be done entirely in 32bit, since the max
-    ; coef values are 17+sign bit, and the coef is 14bit, so 31+sign easily
-    ; fits in 32bit
-    DEFINE_ARGS dst, stride, block, coef
-    pxor                m2, m2
-    DC_ONLY              6, m2
-    movd                m1, coefd
-    pshuflw             m1, m1, q0000
-    punpcklqdq          m1, m1
-    DEFINE_ARGS dst, stride, cnt
-    mov               cntd, 8
-.loop_dc:
-    STORE_2x8            3, 4, 1, m2, m0, dstq,         mmsize
-    STORE_2x8            3, 4, 1, m2, m0, dstq+strideq, mmsize
-    lea               dstq, [dstq+strideq*2]
-    dec               cntd
-    jg .loop_dc
-    RET
-
-.idctfull:
-    mova   [rsp+64*mmsize], m0
-    DEFINE_ARGS dst, stride, block, cnt, ptr, skip, dstbak
-%if ARCH_X86_64
-    mov            dstbakq, dstq
-    movsxd            cntq, cntd
-%endif
-%ifdef PIC
-    lea               ptrq, [default_16x16]
-    movzx             cntd, byte [ptrq+cntq-1]
-%else
-    movzx             cntd, byte [default_16x16+cntq-1]
-%endif
-    mov              skipd, 4
-    sub              skipd, cntd
-    mov               ptrq, rsp
-.loop_1:
-    IDCT16_1D       blockq
-
-    TRANSPOSE4x4D        0, 1, 2, 3, 7
-    mova  [ptrq+ 1*mmsize], m0
-    mova  [ptrq+ 5*mmsize], m1
-    mova  [ptrq+ 9*mmsize], m2
-    mova  [ptrq+13*mmsize], m3
-    mova                m7, [rsp+65*mmsize]
-    TRANSPOSE4x4D        4, 5, 6, 7, 0
-    mova  [ptrq+ 2*mmsize], m4
-    mova  [ptrq+ 6*mmsize], m5
-    mova  [ptrq+10*mmsize], m6
-    mova  [ptrq+14*mmsize], m7
-    UNSCRATCH               0, 8, rsp+67*mmsize
-    UNSCRATCH               1, 9, rsp+68*mmsize
-    UNSCRATCH               2, 10, rsp+69*mmsize
-    UNSCRATCH               3, 11, rsp+70*mmsize
-    TRANSPOSE4x4D        0, 1, 2, 3, 7
-    mova  [ptrq+ 0*mmsize], m0
-    mova  [ptrq+ 4*mmsize], m1
-    mova  [ptrq+ 8*mmsize], m2
-    mova  [ptrq+12*mmsize], m3
-    UNSCRATCH               4, 12, rsp+71*mmsize
-    UNSCRATCH               5, 13, rsp+72*mmsize
-    UNSCRATCH               6, 14, rsp+73*mmsize
-    UNSCRATCH               7, 15, rsp+74*mmsize
-    TRANSPOSE4x4D        4, 5, 6, 7, 0
-    mova  [ptrq+ 3*mmsize], m4
-    mova  [ptrq+ 7*mmsize], m5
-    mova  [ptrq+11*mmsize], m6
-    mova  [ptrq+15*mmsize], m7
-    add               ptrq, 16 * mmsize
-    add             blockq, mmsize
-    dec               cntd
-    jg .loop_1
-
-    ; zero-pad the remainder (skipped cols)
-    test             skipd, skipd
-    jz .end
-    add              skipd, skipd
-    lea             blockq, [blockq+skipq*(mmsize/2)]
-    pxor                m0, m0
-.loop_z:
-    mova   [ptrq+mmsize*0], m0
-    mova   [ptrq+mmsize*1], m0
-    mova   [ptrq+mmsize*2], m0
-    mova   [ptrq+mmsize*3], m0
-    mova   [ptrq+mmsize*4], m0
-    mova   [ptrq+mmsize*5], m0
-    mova   [ptrq+mmsize*6], m0
-    mova   [ptrq+mmsize*7], m0
-    add               ptrq, 8 * mmsize
-    dec              skipd
-    jg .loop_z
-.end:
-
-    DEFINE_ARGS dst, stride, block, cnt, ptr, stride3, dstbak
-    lea           stride3q, [strideq*3]
-    mov               cntd, 4
-    mov               ptrq, rsp
-.loop_2:
-    IDCT16_1D         ptrq
-
-    pxor               m7, m7
-    lea               dstq, [dstq+strideq*4]
-    ROUND_AND_STORE_4x4  0, 1, 2, 3, m7, [rsp+64*mmsize], [pd_32], 6
-    lea               dstq, [dstq+strideq*4]
-    mova                m0, [rsp+65*mmsize]
-    mova                m1, [rsp+64*mmsize]
-    mova                m2, [pd_32]
-    ROUND_AND_STORE_4x4  4, 5, 6, 0, m7, m1, m2, 6
-
-%if ARCH_X86_64
-    DEFINE_ARGS dstbak, stride, block, cnt, ptr, stride3, dst
-%else
-    mov               dstq, dstm
-%endif
-    UNSCRATCH               0, 8, rsp+67*mmsize
-    UNSCRATCH               4, 9, rsp+68*mmsize
-    UNSCRATCH               5, 10, rsp+69*mmsize
-    UNSCRATCH               3, 11, rsp+70*mmsize
-    ROUND_AND_STORE_4x4  0, 4, 5, 3, m7, m1, m2, 6
-%if ARCH_X86_64
-    DEFINE_ARGS dst, stride, block, cnt, ptr, stride3, dstbak
-    lea               dstq, [dstbakq+stride3q*4]
-%else
-    lea               dstq, [dstq+stride3q*4]
-%endif
-    UNSCRATCH               4, 12, rsp+71*mmsize
-    UNSCRATCH               5, 13, rsp+72*mmsize
-    UNSCRATCH               6, 14, rsp+73*mmsize
-    UNSCRATCH               0, 15, rsp+74*mmsize
-    ROUND_AND_STORE_4x4  4, 5, 6, 0, m7, m1, m2, 6
-
-    add               ptrq, mmsize
-%if ARCH_X86_64
-    add            dstbakq, 8
-    mov               dstq, dstbakq
-%else
-    add         dword dstm, 8
-    mov               dstq, dstm
-%endif
-    dec               cntd
-    jg .loop_2
-
-    ; m7 is still zero
-    ZERO_BLOCK blockq-4*mmsize, 64, 16, m7
-    RET
-
-INIT_XMM sse2
-cglobal vp9_idct_idct_16x16_add_12, 4, 6 + ARCH_X86_64, 16, \
-                                    67 * mmsize + ARCH_X86_32 * 8 * mmsize, \
-                                    dst, stride, block, eob
-    mova                m0, [pw_4095]
-    cmp               eobd, 1
-    jg mangle(private_prefix %+ _ %+ vp9_idct_idct_16x16_add_10 %+ SUFFIX).idctfull
-
-    ; dc-only - unfortunately, this one can overflow, since coefs are 19+sign
-    ; bpp, and 19+14+sign does not fit in 32bit, so we do 2-stage multiplies
-    DEFINE_ARGS dst, stride, block, coef, coefl
-    pxor                m2, m2
-    DC_ONLY_64BIT        6, m2
-    movd                m1, coefd
-    pshuflw             m1, m1, q0000
-    punpcklqdq          m1, m1
-    DEFINE_ARGS dst, stride, cnt
-    mov               cntd, 8
-.loop_dc:
-    STORE_2x8            3, 4, 1, m2, m0, dstq,         mmsize
-    STORE_2x8            3, 4, 1, m2, m0, dstq+strideq, mmsize
-    lea               dstq, [dstq+strideq*2]
-    dec               cntd
-    jg .loop_dc
-    RET
-
-; r65-69 are available for spills
-; r70-77 are available on x86-32 only (x86-64 should use m8-15)
-; output should be in m8-11|r70-73, m0-6,r65 and m12-15|r74-77
-%macro IADST16_1D 1 ; src
-    mova                m0, [%1+ 0*4*mmsize]        ; in0
-    mova                m1, [%1+ 7*4*mmsize]        ; in7
-    mova                m2, [%1+ 8*4*mmsize]        ; in8
-    mova                m3, [%1+15*4*mmsize]        ; in15
-    SUMSUB_MUL_D         3, 0, 4, 5, 16364,  804    ; m3/4=t0, m0/5=t1
-    SUMSUB_MUL_D         1, 2, 6, 7, 11003, 12140   ; m1/6=t8, m2/7=t9
-    SCRATCH              0, 8, rsp+70*mmsize
-    SUMSUB_PACK_D        1, 3, 6, 4, 0              ; m1=t0a, m3=t8a
-    UNSCRATCH            0, 8, rsp+70*mmsize
-    SUMSUB_PACK_D        2, 0, 7, 5, 4              ; m2=t1a, m0=t9a
-    mova   [rsp+67*mmsize], m1
-    SCRATCH              2, 9, rsp+71*mmsize
-    SCRATCH              3, 12, rsp+74*mmsize
-    SCRATCH              0, 13, rsp+75*mmsize
-
-    mova                m0, [%1+ 3*4*mmsize]        ; in3
-    mova                m1, [%1+ 4*4*mmsize]        ; in4
-    mova                m2, [%1+11*4*mmsize]        ; in11
-    mova                m3, [%1+12*4*mmsize]        ; in12
-    SUMSUB_MUL_D         2, 1, 4, 5, 14811,  7005   ; m2/4=t4, m1/5=t5
-    SUMSUB_MUL_D         0, 3, 6, 7,  5520, 15426   ; m0/6=t12, m3/7=t13
-    SCRATCH              1, 10, rsp+72*mmsize
-    SUMSUB_PACK_D        0, 2, 6, 4, 1              ; m0=t4a, m2=t12a
-    UNSCRATCH            1, 10, rsp+72*mmsize
-    SUMSUB_PACK_D        3, 1, 7, 5, 4              ; m3=t5a, m1=t13a
-    SCRATCH              0, 15, rsp+77*mmsize
-    SCRATCH              3, 11, rsp+73*mmsize
-
-    UNSCRATCH            0, 12, rsp+74*mmsize       ; t8a
-    UNSCRATCH            3, 13, rsp+75*mmsize       ; t9a
-    SUMSUB_MUL_D         0, 3, 4, 5, 16069,  3196   ; m0/4=t8, m3/5=t9
-    SUMSUB_MUL_D         1, 2, 6, 7,  3196, 16069   ; m1/6=t13, m2/7=t12
-    SCRATCH              1, 12, rsp+74*mmsize
-    SUMSUB_PACK_D        2, 0, 7, 4, 1              ; m2=t8a, m0=t12a
-    UNSCRATCH            1, 12, rsp+74*mmsize
-    SUMSUB_PACK_D        1, 3, 6, 5, 4              ; m1=t9a, m3=t13a
-    mova   [rsp+65*mmsize], m2
-    mova   [rsp+66*mmsize], m1
-    SCRATCH              0, 8, rsp+70*mmsize
-    SCRATCH              3, 12, rsp+74*mmsize
-
-    mova                m0, [%1+ 2*4*mmsize]        ; in2
-    mova                m1, [%1+ 5*4*mmsize]        ; in5
-    mova                m2, [%1+10*4*mmsize]        ; in10
-    mova                m3, [%1+13*4*mmsize]        ; in13
-    SUMSUB_MUL_D         3, 0, 4, 5, 15893,  3981   ; m3/4=t2, m0/5=t3
-    SUMSUB_MUL_D         1, 2, 6, 7,  8423, 14053   ; m1/6=t10, m2/7=t11
-    SCRATCH              0, 10, rsp+72*mmsize
-    SUMSUB_PACK_D        1, 3, 6, 4, 0              ; m1=t2a, m3=t10a
-    UNSCRATCH            0, 10, rsp+72*mmsize
-    SUMSUB_PACK_D        2, 0, 7, 5, 4              ; m2=t3a, m0=t11a
-    mova   [rsp+68*mmsize], m1
-    mova   [rsp+69*mmsize], m2
-    SCRATCH              3, 13, rsp+75*mmsize
-    SCRATCH              0, 14, rsp+76*mmsize
-
-    mova                m0, [%1+ 1*4*mmsize]        ; in1
-    mova                m1, [%1+ 6*4*mmsize]        ; in6
-    mova                m2, [%1+ 9*4*mmsize]        ; in9
-    mova                m3, [%1+14*4*mmsize]        ; in14
-    SUMSUB_MUL_D         2, 1, 4, 5, 13160,  9760   ; m2/4=t6, m1/5=t7
-    SUMSUB_MUL_D         0, 3, 6, 7,  2404, 16207   ; m0/6=t14, m3/7=t15
-    SCRATCH              1, 10, rsp+72*mmsize
-    SUMSUB_PACK_D        0, 2, 6, 4, 1              ; m0=t6a, m2=t14a
-    UNSCRATCH            1, 10, rsp+72*mmsize
-    SUMSUB_PACK_D        3, 1, 7, 5, 4              ; m3=t7a, m1=t15a
-
-    UNSCRATCH            4, 13, rsp+75*mmsize       ; t10a
-    UNSCRATCH            5, 14, rsp+76*mmsize       ; t11a
-    SCRATCH              0, 13, rsp+75*mmsize
-    SCRATCH              3, 14, rsp+76*mmsize
-    SUMSUB_MUL_D         4, 5, 6, 7,  9102, 13623   ; m4/6=t10, m5/7=t11
-    SUMSUB_MUL_D         1, 2, 0, 3, 13623,  9102   ; m1/0=t15, m2/3=t14
-    SCRATCH              0, 10, rsp+72*mmsize
-    SUMSUB_PACK_D        2, 4, 3, 6, 0              ; m2=t10a, m4=t14a
-    UNSCRATCH            0, 10, rsp+72*mmsize
-    SUMSUB_PACK_D        1, 5, 0, 7, 6              ; m1=t11a, m5=t15a
-
-    UNSCRATCH            0, 8, rsp+70*mmsize        ; t12a
-    UNSCRATCH            3, 12, rsp+74*mmsize       ; t13a
-    SCRATCH              2, 8, rsp+70*mmsize
-    SCRATCH              1, 12, rsp+74*mmsize
-    SUMSUB_MUL_D         0, 3, 1, 2, 15137,  6270   ; m0/1=t12, m3/2=t13
-    SUMSUB_MUL_D         5, 4, 7, 6,  6270, 15137   ; m5/7=t15, m4/6=t14
-    SCRATCH              2, 10, rsp+72*mmsize
-    SUMSUB_PACK_D        4, 0, 6, 1, 2              ; m4=out2, m0=t14a
-    UNSCRATCH            2, 10, rsp+72*mmsize
-    SUMSUB_PACK_D        5, 3, 7, 2, 1              ; m5=-out13, m3=t15a
-    NEGD                m5                          ; m5=out13
-
-    UNSCRATCH            1, 9, rsp+71*mmsize        ; t1a
-    mova                m2, [rsp+68*mmsize]         ; t2a
-    UNSCRATCH            6, 13, rsp+75*mmsize       ; t6a
-    UNSCRATCH            7, 14, rsp+76*mmsize       ; t7a
-    SCRATCH              4, 10, rsp+72*mmsize
-    SCRATCH              5, 13, rsp+75*mmsize
-    UNSCRATCH            4, 15, rsp+77*mmsize       ; t4a
-    UNSCRATCH            5, 11, rsp+73*mmsize       ; t5a
-    SCRATCH              0, 14, rsp+76*mmsize
-    SCRATCH              3, 15, rsp+77*mmsize
-    mova                m0, [rsp+67*mmsize]         ; t0a
-    SUMSUB_BA         d, 4, 0, 3                    ; m4=t0, m0=t4
-    SUMSUB_BA         d, 5, 1, 3                    ; m5=t1, m1=t5
-    SUMSUB_BA         d, 6, 2, 3                    ; m6=t2, m2=t6
-    SCRATCH              4, 9, rsp+71*mmsize
-    mova                m3, [rsp+69*mmsize]         ; t3a
-    SUMSUB_BA         d, 7, 3, 4                    ; m7=t3, m3=t7
-
-    mova   [rsp+67*mmsize], m5
-    mova   [rsp+68*mmsize], m6
-    mova   [rsp+69*mmsize], m7
-    SUMSUB_MUL_D         0, 1, 4, 5, 15137,  6270   ; m0/4=t4a, m1/5=t5a
-    SUMSUB_MUL_D         3, 2, 7, 6,  6270, 15137   ; m3/7=t7a, m2/6=t6a
-    SCRATCH              1, 11, rsp+73*mmsize
-    SUMSUB_PACK_D        2, 0, 6, 4, 1              ; m2=-out3, m0=t6
-    NEGD                m2                          ; m2=out3
-    UNSCRATCH            1, 11, rsp+73*mmsize
-    SUMSUB_PACK_D        3, 1, 7, 5, 4              ; m3=out12, m1=t7
-    SCRATCH              2, 11, rsp+73*mmsize
-    UNSCRATCH            2, 12, rsp+74*mmsize       ; t11a
-    SCRATCH              3, 12, rsp+74*mmsize
-
-    UNSCRATCH            3, 8, rsp+70*mmsize        ; t10a
-    mova                m4, [rsp+65*mmsize]         ; t8a
-    mova                m5, [rsp+66*mmsize]         ; t9a
-    SUMSUB_BA         d, 3, 4, 6                    ; m3=-out1, m4=t10
-    NEGD                m3                          ; m3=out1
-    SUMSUB_BA         d, 2, 5, 6                    ; m2=out14, m5=t11
-    UNSCRATCH            6, 9, rsp+71*mmsize        ; t0
-    UNSCRATCH            7, 14, rsp+76*mmsize       ; t14a
-    SCRATCH              3, 9, rsp+71*mmsize
-    SCRATCH              2, 14, rsp+76*mmsize
-
-    SUMSUB_MUL           1, 0, 2, 3, 11585, 11585   ; m1=out4, m0=out11
-    mova   [rsp+65*mmsize], m0
-    SUMSUB_MUL           5, 4, 2, 3, 11585, 11585   ; m5=out6, m4=out9
-    UNSCRATCH            0, 15, rsp+77*mmsize       ; t15a
-    SUMSUB_MUL           7, 0, 2, 3, 11585, m11585  ; m7=out10, m0=out5
-
-    mova                m2, [rsp+68*mmsize]         ; t2
-    SUMSUB_BA         d, 2, 6, 3                    ; m2=out0, m6=t2a
-    SCRATCH              2, 8, rsp+70*mmsize
-    mova                m2, [rsp+67*mmsize]         ; t1
-    mova                m3, [rsp+69*mmsize]         ; t3
-    mova   [rsp+67*mmsize], m7
-    SUMSUB_BA         d, 3, 2, 7                    ; m3=-out15, m2=t3a
-    NEGD                m3                          ; m3=out15
-    SCRATCH              3, 15, rsp+77*mmsize
-    SUMSUB_MUL           6, 2, 7, 3, 11585, m11585  ; m6=out8, m2=out7
-    mova                m7, [rsp+67*mmsize]
-
-    SWAP                 0, 1
-    SWAP                 2, 5, 4, 6, 7, 3
-%endmacro
-
-%macro IADST16_FN 7
-cglobal vp9_%1_%4_16x16_add_10, 4, 6 + ARCH_X86_64, 16, \
-                                70 * mmsize + ARCH_X86_32 * 8 * mmsize, \
-                                dst, stride, block, eob
-    mova                m0, [pw_1023]
-
-.body:
-    mova   [rsp+64*mmsize], m0
-    DEFINE_ARGS dst, stride, block, cnt, ptr, skip, dstbak
-%if ARCH_X86_64
-    mov            dstbakq, dstq
-    movsxd            cntq, cntd
-%endif
-%ifdef PIC
-    lea               ptrq, [%7_16x16]
-    movzx             cntd, byte [ptrq+cntq-1]
-%else
-    movzx             cntd, byte [%7_16x16+cntq-1]
-%endif
-    mov              skipd, 4
-    sub              skipd, cntd
-    mov               ptrq, rsp
-.loop_1:
-    %2_1D           blockq
-
-    TRANSPOSE4x4D        0, 1, 2, 3, 7
-    mova  [ptrq+ 1*mmsize], m0
-    mova  [ptrq+ 5*mmsize], m1
-    mova  [ptrq+ 9*mmsize], m2
-    mova  [ptrq+13*mmsize], m3
-    mova                m7, [rsp+65*mmsize]
-    TRANSPOSE4x4D        4, 5, 6, 7, 0
-    mova  [ptrq+ 2*mmsize], m4
-    mova  [ptrq+ 6*mmsize], m5
-    mova  [ptrq+10*mmsize], m6
-    mova  [ptrq+14*mmsize], m7
-    UNSCRATCH               0, 8, rsp+(%3+0)*mmsize
-    UNSCRATCH               1, 9, rsp+(%3+1)*mmsize
-    UNSCRATCH               2, 10, rsp+(%3+2)*mmsize
-    UNSCRATCH               3, 11, rsp+(%3+3)*mmsize
-    TRANSPOSE4x4D        0, 1, 2, 3, 7
-    mova  [ptrq+ 0*mmsize], m0
-    mova  [ptrq+ 4*mmsize], m1
-    mova  [ptrq+ 8*mmsize], m2
-    mova  [ptrq+12*mmsize], m3
-    UNSCRATCH               4, 12, rsp+(%3+4)*mmsize
-    UNSCRATCH               5, 13, rsp+(%3+5)*mmsize
-    UNSCRATCH               6, 14, rsp+(%3+6)*mmsize
-    UNSCRATCH               7, 15, rsp+(%3+7)*mmsize
-    TRANSPOSE4x4D        4, 5, 6, 7, 0
-    mova  [ptrq+ 3*mmsize], m4
-    mova  [ptrq+ 7*mmsize], m5
-    mova  [ptrq+11*mmsize], m6
-    mova  [ptrq+15*mmsize], m7
-    add               ptrq, 16 * mmsize
-    add             blockq, mmsize
-    dec               cntd
-    jg .loop_1
-
-    ; zero-pad the remainder (skipped cols)
-    test             skipd, skipd
-    jz .end
-    add              skipd, skipd
-    lea             blockq, [blockq+skipq*(mmsize/2)]
-    pxor                m0, m0
-.loop_z:
-    mova   [ptrq+mmsize*0], m0
-    mova   [ptrq+mmsize*1], m0
-    mova   [ptrq+mmsize*2], m0
-    mova   [ptrq+mmsize*3], m0
-    mova   [ptrq+mmsize*4], m0
-    mova   [ptrq+mmsize*5], m0
-    mova   [ptrq+mmsize*6], m0
-    mova   [ptrq+mmsize*7], m0
-    add               ptrq, 8 * mmsize
-    dec              skipd
-    jg .loop_z
-.end:
-
-    DEFINE_ARGS dst, stride, block, cnt, ptr, stride3, dstbak
-    lea           stride3q, [strideq*3]
-    mov               cntd, 4
-    mov               ptrq, rsp
-.loop_2:
-    %5_1D             ptrq
-
-    pxor                m7, m7
-    lea               dstq, [dstq+strideq*4]
-    ROUND_AND_STORE_4x4  0, 1, 2, 3, m7, [rsp+64*mmsize], [pd_32], 6
-    lea               dstq, [dstq+strideq*4]
-    mova                m0, [rsp+65*mmsize]
-    mova                m1, [rsp+64*mmsize]
-    mova                m2, [pd_32]
-    ROUND_AND_STORE_4x4  4, 5, 6, 0, m7, m1, m2, 6
-
-%if ARCH_X86_64
-    DEFINE_ARGS dstbak, stride, block, cnt, ptr, stride3, dst
-%else
-    mov               dstq, dstm
-%endif
-    UNSCRATCH               0, 8, rsp+(%6+0)*mmsize
-    UNSCRATCH               4, 9, rsp+(%6+1)*mmsize
-    UNSCRATCH               5, 10, rsp+(%6+2)*mmsize
-    UNSCRATCH               3, 11, rsp+(%6+3)*mmsize
-    ROUND_AND_STORE_4x4  0, 4, 5, 3, m7, m1, m2, 6
-%if ARCH_X86_64
-    DEFINE_ARGS dst, stride, block, cnt, ptr, stride3, dstbak
-    lea               dstq, [dstbakq+stride3q*4]
-%else
-    lea               dstq, [dstq+stride3q*4]
-%endif
-    UNSCRATCH               4, 12, rsp+(%6+4)*mmsize
-    UNSCRATCH               5, 13, rsp+(%6+5)*mmsize
-    UNSCRATCH               6, 14, rsp+(%6+6)*mmsize
-    UNSCRATCH               0, 15, rsp+(%6+7)*mmsize
-    ROUND_AND_STORE_4x4  4, 5, 6, 0, m7, m1, m2, 6
-
-    add               ptrq, mmsize
-%if ARCH_X86_64
-    add            dstbakq, 8
-    mov               dstq, dstbakq
-%else
-    add         dword dstm, 8
-    mov               dstq, dstm
-%endif
-    dec               cntd
-    jg .loop_2
-
-    ; m7 is still zero
-    ZERO_BLOCK blockq-4*mmsize, 64, 16, m7
-    RET
-
-cglobal vp9_%1_%4_16x16_add_12, 4, 6 + ARCH_X86_64, 16, \
-                                70 * mmsize + ARCH_X86_32 * 8 * mmsize, \
-                                dst, stride, block, eob
-    mova                m0, [pw_4095]
-    jmp mangle(private_prefix %+ _ %+ vp9_%1_%4_16x16_add_10 %+ SUFFIX).body
-%endmacro
-
-INIT_XMM sse2
-IADST16_FN idct,  IDCT16,  67, iadst, IADST16, 70, row
-IADST16_FN iadst, IADST16, 70, idct,  IDCT16,  67, col
-IADST16_FN iadst, IADST16, 70, iadst, IADST16, 70, default
-
-%macro IDCT32_1D 2-3 8 * mmsize; pass[1/2], src, src_stride
-    IDCT16_1D %2, 2 * %3, 272, 257
-%if ARCH_X86_64
-    mova  [rsp+257*mmsize], m8
-    mova  [rsp+258*mmsize], m9
-    mova  [rsp+259*mmsize], m10
-    mova  [rsp+260*mmsize], m11
-    mova  [rsp+261*mmsize], m12
-    mova  [rsp+262*mmsize], m13
-    mova  [rsp+263*mmsize], m14
-    mova  [rsp+264*mmsize], m15
-%endif
-    mova  [rsp+265*mmsize], m0
-    mova  [rsp+266*mmsize], m1
-    mova  [rsp+267*mmsize], m2
-    mova  [rsp+268*mmsize], m3
-    mova  [rsp+269*mmsize], m4
-    mova  [rsp+270*mmsize], m5
-    mova  [rsp+271*mmsize], m6
-
-    ; r257-260: t0-3
-    ; r265-272: t4/5a/6a/7/8/9a/10/11a
-    ; r261-264: t12a/13/14a/15
-    ; r273-274 is free as scratch space, and 275-282 mirrors m8-15 on 32bit
-
-    mova                m0, [%2+ 1*%3]              ; in1
-    mova                m1, [%2+15*%3]              ; in15
-    mova                m2, [%2+17*%3]              ; in17
-    mova                m3, [%2+31*%3]              ; in31
-    SUMSUB_MUL           0, 3, 4, 5, 16364,  804    ; m0=t31a, m3=t16a
-    SUMSUB_MUL           2, 1, 4, 5, 11003, 12140   ; m2=t30a, m1=t17a
-    SUMSUB_BA         d, 1, 3, 4                    ; m1=t16, m3=t17
-    SUMSUB_BA         d, 2, 0, 4                    ; m2=t31, m0=t30
-    SUMSUB_MUL           0, 3, 4, 5, 16069,  3196   ; m0=t30a, m3=t17a
-    SCRATCH              0, 8, rsp+275*mmsize
-    SCRATCH              2, 9, rsp+276*mmsize
-
-    ; end of stage 1-3 first quart
-
-    mova                m0, [%2+ 7*%3]              ; in7
-    mova                m2, [%2+ 9*%3]              ; in9
-    mova                m4, [%2+23*%3]              ; in23
-    mova                m5, [%2+25*%3]              ; in25
-    SUMSUB_MUL           2, 4, 6, 7, 14811,  7005   ; m2=t29a, m4=t18a
-    SUMSUB_MUL           5, 0, 6, 7,  5520, 15426   ; m5=t28a, m0=t19a
-    SUMSUB_BA         d, 4, 0, 6                    ; m4=t19, m0=t18
-    SUMSUB_BA         d, 2, 5, 6                    ; m2=t28, m5=t29
-    SUMSUB_MUL           5, 0, 6, 7,  3196, m16069  ; m5=t29a, m0=t18a
-
-    ; end of stage 1-3 second quart
-
-    SUMSUB_BA         d, 4, 1, 6                    ; m4=t16a, m1=t19a
-    SUMSUB_BA         d, 0, 3, 6                    ; m0=t17, m3=t18
-    UNSCRATCH            6, 8, rsp+275*mmsize       ; t30a
-    UNSCRATCH            7, 9, rsp+276*mmsize       ; t31
-    mova  [rsp+273*mmsize], m4
-    mova  [rsp+274*mmsize], m0
-    SUMSUB_BA         d, 2, 7, 0                    ; m2=t31a, m7=t28a
-    SUMSUB_BA         d, 5, 6, 0                    ; m5=t30, m6=t29
-    SUMSUB_MUL           6, 3, 0, 4, 15137,  6270   ; m6=t29a, m3=t18a
-    SUMSUB_MUL           7, 1, 0, 4, 15137,  6270   ; m7=t28, m1=t19
-    SCRATCH              3, 10, rsp+277*mmsize
-    SCRATCH              1, 11, rsp+278*mmsize
-    SCRATCH              7, 12, rsp+279*mmsize
-    SCRATCH              6, 13, rsp+280*mmsize
-    SCRATCH              5, 14, rsp+281*mmsize
-    SCRATCH              2, 15, rsp+282*mmsize
-
-    ; end of stage 4-5 first half
-
-    mova                m0, [%2+ 5*%3]              ; in5
-    mova                m1, [%2+11*%3]              ; in11
-    mova                m2, [%2+21*%3]              ; in21
-    mova                m3, [%2+27*%3]              ; in27
-    SUMSUB_MUL           0, 3, 4, 5, 15893,  3981   ; m0=t27a, m3=t20a
-    SUMSUB_MUL           2, 1, 4, 5,  8423, 14053   ; m2=t26a, m1=t21a
-    SUMSUB_BA         d, 1, 3, 4                    ; m1=t20, m3=t21
-    SUMSUB_BA         d, 2, 0, 4                    ; m2=t27, m0=t26
-    SUMSUB_MUL           0, 3, 4, 5,  9102, 13623   ; m0=t26a, m3=t21a
-    SCRATCH              0, 8, rsp+275*mmsize
-    SCRATCH              2, 9, rsp+276*mmsize
-
-    ; end of stage 1-3 third quart
-
-    mova                m0, [%2+ 3*%3]              ; in3
-    mova                m2, [%2+13*%3]              ; in13
-    mova                m4, [%2+19*%3]              ; in19
-    mova                m5, [%2+29*%3]              ; in29
-    SUMSUB_MUL           2, 4, 6, 7, 13160,  9760   ; m2=t25a, m4=t22a
-    SUMSUB_MUL           5, 0, 6, 7,  2404, 16207   ; m5=t24a, m0=t23a
-    SUMSUB_BA         d, 4, 0, 6                    ; m4=t23, m0=t22
-    SUMSUB_BA         d, 2, 5, 6                    ; m2=t24, m5=t25
-    SUMSUB_MUL           5, 0, 6, 7, 13623, m9102   ; m5=t25a, m0=t22a
-
-    ; end of stage 1-3 fourth quart
-
-    SUMSUB_BA         d, 1, 4, 6                    ; m1=t23a, m4=t20a
-    SUMSUB_BA         d, 3, 0, 6                    ; m3=t22, m0=t21
-    UNSCRATCH            6, 8, rsp+275*mmsize       ; t26a
-    UNSCRATCH            7, 9, rsp+276*mmsize       ; t27
-    SCRATCH              3, 8, rsp+275*mmsize
-    SCRATCH              1, 9, rsp+276*mmsize
-    SUMSUB_BA         d, 7, 2, 1                    ; m7=t24a, m2=t27a
-    SUMSUB_BA         d, 6, 5, 1                    ; m6=t25, m5=t26
-    SUMSUB_MUL           2, 4, 1, 3,  6270, m15137  ; m2=t27, m4=t20
-    SUMSUB_MUL           5, 0, 1, 3,  6270, m15137  ; m5=t26a, m0=t21a
-
-    ; end of stage 4-5 second half
-
-    UNSCRATCH            1, 12, rsp+279*mmsize      ; t28
-    UNSCRATCH            3, 13, rsp+280*mmsize      ; t29a
-    SCRATCH              4, 12, rsp+279*mmsize
-    SCRATCH              0, 13, rsp+280*mmsize
-    SUMSUB_BA         d, 5, 3, 0                    ; m5=t29, m3=t26
-    SUMSUB_BA         d, 2, 1, 0                    ; m2=t28a, m1=t27a
-    UNSCRATCH            0, 14, rsp+281*mmsize      ; t30
-    UNSCRATCH            4, 15, rsp+282*mmsize      ; t31a
-    SCRATCH              2, 14, rsp+281*mmsize
-    SCRATCH              5, 15, rsp+282*mmsize
-    SUMSUB_BA         d, 6, 0, 2                    ; m6=t30a, m0=t25a
-    SUMSUB_BA         d, 7, 4, 2                    ; m7=t31, m4=t24
-
-    mova                m2, [rsp+273*mmsize]        ; t16a
-    mova                m5, [rsp+274*mmsize]        ; t17
-    mova  [rsp+273*mmsize], m6
-    mova  [rsp+274*mmsize], m7
-    UNSCRATCH            6, 10, rsp+277*mmsize      ; t18a
-    UNSCRATCH            7, 11, rsp+278*mmsize      ; t19
-    SCRATCH              4, 10, rsp+277*mmsize
-    SCRATCH              0, 11, rsp+278*mmsize
-    UNSCRATCH            4, 12, rsp+279*mmsize      ; t20
-    UNSCRATCH            0, 13, rsp+280*mmsize      ; t21a
-    SCRATCH              3, 12, rsp+279*mmsize
-    SCRATCH              1, 13, rsp+280*mmsize
-    SUMSUB_BA         d, 0, 6, 1                    ; m0=t18, m6=t21
-    SUMSUB_BA         d, 4, 7, 1                    ; m4=t19a, m7=t20a
-    UNSCRATCH            3, 8, rsp+275*mmsize       ; t22
-    UNSCRATCH            1, 9, rsp+276*mmsize       ; t23a
-    SCRATCH              0, 8, rsp+275*mmsize
-    SCRATCH              4, 9, rsp+276*mmsize
-    SUMSUB_BA         d, 3, 5, 0                    ; m3=t17a, m5=t22a
-    SUMSUB_BA         d, 1, 2, 0                    ; m1=t16, m2=t23
-
-    ; end of stage 6
-
-    UNSCRATCH            0, 10, rsp+277*mmsize      ; t24
-    UNSCRATCH            4, 11, rsp+278*mmsize      ; t25a
-    SCRATCH              1, 10, rsp+277*mmsize
-    SCRATCH              3, 11, rsp+278*mmsize
-    SUMSUB_MUL           0, 2, 1, 3, 11585, 11585   ; m0=t24a, m2=t23a
-    SUMSUB_MUL           4, 5, 1, 3, 11585, 11585   ; m4=t25, m5=t22
-    UNSCRATCH            1, 12, rsp+279*mmsize      ; t26
-    UNSCRATCH            3, 13, rsp+280*mmsize      ; t27a
-    SCRATCH              0, 12, rsp+279*mmsize
-    SCRATCH              4, 13, rsp+280*mmsize
-    SUMSUB_MUL           3, 7, 0, 4, 11585, 11585   ; m3=t27, m7=t20
-    SUMSUB_MUL           1, 6, 0, 4, 11585, 11585   ; m1=t26a, m6=t21a
-
-    ; end of stage 7
-
-    mova                m0, [rsp+269*mmsize]        ; t8
-    mova                m4, [rsp+270*mmsize]        ; t9a
-    mova  [rsp+269*mmsize], m1                      ; t26a
-    mova  [rsp+270*mmsize], m3                      ; t27
-    mova                m3, [rsp+271*mmsize]        ; t10
-    SUMSUB_BA         d, 2, 0, 1                    ; m2=out8, m0=out23
-    SUMSUB_BA         d, 5, 4, 1                    ; m5=out9, m4=out22
-    SUMSUB_BA         d, 6, 3, 1                    ; m6=out10, m3=out21
-    mova                m1, [rsp+272*mmsize]        ; t11a
-    mova  [rsp+271*mmsize], m0
-    SUMSUB_BA         d, 7, 1, 0                    ; m7=out11, m1=out20
-
-%if %1 == 1
-    TRANSPOSE4x4D        2, 5, 6, 7, 0
-    mova  [ptrq+ 2*mmsize], m2
-    mova  [ptrq+10*mmsize], m5
-    mova  [ptrq+18*mmsize], m6
-    mova  [ptrq+26*mmsize], m7
-%else ; %1 == 2
-    pxor                m0, m0
-    lea               dstq, [dstq+strideq*8]
-    ROUND_AND_STORE_4x4  2, 5, 6, 7, m0, [rsp+256*mmsize], [pd_32], 6
-%endif
-    mova                m2, [rsp+271*mmsize]
-%if %1 == 1
-    TRANSPOSE4x4D        1, 3, 4, 2, 0
-    mova  [ptrq+ 5*mmsize], m1
-    mova  [ptrq+13*mmsize], m3
-    mova  [ptrq+21*mmsize], m4
-    mova  [ptrq+29*mmsize], m2
-%else ; %1 == 2
-    lea               dstq, [dstq+stride3q*4]
-    ROUND_AND_STORE_4x4  1, 3, 4, 2, m0, [rsp+256*mmsize], [pd_32], 6
-%endif
-
-    ; end of last stage + store for out8-11 and out20-23
-
-    UNSCRATCH            0, 9, rsp+276*mmsize       ; t19a
-    UNSCRATCH            1, 8, rsp+275*mmsize       ; t18
-    UNSCRATCH            2, 11, rsp+278*mmsize      ; t17a
-    UNSCRATCH            3, 10, rsp+277*mmsize      ; t16
-    mova                m7, [rsp+261*mmsize]        ; t12a
-    mova                m6, [rsp+262*mmsize]        ; t13
-    mova                m5, [rsp+263*mmsize]        ; t14a
-    SUMSUB_BA         d, 0, 7, 4                    ; m0=out12, m7=out19
-    SUMSUB_BA         d, 1, 6, 4                    ; m1=out13, m6=out18
-    SUMSUB_BA         d, 2, 5, 4                    ; m2=out14, m5=out17
-    mova                m4, [rsp+264*mmsize]        ; t15
-    SCRATCH              7, 8, rsp+275*mmsize
-    SUMSUB_BA         d, 3, 4, 7                    ; m3=out15, m4=out16
-
-%if %1 == 1
-    TRANSPOSE4x4D        0, 1, 2, 3, 7
-    mova  [ptrq+ 3*mmsize], m0
-    mova  [ptrq+11*mmsize], m1
-    mova  [ptrq+19*mmsize], m2
-    mova  [ptrq+27*mmsize], m3
-%else ; %1 == 2
-%if ARCH_X86_64
-    SWAP                 7, 9
-    lea               dstq, [dstbakq+stride3q*4]
-%else ; x86-32
-    pxor                m7, m7
-    mov               dstq, dstm
-    lea               dstq, [dstq+stride3q*4]
-%endif
-    ROUND_AND_STORE_4x4  0, 1, 2, 3, m7, [rsp+256*mmsize], [pd_32], 6
-%endif
-    UNSCRATCH            0, 8, rsp+275*mmsize       ; out19
-%if %1 == 1
-    TRANSPOSE4x4D        4, 5, 6, 0, 7
-    mova  [ptrq+ 4*mmsize], m4
-    mova  [ptrq+12*mmsize], m5
-    mova  [ptrq+20*mmsize], m6
-    mova  [ptrq+28*mmsize], m0
-%else ; %1 == 2
-    lea               dstq, [dstq+strideq*4]
-    ROUND_AND_STORE_4x4  4, 5, 6, 0, m7, [rsp+256*mmsize], [pd_32], 6
-%endif
-
-    ; end of last stage + store for out12-19
-
-%if ARCH_X86_64
-    SWAP                 7, 8
-%endif
-    mova                m7, [rsp+257*mmsize]        ; t0
-    mova                m6, [rsp+258*mmsize]        ; t1
-    mova                m5, [rsp+259*mmsize]        ; t2
-    mova                m4, [rsp+260*mmsize]        ; t3
-    mova                m0, [rsp+274*mmsize]        ; t31
-    mova                m1, [rsp+273*mmsize]        ; t30a
-    UNSCRATCH            2, 15, rsp+282*mmsize      ; t29
-    SUMSUB_BA         d, 0, 7, 3                    ; m0=out0, m7=out31
-    SUMSUB_BA         d, 1, 6, 3                    ; m1=out1, m6=out30
-    SUMSUB_BA         d, 2, 5, 3                    ; m2=out2, m5=out29
-    SCRATCH              0, 9, rsp+276*mmsize
-    UNSCRATCH            3, 14, rsp+281*mmsize      ; t28a
-    SUMSUB_BA         d, 3, 4, 0                    ; m3=out3, m4=out28
-
-%if %1 == 1
-    TRANSPOSE4x4D        4, 5, 6, 7, 0
-    mova  [ptrq+ 7*mmsize], m4
-    mova  [ptrq+15*mmsize], m5
-    mova  [ptrq+23*mmsize], m6
-    mova  [ptrq+31*mmsize], m7
-%else ; %1 == 2
-%if ARCH_X86_64
-    SWAP                 0, 8
-%else ; x86-32
-    pxor                m0, m0
-%endif
-    lea               dstq, [dstq+stride3q*4]
-    ROUND_AND_STORE_4x4  4, 5, 6, 7, m0, [rsp+256*mmsize], [pd_32], 6
-%endif
-    UNSCRATCH            7, 9, rsp+276*mmsize       ; out0
-%if %1 == 1
-    TRANSPOSE4x4D        7, 1, 2, 3, 0
-    mova  [ptrq+ 0*mmsize], m7
-    mova  [ptrq+ 8*mmsize], m1
-    mova  [ptrq+16*mmsize], m2
-    mova  [ptrq+24*mmsize], m3
-%else ; %1 == 2
-%if ARCH_X86_64
-    DEFINE_ARGS dstbak, stride, block, cnt, ptr, stride3, dst
-%else ; x86-32
-    mov               dstq, dstm
-%endif
-    ROUND_AND_STORE_4x4  7, 1, 2, 3, m0, [rsp+256*mmsize], [pd_32], 6
-%if ARCH_X86_64
-    DEFINE_ARGS dst, stride, block, cnt, ptr, stride3, dstbak
-%endif
-%endif
-
-    ; end of last stage + store for out0-3 and out28-31
-
-%if ARCH_X86_64
-    SWAP                 0, 8
-%endif
-    mova                m7, [rsp+265*mmsize]        ; t4
-    mova                m6, [rsp+266*mmsize]        ; t5a
-    mova                m5, [rsp+267*mmsize]        ; t6a
-    mova                m4, [rsp+268*mmsize]        ; t7
-    mova                m0, [rsp+270*mmsize]        ; t27
-    mova                m1, [rsp+269*mmsize]        ; t26a
-    UNSCRATCH            2, 13, rsp+280*mmsize      ; t25
-    SUMSUB_BA         d, 0, 7, 3                    ; m0=out4, m7=out27
-    SUMSUB_BA         d, 1, 6, 3                    ; m1=out5, m6=out26
-    SUMSUB_BA         d, 2, 5, 3                    ; m2=out6, m5=out25
-    UNSCRATCH            3, 12, rsp+279*mmsize      ; t24a
-    SCRATCH              7, 9, rsp+276*mmsize
-    SUMSUB_BA         d, 3, 4, 7                    ; m3=out7, m4=out24
-
-%if %1 == 1
-    TRANSPOSE4x4D        0, 1, 2, 3, 7
-    mova  [ptrq+ 1*mmsize], m0
-    mova  [ptrq+ 9*mmsize], m1
-    mova  [ptrq+17*mmsize], m2
-    mova  [ptrq+25*mmsize], m3
-%else ; %1 == 2
-%if ARCH_X86_64
-    SWAP                 7, 8
-    lea               dstq, [dstbakq+strideq*4]
-%else ; x86-32
-    pxor                m7, m7
-    lea               dstq, [dstq+strideq*4]
-%endif
-    ROUND_AND_STORE_4x4  0, 1, 2, 3, m7, [rsp+256*mmsize], [pd_32], 6
-%endif
-    UNSCRATCH            0, 9, rsp+276*mmsize       ; out27
-%if %1 == 1
-    TRANSPOSE4x4D        4, 5, 6, 0, 7
-    mova  [ptrq+ 6*mmsize], m4
-    mova  [ptrq+14*mmsize], m5
-    mova  [ptrq+22*mmsize], m6
-    mova  [ptrq+30*mmsize], m0
-%else ; %1 == 2
-%if ARCH_X86_64
-    lea               dstq, [dstbakq+stride3q*8]
-%else
-    mov               dstq, dstm
-    lea               dstq, [dstq+stride3q*8]
-%endif
-    ROUND_AND_STORE_4x4  4, 5, 6, 0, m7, [rsp+256*mmsize], [pd_32], 6
-%endif
-
-    ; end of last stage + store for out4-7 and out24-27
-%endmacro
-
-INIT_XMM sse2
-cglobal vp9_idct_idct_32x32_add_10, 4, 6 + ARCH_X86_64, 16, \
-                                    275 * mmsize + ARCH_X86_32 * 8 * mmsize, \
-                                    dst, stride, block, eob
-    mova                m0, [pw_1023]
-    cmp               eobd, 1
-    jg .idctfull
-
-    ; dc-only - the 10bit version can be done entirely in 32bit, since the max
-    ; coef values are 17+sign bit, and the coef is 14bit, so 31+sign easily
-    ; fits in 32bit
-    DEFINE_ARGS dst, stride, block, coef
-    pxor                m2, m2
-    DC_ONLY              6, m2
-    movd                m1, coefd
-    pshuflw             m1, m1, q0000
-    punpcklqdq          m1, m1
-    DEFINE_ARGS dst, stride, cnt
-    mov               cntd, 32
-.loop_dc:
-    STORE_2x8            3, 4, 1, m2, m0, dstq,          mmsize
-    STORE_2x8            3, 4, 1, m2, m0, dstq+mmsize*2, mmsize
-    add               dstq, strideq
-    dec               cntd
-    jg .loop_dc
-    RET
-
-.idctfull:
-    mova  [rsp+256*mmsize], m0
-    DEFINE_ARGS dst, stride, block, cnt, ptr, skip, dstbak
-%if ARCH_X86_64
-    mov            dstbakq, dstq
-    movsxd            cntq, cntd
-%endif
-%ifdef PIC
-    lea               ptrq, [default_32x32]
-    movzx             cntd, byte [ptrq+cntq-1]
-%else
-    movzx             cntd, byte [default_32x32+cntq-1]
-%endif
-    mov              skipd, 8
-    sub              skipd, cntd
-    mov               ptrq, rsp
-.loop_1:
-    IDCT32_1D            1, blockq
-
-    add               ptrq, 32 * mmsize
-    add             blockq, mmsize
-    dec               cntd
-    jg .loop_1
-
-    ; zero-pad the remainder (skipped cols)
-    test             skipd, skipd
-    jz .end
-    shl              skipd, 2
-    lea             blockq, [blockq+skipq*(mmsize/4)]
-    pxor                m0, m0
-.loop_z:
-    mova   [ptrq+mmsize*0], m0
-    mova   [ptrq+mmsize*1], m0
-    mova   [ptrq+mmsize*2], m0
-    mova   [ptrq+mmsize*3], m0
-    mova   [ptrq+mmsize*4], m0
-    mova   [ptrq+mmsize*5], m0
-    mova   [ptrq+mmsize*6], m0
-    mova   [ptrq+mmsize*7], m0
-    add               ptrq, 8 * mmsize
-    dec              skipd
-    jg .loop_z
-.end:
-
-    DEFINE_ARGS dst, stride, block, cnt, ptr, stride3, dstbak
-    lea           stride3q, [strideq*3]
-    mov               cntd, 8
-    mov               ptrq, rsp
-.loop_2:
-    IDCT32_1D            2, ptrq
-
-    add               ptrq, mmsize
-%if ARCH_X86_64
-    add            dstbakq, 8
-    mov               dstq, dstbakq
-%else
-    add         dword dstm, 8
-    mov               dstq, dstm
-%endif
-    dec               cntd
-    jg .loop_2
-
-    ; m7 is still zero
-    ZERO_BLOCK blockq-8*mmsize, 128, 32, m7
-    RET
-
-INIT_XMM sse2
-cglobal vp9_idct_idct_32x32_add_12, 4, 6 + ARCH_X86_64, 16, \
-                                    275 * mmsize + ARCH_X86_32 * 8 * mmsize, \
-                                    dst, stride, block, eob
-    mova                m0, [pw_4095]
-    cmp               eobd, 1
-    jg mangle(private_prefix %+ _ %+ vp9_idct_idct_32x32_add_10 %+ SUFFIX).idctfull
-
-    ; dc-only - unfortunately, this one can overflow, since coefs are 19+sign
-    ; bpp, and 19+14+sign does not fit in 32bit, so we do 2-stage multiplies
-    DEFINE_ARGS dst, stride, block, coef, coefl
-    pxor                m2, m2
-    DC_ONLY_64BIT        6, m2
-    movd                m1, coefd
-    pshuflw             m1, m1, q0000
-    punpcklqdq          m1, m1
-    DEFINE_ARGS dst, stride, cnt
-    mov               cntd, 32
-.loop_dc:
-    STORE_2x8            3, 4, 1, m2, m0, dstq,          mmsize
-    STORE_2x8            3, 4, 1, m2, m0, dstq+mmsize*2, mmsize
-    add               dstq, strideq
-    dec               cntd
-    jg .loop_dc
-    RET
diff -uparN ffmpeg-4.1/libavcodec/x86/vp9itxfm.asm ffmpeg-y/libavcodec/x86/vp9itxfm.asm
--- ffmpeg-4.1/libavcodec/x86/vp9itxfm.asm	2018-11-02 02:34:25.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/vp9itxfm.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,3197 +0,0 @@
-;******************************************************************************
-;* VP9 IDCT SIMD optimizations
-;*
-;* Copyright (C) 2013 Clément Bœsch <u pkh me>
-;* Copyright (C) 2013 Ronald S. Bultje <rsbultje gmail com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-%include "vp9itxfm_template.asm"
-
-SECTION_RODATA 32
-
-%macro VP9_IDCT_COEFFS 2-3 0
-const pw_m%1_%2
-times 8 dw -%1,  %2
-const pw_%2_%1
-times 8 dw  %2,  %1
-
-%if %3 == 1
-const pw_m%2_m%1
-times 8 dw -%2, -%1
-%if %1 != %2
-const pw_m%2_%1
-times 8 dw -%2,  %1
-const pw_%1_%2
-times 8 dw  %1,  %2
-%endif
-%endif
-
-%if %1 < 11585
-pw_m%1x2:   times 16 dw -%1*2
-%elif %1 > 11585
-pw_%1x2:    times 16 dw  %1*2
-%else
-const pw_%1x2
-times 16 dw %1*2
-%endif
-
-%if %2 != %1
-pw_%2x2:    times 16 dw  %2*2
-%endif
-%endmacro
-
-VP9_IDCT_COEFFS 16364,   804
-VP9_IDCT_COEFFS 16305,  1606
-VP9_IDCT_COEFFS 16069,  3196, 1
-VP9_IDCT_COEFFS 15893,  3981
-VP9_IDCT_COEFFS 15137,  6270, 1
-VP9_IDCT_COEFFS 14811,  7005
-VP9_IDCT_COEFFS 14449,  7723
-VP9_IDCT_COEFFS 13160,  9760
-VP9_IDCT_COEFFS 11585, 11585, 1
-VP9_IDCT_COEFFS 11003, 12140
-VP9_IDCT_COEFFS 10394, 12665
-VP9_IDCT_COEFFS  9102, 13623, 1
-VP9_IDCT_COEFFS  8423, 14053
-VP9_IDCT_COEFFS  5520, 15426
-VP9_IDCT_COEFFS  4756, 15679
-VP9_IDCT_COEFFS  2404, 16207
-
-const pw_5283_13377
-times 4 dw 5283, 13377
-const pw_9929_13377
-times 4 dw 9929, 13377
-const pw_15212_m13377
-times 4 dw 15212, -13377
-const pw_15212_9929
-times 4 dw 15212, 9929
-const pw_m5283_m15212
-times 4 dw -5283, -15212
-const pw_13377x2
-times 8 dw 13377*2
-const pw_m13377_13377
-times 4 dw -13377, 13377
-const pw_13377_0
-times 4 dw 13377, 0
-
-cextern pw_8
-cextern pw_16
-cextern pw_32
-cextern pw_512
-cextern pw_1024
-cextern pw_2048
-cextern pw_m1
-cextern pd_8192
-
-SECTION .text
-
-%macro VP9_UNPACK_MULSUB_2D_4X 6 ; dst1 [src1], dst2 [src2], dst3, dst4, mul1, mul2
-    punpckhwd          m%4, m%2, m%1
-    punpcklwd          m%2, m%1
-    pmaddwd            m%3, m%4, [pw_m%5_%6]
-    pmaddwd            m%4, [pw_%6_%5]
-    pmaddwd            m%1, m%2, [pw_m%5_%6]
-    pmaddwd            m%2, [pw_%6_%5]
-%endmacro
-
-%macro VP9_RND_SH_SUMSUB_BA 6 ; dst1 [src1], dst2 [src2], src3, src4, tmp, round
-    SUMSUB_BA            d, %1, %2, %5
-    SUMSUB_BA            d, %3, %4, %5
-    paddd              m%1, %6
-    paddd              m%2, %6
-    paddd              m%3, %6
-    paddd              m%4, %6
-    psrad              m%1, 14
-    psrad              m%2, 14
-    psrad              m%3, 14
-    psrad              m%4, 14
-    packssdw           m%1, m%3
-    packssdw           m%2, m%4
-%endmacro
-
-%macro VP9_STORE_2X 5-6 dstq ; reg1, reg2, tmp1, tmp2, zero, dst
-%if mmsize == 32
-    pmovzxbw           m%3, [%6]
-    pmovzxbw           m%4, [%6+strideq]
-%else
-    movh               m%3, [%6]
-    movh               m%4, [%6+strideq]
-    punpcklbw          m%3, m%5
-    punpcklbw          m%4, m%5
-%endif
-    paddw              m%3, m%1
-    paddw              m%4, m%2
-%if mmsize == 32
-    packuswb           m%3, m%4
-    ; Intel...
-    vpermq             m%3, m%3, q3120
-    mova              [%6], xm%3
-    vextracti128 [%6+strideq], m%3, 1
-%elif mmsize == 16
-    packuswb           m%3, m%4
-    movh              [%6], m%3
-    movhps    [%6+strideq], m%3
-%else
-    packuswb           m%3, m%5
-    packuswb           m%4, m%5
-    movh              [%6], m%3
-    movh      [%6+strideq], m%4
-%endif
-%endmacro
-
-%macro ZERO_BLOCK 4 ; mem, stride, nnzcpl, zero_reg
-%assign %%y 0
-%rep %3
-%assign %%x 0
-%rep %3*2/mmsize
-    mova      [%1+%%y+%%x], %4
-%assign %%x (%%x+mmsize)
-%endrep
-%assign %%y (%%y+%2)
-%endrep
-%endmacro
-
-;-------------------------------------------------------------------------------------------
-; void vp9_iwht_iwht_4x4_add_<opt>(uint8_t *dst, ptrdiff_t stride, int16_t *block, int eob);
-;-------------------------------------------------------------------------------------------
-
-INIT_MMX mmx
-cglobal vp9_iwht_iwht_4x4_add, 3, 3, 0, dst, stride, block, eob
-    mova                m0, [blockq+0*8]
-    mova                m1, [blockq+1*8]
-    mova                m2, [blockq+2*8]
-    mova                m3, [blockq+3*8]
-    psraw               m0, 2
-    psraw               m1, 2
-    psraw               m2, 2
-    psraw               m3, 2
-
-    VP9_IWHT4_1D
-    TRANSPOSE4x4W        0, 1, 2, 3, 4
-    VP9_IWHT4_1D
-
-    pxor                m4, m4
-    VP9_STORE_2X         0, 1, 5, 6, 4
-    lea               dstq, [dstq+strideq*2]
-    VP9_STORE_2X         2, 3, 5, 6, 4
-    ZERO_BLOCK      blockq, 8, 4, m4
-    RET
-
-;-------------------------------------------------------------------------------------------
-; void vp9_idct_idct_4x4_add_<opt>(uint8_t *dst, ptrdiff_t stride, int16_t *block, int eob);
-;-------------------------------------------------------------------------------------------
-
-; 2x2 top left corner
-%macro VP9_IDCT4_2x2_1D 0
-    pmulhrsw            m0, m5                              ; m0=t1
-    mova                m2, m0                              ; m2=t0
-    mova                m3, m1
-    pmulhrsw            m1, m6                              ; m1=t2
-    pmulhrsw            m3, m7                              ; m3=t3
-    VP9_IDCT4_1D_FINALIZE
-%endmacro
-
-%macro VP9_IDCT4_WRITEOUT 0
-%if cpuflag(ssse3)
-    mova                m5, [pw_2048]
-    pmulhrsw            m0, m5              ; (x*2048 + (1<<14))>>15 <=> (x+8)>>4
-    pmulhrsw            m1, m5
-%else
-    mova                m5, [pw_8]
-    paddw               m0, m5
-    paddw               m1, m5
-    psraw               m0, 4
-    psraw               m1, 4
-%endif
-    VP9_STORE_2X         0,  1,  6,  7,  4
-    lea               dstq, [dstq+2*strideq]
-%if cpuflag(ssse3)
-    pmulhrsw            m2, m5
-    pmulhrsw            m3, m5
-%else
-    paddw               m2, m5
-    paddw               m3, m5
-    psraw               m2, 4
-    psraw               m3, 4
-%endif
-    VP9_STORE_2X         2,  3,  6,  7,  4
-%endmacro
-
-%macro IDCT_4x4_FN 1
-INIT_MMX %1
-cglobal vp9_idct_idct_4x4_add, 4, 4, 0, dst, stride, block, eob
-
-%if cpuflag(ssse3)
-    cmp eobd, 4 ; 2x2 or smaller
-    jg .idctfull
-
-    cmp eobd, 1 ; faster path for when only DC is set
-    jne .idct2x2
-%else
-    cmp eobd, 1
-    jg .idctfull
-%endif
-
-%if cpuflag(ssse3)
-    movd                m0, [blockq]
-    mova                m5, [pw_11585x2]
-    pmulhrsw            m0, m5
-    pmulhrsw            m0, m5
-%else
-    DEFINE_ARGS dst, stride, block, coef
-    movsx            coefd, word [blockq]
-    imul             coefd, 11585
-    add              coefd, 8192
-    sar              coefd, 14
-    imul             coefd, 11585
-    add              coefd, (8 << 14) + 8192
-    sar              coefd, 14 + 4
-    movd                m0, coefd
-%endif
-    pshufw              m0, m0, 0
-    pxor                m4, m4
-    movh          [blockq], m4
-%if cpuflag(ssse3)
-    pmulhrsw            m0, [pw_2048]       ; (x*2048 + (1<<14))>>15 <=> (x+8)>>4
-%endif
-    VP9_STORE_2X         0,  0,  6,  7,  4
-    lea               dstq, [dstq+2*strideq]
-    VP9_STORE_2X         0,  0,  6,  7,  4
-    RET
-
-%if cpuflag(ssse3)
-; faster path for when only top left 2x2 block is set
-.idct2x2:
-    movd                m0, [blockq+0]
-    movd                m1, [blockq+8]
-    mova                m5, [pw_11585x2]
-    mova                m6, [pw_6270x2]
-    mova                m7, [pw_15137x2]
-    VP9_IDCT4_2x2_1D
-    ; partial 2x4 transpose
-    punpcklwd           m0, m1
-    punpcklwd           m2, m3
-    SBUTTERFLY          dq, 0, 2, 1
-    SWAP                1, 2
-    VP9_IDCT4_2x2_1D
-    pxor                m4, m4  ; used for the block reset, and VP9_STORE_2X
-    movh       [blockq+ 0], m4
-    movh       [blockq+ 8], m4
-    VP9_IDCT4_WRITEOUT
-    RET
-%endif
-
-.idctfull: ; generic full 4x4 idct/idct
-    mova                m0, [blockq+ 0]
-    mova                m1, [blockq+ 8]
-    mova                m2, [blockq+16]
-    mova                m3, [blockq+24]
-%if cpuflag(ssse3)
-    mova                m6, [pw_11585x2]
-%endif
-    mova                m7, [pd_8192]       ; rounding
-    VP9_IDCT4_1D
-    TRANSPOSE4x4W  0, 1, 2, 3, 4
-    VP9_IDCT4_1D
-    pxor                m4, m4  ; used for the block reset, and VP9_STORE_2X
-    mova       [blockq+ 0], m4
-    mova       [blockq+ 8], m4
-    mova       [blockq+16], m4
-    mova       [blockq+24], m4
-    VP9_IDCT4_WRITEOUT
-    RET
-%endmacro
-
-IDCT_4x4_FN mmxext
-IDCT_4x4_FN ssse3
-
-;-------------------------------------------------------------------------------------------
-; void vp9_iadst_iadst_4x4_add_<opt>(uint8_t *dst, ptrdiff_t stride, int16_t *block, int eob);
-;-------------------------------------------------------------------------------------------
-
-%macro IADST4_FN 5
-INIT_MMX %5
-cglobal vp9_%1_%3_4x4_add, 3, 3, 0, dst, stride, block, eob
-%if WIN64 && notcpuflag(ssse3)
-    WIN64_SPILL_XMM 8
-%endif
-    movdqa            xmm5, [pd_8192]
-    mova                m0, [blockq+ 0]
-    mova                m1, [blockq+ 8]
-    mova                m2, [blockq+16]
-    mova                m3, [blockq+24]
-%if cpuflag(ssse3)
-    mova                m6, [pw_11585x2]
-%endif
-%ifnidn %1%3, iadstiadst
-    movdq2q             m7, xmm5
-%endif
-    VP9_%2_1D
-    TRANSPOSE4x4W  0, 1, 2, 3, 4
-    VP9_%4_1D
-    pxor                m4, m4  ; used for the block reset, and VP9_STORE_2X
-    mova       [blockq+ 0], m4
-    mova       [blockq+ 8], m4
-    mova       [blockq+16], m4
-    mova       [blockq+24], m4
-    VP9_IDCT4_WRITEOUT
-    RET
-%endmacro
-
-IADST4_FN idct,  IDCT4,  iadst, IADST4, sse2
-IADST4_FN iadst, IADST4, idct,  IDCT4,  sse2
-IADST4_FN iadst, IADST4, iadst, IADST4, sse2
-
-IADST4_FN idct,  IDCT4,  iadst, IADST4, ssse3
-IADST4_FN iadst, IADST4, idct,  IDCT4,  ssse3
-IADST4_FN iadst, IADST4, iadst, IADST4, ssse3
-
-%macro SCRATCH 3
-%if ARCH_X86_64
-    SWAP                %1, %2
-%else
-    mova              [%3], m%1
-%endif
-%endmacro
-
-%macro UNSCRATCH 3
-%if ARCH_X86_64
-    SWAP                %1, %2
-%else
-    mova               m%1, [%3]
-%endif
-%endmacro
-
-;-------------------------------------------------------------------------------------------
-; void vp9_idct_idct_8x8_add_<opt>(uint8_t *dst, ptrdiff_t stride, int16_t *block, int eob);
-;-------------------------------------------------------------------------------------------
-
-%macro VP9_IDCT8_1D_FINALIZE 0
-    SUMSUB_BA            w,  3,  6, 5                       ; m3=t0+t7, m6=t0-t7
-    SUMSUB_BA            w,  1,  2, 5                       ; m1=t1+t6, m2=t1-t6
-    SUMSUB_BA            w,  7,  0, 5                       ; m7=t2+t5, m0=t2-t5
-
-    UNSCRATCH            5, 8, blockq+ 0
-    SCRATCH              2, 8, blockq+ 0
-
-    SUMSUB_BA            w,  5,  4, 2                       ; m5=t3+t4, m4=t3-t4
-    SWAP                 7,  6,  2
-    SWAP                 3,  5,  0
-
-%if ARCH_X86_64
-    SWAP                 6, 8
-%endif
-%endmacro
-
-; x86-32
-; - in: m0/m4 is in mem
-; - out: m6 is in mem
-; x86-64:
-; - everything is in registers (m0-7)
-%macro VP9_IDCT8_1D 0
-%if ARCH_X86_64
-    SWAP                 0, 8
-    SWAP                 4, 9
-%endif
-
-    VP9_UNPACK_MULSUB_2W_4X 5,  3,  9102, 13623, D_8192_REG, 0, 4  ; m5=t5a, m3=t6a
-    VP9_UNPACK_MULSUB_2W_4X 1,  7, 16069,  3196, D_8192_REG, 0, 4  ; m1=t4a, m7=t7a
-    SUMSUB_BA            w,  5,  1, 0                       ; m5=t4a+t5a (t4), m1=t4a-t5a (t5a)
-    SUMSUB_BA            w,  3,  7, 0                       ; m3=t7a+t6a (t7), m7=t7a-t6a (t6a)
-%if cpuflag(ssse3)
-    SUMSUB_BA            w,  1,  7, 0                       ; m1=t6a+t5a (t6), m7=t6a-t5a (t5)
-    pmulhrsw            m1, W_11585x2_REG                   ; m1=t6
-    pmulhrsw            m7, W_11585x2_REG                   ; m7=t5
-%else
-    VP9_UNPACK_MULSUB_2W_4X 7,  1, 11585, 11585, D_8192_REG, 0, 4
-%endif
-    VP9_UNPACK_MULSUB_2W_4X 2,  6, 15137,  6270, D_8192_REG, 0, 4  ; m2=t2a, m6=t3a
-
-    UNSCRATCH            0, 8, blockq+ 0    ; IN(0)
-    UNSCRATCH            4, 9, blockq+64    ; IN(4)
-    SCRATCH              5, 8, blockq+ 0
-
-%if cpuflag(ssse3)
-    SUMSUB_BA            w, 4, 0, 5                         ; m4=IN(0)+IN(4) m0=IN(0)-IN(4)
-    pmulhrsw            m4, W_11585x2_REG                   ; m4=t0a
-    pmulhrsw            m0, W_11585x2_REG                   ; m0=t1a
-%else
-    SCRATCH              7, 9, blockq+64
-    VP9_UNPACK_MULSUB_2W_4X 0,  4, 11585, 11585, D_8192_REG, 5, 7
-    UNSCRATCH            7, 9, blockq+64
-%endif
-    SUMSUB_BA            w,  6,  4, 5                       ; m6=t0a+t3a (t0), m4=t0a-t3a (t3)
-    SUMSUB_BA            w,  2,  0, 5                       ; m2=t1a+t2a (t1), m0=t1a-t2a (t2)
-
-    VP9_IDCT8_1D_FINALIZE
-%endmacro
-
-%macro VP9_IDCT8_4x4_1D 0
-    pmulhrsw            m0, W_11585x2_REG                   ; m0=t1a/t0a
-    pmulhrsw            m6, m2, [pw_15137x2]                ; m6=t3a
-    pmulhrsw            m2, [pw_6270x2]                     ; m2=t2a
-    pmulhrsw            m7, m1, [pw_16069x2]                ; m7=t7a
-    pmulhrsw            m1, [pw_3196x2]                     ; m1=t4a
-    pmulhrsw            m5, m3, [pw_m9102x2]                ; m5=t5a
-    pmulhrsw            m3, [pw_13623x2]                    ; m3=t6a
-    SUMSUB_BA            w,  5,  1, 4                       ; m1=t4a+t5a (t4), m5=t4a-t5a (t5a)
-    SUMSUB_BA            w,  3,  7, 4                       ; m3=t7a+t6a (t7), m7=t7a-t6a (t6a)
-    SUMSUB_BA            w,  1,  7, 4                       ; m1=t6a+t5a (t6), m7=t6a-t5a (t5)
-    pmulhrsw            m1, W_11585x2_REG                   ; m1=t6
-    pmulhrsw            m7, W_11585x2_REG                   ; m7=t5
-    psubw               m4, m0, m6                          ; m4=t0a-t3a (t3)
-    paddw               m6, m0                              ; m6=t0a+t3a (t0)
-    SCRATCH              5,  8, blockq+ 0
-    SUMSUB_BA            w,  2,  0, 5                       ; m2=t1a+t2a (t1), m0=t1a-t2a (t2)
-    VP9_IDCT8_1D_FINALIZE
-%endmacro
-
-%macro VP9_IDCT8_2x2_1D 1
-    pmulhrsw            m0, W_11585x2_REG                   ; m0=t0
-    pmulhrsw            m3, m1, W_16069x2_REG               ; m3=t7
-    pmulhrsw            m1, W_3196x2_REG                    ; m1=t4
-    psubw               m7, m3, m1                          ; t5 = t7a - t4a
-    paddw               m5, m3, m1                          ; t6 = t7a + t4a
-    pmulhrsw            m7, W_11585x2_REG                   ; m7=t5
-    pmulhrsw            m5, W_11585x2_REG                   ; m5=t6
-    SWAP                 5,  1
-    ; merged VP9_IDCT8_1D_FINALIZE to make register-sharing w/ avx easier
-    psubw               m6, m0, m3                          ; m6=t0-t7
-    paddw               m3, m0                              ; m3=t0+t7
-    psubw               m2, m0, m1                          ; m2=t1-t6
-    paddw               m1, m0                              ; m1=t1+t6
-%if %1 == 1
-    punpcklwd           m3, m1
-%define SCRATCH_REG 1
-%elif ARCH_X86_32
-    mova       [blockq+ 0], m2
-%define SCRATCH_REG 2
-%else
-%define SCRATCH_REG 8
-%endif
-    psubw               m4, m0, m5                          ; m4=t3-t4
-    paddw               m5, m0                              ; m5=t3+t4
-    SUMSUB_BA            w,  7,  0, SCRATCH_REG             ; m7=t2+t5, m0=t2-t5
-    SWAP                 7,  6,  2
-    SWAP                 3,  5,  0
-%undef SCRATCH_REG
-%endmacro
-
-%macro VP9_IDCT8_WRITEx2 6-8 5 ; line1, line2, tmp1, tmp2, zero, pw_1024/pw_16, shift
-%if cpuflag(ssse3)
-    pmulhrsw           m%1, %6              ; (x*1024 + (1<<14))>>15 <=> (x+16)>>5
-    pmulhrsw           m%2, %6
-%else
-    paddw              m%1, %6
-    paddw              m%2, %6
-    psraw              m%1, %7
-    psraw              m%2, %7
-%endif
-%if %0 <= 7
-    VP9_STORE_2X        %1, %2, %3, %4, %5
-%else
-    VP9_STORE_2X        %1, %2, %3, %4, %5, %8
-%endif
-%endmacro
-
-; x86-32:
-; - m6 is in mem
-; x86-64:
-; - m8 holds m6 (SWAP)
-; m6 holds zero
-%macro VP9_IDCT8_WRITEOUT 0
-%if ARCH_X86_64
-%if cpuflag(ssse3)
-    mova                m9, [pw_1024]
-%else
-    mova                m9, [pw_16]
-%endif
-%define ROUND_REG m9
-%else
-%if cpuflag(ssse3)
-%define ROUND_REG [pw_1024]
-%else
-%define ROUND_REG [pw_16]
-%endif
-%endif
-    SCRATCH              5, 10, blockq+16
-    SCRATCH              7, 11, blockq+32
-    VP9_IDCT8_WRITEx2    0,  1, 5, 7, 6, ROUND_REG
-    lea               dstq, [dstq+2*strideq]
-    VP9_IDCT8_WRITEx2    2,  3, 5, 7, 6, ROUND_REG
-    lea               dstq, [dstq+2*strideq]
-    UNSCRATCH            5, 10, blockq+16
-    UNSCRATCH            7, 11, blockq+32
-    VP9_IDCT8_WRITEx2    4,  5, 0, 1, 6, ROUND_REG
-    lea               dstq, [dstq+2*strideq]
-    UNSCRATCH            5, 8, blockq+ 0
-    VP9_IDCT8_WRITEx2    5,  7, 0, 1, 6, ROUND_REG
-
-%undef ROUND_REG
-%endmacro
-
-%macro VP9_IDCT_IDCT_8x8_ADD_XMM 2
-INIT_XMM %1
-cglobal vp9_idct_idct_8x8_add, 4, 4, %2, dst, stride, block, eob
-
-%if cpuflag(ssse3)
-%if ARCH_X86_64
-    mova               m12, [pw_11585x2]    ; often used
-%define W_11585x2_REG m12
-%else
-%define W_11585x2_REG [pw_11585x2]
-%endif
-
-    cmp eobd, 12 ; top left half or less
-    jg .idctfull
-
-    cmp eobd, 3  ; top left corner or less
-    jg .idcthalf
-
-    cmp eobd, 1 ; faster path for when only DC is set
-    jne .idcttopleftcorner
-%else
-    cmp eobd, 1
-    jg .idctfull
-%endif
-
-%if cpuflag(ssse3)
-    movd                m0, [blockq]
-    pmulhrsw            m0, W_11585x2_REG
-    pmulhrsw            m0, W_11585x2_REG
-%else
-    DEFINE_ARGS dst, stride, block, coef
-    movsx            coefd, word [blockq]
-    imul             coefd, 11585
-    add              coefd, 8192
-    sar              coefd, 14
-    imul             coefd, 11585
-    add              coefd, (16 << 14) + 8192
-    sar              coefd, 14 + 5
-    movd                m0, coefd
-%endif
-    SPLATW              m0, m0, 0
-    pxor                m4, m4
-    movd          [blockq], m4
-%if cpuflag(ssse3)
-    pmulhrsw            m0, [pw_1024]       ; (x*1024 + (1<<14))>>15 <=> (x+16)>>5
-%endif
-%rep 3
-    VP9_STORE_2X         0,  0,  6,  7,  4
-    lea               dstq, [dstq+2*strideq]
-%endrep
-    VP9_STORE_2X         0,  0,  6,  7,  4
-    RET
-
-%if cpuflag(ssse3)
-; faster path for when only left corner is set (3 input: DC, right to DC, below
-; to DC). Note: also working with a 2x2 block
-.idcttopleftcorner:
-    movd                m0, [blockq+0]
-    movd                m1, [blockq+16]
-%if ARCH_X86_64
-    mova               m10, [pw_3196x2]
-    mova               m11, [pw_16069x2]
-%define W_3196x2_REG m10
-%define W_16069x2_REG m11
-%else
-%define W_3196x2_REG [pw_3196x2]
-%define W_16069x2_REG [pw_16069x2]
-%endif
-    VP9_IDCT8_2x2_1D 1
-    ; partial 2x8 transpose
-    ; punpcklwd m0, m1 already done inside idct
-    punpcklwd           m2, m3
-    punpcklwd           m4, m5
-    punpcklwd           m6, m7
-    punpckldq           m0, m2
-    punpckldq           m4, m6
-    SBUTTERFLY         qdq, 0, 4, 1
-    SWAP                 1, 4
-    VP9_IDCT8_2x2_1D 2
-%if ARCH_X86_64
-    SWAP                 6, 8
-%endif
-    pxor                m6, m6  ; used for the block reset, and VP9_STORE_2X
-    VP9_IDCT8_WRITEOUT
-%if ARCH_X86_64
-    movd       [blockq+ 0], m6
-    movd       [blockq+16], m6
-%else
-    mova       [blockq+ 0], m6
-    mova       [blockq+16], m6
-    mova       [blockq+32], m6
-%endif
-    RET
-
-.idcthalf:
-    movh                m0, [blockq + 0]
-    movh                m1, [blockq +16]
-    movh                m2, [blockq +32]
-    movh                m3, [blockq +48]
-    VP9_IDCT8_4x4_1D
-    ; partial 4x8 transpose
-%if ARCH_X86_32
-    mova                m6, [blockq+ 0]
-%endif
-    punpcklwd           m0, m1
-    punpcklwd           m2, m3
-    punpcklwd           m4, m5
-    punpcklwd           m6, m7
-    SBUTTERFLY          dq, 0, 2, 1
-    SBUTTERFLY          dq, 4, 6, 5
-    SBUTTERFLY         qdq, 0, 4, 1
-    SBUTTERFLY         qdq, 2, 6, 5
-    SWAP                 1, 4
-    SWAP                 3, 6
-    VP9_IDCT8_4x4_1D
-%if ARCH_X86_64
-    SWAP                 6, 8
-%endif
-    pxor                m6, m6
-    VP9_IDCT8_WRITEOUT
-%if ARCH_X86_64
-    movh       [blockq+ 0], m6
-    movh       [blockq+16], m6
-    movh       [blockq+32], m6
-%else
-    mova       [blockq+ 0], m6
-    mova       [blockq+16], m6
-    mova       [blockq+32], m6
-%endif
-    movh       [blockq+48], m6
-    RET
-%endif
-
-.idctfull: ; generic full 8x8 idct/idct
-%if ARCH_X86_64
-    mova                m0, [blockq+  0]    ; IN(0)
-%endif
-    mova                m1, [blockq+ 16]    ; IN(1)
-    mova                m2, [blockq+ 32]    ; IN(2)
-    mova                m3, [blockq+ 48]    ; IN(3)
-%if ARCH_X86_64
-    mova                m4, [blockq+ 64]    ; IN(4)
-%endif
-    mova                m5, [blockq+ 80]    ; IN(5)
-    mova                m6, [blockq+ 96]    ; IN(6)
-    mova                m7, [blockq+112]    ; IN(7)
-%if ARCH_X86_64
-    mova               m11, [pd_8192]       ; rounding
-%define D_8192_REG m11
-%else
-%define D_8192_REG [pd_8192]
-%endif
-    VP9_IDCT8_1D
-%if ARCH_X86_64
-    TRANSPOSE8x8W  0, 1, 2, 3, 4, 5, 6, 7, 8
-%else
-    TRANSPOSE8x8W  0, 1, 2, 3, 4, 5, 6, 7, [blockq+0], [blockq+64], 1
-    mova        [blockq+0], m0
-%endif
-    VP9_IDCT8_1D
-
-%if ARCH_X86_64
-    SWAP                 6, 8
-%endif
-    pxor                m6, m6  ; used for the block reset, and VP9_STORE_2X
-    VP9_IDCT8_WRITEOUT
-    ZERO_BLOCK      blockq, 16, 8, m6
-    RET
-%undef W_11585x2_REG
-%endmacro
-
-VP9_IDCT_IDCT_8x8_ADD_XMM sse2, 12
-VP9_IDCT_IDCT_8x8_ADD_XMM ssse3, 13
-VP9_IDCT_IDCT_8x8_ADD_XMM avx, 13
-
-;---------------------------------------------------------------------------------------------
-; void vp9_iadst_iadst_8x8_add_<opt>(uint8_t *dst, ptrdiff_t stride, int16_t *block, int eob);
-;---------------------------------------------------------------------------------------------
-
-; x86-32:
-; - in: m0/3/4/7 are in mem [blockq+N*16]
-; - out: m6 is in mem [blockq+0]
-; x86-64:
-; - everything is in registers
-%macro VP9_IADST8_1D 0 ; input/output=m0/1/2/3/4/5/6/7
-%if ARCH_X86_64
-    SWAP                     0, 8
-    SWAP                     3, 9
-    SWAP                     4, 10
-    SWAP                     7, 11
-%endif
-
-    VP9_UNPACK_MULSUB_2D_4X  5,  2,  0,  3, 14449,  7723    ; m5/2=t3[d], m2/4=t2[d]
-    VP9_UNPACK_MULSUB_2D_4X  1,  6,  4,  7,  4756, 15679    ; m1/4=t7[d], m6/7=t6[d]
-    SCRATCH                  4, 12, blockq+1*16
-    VP9_RND_SH_SUMSUB_BA     6,  2,  7,  3, 4, D_8192_REG  ; m6=t2[w], m2=t6[w]
-    UNSCRATCH                4, 12, blockq+1*16
-    VP9_RND_SH_SUMSUB_BA     1,  5,  4,  0, 3, D_8192_REG  ; m1=t3[w], m5=t7[w]
-
-    UNSCRATCH                0,  8, blockq+16*0
-    UNSCRATCH                3,  9, blockq+16*3
-    UNSCRATCH                4, 10, blockq+16*4
-    UNSCRATCH                7, 11, blockq+16*7
-    SCRATCH                  1,  8, blockq+16*1
-    SCRATCH                  2,  9, blockq+16*2
-    SCRATCH                  5, 10, blockq+16*5
-    SCRATCH                  6, 11, blockq+16*6
-
-    VP9_UNPACK_MULSUB_2D_4X  7,  0,  1,  2, 16305,  1606    ; m7/1=t1[d], m0/2=t0[d]
-    VP9_UNPACK_MULSUB_2D_4X  3,  4,  5,  6, 10394, 12665    ; m3/5=t5[d], m4/6=t4[d]
-    SCRATCH                  1, 12, blockq+ 0*16
-    VP9_RND_SH_SUMSUB_BA     4,  0,  6,  2, 1, D_8192_REG  ; m4=t0[w], m0=t4[w]
-    UNSCRATCH                1, 12, blockq+ 0*16
-    VP9_RND_SH_SUMSUB_BA     3,  7,  5,  1, 2, D_8192_REG  ; m3=t1[w], m7=t5[w]
-
-    UNSCRATCH                2,  9, blockq+16*2
-    UNSCRATCH                5, 10, blockq+16*5
-    SCRATCH                  3,  9, blockq+16*3
-    SCRATCH                  4, 10, blockq+16*4
-
-    ; m4=t0, m3=t1, m6=t2, m1=t3, m0=t4, m7=t5, m2=t6, m5=t7
-
-    VP9_UNPACK_MULSUB_2D_4X  0,  7,  1,  3, 15137,  6270    ; m0/1=t5[d], m7/3=t4[d]
-    VP9_UNPACK_MULSUB_2D_4X  5,  2,  4,  6,  6270, 15137    ; m5/4=t6[d], m2/6=t7[d]
-    SCRATCH                  1, 12, blockq+ 0*16
-    VP9_RND_SH_SUMSUB_BA     5,  7,  4,  3, 1, D_8192_REG
-    UNSCRATCH                1, 12, blockq+ 0*16
-    PSIGNW                  m5, W_M1_REG                    ; m5=out1[w], m7=t6[w]
-    VP9_RND_SH_SUMSUB_BA     2,  0,  6,  1, 3, D_8192_REG   ; m2=out6[w], m0=t7[w]
-
-    UNSCRATCH                1,  8, blockq+16*1
-    UNSCRATCH                3,  9, blockq+16*3
-    UNSCRATCH                4, 10, blockq+16*4
-    UNSCRATCH                6, 11, blockq+16*6
-    SCRATCH                  2,  8, blockq+16*0
-
-    SUMSUB_BA                w,  6,  4, 2                   ; m6=out0[w], m4=t2[w]
-    SUMSUB_BA                w,  1,  3, 2
-    PSIGNW                  m1, W_M1_REG                    ; m1=out7[w], m3=t3[w]
-
-    ; m6=out0, m5=out1, m4=t2, m3=t3, m7=t6, m0=t7, m2=out6, m1=out7
-
-    ; unfortunately, the code below overflows in some cases
-%if 0; cpuflag(ssse3)
-    SUMSUB_BA                w,  3,  4,  2
-    SUMSUB_BA                w,  0,  7,  2
-    pmulhrsw                m3, W_11585x2_REG
-    pmulhrsw                m7, W_11585x2_REG
-    pmulhrsw                m4, W_11585x2_REG               ; out4
-    pmulhrsw                m0, W_11585x2_REG               ; out2
-%else
-    SCRATCH                  5,  9, blockq+16*1
-    VP9_UNPACK_MULSUB_2W_4X  4, 3, 11585, 11585, D_8192_REG, 2, 5
-    VP9_UNPACK_MULSUB_2W_4X  7, 0, 11585, 11585, D_8192_REG, 2, 5
-    UNSCRATCH                5,  9, blockq+16*1
-%endif
-    PSIGNW                  m3, W_M1_REG                    ; out3
-    PSIGNW                  m7, W_M1_REG                    ; out5
-
-    ; m6=out0, m5=out1, m0=out2, m3=out3, m4=out4, m7=out5, m2=out6, m1=out7
-
-%if ARCH_X86_64
-    SWAP                     2, 8
-%endif
-    SWAP                     0, 6, 2
-    SWAP                     7, 1, 5
-%endmacro
-
-%macro IADST8_FN 6
-INIT_XMM %5
-cglobal vp9_%1_%3_8x8_add, 3, 3, %6, dst, stride, block, eob
-
-%ifidn %1, idct
-%define first_is_idct 1
-%else
-%define first_is_idct 0
-%endif
-
-%ifidn %3, idct
-%define second_is_idct 1
-%else
-%define second_is_idct 0
-%endif
-
-%if ARCH_X86_64
-    mova                m0, [blockq+  0]    ; IN(0)
-%endif
-    mova                m1, [blockq+ 16]    ; IN(1)
-    mova                m2, [blockq+ 32]    ; IN(2)
-%if ARCH_X86_64 || first_is_idct
-    mova                m3, [blockq+ 48]    ; IN(3)
-%endif
-%if ARCH_X86_64
-    mova                m4, [blockq+ 64]    ; IN(4)
-%endif
-    mova                m5, [blockq+ 80]    ; IN(5)
-    mova                m6, [blockq+ 96]    ; IN(6)
-%if ARCH_X86_64 || first_is_idct
-    mova                m7, [blockq+112]    ; IN(7)
-%endif
-%if ARCH_X86_64
-%if cpuflag(ssse3)
-    mova               m15, [pw_11585x2]    ; often used
-%endif
-    mova               m13, [pd_8192]       ; rounding
-    mova               m14, [pw_m1]
-%define W_11585x2_REG m15
-%define D_8192_REG m13
-%define W_M1_REG m14
-%else
-%define W_11585x2_REG [pw_11585x2]
-%define D_8192_REG [pd_8192]
-%define W_M1_REG [pw_m1]
-%endif
-
-    ; note different calling conventions for idct8 vs. iadst8 on x86-32
-    VP9_%2_1D
-%if ARCH_X86_64
-    TRANSPOSE8x8W  0, 1, 2, 3, 4, 5, 6, 7, 8
-%else
-    TRANSPOSE8x8W  0, 1, 2, 3, 4, 5, 6, 7, [blockq+0], [blockq+64], 1
-    mova      [blockq+  0], m0
-%if second_is_idct == 0
-    mova      [blockq+ 48], m3
-    mova      [blockq+112], m7
-%endif
-%endif
-    VP9_%4_1D
-
-%if ARCH_X86_64
-    SWAP                 6, 8
-%endif
-    pxor                m6, m6  ; used for the block reset, and VP9_STORE_2X
-    VP9_IDCT8_WRITEOUT
-    ZERO_BLOCK      blockq, 16, 8, m6
-    RET
-
-%undef W_11585x2_REG
-%undef first_is_idct
-%undef second_is_idct
-
-%endmacro
-
-IADST8_FN idct,  IDCT8,  iadst, IADST8, sse2, 15
-IADST8_FN iadst, IADST8, idct,  IDCT8,  sse2, 15
-IADST8_FN iadst, IADST8, iadst, IADST8, sse2, 15
-IADST8_FN idct,  IDCT8,  iadst, IADST8, ssse3, 16
-IADST8_FN idct,  IDCT8,  iadst, IADST8, avx, 16
-IADST8_FN iadst, IADST8, idct,  IDCT8,  ssse3, 16
-IADST8_FN iadst, IADST8, idct,  IDCT8,  avx, 16
-IADST8_FN iadst, IADST8, iadst, IADST8, ssse3, 16
-IADST8_FN iadst, IADST8, iadst, IADST8, avx, 16
-
-;---------------------------------------------------------------------------------------------
-; void vp9_idct_idct_16x16_add_<opt>(uint8_t *dst, ptrdiff_t stride, int16_t *block, int eob);
-;---------------------------------------------------------------------------------------------
-
-; x86-64:
-; at the end of this macro, m7 is stored in [%4+15*%5]
-; everything else (t0-6 and t8-15) is stored in m0-6 and m8-15
-; the following sumsubs have not been done yet:
-;    SUMSUB_BA            w,  6,  9, 15      ; t6, t9
-;    SUMSUB_BA            w,  7,  8, 15      ; t7, t8
-; or (x86-32) t0-t5 are in m0-m5, t10-t15 are in x11/9/7/5/3/1,
-; and the following simsubs have not been done yet:
-;    SUMSUB_BA            w, x13, x14, 7       ; t6, t9
-;    SUMSUB_BA            w, x15, x12, 7       ; t7, t8
-
-%macro VP9_IDCT16_1D_START 6 ; src, nnzc, stride, scratch, scratch_stride, is_iadst
-%if %2 <= 4
-    mova                m3, [%1+ 1*%3]      ; IN(1)
-    mova                m0, [%1+ 3*%3]      ; IN(3)
-
-    pmulhrsw            m4, m3,  [pw_16305x2]       ; t14-15
-    pmulhrsw            m3, [pw_1606x2]             ; t8-9
-    pmulhrsw            m7, m0,  [pw_m4756x2]       ; t10-11
-    pmulhrsw            m0, [pw_15679x2]            ; t12-13
-
-    ; m8=t0, m9=t1, m10=t2, m11=t3, m12=t4, m14=t5, m13=t6, m15=t7
-    ; m3=t8, m5=t9, m1=t10, m7=t11, m0=t12, m6=t13, m2=t14, m4=t15
-
-    VP9_UNPACK_MULSUB_2W_4X 2, 5, 4, 3, 15137,  6270, [pd_8192], 1, 6 ; t9,  t14
-    SCRATCH              4, 10, %4+ 1*%5
-    SCRATCH              5, 11, %4+ 7*%5
-    VP9_UNPACK_MULSUB_2W_4X 6, 1, 0, 7, 6270, m15137, [pd_8192], 4, 5 ; t10, t13
-    UNSCRATCH            5, 11, %4+ 7*%5
-
-    ; m15=t0, m14=t1, m13=t2, m12=t3, m11=t4, m10=t5, m9=t6, m8=t7
-    ; m7=t8, m6=t9, m2=t10, m3=t11, m4=t12, m5=t13, m1=t14, m0=t15
-%else
-    mova                m5, [%1+ 1*%3]      ; IN(1)
-    mova                m4, [%1+ 7*%3]      ; IN(7)
-%if %2 <= 8
-    pmulhrsw            m2, m5,  [pw_16305x2]       ; t15
-    pmulhrsw            m5, [pw_1606x2]             ; t8
-    pmulhrsw            m3, m4,  [pw_m10394x2]      ; t9
-    pmulhrsw            m4, [pw_12665x2]            ; t14
-%else
-    mova                m3, [%1+ 9*%3]      ; IN(9)
-    mova                m2, [%1+15*%3]      ; IN(15)
-
-    ; m10=in0, m5=in1, m14=in2, m6=in3, m9=in4, m7=in5, m15=in6, m4=in7
-    ; m11=in8, m3=in9, m12=in10 m0=in11, m8=in12, m1=in13, m13=in14, m2=in15
-
-    VP9_UNPACK_MULSUB_2W_4X   5,   2, 16305,  1606, [pd_8192], 0, 1 ; t8,  t15
-    VP9_UNPACK_MULSUB_2W_4X   3,   4, 10394, 12665, [pd_8192], 0, 1 ; t9,  t14
-%endif
-
-    SUMSUB_BA            w,  3,  5, 0       ; t8,  t9
-    SUMSUB_BA            w,  4,  2, 0       ; t15, t14
-
-    VP9_UNPACK_MULSUB_2W_4X   2,   5, 15137,  6270, [pd_8192], 0, 1 ; t9,  t14
-
-    SCRATCH              4, 10, %4+ 1*%5
-    SCRATCH              5, 11, %4+ 7*%5
-
-    mova                m6, [%1+ 3*%3]      ; IN(3)
-    mova                m7, [%1+ 5*%3]      ; IN(5)
-%if %2 <= 8
-    pmulhrsw            m0, m7,  [pw_14449x2]       ; t13
-    pmulhrsw            m7, [pw_7723x2]             ; t10
-    pmulhrsw            m1, m6,  [pw_m4756x2]       ; t11
-    pmulhrsw            m6, [pw_15679x2]            ; t12
-%else
-    mova                m0, [%1+11*%3]      ; IN(11)
-    mova                m1, [%1+13*%3]      ; IN(13)
-
-    VP9_UNPACK_MULSUB_2W_4X   7,   0, 14449,  7723, [pd_8192], 4, 5 ; t10, t13
-    VP9_UNPACK_MULSUB_2W_4X   1,   6,  4756, 15679, [pd_8192], 4, 5 ; t11, t12
-%endif
-
-    ; m11=t0, m10=t1, m9=t2, m8=t3, m14=t4, m12=t5, m15=t6, m13=t7
-    ; m5=t8, m3=t9, m7=t10, m1=t11, m6=t12, m0=t13, m4=t14, m2=t15
-
-    SUMSUB_BA            w,  7,  1, 4       ; t11, t10
-    SUMSUB_BA            w,  0,  6, 4       ; t12, t13
-
-    ; m8=t0, m9=t1, m10=t2, m11=t3, m12=t4, m14=t5, m13=t6, m15=t7
-    ; m3=t8, m5=t9, m1=t10, m7=t11, m0=t12, m6=t13, m2=t14, m4=t15
-
-    VP9_UNPACK_MULSUB_2W_4X   6,   1, 6270, m15137, [pd_8192], 4, 5 ; t10, t13
-
-    UNSCRATCH            5, 11, %4+ 7*%5
-%endif
-
-    ; m8=t0, m9=t1, m10=t2, m11=t3, m12=t4, m13=t5, m14=t6, m15=t7
-    ; m3=t8, m2=t9, m6=t10, m7=t11, m0=t12, m1=t13, m5=t14, m4=t15
-
-    SUMSUB_BA            w,  7,  3, 4       ; t8,  t11
-
-    ; backup first register
-    mova        [%4+15*%5], m7
-
-    SUMSUB_BA            w,  6,  2, 7       ; t9,  t10
-    UNSCRATCH            4, 10, %4+ 1*%5
-    SUMSUB_BA            w,  0,  4, 7       ; t15, t12
-    SUMSUB_BA            w,  1,  5, 7       ; t14. t13
-
-    ; m15=t0, m14=t1, m13=t2, m12=t3, m11=t4, m10=t5, m9=t6, m8=t7
-    ; m7=t8, m6=t9, m2=t10, m3=t11, m4=t12, m5=t13, m1=t14, m0=t15
-
-%if cpuflag(ssse3) && %6 == 0
-    SUMSUB_BA            w,  2,  5, 7
-    SUMSUB_BA            w,  3,  4, 7
-    pmulhrsw            m5, [pw_11585x2]    ; t10
-    pmulhrsw            m4, [pw_11585x2]    ; t11
-    pmulhrsw            m3, [pw_11585x2]    ; t12
-    pmulhrsw            m2, [pw_11585x2]    ; t13
-%else
-    SCRATCH              6, 10, %4+ 1*%5
-    VP9_UNPACK_MULSUB_2W_4X   5,   2, 11585, 11585, [pd_8192], 6, 7 ; t10, t13
-    VP9_UNPACK_MULSUB_2W_4X   4,   3, 11585, 11585, [pd_8192], 6, 7 ; t11, t12
-    UNSCRATCH            6, 10, %4+ 1*%5
-%endif
-
-    ; m15=t0, m14=t1, m13=t2, m12=t3, m11=t4, m10=t5, m9=t6, m8=t7
-    ; m7=t8, m6=t9, m5=t10, m4=t11, m3=t12, m2=t13, m1=t14, m0=t15
-
-    SCRATCH              0,  8, %4+ 1*%5
-    SCRATCH              1,  9, %4+ 3*%5
-    SCRATCH              2, 10, %4+ 5*%5
-    SCRATCH              3, 11, %4+ 7*%5
-    SCRATCH              4, 12, %4+ 9*%5
-    SCRATCH              5, 13, %4+11*%5
-    SCRATCH              6, 14, %4+13*%5
-
-    ; even (tx8x8)
-%if %2 <= 4
-    mova                m3, [%1+ 0*%3]      ; IN(0)
-    mova                m4, [%1+ 2*%3]      ; IN(2)
-
-    pmulhrsw            m3, [pw_11585x2]    ; t0-t3
-    pmulhrsw            m7, m4, [pw_16069x2]        ; t6-7
-    pmulhrsw            m4, [pw_3196x2]             ; t4-5
-
-%if 0 ; overflows :(
-    paddw               m6, m7, m4
-    psubw               m5, m7, m4
-    pmulhrsw            m5, [pw_11585x2]            ; t5
-    pmulhrsw            m6, [pw_11585x2]            ; t6
-%else
-    VP9_UNPACK_MULSUB_2W_4X  5, 6, 7, 4, 11585, 11585, [pd_8192], 0, 1 ; t5,  t6
-%endif
-
-    psubw               m0, m3, m7
-    paddw               m7, m3
-    psubw               m1, m3, m6
-    paddw               m6, m3
-    psubw               m2, m3, m5
-    paddw               m5, m3
-
-%if ARCH_X86_32
-    SWAP                 0, 7
-%endif
-    SCRATCH              7, 15, %4+12*%5
-%else
-    mova                m6, [%1+ 2*%3]      ; IN(2)
-    mova                m1, [%1+ 4*%3]      ; IN(4)
-    mova                m7, [%1+ 6*%3]      ; IN(6)
-%if %2 <= 8
-    pmulhrsw            m0, m1,  [pw_15137x2]       ; t3
-    pmulhrsw            m1, [pw_6270x2]             ; t2
-    pmulhrsw            m5, m6, [pw_16069x2]        ; t7
-    pmulhrsw            m6, [pw_3196x2]             ; t4
-    pmulhrsw            m4, m7, [pw_m9102x2]        ; t5
-    pmulhrsw            m7, [pw_13623x2]            ; t6
-%else
-    mova                m4, [%1+10*%3]      ; IN(10)
-    mova                m0, [%1+12*%3]      ; IN(12)
-    mova                m5, [%1+14*%3]      ; IN(14)
-
-    VP9_UNPACK_MULSUB_2W_4X   1,   0, 15137,  6270, [pd_8192], 2, 3 ; t2,  t3
-    VP9_UNPACK_MULSUB_2W_4X   6,   5, 16069,  3196, [pd_8192], 2, 3 ; t4,  t7
-    VP9_UNPACK_MULSUB_2W_4X   4,   7,  9102, 13623, [pd_8192], 2, 3 ; t5,  t6
-%endif
-
-    SUMSUB_BA            w,  4,  6, 2       ; t4,  t5
-    SUMSUB_BA            w,  7,  5, 2       ; t7,  t6
-
-%if cpuflag(ssse3) && %6 == 0
-    SUMSUB_BA            w,  6,  5, 2
-    pmulhrsw            m5, [pw_11585x2]                              ; t5
-    pmulhrsw            m6, [pw_11585x2]                              ; t6
-%else
-    VP9_UNPACK_MULSUB_2W_4X  5,  6, 11585, 11585, [pd_8192], 2, 3 ; t5,  t6
-%endif
-
-    SCRATCH              5, 15, %4+10*%5
-    mova                m2, [%1+ 0*%3]      ; IN(0)
-%if %2 <= 8
-    pmulhrsw            m2, [pw_11585x2]    ; t0 and t1
-    psubw               m3, m2, m0
-    paddw               m0, m2
-
-    SUMSUB_BA            w,  7,  0, 5       ; t0,  t7
-%else
-    mova                m3, [%1+ 8*%3]      ; IN(8)
-
-    ; from 3 stages back
-%if cpuflag(ssse3) && %6 == 0
-    SUMSUB_BA            w,  3,  2, 5
-    pmulhrsw            m3, [pw_11585x2]    ; t0
-    pmulhrsw            m2, [pw_11585x2]    ; t1
-%else
-    mova        [%1+ 0*%3], m0
-    VP9_UNPACK_MULSUB_2W_4X  2,  3, 11585,  11585, [pd_8192], 5, 0 ; t0, t1
-    mova                m0, [%1+ 0*%3]
-%endif
-
-    ; from 2 stages back
-    SUMSUB_BA            w,  0,  3, 5      ; t0,  t3
-
-    SUMSUB_BA            w,  7,  0, 5      ; t0,  t7
-%endif
-    UNSCRATCH            5, 15, %4+10*%5
-%if ARCH_X86_32
-    SWAP                 0, 7
-%endif
-    SCRATCH              7, 15, %4+12*%5
-    SUMSUB_BA            w,  1,  2, 7       ; t1,  t2
-
-    ; from 1 stage back
-    SUMSUB_BA            w,  6,  1, 7       ; t1,  t6
-    SUMSUB_BA            w,  5,  2, 7       ; t2,  t5
-%endif
-    SUMSUB_BA            w,  4,  3, 7       ; t3,  t4
-
-%if ARCH_X86_64
-    SWAP                 0, 8
-    SWAP                 1, 9
-    SWAP                 2, 10
-    SWAP                 3, 11
-    SWAP                 4, 12
-    SWAP                 5, 13
-    SWAP                 6, 14
-
-    SUMSUB_BA            w,  0, 15, 7       ; t0, t15
-    SUMSUB_BA            w,  1, 14, 7       ; t1, t14
-    SUMSUB_BA            w,  2, 13, 7       ; t2, t13
-    SUMSUB_BA            w,  3, 12, 7       ; t3, t12
-    SUMSUB_BA            w,  4, 11, 7       ; t4, t11
-    SUMSUB_BA            w,  5, 10, 7       ; t5, t10
-%else
-    SWAP                 1, 6
-    SWAP                 2, 5
-    SWAP                 3, 4
-    mova        [%4+14*%5], m6
-
-%macro %%SUMSUB_BA_STORE 5 ; reg, from_mem, to_mem, scratch, scratch_stride
-    mova                m6, [%4+%2*%5]
-    SUMSUB_BA            w,  6, %1, 7
-    SWAP                %1, 6
-    mova        [%4+%3*%5], m6
-%endmacro
-
-    %%SUMSUB_BA_STORE    0,  1,  1, %4, %5  ; t0, t15
-    %%SUMSUB_BA_STORE    1,  3,  3, %4, %5  ; t1, t14
-    %%SUMSUB_BA_STORE    2,  5,  5, %4, %5  ; t2, t13
-    %%SUMSUB_BA_STORE    3,  7,  7, %4, %5  ; t3, t12
-    %%SUMSUB_BA_STORE    4,  9,  9, %4, %5  ; t4, t11
-    %%SUMSUB_BA_STORE    5, 11, 11, %4, %5  ; t5, t10
-%endif
-%endmacro
-
-%macro VP9_IDCT16_1D 2-4 16, 1 ; src, pass, nnzc, is_iadst
-%if %2 == 1
-    VP9_IDCT16_1D_START %1, %3, 32, tmpq, 16, %4
-
-%if ARCH_X86_64
-    ; backup a different register
-    mova                m7, [tmpq+15*16]
-    mova      [tmpq+ 1*16], m15
-
-    SUMSUB_BA            w,  6,  9, 15      ; t6, t9
-    SUMSUB_BA            w,  7,  8, 15      ; t7, t8
-
-    TRANSPOSE8x8W        0, 1, 2, 3, 4, 5, 6, 7, 15
-    mova        [tmpq+  0], m0
-    mova        [tmpq+ 32], m1
-    mova        [tmpq+ 64], m2
-    mova        [tmpq+ 96], m3
-    mova        [tmpq+128], m4
-    mova        [tmpq+160], m5
-    mova        [tmpq+192], m6
-    mova        [tmpq+224], m7
-
-    mova               m15, [tmpq+ 1*16]
-    TRANSPOSE8x8W        8, 9, 10, 11, 12, 13, 14, 15, 0
-    mova        [tmpq+ 16], m8
-    mova        [tmpq+ 48], m9
-    mova        [tmpq+ 80], m10
-    mova        [tmpq+112], m11
-    mova        [tmpq+144], m12
-    mova        [tmpq+176], m13
-    mova        [tmpq+208], m14
-    mova        [tmpq+240], m15
-%else
-    mova                m6, [tmpq+13*16]
-    mova                m7, [tmpq+14*16]
-    SUMSUB_BA            w, 6, 7                ; t6, t9
-    mova      [tmpq+14*16], m6
-    mova      [tmpq+13*16], m7
-    mova                m7, [tmpq+15*16]
-    mova                m6, [tmpq+12*16]
-    SUMSUB_BA            w, 7, 6                ; t7, t8
-    mova      [tmpq+15*16], m6
-
-    TRANSPOSE8x8W       0, 1, 2, 3, 4, 5, 6, 7, [tmpq+14*16], [tmpq+ 8*16], 1
-    mova     [tmpq+ 0*16], m0
-    mova     [tmpq+ 2*16], m1
-    mova     [tmpq+ 4*16], m2
-    mova     [tmpq+ 6*16], m3
-    mova     [tmpq+10*16], m5
-    mova     [tmpq+12*16], m6
-    mova     [tmpq+14*16], m7
-
-    mova                m0, [tmpq+15*16]
-    mova                m1, [tmpq+13*16]
-    mova                m2, [tmpq+11*16]
-    mova                m3, [tmpq+ 9*16]
-    mova                m4, [tmpq+ 7*16]
-    mova                m5, [tmpq+ 5*16]
-    mova                m7, [tmpq+ 1*16]
-    TRANSPOSE8x8W       0, 1, 2, 3, 4, 5, 6, 7, [tmpq+ 3*16], [tmpq+ 9*16], 1
-    mova     [tmpq+ 1*16], m0
-    mova     [tmpq+ 3*16], m1
-    mova     [tmpq+ 5*16], m2
-    mova     [tmpq+ 7*16], m3
-    mova     [tmpq+11*16], m5
-    mova     [tmpq+13*16], m6
-    mova     [tmpq+15*16], m7
-%endif
-%else ; %2 == 2
-    VP9_IDCT16_1D_START %1, %3, 32, %1, 32, %4
-
-%if cpuflag(ssse3)
-%define ROUND_REG [pw_512]
-%else
-%define ROUND_REG [pw_32]
-%endif
-
-    pxor                m7, m7
-%if ARCH_X86_64
-    ; backup more registers
-    mova        [%1+ 2*32], m8
-    mova        [%1+ 3*32], m9
-
-    VP9_IDCT8_WRITEx2    0,  1, 8, 9, 7, ROUND_REG, 6
-    lea               dstq, [dstq+strideq*2]
-    VP9_IDCT8_WRITEx2    2,  3, 8, 9, 7, ROUND_REG, 6
-    lea               dstq, [dstq+strideq*2]
-    VP9_IDCT8_WRITEx2    4,  5, 8, 9, 7, ROUND_REG, 6
-    lea               dstq, [dstq+strideq*2]
-
-    ; restore from cache
-    SWAP                 0, 7               ; move zero from m7 to m0
-    mova                m7, [%1+15*32]
-    mova                m8, [%1+ 2*32]
-    mova                m9, [%1+ 3*32]
-
-    SUMSUB_BA            w,  6,  9, 3       ; t6, t9
-    SUMSUB_BA            w,  7,  8, 3       ; t7, t8
-
-    VP9_IDCT8_WRITEx2    6,  7, 3, 4, 0, ROUND_REG, 6
-    lea               dstq, [dstq+strideq*2]
-    VP9_IDCT8_WRITEx2    8,  9, 3, 4, 0, ROUND_REG, 6
-    lea               dstq, [dstq+strideq*2]
-    VP9_IDCT8_WRITEx2   10, 11, 1, 2, 0, ROUND_REG, 6
-    lea               dstq, [dstq+strideq*2]
-    VP9_IDCT8_WRITEx2   12, 13, 1, 2, 0, ROUND_REG, 6
-    lea               dstq, [dstq+strideq*2]
-    VP9_IDCT8_WRITEx2   14, 15, 1, 2, 0, ROUND_REG, 6
-%else
-    mova      [tmpq+ 0*32], m5
-
-    VP9_IDCT8_WRITEx2    0,  1, 5, 6, 7, ROUND_REG, 6
-    lea               dstq, [dstq+strideq*2]
-    VP9_IDCT8_WRITEx2    2,  3, 5, 6, 7, ROUND_REG, 6
-    lea               dstq, [dstq+strideq*2]
-
-    SWAP                 0, 7               ; move zero from m7 to m0
-    mova                m5, [tmpq+ 0*32]
-
-    VP9_IDCT8_WRITEx2    4,  5, 1, 2, 0, ROUND_REG, 6
-    lea               dstq, [dstq+strideq*2]
-
-    mova                m4, [tmpq+13*32]
-    mova                m7, [tmpq+14*32]
-    mova                m5, [tmpq+15*32]
-    mova                m6, [tmpq+12*32]
-    SUMSUB_BADC w, 4, 7, 5, 6, 1
-
-    VP9_IDCT8_WRITEx2    4,  5, 1, 2, 0, ROUND_REG, 6
-    lea               dstq, [dstq+strideq*2]
-    VP9_IDCT8_WRITEx2    6,  7, 1, 2, 0, ROUND_REG, 6
-    lea               dstq, [dstq+strideq*2]
-
-    mova                m4, [tmpq+11*32]
-    mova                m5, [tmpq+ 9*32]
-    mova                m6, [tmpq+ 7*32]
-    mova                m7, [tmpq+ 5*32]
-
-    VP9_IDCT8_WRITEx2    4,  5, 1, 2, 0, ROUND_REG, 6
-    lea               dstq, [dstq+strideq*2]
-    VP9_IDCT8_WRITEx2    6,  7, 1, 2, 0, ROUND_REG, 6
-    lea               dstq, [dstq+strideq*2]
-
-    mova                m4, [tmpq+ 3*32]
-    mova                m5, [tmpq+ 1*32]
-
-    VP9_IDCT8_WRITEx2    4,  5, 1, 2, 0, ROUND_REG, 6
-    lea               dstq, [dstq+strideq*2]
-%endif
-
-%undef ROUND_REG
-%endif ; %2 == 1/2
-%endmacro
-
-%macro VP9_STORE_2XFULL 6-7 strideq; dc, tmp1, tmp2, tmp3, tmp4, zero, stride
-    mova               m%3, [dstq]
-    mova               m%5, [dstq+%7]
-    punpcklbw          m%2, m%3, m%6
-    punpckhbw          m%3, m%6
-    punpcklbw          m%4, m%5, m%6
-    punpckhbw          m%5, m%6
-    paddw              m%2, m%1
-    paddw              m%3, m%1
-    paddw              m%4, m%1
-    paddw              m%5, m%1
-    packuswb           m%2, m%3
-    packuswb           m%4, m%5
-    mova            [dstq], m%2
-    mova         [dstq+%7], m%4
-%endmacro
-
-%macro VP9_IDCT_IDCT_16x16_ADD_XMM 1
-INIT_XMM %1
-cglobal vp9_idct_idct_16x16_add, 4, 6, 16, 512, dst, stride, block, eob
-%if cpuflag(ssse3)
-    ; 2x2=eob=3, 4x4=eob=10
-    cmp eobd, 38
-    jg .idctfull
-    cmp eobd, 1 ; faster path for when only DC is set
-    jne .idct8x8
-%else
-    cmp eobd, 1 ; faster path for when only DC is set
-    jg .idctfull
-%endif
-
-    ; dc-only
-%if cpuflag(ssse3)
-    movd                m0, [blockq]
-    mova                m1, [pw_11585x2]
-    pmulhrsw            m0, m1
-    pmulhrsw            m0, m1
-%else
-    DEFINE_ARGS dst, stride, block, coef
-    movsx            coefd, word [blockq]
-    imul             coefd, 11585
-    add              coefd, 8192
-    sar              coefd, 14
-    imul             coefd, 11585
-    add              coefd, (32 << 14) + 8192
-    sar              coefd, 14 + 6
-    movd                m0, coefd
-%endif
-    SPLATW              m0, m0, q0000
-%if cpuflag(ssse3)
-    pmulhrsw            m0, [pw_512]
-%endif
-    pxor                m5, m5
-    movd          [blockq], m5
-%rep 7
-    VP9_STORE_2XFULL    0, 1, 2, 3, 4, 5
-    lea               dstq, [dstq+2*strideq]
-%endrep
-    VP9_STORE_2XFULL    0, 1, 2, 3, 4, 5
-    RET
-
-    DEFINE_ARGS dst, stride, block, cnt, dst_bak, tmp
-%if cpuflag(ssse3)
-.idct8x8:
-    mov               tmpq, rsp
-    VP9_IDCT16_1D   blockq, 1, 8, 0
-
-    mov               cntd, 2
-    mov           dst_bakq, dstq
-.loop2_8x8:
-    VP9_IDCT16_1D     tmpq, 2, 8, 0
-    lea               dstq, [dst_bakq+8]
-    add               tmpq, 16
-    dec               cntd
-    jg .loop2_8x8
-
-    ; at the end of the loop, m0 should still be zero
-    ; use that to zero out block coefficients
-    ZERO_BLOCK      blockq, 32, 8, m0
-    RET
-%endif
-
-.idctfull:
-    mov               cntd, 2
-    mov               tmpq, rsp
-.loop1_full:
-    VP9_IDCT16_1D   blockq, 1, 16, 0
-    add             blockq, 16
-    add               tmpq, 256
-    dec               cntd
-    jg .loop1_full
-    sub             blockq, 32
-
-    mov               cntd, 2
-    mov               tmpq, rsp
-    mov           dst_bakq, dstq
-.loop2_full:
-    VP9_IDCT16_1D     tmpq, 2, 16, 0
-    lea               dstq, [dst_bakq+8]
-    add               tmpq, 16
-    dec               cntd
-    jg .loop2_full
-
-    ; at the end of the loop, m0 should still be zero
-    ; use that to zero out block coefficients
-    ZERO_BLOCK      blockq, 32, 16, m0
-    RET
-%endmacro
-
-VP9_IDCT_IDCT_16x16_ADD_XMM sse2
-VP9_IDCT_IDCT_16x16_ADD_XMM ssse3
-VP9_IDCT_IDCT_16x16_ADD_XMM avx
-
-%macro VP9_IDCT16_YMM_1D 0
-    VP9_UNPACK_MULSUB_2W_4X  1,  15, 16305,  1606, [pd_8192], 0, 4 ; t8,  t15
-    VP9_UNPACK_MULSUB_2W_4X  9,   7, 10394, 12665, [pd_8192], 0, 4 ; t9,  t14
-
-    SUMSUB_BA            w,  9,   1, 0      ; t8,  t9
-    SUMSUB_BA            w,  7,  15, 0      ; t15, t14
-
-    VP9_UNPACK_MULSUB_2W_4X 15,   1, 15137,  6270, [pd_8192], 0, 4 ; t9,  t14
-
-    VP9_UNPACK_MULSUB_2W_4X  5,  11, 14449,  7723, [pd_8192], 0, 4 ; t10, t13
-    VP9_UNPACK_MULSUB_2W_4X 13,   3,  4756, 15679, [pd_8192], 0, 4 ; t11, t12
-
-    SUMSUB_BA            w,  5,  13, 0      ; t11, t10
-    SUMSUB_BA            w, 11,   3, 0      ; t12, t13
-
-    VP9_UNPACK_MULSUB_2W_4X  3,  13, 6270, m15137, [pd_8192], 0, 4 ; t10, t13
-
-    SUMSUB_BA            w,  5,   9, 0      ; t8,  t11
-    SUMSUB_BA            w,  3,  15, 0      ; t9,  t10
-    SUMSUB_BA            w, 11,   7, 0      ; t15, t12
-    SUMSUB_BA            w, 13,   1, 0      ; t14, t13
-
-    SUMSUB_BA            w, 15,   1, 0
-    SUMSUB_BA            w,  9,   7, 0
-    pmulhrsw            m1, [pw_11585x2]    ; t10
-    pmulhrsw            m7, [pw_11585x2]    ; t11
-    pmulhrsw            m9, [pw_11585x2]    ; t12
-    pmulhrsw           m15, [pw_11585x2]    ; t13
-
-    ; even (tx8x8)
-    mova                m4, [blockq+128]
-    mova      [blockq+128], m5
-    VP9_UNPACK_MULSUB_2W_4X   4,  12, 15137,  6270, [pd_8192], 0, 5 ; t2,  t3
-    VP9_UNPACK_MULSUB_2W_4X   2,  14, 16069,  3196, [pd_8192], 0, 5 ; t4,  t7
-    VP9_UNPACK_MULSUB_2W_4X  10,   6,  9102, 13623, [pd_8192], 0, 5 ; t5,  t6
-    mova                m0, [blockq+  0]
-    SUMSUB_BA            w,   8,   0, 5
-    pmulhrsw            m8, [pw_11585x2]    ; t0
-    pmulhrsw            m0, [pw_11585x2]    ; t1
-
-    SUMSUB_BA            w,  10,   2, 5     ; t4,  t5
-    SUMSUB_BA            w,   6,  14, 5     ; t7,  t6
-    SUMSUB_BA            w,  12,   8, 5     ; t0,  t3
-    SUMSUB_BA            w,   4,   0, 5     ; t1,  t2
-
-    SUMSUB_BA            w,   2,  14, 5
-    pmulhrsw           m14, [pw_11585x2]    ; t5
-    pmulhrsw            m2, [pw_11585x2]    ; t6
-
-    SUMSUB_BA            w,   6,  12, 5     ; t0,  t7
-    SUMSUB_BA            w,   2,   4, 5     ; t1,  t6
-    SUMSUB_BA            w,  14,   0, 5     ; t2,  t5
-    SUMSUB_BA            w,  10,   8, 5     ; t3,  t4
-
-    ; final stage
-    SUMSUB_BA            w, 11,  6,  5      ; out0, out15
-    SUMSUB_BA            w, 13,  2,  5      ; out1, out14
-    SUMSUB_BA            w, 15, 14,  5      ; out2, out13
-    SUMSUB_BA            w,  9, 10,  5      ; out3, out12
-    SUMSUB_BA            w,  7,  8,  5      ; out4, out11
-    SUMSUB_BA            w,  1,  0,  5      ; out5, out10
-    SUMSUB_BA            w,  3,  4,  5      ; out6, out9
-    mova                m5, [blockq+128]
-    mova      [blockq+192], m3
-    SUMSUB_BA            w,  5, 12,  3      ; out7, out8
-
-    SWAP  0, 11,  8, 12, 10
-    SWAP  1, 13, 14,  2, 15,  6,  3,  9,  4,  7,  5
-%endmacro
-
-; this is almost identical to VP9_STORE_2X, but it does two rows
-; for slightly improved interleaving, and it omits vpermq since the
-; input is DC so all values are identical
-%macro VP9_STORE_YMM_DC_4X 6 ; reg, tmp1, tmp2, tmp3, tmp4, zero
-    mova              xm%2, [dstq]
-    mova              xm%4, [dstq+strideq*2]
-    vinserti128        m%2, m%2, [dstq+strideq], 1
-    vinserti128        m%4, m%4, [dstq+stride3q], 1
-    punpckhbw          m%3, m%2, m%6
-    punpcklbw          m%2, m%6
-    punpckhbw          m%5, m%4, m%6
-    punpcklbw          m%4, m%6
-    paddw              m%3, m%1
-    paddw              m%2, m%1
-    paddw              m%5, m%1
-    paddw              m%4, m%1
-    packuswb           m%2, m%3
-    packuswb           m%4, m%5
-    mova            [dstq], xm%2
-    mova        [dstq+strideq*2], xm%4
-    vextracti128  [dstq+strideq], m%2, 1
-    vextracti128 [dstq+stride3q], m%4, 1
-%endmacro
-
-%if ARCH_X86_64 && HAVE_AVX2_EXTERNAL
-INIT_YMM avx2
-cglobal vp9_idct_idct_16x16_add, 4, 4, 16, dst, stride, block, eob
-    cmp eobd, 1 ; faster path for when only DC is set
-    jg .idctfull
-
-    ; dc-only
-    mova                m1, [pw_11585x2]
-    vpbroadcastw        m0, [blockq]
-    pmulhrsw            m0, m1
-    pmulhrsw            m0, m1
-    pxor                m5, m5
-    pmulhrsw            m0, [pw_512]
-    movd          [blockq], xm5
-
-    DEFINE_ARGS dst, stride, stride3, cnt
-    mov               cntd, 4
-    lea           stride3q, [strideq*3]
-.loop_dc:
-    VP9_STORE_YMM_DC_4X  0, 1, 2, 3, 4, 5
-    lea               dstq, [dstq+4*strideq]
-    dec               cntd
-    jg .loop_dc
-    RET
-
-    DEFINE_ARGS dst, stride, block, eob
-.idctfull:
-    mova                m1, [blockq+ 32]
-    mova                m2, [blockq+ 64]
-    mova                m3, [blockq+ 96]
-    mova                m5, [blockq+160]
-    mova                m6, [blockq+192]
-    mova                m7, [blockq+224]
-    mova                m8, [blockq+256]
-    mova                m9, [blockq+288]
-    mova               m10, [blockq+320]
-    mova               m11, [blockq+352]
-    mova               m12, [blockq+384]
-    mova               m13, [blockq+416]
-    mova               m14, [blockq+448]
-    mova               m15, [blockq+480]
-
-    VP9_IDCT16_YMM_1D
-    TRANSPOSE16x16W      0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, \
-                         [blockq+192], [blockq+128], 1
-    mova      [blockq+  0], m0
-    VP9_IDCT16_YMM_1D
-
-    mova      [blockq+224], m7
-
-    ; store
-    VP9_IDCT8_WRITEx2    0,  1, 6, 7, unused, [pw_512], 6
-    lea               dstq, [dstq+2*strideq]
-    VP9_IDCT8_WRITEx2    2,  3, 6, 7, unused, [pw_512], 6
-    lea               dstq, [dstq+2*strideq]
-    VP9_IDCT8_WRITEx2    4,  5, 6, 7, unused, [pw_512], 6
-    lea               dstq, [dstq+2*strideq]
-    mova                m6, [blockq+192]
-    mova                m7, [blockq+224]
-    VP9_IDCT8_WRITEx2    6,  7, 1, 2, unused, [pw_512], 6
-    lea               dstq, [dstq+2*strideq]
-    VP9_IDCT8_WRITEx2    8,  9, 1, 2, unused, [pw_512], 6
-    lea               dstq, [dstq+2*strideq]
-    VP9_IDCT8_WRITEx2   10, 11, 1, 2, unused, [pw_512], 6
-    lea               dstq, [dstq+2*strideq]
-    VP9_IDCT8_WRITEx2   12, 13, 1, 2, unused, [pw_512], 6
-    lea               dstq, [dstq+2*strideq]
-    VP9_IDCT8_WRITEx2   14, 15, 1, 2, unused, [pw_512], 6
-    lea               dstq, [dstq+2*strideq]
-
-    ; at the end of the loop, m0 should still be zero
-    ; use that to zero out block coefficients
-    pxor                m0, m0
-    ZERO_BLOCK      blockq, 32, 16, m0
-    RET
-%endif
-
-;---------------------------------------------------------------------------------------------
-; void vp9_iadst_iadst_16x16_add_<opt>(uint8_t *dst, ptrdiff_t stride, int16_t *block, int eob);
-;---------------------------------------------------------------------------------------------
-
-%macro VP9_IADST16_1D 2 ; src, pass
-%assign %%str 16*%2
-    mova                m0, [%1+ 0*32]  ; in0
-    mova                m1, [%1+15*32]  ; in15
-    mova                m2, [%1+ 7*32]  ; in7
-    mova                m3, [%1+ 8*32]  ; in8
-
-    VP9_UNPACK_MULSUB_2D_4X  1,  0,  4,  5, 16364,   804    ; m1/4=t1[d], m0/5=t0[d]
-    VP9_UNPACK_MULSUB_2D_4X  2,  3,  7,  6, 11003, 12140    ; m2/7=t9[d], m3/6=t8[d]
-    SCRATCH              4, 8, tmpq+ 0*%%str
-    VP9_RND_SH_SUMSUB_BA     3,  0,  6,  5,  4, [pd_8192]   ; m3=t0[w], m0=t8[w]
-    UNSCRATCH            4, 8, tmpq+ 0*%%str
-    VP9_RND_SH_SUMSUB_BA     2,  1,  7,  4,  5, [pd_8192]   ; m2=t1[w], m1=t9[w]
-
-    SCRATCH              0, 10, tmpq+ 0*%%str
-    SCRATCH              1, 11, tmpq+15*%%str
-    mova   [tmpq+ 7*%%str], m2
-    mova   [tmpq+ 8*%%str], m3
-
-    mova                m1, [%1+ 2*32]  ; in2
-    mova                m0, [%1+13*32]  ; in13
-    mova                m3, [%1+ 5*32]  ; in5
-    mova                m2, [%1+10*32]  ; in10
-
-    VP9_UNPACK_MULSUB_2D_4X  0,  1,  6,  7, 15893,  3981    ; m0/6=t3[d], m1/7=t2[d]
-    VP9_UNPACK_MULSUB_2D_4X  3,  2,  4,  5,  8423, 14053    ; m3/4=t11[d], m2/5=t10[d]
-    SCRATCH              4, 12, tmpq+ 2*%%str
-    VP9_RND_SH_SUMSUB_BA     2,  1,  5,  7,  4, [pd_8192]   ; m2=t2[w], m1=t10[w]
-    UNSCRATCH            4, 12, tmpq+ 2*%%str
-    VP9_RND_SH_SUMSUB_BA     3,  0,  4,  6,  5, [pd_8192]   ; m3=t3[w], m0=t11[w]
-
-    SCRATCH              0, 12, tmpq+ 2*%%str
-    SCRATCH              1, 13, tmpq+13*%%str
-    mova   [tmpq+ 5*%%str], m2
-    mova   [tmpq+10*%%str], m3
-
-    mova                m2, [%1+ 4*32]  ; in4
-    mova                m3, [%1+11*32]  ; in11
-    mova                m0, [%1+ 3*32]  ; in3
-    mova                m1, [%1+12*32]  ; in12
-
-    VP9_UNPACK_MULSUB_2D_4X  3,  2,  7,  6, 14811,  7005    ; m3/7=t5[d], m2/6=t4[d]
-    VP9_UNPACK_MULSUB_2D_4X  0,  1,  4,  5,  5520, 15426    ; m0/4=t13[d], m1/5=t12[d]
-    SCRATCH              4, 9, tmpq+ 4*%%str
-    VP9_RND_SH_SUMSUB_BA     1,  2,  5,  6,  4, [pd_8192]   ; m1=t4[w], m2=t12[w]
-    UNSCRATCH            4, 9, tmpq+ 4*%%str
-    VP9_RND_SH_SUMSUB_BA     0,  3,  4,  7,  6, [pd_8192]   ; m0=t5[w], m3=t13[w]
-
-    SCRATCH              0,  8, tmpq+ 4*%%str
-    mova   [tmpq+11*%%str], m1          ; t4:m1->r11
-    UNSCRATCH            0, 10, tmpq+ 0*%%str
-    UNSCRATCH            1, 11, tmpq+15*%%str
-
-    ; round 2 interleaved part 1
-    VP9_UNPACK_MULSUB_2D_4X  0,  1,  6,  7, 16069,  3196    ; m1/7=t8[d], m0/6=t9[d]
-    VP9_UNPACK_MULSUB_2D_4X  3,  2,  5,  4,  3196, 16069    ; m3/5=t12[d], m2/4=t13[d]
-    SCRATCH              4, 9, tmpq+ 3*%%str
-    VP9_RND_SH_SUMSUB_BA     3,  1,  5,  7,  4, [pd_8192]   ; m3=t8[w], m1=t12[w]
-    UNSCRATCH            4, 9, tmpq+ 3*%%str
-    VP9_RND_SH_SUMSUB_BA     2,  0,  4,  6,  5, [pd_8192]   ; m2=t9[w], m0=t13[w]
-
-    SCRATCH              0, 10, tmpq+ 0*%%str
-    SCRATCH              1, 11, tmpq+15*%%str
-    SCRATCH              2, 14, tmpq+ 3*%%str
-    SCRATCH              3, 15, tmpq+12*%%str
-
-    mova                m2, [%1+ 6*32]  ; in6
-    mova                m3, [%1+ 9*32]  ; in9
-    mova                m0, [%1+ 1*32]  ; in1
-    mova                m1, [%1+14*32]  ; in14
-
-    VP9_UNPACK_MULSUB_2D_4X  3,  2,  7,  6, 13160,  9760    ; m3/7=t7[d], m2/6=t6[d]
-    VP9_UNPACK_MULSUB_2D_4X  0,  1,  4,  5,  2404, 16207    ; m0/4=t15[d], m1/5=t14[d]
-    SCRATCH              4, 9, tmpq+ 6*%%str
-    VP9_RND_SH_SUMSUB_BA     1,  2,  5,  6,  4, [pd_8192]   ; m1=t6[w], m2=t14[w]
-    UNSCRATCH            4, 9, tmpq+ 6*%%str
-    VP9_RND_SH_SUMSUB_BA     0,  3,  4,  7,  6, [pd_8192]   ; m0=t7[w], m3=t15[w]
-
-    ; r8=t0, r7=t1, r5=t2, r10=t3, r11=t4, m8|r4=t5, m1=t6, m0=t7
-    ; m10|r0=t8, m11|r15=t9, m13|r13=t10, m12|r2=t11, m14|r3=t12, m15|r12=t13, m2=t14, m3=t15
-
-    UNSCRATCH            4, 12, tmpq+ 2*%%str
-    UNSCRATCH            5, 13, tmpq+13*%%str
-    SCRATCH              0, 12, tmpq+ 1*%%str
-    SCRATCH              1, 13, tmpq+14*%%str
-
-    ; remainder of round 2 (rest of t8-15)
-    VP9_UNPACK_MULSUB_2D_4X  5,  4,  6,  7,  9102, 13623    ; m5/6=t11[d], m4/7=t10[d]
-    VP9_UNPACK_MULSUB_2D_4X  3,  2,  1,  0, 13623,  9102    ; m3/1=t14[d], m2/0=t15[d]
-    SCRATCH              0, 9, tmpq+ 6*%%str
-    VP9_RND_SH_SUMSUB_BA     3,  4,  1,  7,  0, [pd_8192]   ; m3=t10[w], m4=t14[w]
-    UNSCRATCH            0, 9, tmpq+ 6*%%str
-    VP9_RND_SH_SUMSUB_BA     2,  5,  0,  6,  1, [pd_8192]   ; m2=t11[w], m5=t15[w]
-
-    ; m15|r12=t8, m14|r3=t9, m3=t10, m2=t11, m11|r15=t12, m10|r0=t13, m4=t14, m5=t15
-
-    UNSCRATCH            6, 14, tmpq+ 3*%%str
-    UNSCRATCH            7, 15, tmpq+12*%%str
-
-    SUMSUB_BA                w,  3,  7,  1
-    PSIGNW                  m3, [pw_m1]                     ; m3=out1[w], m7=t10[w]
-    SUMSUB_BA                w,  2,  6,  1                  ; m2=out14[w], m6=t11[w]
-
-    ; unfortunately, the code below overflows in some cases, e.g.
-    ; http://downloads.webmproject.org/test_data/libvpx/vp90-2-14-resize-fp-tiles-16-8.webm
-%if 0; cpuflag(ssse3)
-    SUMSUB_BA                w,  7,  6,  1
-    pmulhrsw                m7, [pw_11585x2]                ; m7=out6[w]
-    pmulhrsw                m6, [pw_11585x2]                ; m6=out9[w]
-%else
-    VP9_UNPACK_MULSUB_2W_4X  6,  7, 11585, 11585, [pd_8192], 1, 0
-%endif
-
-    mova       [tmpq+ 3*%%str], m6
-    mova       [tmpq+ 6*%%str], m7
-    UNSCRATCH                6, 10, tmpq+ 0*%%str
-    UNSCRATCH                7, 11, tmpq+15*%%str
-    mova       [tmpq+13*%%str], m2
-    SCRATCH                  3, 11, tmpq+ 9*%%str
-
-    VP9_UNPACK_MULSUB_2D_4X  7,  6,  2,  3, 15137,  6270    ; m6/3=t13[d], m7/2=t12[d]
-    VP9_UNPACK_MULSUB_2D_4X  5,  4,  1,  0,  6270, 15137    ; m5/1=t14[d], m4/0=t15[d]
-    SCRATCH              0, 9, tmpq+ 2*%%str
-    VP9_RND_SH_SUMSUB_BA     5,  6,  1,  3,  0, [pd_8192]   ; m5=out2[w], m6=t14[w]
-    UNSCRATCH            0, 9, tmpq+ 2*%%str
-    VP9_RND_SH_SUMSUB_BA     4,  7,  0,  2,  1, [pd_8192]
-    PSIGNW                  m4, [pw_m1]                     ; m4=out13[w], m7=t15[w]
-
-    ; unfortunately, the code below overflows in some cases
-%if 0; cpuflag(ssse3)
-    SUMSUB_BA                w,  7,  6,  1
-    pmulhrsw                m7, [pw_m11585x2]               ; m7=out5[w]
-    pmulhrsw                m6, [pw_11585x2]                ; m6=out10[w]
-%else
-    PSIGNW                  m7, [pw_m1]
-    VP9_UNPACK_MULSUB_2W_4X  7,  6, 11585, 11585, [pd_8192], 1, 0
-%endif
-
-    ; m11|r13=out1, m5=out2, m7=out5, r15=out6, r3=out9, m6=out10, m4=out13, r2=out14
-
-    mova                    m2, [tmpq+ 8*%%str]
-    mova                    m3, [tmpq+ 7*%%str]
-    mova                    m1, [tmpq+11*%%str]
-    mova       [tmpq+ 7*%%str], m6
-    mova       [tmpq+11*%%str], m4
-    mova                    m4, [tmpq+ 5*%%str]
-    SCRATCH                  5, 14, tmpq+ 5*%%str
-    SCRATCH                  7, 15, tmpq+ 8*%%str
-    UNSCRATCH                6,  8, tmpq+ 4*%%str
-    UNSCRATCH                5, 12, tmpq+ 1*%%str
-    UNSCRATCH                7, 13, tmpq+14*%%str
-
-    ; m2=t0, m3=t1, m9=t2, m0=t3, m1=t4, m8=t5, m13=t6, m12=t7
-    ; m11|r13=out1, m5=out2, m7=out5, r15=out6, r3=out9, r10=out10, r11=out13, r2=out14
-
-    SUMSUB_BA                w,  1,  2, 0                   ; m1=t0[w], m2=t4[w]
-    mova                    m0, [tmpq+10*%%str]
-    SCRATCH                  1, 12, tmpq+ 1*%%str
-    SUMSUB_BA                w,  6,  3, 1                   ; m8=t1[w], m3=t5[w]
-    SCRATCH                  6, 13, tmpq+ 4*%%str
-    SUMSUB_BA                w,  7,  4, 1                   ; m13=t2[w], m9=t6[w]
-    SCRATCH                  7,  8, tmpq+10*%%str
-    SUMSUB_BA                w,  5,  0, 1                   ; m12=t3[w], m0=t7[w]
-    SCRATCH                  5,  9, tmpq+14*%%str
-
-    VP9_UNPACK_MULSUB_2D_4X  2,  3,  7,  5, 15137,  6270    ; m2/6=t5[d], m3/10=t4[d]
-    VP9_UNPACK_MULSUB_2D_4X  0,  4,  1,  6,  6270, 15137    ; m0/14=t6[d], m9/15=t7[d]
-    SCRATCH                  6, 10, tmpq+ 0*%%str
-    VP9_RND_SH_SUMSUB_BA     0,  3,  1,  5,  6, [pd_8192]
-    UNSCRATCH                6, 10, tmpq+ 0*%%str
-    PSIGNW                  m0, [pw_m1]                     ; m0=out3[w], m3=t6[w]
-    VP9_RND_SH_SUMSUB_BA     4,  2,  6,  7,  5, [pd_8192]   ; m9=out12[w], m2=t7[w]
-
-    UNSCRATCH                1,  8, tmpq+10*%%str
-    UNSCRATCH                5,  9, tmpq+14*%%str
-    UNSCRATCH                6, 12, tmpq+ 1*%%str
-    UNSCRATCH                7, 13, tmpq+ 4*%%str
-    SCRATCH                  4,  9, tmpq+14*%%str
-
-    SUMSUB_BA                w,  1,  6,  4                  ; m13=out0[w], m1=t2[w]
-    SUMSUB_BA                w,  5,  7,  4
-    PSIGNW                  m5, [pw_m1]                     ; m12=out15[w], m8=t3[w]
-
-    ; unfortunately, the code below overflows in some cases, e.g.
-    ; http://downloads.webmproject.org/test_data/libvpx/vp90-2-14-resize-fp-tiles-16-8-4-2-1.webm
-%if 0 ; cpuflag(ssse3)
-    SUMSUB_BA               w,   7,  6,  4
-    pmulhrsw                m7, [pw_m11585x2]               ; m8=out7[w]
-    pmulhrsw                m6, [pw_11585x2]                ; m1=out8[w]
-    SWAP                     6,  7
-    SUMSUB_BA                w,  3,  2,  4
-    pmulhrsw                m3, [pw_11585x2]                ; m3=out4[w]
-    pmulhrsw                m2, [pw_11585x2]                ; m2=out11[w]
-%else
-    SCRATCH                  5,  8, tmpq+10*%%str
-    VP9_UNPACK_MULSUB_2W_4X  6,  7, 11585, m11585, [pd_8192],  5,  4
-    VP9_UNPACK_MULSUB_2W_4X  2,  3, 11585, 11585, [pd_8192],  5,  4
-    UNSCRATCH                5,  8, tmpq+10*%%str
-%endif
-
-    ; m13=out0, m0=out3, m3=out4, m8=out7, m1=out8, m2=out11, m9=out12, m12=out15
-    ; m11|r13=out1, m5=out2, m7=out5, r15=out6, r3=out9, r10=out10, r11=out13, r2=out14
-
-%if %2 == 1
-%if ARCH_X86_64
-    mova                   m13, [tmpq+ 6*%%str]
-    TRANSPOSE8x8W            1, 11, 14, 0, 3, 15, 13, 6, 10
-    mova          [tmpq+ 0*16], m1
-    mova          [tmpq+ 2*16], m11
-    mova          [tmpq+ 4*16], m14
-    mova          [tmpq+ 6*16], m0
-    mova                    m1, [tmpq+ 3*%%str]
-    mova                   m11, [tmpq+ 7*%%str]
-    mova                   m14, [tmpq+11*%%str]
-    mova                    m0, [tmpq+13*%%str]
-    mova          [tmpq+ 8*16], m3
-    mova          [tmpq+10*16], m15
-    mova          [tmpq+12*16], m13
-    mova          [tmpq+14*16], m6
-
-    TRANSPOSE8x8W            7, 1, 11, 2, 9, 14, 0, 5, 10
-    mova          [tmpq+ 1*16], m7
-    mova          [tmpq+ 3*16], m1
-    mova          [tmpq+ 5*16], m11
-    mova          [tmpq+ 7*16], m2
-    mova          [tmpq+ 9*16], m9
-    mova          [tmpq+11*16], m14
-    mova          [tmpq+13*16], m0
-    mova          [tmpq+15*16], m5
-%else
-    mova       [tmpq+12*%%str], m2
-    mova       [tmpq+ 1*%%str], m5
-    mova       [tmpq+15*%%str], m7
-    mova                    m2, [tmpq+ 9*%%str]
-    mova                    m5, [tmpq+ 5*%%str]
-    mova                    m7, [tmpq+ 8*%%str]
-    TRANSPOSE8x8W            1, 2, 5, 0, 3, 7, 4, 6, [tmpq+ 6*%%str], [tmpq+ 8*%%str], 1
-    mova          [tmpq+ 0*16], m1
-    mova          [tmpq+ 2*16], m2
-    mova          [tmpq+ 4*16], m5
-    mova          [tmpq+ 6*16], m0
-    mova          [tmpq+10*16], m7
-    mova                    m3, [tmpq+12*%%str]
-    mova          [tmpq+12*16], m4
-    mova                    m4, [tmpq+14*%%str]
-    mova          [tmpq+14*16], m6
-
-    mova                    m0, [tmpq+15*%%str]
-    mova                    m1, [tmpq+ 3*%%str]
-    mova                    m2, [tmpq+ 7*%%str]
-    mova                    m5, [tmpq+11*%%str]
-    mova                    m7, [tmpq+ 1*%%str]
-    TRANSPOSE8x8W            0, 1, 2, 3, 4, 5, 6, 7, [tmpq+13*%%str], [tmpq+ 9*%%str], 1
-    mova          [tmpq+ 1*16], m0
-    mova          [tmpq+ 3*16], m1
-    mova          [tmpq+ 5*16], m2
-    mova          [tmpq+ 7*16], m3
-    mova          [tmpq+11*16], m5
-    mova          [tmpq+13*16], m6
-    mova          [tmpq+15*16], m7
-%endif
-%else
-    pxor                    m4, m4
-
-%if cpuflag(ssse3)
-%define ROUND_REG [pw_512]
-%else
-%define ROUND_REG [pw_32]
-%endif
-
-%if ARCH_X86_64
-    mova                   m12, [tmpq+ 6*%%str]
-    VP9_IDCT8_WRITEx2        1, 11, 10,  8,  4, ROUND_REG, 6
-    lea                   dstq, [dstq+strideq*2]
-    VP9_IDCT8_WRITEx2       14,  0, 10,  8,  4, ROUND_REG, 6
-    lea                   dstq, [dstq+strideq*2]
-    VP9_IDCT8_WRITEx2        3, 15, 10,  8,  4, ROUND_REG, 6
-    lea                   dstq, [dstq+strideq*2]
-    VP9_IDCT8_WRITEx2       12,  6, 10,  8,  4, ROUND_REG, 6
-    lea                   dstq, [dstq+strideq*2]
-
-    mova                    m1, [tmpq+ 3*%%str]
-    mova                   m11, [tmpq+ 7*%%str]
-    mova                   m14, [tmpq+11*%%str]
-    mova                    m0, [tmpq+13*%%str]
-
-    VP9_IDCT8_WRITEx2        7,  1, 10,  8,  4, ROUND_REG, 6
-    lea                   dstq, [dstq+strideq*2]
-    VP9_IDCT8_WRITEx2       11,  2, 10,  8,  4, ROUND_REG, 6
-    lea                   dstq, [dstq+strideq*2]
-    VP9_IDCT8_WRITEx2        9, 14, 10,  8,  4, ROUND_REG, 6
-    lea                   dstq, [dstq+strideq*2]
-    VP9_IDCT8_WRITEx2        0,  5, 10,  8,  4, ROUND_REG, 6
-%else
-    mova       [tmpq+ 0*%%str], m2
-    mova       [tmpq+ 1*%%str], m5
-    mova       [tmpq+ 2*%%str], m7
-    mova                    m2, [tmpq+ 9*%%str]
-    VP9_IDCT8_WRITEx2        1,  2,  5,  7,  4, ROUND_REG, 6
-    lea                   dstq, [dstq+strideq*2]
-    mova                    m5, [tmpq+ 5*%%str]
-    VP9_IDCT8_WRITEx2        5,  0,  1,  2,  4, ROUND_REG, 6
-    lea                   dstq, [dstq+strideq*2]
-    mova                    m5, [tmpq+ 8*%%str]
-    VP9_IDCT8_WRITEx2        3,  5,  1,  2,  4, ROUND_REG, 6
-    lea                   dstq, [dstq+strideq*2]
-    mova                    m5, [tmpq+ 6*%%str]
-    VP9_IDCT8_WRITEx2        5,  6,  1,  2,  4, ROUND_REG, 6
-    lea                   dstq, [dstq+strideq*2]
-
-    mova                    m0, [tmpq+ 2*%%str]
-    mova                    m3, [tmpq+ 3*%%str]
-    VP9_IDCT8_WRITEx2        0,  3,  1,  2,  4, ROUND_REG, 6
-    lea                   dstq, [dstq+strideq*2]
-    mova                    m0, [tmpq+ 7*%%str]
-    mova                    m3, [tmpq+ 0*%%str]
-    VP9_IDCT8_WRITEx2        0,  3,  1,  2,  4, ROUND_REG, 6
-    lea                   dstq, [dstq+strideq*2]
-    mova                    m0, [tmpq+14*%%str]
-    mova                    m3, [tmpq+11*%%str]
-    VP9_IDCT8_WRITEx2        0,  3,  1,  2,  4, ROUND_REG, 6
-    lea                   dstq, [dstq+strideq*2]
-    mova                    m0, [tmpq+13*%%str]
-    mova                    m3, [tmpq+ 1*%%str]
-    VP9_IDCT8_WRITEx2        0,  3,  1,  2,  4, ROUND_REG, 6
-%endif
-
-    SWAP                     0,  4 ; zero
-%undef ROUND_REG
-%endif
-%endmacro
-
-%macro IADST16_FN 5
-INIT_XMM %5
-cglobal vp9_%1_%3_16x16_add, 3, 6, 16, 512, dst, stride, block, cnt, dst_bak, tmp
-    mov               cntd, 2
-    mov               tmpq, rsp
-.loop1_full:
-    VP9_%2_1D       blockq, 1
-    add             blockq, 16
-    add               tmpq, 256
-    dec               cntd
-    jg .loop1_full
-    sub             blockq, 32
-
-    mov               cntd, 2
-    mov               tmpq, rsp
-    mov           dst_bakq, dstq
-.loop2_full:
-    VP9_%4_1D         tmpq, 2
-    lea               dstq, [dst_bakq+8]
-    add               tmpq, 16
-    dec               cntd
-    jg .loop2_full
-
-    ; at the end of the loop, m0 should still be zero
-    ; use that to zero out block coefficients
-    ZERO_BLOCK      blockq, 32, 16, m0
-    RET
-%endmacro
-
-IADST16_FN idct,  IDCT16,  iadst, IADST16, sse2
-IADST16_FN iadst, IADST16, idct,  IDCT16,  sse2
-IADST16_FN iadst, IADST16, iadst, IADST16, sse2
-IADST16_FN idct,  IDCT16,  iadst, IADST16, ssse3
-IADST16_FN iadst, IADST16, idct,  IDCT16,  ssse3
-IADST16_FN iadst, IADST16, iadst, IADST16, ssse3
-IADST16_FN idct,  IDCT16,  iadst, IADST16, avx
-IADST16_FN iadst, IADST16, idct,  IDCT16,  avx
-IADST16_FN iadst, IADST16, iadst, IADST16, avx
-
-; in: data in m[0-15] except m0/m4, which are in [blockq+0] and [blockq+128]
-; out: m[0-15] except m6, which is in [blockq+192]
-; uses blockq as scratch space
-%macro VP9_IADST16_YMM_1D 0
-    mova          [blockq+ 32], m3
-    mova          [blockq+ 64], m7
-    mova          [blockq+ 96], m8
-
-    ; first half of round 1
-    VP9_UNPACK_MULSUB_2D_4X  9,  6,  0,  3, 13160,  9760    ; m9/x=t7[d], m6/x=t6[d]
-    VP9_UNPACK_MULSUB_2D_4X  1, 14,  4,  7,  2404, 16207    ; m1/x=t15[d], m14/x=t14[d]
-    VP9_RND_SH_SUMSUB_BA    14,  6,  7,  3,  8, [pd_8192]   ; m14=t6[w], m6=t14[w]
-    VP9_RND_SH_SUMSUB_BA     1,  9,  4,  0,  8, [pd_8192]   ; m1=t7[w], m9=t15[w]
-
-    VP9_UNPACK_MULSUB_2D_4X 13,  2,  4,  7, 15893,  3981    ; m13/x=t3[d], m2/x=t2[d]
-    VP9_UNPACK_MULSUB_2D_4X  5, 10,  0,  3,  8423, 14053    ; m5/x=t11[d], m10/x=t10[d]
-    VP9_RND_SH_SUMSUB_BA    10,  2,  3,  7,  8, [pd_8192]   ; m10=t2[w], m2=t10[w]
-    VP9_RND_SH_SUMSUB_BA     5, 13,  0,  4,  8, [pd_8192]   ; m5=t3[w], m13=t11[w]
-
-    ; half of round 2 t8-15
-    VP9_UNPACK_MULSUB_2D_4X  2, 13,  4,  7,  9102, 13623    ; m2/x=t11[d], m13/x=t10[d]
-    VP9_UNPACK_MULSUB_2D_4X  9,  6,  3,  0, 13623,  9102    ; m9/x=t14[d], m6/x=t15[d]
-    VP9_RND_SH_SUMSUB_BA     9, 13,  3,  7,  8, [pd_8192]   ; m9=t10[w], m13=t14[w]
-    VP9_RND_SH_SUMSUB_BA     6,  2,  0,  4,  8, [pd_8192]   ; m6=t11[w], m2=t15[w]
-
-    SUMSUB_BA            w, 14, 10,  8                      ; m14=t2, m10=t6
-    SUMSUB_BA            w,  1,  5,  8                      ; m1=t3, m5=t7
-
-    mova                    m0, [blockq+  0]
-    mova                    m4, [blockq+128]
-    mova                    m3, [blockq+ 32]
-    mova                    m7, [blockq+ 64]
-    mova                    m8, [blockq+ 96]
-    mova          [blockq+  0], m1
-    mova          [blockq+128], m14
-    mova          [blockq+ 32], m6
-    mova          [blockq+ 64], m9
-    mova          [blockq+ 96], m10
-
-    ; second half of round 1
-    VP9_UNPACK_MULSUB_2D_4X 15,  0,  1,  9, 16364,   804    ; m15/x=t1[d], m0/x=t0[d]
-    VP9_UNPACK_MULSUB_2D_4X  7,  8, 10,  6, 11003, 12140    ; m7/x=t9[d], m8/x=t8[d]
-    VP9_RND_SH_SUMSUB_BA     8,  0,  6,  9, 14, [pd_8192]   ; m8=t0[w], m0=t8[w]
-    VP9_RND_SH_SUMSUB_BA     7, 15, 10,  1, 14, [pd_8192]   ; m7=t1[w], m15=t9[w]
-
-    VP9_UNPACK_MULSUB_2D_4X 11,  4, 10,  6, 14811,  7005    ; m11/x=t5[d], m4/x=t4[d]
-    VP9_UNPACK_MULSUB_2D_4X  3, 12,  1,  9,  5520, 15426    ; m3/x=t13[d], m12/x=t12[d]
-    VP9_RND_SH_SUMSUB_BA    12,  4,  9,  6, 14, [pd_8192]   ; m12=t4[w], m4=t12[w]
-    VP9_RND_SH_SUMSUB_BA     3, 11,  1, 10, 14, [pd_8192]   ; m3=t5[w], m11=t13[w]
-
-    ; second half of round 2 t8-15
-    VP9_UNPACK_MULSUB_2D_4X  0, 15,  6, 10, 16069,  3196    ; m15/x=t8[d], m0/x=t9[d]
-    VP9_UNPACK_MULSUB_2D_4X 11,  4,  9,  1,  3196, 16069    ; m11/x=t12[d], m4/x=t13[d]
-    VP9_RND_SH_SUMSUB_BA    11, 15,  9, 10, 14, [pd_8192]   ; m11=t8[w], m15=t12[w]
-    VP9_RND_SH_SUMSUB_BA     4,  0,  1,  6, 14, [pd_8192]   ; m4=t9[w], m0=t13[w]
-
-    SUMSUB_BA            w, 12,  8, 14                      ; m12=t0, m8=t4
-    SUMSUB_BA            w,  3,  7, 14                      ; m3=t1, m7=t5
-
-    mova                   m10, [blockq+ 96]
-    mova          [blockq+ 96], m12
-
-    ; round 3
-    VP9_UNPACK_MULSUB_2D_4X 15,  0,  9, 12, 15137,  6270    ; m15/x=t13[d], m0/x=t12[d]
-    VP9_UNPACK_MULSUB_2D_4X  2, 13,  1,  6,  6270, 15137    ; m2/x=t14[d], m13/x=t15[d]
-    VP9_RND_SH_SUMSUB_BA     2,  0,  1, 12, 14, [pd_8192]   ; m2=out2[w], m0=t14a[w]
-    VP9_RND_SH_SUMSUB_BA    13, 15,  6,  9, 14, [pd_8192]
-    PSIGNW                 m13, [pw_m1]                     ; m13=out13[w], m15=t15a[w]
-
-    VP9_UNPACK_MULSUB_2D_4X  8,  7, 12,  9, 15137,  6270    ; m8/x=t5[d], m7/x=t4[d]
-    VP9_UNPACK_MULSUB_2D_4X  5, 10,  1,  6,  6270, 15137    ; m5/x=t6[d], m10/x=t7[d]
-    VP9_RND_SH_SUMSUB_BA     5,  7,  1,  9, 14, [pd_8192]
-    PSIGNW                  m5, [pw_m1]                     ; m5=out3[w], m7=t6[w]
-    VP9_RND_SH_SUMSUB_BA    10,  8,  6, 12, 14, [pd_8192]   ; m10=out12[w], m8=t7[w]
-
-    mova                    m1, [blockq+  0]
-    mova                   m14, [blockq+128]
-    mova                    m6, [blockq+ 32]
-    mova                    m9, [blockq+ 64]
-    mova                   m12, [blockq+ 96]
-    mova          [blockq+  0], m10
-    mova          [blockq+128], m5
-
-    SUMSUB_BA            w, 14, 12,  5                      ; m14=out0, m12=t2a
-    SUMSUB_BA            w,  1,  3,  5
-    PSIGNW                  m1, [pw_m1]                     ; m1=out15, m3=t3a
-
-    SUMSUB_BA            w,  9, 11,  5
-    PSIGNW                  m9, [pw_m1]                     ; m9=out1, m11=t10
-    SUMSUB_BA            w,  6,  4,  5                      ; m6=out14, m4=t11
-
-    VP9_UNPACK_MULSUB_2W_4X  4, 11, 11585, 11585, [pd_8192],  5, 10 ; m4=out9, m11=out6
-    mova                    m5, [blockq+128]
-    mova          [blockq+192], m11
-    PSIGNW                 m15, [pw_m1]
-    VP9_UNPACK_MULSUB_2W_4X 15,  0, 11585, 11585, [pd_8192], 10, 11 ; m15=out5, m0=out10
-
-    PSIGNW                  m3, [pw_m1]
-    VP9_UNPACK_MULSUB_2W_4X  3, 12, 11585, 11585, [pd_8192], 10, 11 ; m3=out7,m12=out8
-    VP9_UNPACK_MULSUB_2W_4X  8,  7, 11585, 11585, [pd_8192], 10, 11 ; m8=out11,m7=out4
-
-    mova                   m10, [blockq+  0]
-
-    SWAP                     0, 14,  6, 11,  8, 12, 10
-    SWAP                     1,  9, 15,  4,  7,  3,  5
-    SWAP                     5,  9, 15
-%endmacro
-
-%if ARCH_X86_64 && HAVE_AVX2_EXTERNAL
-%macro IADST16_YMM_FN 4
-INIT_YMM avx2
-cglobal vp9_%1_%3_16x16_add, 4, 4, 16, dst, stride, block, eob
-    mova                m1, [blockq+ 32]
-    mova                m2, [blockq+ 64]
-    mova                m3, [blockq+ 96]
-    mova                m5, [blockq+160]
-    mova                m6, [blockq+192]
-    mova                m7, [blockq+224]
-    mova                m8, [blockq+256]
-    mova                m9, [blockq+288]
-    mova               m10, [blockq+320]
-    mova               m11, [blockq+352]
-    mova               m12, [blockq+384]
-    mova               m13, [blockq+416]
-    mova               m14, [blockq+448]
-    mova               m15, [blockq+480]
-
-    VP9_%2_YMM_1D
-    TRANSPOSE16x16W      0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, \
-                         [blockq+192], [blockq+128], 1
-    mova      [blockq+  0], m0
-    VP9_%4_YMM_1D
-
-    mova      [blockq+224], m7
-
-    ; store
-    VP9_IDCT8_WRITEx2    0,  1, 6, 7, unused, [pw_512], 6
-    lea               dstq, [dstq+2*strideq]
-    VP9_IDCT8_WRITEx2    2,  3, 6, 7, unused, [pw_512], 6
-    lea               dstq, [dstq+2*strideq]
-    VP9_IDCT8_WRITEx2    4,  5, 6, 7, unused, [pw_512], 6
-    lea               dstq, [dstq+2*strideq]
-    mova                m6, [blockq+192]
-    mova                m7, [blockq+224]
-    VP9_IDCT8_WRITEx2    6,  7, 1, 2, unused, [pw_512], 6
-    lea               dstq, [dstq+2*strideq]
-    VP9_IDCT8_WRITEx2    8,  9, 1, 2, unused, [pw_512], 6
-    lea               dstq, [dstq+2*strideq]
-    VP9_IDCT8_WRITEx2   10, 11, 1, 2, unused, [pw_512], 6
-    lea               dstq, [dstq+2*strideq]
-    VP9_IDCT8_WRITEx2   12, 13, 1, 2, unused, [pw_512], 6
-    lea               dstq, [dstq+2*strideq]
-    VP9_IDCT8_WRITEx2   14, 15, 1, 2, unused, [pw_512], 6
-    lea               dstq, [dstq+2*strideq]
-
-    ; at the end of the loop, m0 should still be zero
-    ; use that to zero out block coefficients
-    pxor                m0, m0
-    ZERO_BLOCK      blockq, 32, 16, m0
-    RET
-%endmacro
-
-IADST16_YMM_FN idct,  IDCT16,  iadst, IADST16
-IADST16_YMM_FN iadst, IADST16, idct,  IDCT16
-IADST16_YMM_FN iadst, IADST16, iadst, IADST16
-%endif
-
-;---------------------------------------------------------------------------------------------
-; void vp9_idct_idct_32x32_add_<opt>(uint8_t *dst, ptrdiff_t stride, int16_t *block, int eob);
-;---------------------------------------------------------------------------------------------
-
-%macro VP9_IDCT32_1D 2-3 32 ; src, pass, nnzc
-%if %2 == 1
-%assign %%str mmsize
-%else
-%assign %%str 64
-%endif
-
-    ; first do t0-15, this can be done identical to idct16x16
-    VP9_IDCT16_1D_START %1, %3/2, 64*2, tmpq, 2*%%str, 1
-
-    ; store everything on stack to make space available for t16-31
-    ; we store interleaved with the output of the second half (t16-31)
-    ; so we don't need to allocate extra stack space
-    mova    [tmpq+ 0*%%str], m0     ; t0
-    mova    [tmpq+ 4*%%str], m1     ; t1
-    mova    [tmpq+ 8*%%str], m2     ; t2
-    mova    [tmpq+12*%%str], m3     ; t3
-    mova    [tmpq+16*%%str], m4     ; t4
-    mova    [tmpq+20*%%str], m5     ; t5
-%if ARCH_X86_64
-    mova    [tmpq+22*%%str], m10    ; t10
-    mova    [tmpq+18*%%str], m11    ; t11
-    mova    [tmpq+14*%%str], m12    ; t12
-    mova    [tmpq+10*%%str], m13    ; t13
-    mova    [tmpq+ 6*%%str], m14    ; t14
-    mova    [tmpq+ 2*%%str], m15    ; t15
-%endif
-
-    mova                m0, [tmpq+ 30*%%str]
-    UNSCRATCH            1,  6, tmpq+26*%%str
-    UNSCRATCH            2,  8, tmpq+24*%%str
-    UNSCRATCH            3,  9, tmpq+28*%%str
-    SUMSUB_BA            w,  1,  3, 4       ; t6, t9
-    SUMSUB_BA            w,  0,  2, 4       ; t7, t8
-
-    mova    [tmpq+24*%%str], m1     ; t6
-    mova    [tmpq+28*%%str], m0     ; t7
-    mova    [tmpq+30*%%str], m2     ; t8
-    mova    [tmpq+26*%%str], m3     ; t9
-
-    ; then, secondly, do t16-31
-%if %3 <= 8
-    mova                 m4, [%1+ 1*64]
-    mova                 m7, [%1+ 7*64]
-
-    pmulhrsw             m1,  m4, [pw_16364x2] ;t31
-    pmulhrsw             m4, [pw_804x2] ;t16
-
-    VP9_UNPACK_MULSUB_2W_4X   5,  0,  1,  4, 16069,  3196, [pd_8192], 6,  2 ; t17, t30
-
-    pmulhrsw             m3,  m7, [pw_m5520x2] ;t19
-    pmulhrsw             m7, [pw_15426x2] ;t28
-
-    SCRATCH               4, 13, tmpq+ 1*%%str
-    SCRATCH               5, 12, tmpq+15*%%str
-
-    VP9_UNPACK_MULSUB_2W_4X   2,  6,  7,  3, 3196, m16069, [pd_8192], 4,  5 ; t18, t29
-%else
-    mova                 m0, [%1+ 1*64]
-    mova                 m1, [%1+15*64]
-%if %3 <= 16
-    pmulhrsw             m5, m0, [pw_16364x2]
-    pmulhrsw             m0, [pw_804x2]
-    pmulhrsw             m4, m1, [pw_m11003x2]
-    pmulhrsw             m1, [pw_12140x2]
-%else
-    mova                 m4, [%1+17*64]
-    mova                 m5, [%1+31*64]
-
-    VP9_UNPACK_MULSUB_2W_4X   0,  5, 16364,   804, [pd_8192], 2, 3 ; t16, t31
-    VP9_UNPACK_MULSUB_2W_4X   4,  1, 11003, 12140, [pd_8192], 2, 3 ; t17, t30
-%endif
-    SUMSUB_BA             w,  4,  0,  2
-    SUMSUB_BA             w,  1,  5,  2
-
-    VP9_UNPACK_MULSUB_2W_4X   5,  0, 16069,  3196, [pd_8192], 2, 3 ; t17, t30
-
-    SCRATCH               4, 13, tmpq+ 1*%%str
-    SCRATCH               5, 12, tmpq+15*%%str
-
-    mova                 m2, [%1+ 7*64]
-    mova                 m3, [%1+ 9*64]
-%if %3 <= 16
-    pmulhrsw             m7,  m3, [pw_14811x2]
-    pmulhrsw             m3, [pw_7005x2]
-    pmulhrsw             m6,  m2, [pw_m5520x2]
-    pmulhrsw             m2, [pw_15426x2]
-%else
-    mova                 m7, [%1+23*64]
-    mova                 m6, [%1+25*64]
-
-    VP9_UNPACK_MULSUB_2W_4X   3,  7, 14811,  7005, [pd_8192], 4, 5 ; t18, t29
-    VP9_UNPACK_MULSUB_2W_4X   6,  2,  5520, 15426, [pd_8192], 4, 5 ; t19, t28
-%endif
-    SUMSUB_BA             w,  3,  6,  4
-    SUMSUB_BA             w,  7,  2,  4
-
-    VP9_UNPACK_MULSUB_2W_4X   2,  6, 3196, m16069, [pd_8192], 4, 5 ; t18, t29
-%endif
-
-    UNSCRATCH             5, 12, tmpq+15*%%str
-    SUMSUB_BA             w,  6,  0,  4
-    mova    [tmpq+25*%%str], m6             ; t19
-    UNSCRATCH             4, 13, tmpq+ 1*%%str
-    SUMSUB_BA             w,  7,  1,  6
-    SUMSUB_BA             w,  3,  4,  6
-    mova    [tmpq+23*%%str], m3             ; t16
-    SUMSUB_BA             w,  2,  5,  6
-
-    VP9_UNPACK_MULSUB_2W_4X   0,  5, 15137,  6270, [pd_8192], 6, 3 ; t18, t29
-    VP9_UNPACK_MULSUB_2W_4X   1,  4, 15137,  6270, [pd_8192], 6, 3 ; t19, t28
-
-    SCRATCH               0, 10, tmpq+ 1*%%str
-    SCRATCH               1, 11, tmpq+ 7*%%str
-    SCRATCH               2,  9, tmpq+ 9*%%str
-    SCRATCH               4, 14, tmpq+15*%%str
-    SCRATCH               5, 15, tmpq+17*%%str
-    SCRATCH               7, 13, tmpq+31*%%str
-
-%if %3 <= 8
-    mova                 m0, [%1+ 5*64]
-    mova                 m3, [%1+ 3*64]
-
-    pmulhrsw             m5,  m0, [pw_15893x2] ;t27
-    pmulhrsw             m0, [pw_3981x2] ;t20
-
-    VP9_UNPACK_MULSUB_2W_4X   1,  4,  5,  0,  9102, 13623, [pd_8192], 7,  2 ; t21, t26
-
-    pmulhrsw             m6,  m3, [pw_m2404x2] ;t23
-    pmulhrsw             m3, [pw_16207x2] ;t24
-
-    SCRATCH               5,  8, tmpq+ 5*%%str
-    SCRATCH               4, 12, tmpq+11*%%str
-
-    VP9_UNPACK_MULSUB_2W_4X   7,  2,  3,  6, 13623, m9102, [pd_8192], 4, 5 ; t22, t25
-%else
-    mova                 m4, [%1+ 5*64]
-    mova                 m5, [%1+11*64]
-%if %3 <= 16
-    pmulhrsw             m1, m4, [pw_15893x2]
-    pmulhrsw             m4, [pw_3981x2]
-    pmulhrsw             m0, m5, [pw_m8423x2]
-    pmulhrsw             m5, [pw_14053x2]
-%else
-    mova                 m0, [%1+21*64]
-    mova                 m1, [%1+27*64]
-
-    VP9_UNPACK_MULSUB_2W_4X   4,  1, 15893,  3981, [pd_8192], 2, 3 ; t20, t27
-    VP9_UNPACK_MULSUB_2W_4X   0,  5,  8423, 14053, [pd_8192], 2, 3 ; t21, t26
-%endif
-    SUMSUB_BA             w,  0,  4,  2
-    SUMSUB_BA             w,  5,  1,  2
-
-    VP9_UNPACK_MULSUB_2W_4X   1,  4,  9102, 13623, [pd_8192], 2, 3 ; t21, t26
-
-    SCRATCH               5,  8, tmpq+ 5*%%str
-    SCRATCH               4, 12, tmpq+11*%%str
-
-    mova                 m7, [%1+ 3*64]
-    mova                 m6, [%1+13*64]
-%if %3 <= 16
-    pmulhrsw             m3, m6, [pw_13160x2]
-    pmulhrsw             m6, [pw_9760x2]
-    pmulhrsw             m2, m7, [pw_m2404x2]
-    pmulhrsw             m7, [pw_16207x2]
-%else
-    mova                 m2, [%1+29*64]
-    mova                 m3, [%1+19*64]
-    VP9_UNPACK_MULSUB_2W_4X   6,  3, 13160,  9760, [pd_8192], 4, 5 ; t22, t25
-    VP9_UNPACK_MULSUB_2W_4X   2,  7,  2404, 16207, [pd_8192], 4, 5 ; t23, t24
-%endif
-    SUMSUB_BA             w,  6,  2,  4
-    SUMSUB_BA             w,  3,  7,  4
-
-    VP9_UNPACK_MULSUB_2W_4X   7,  2, 13623, m9102, [pd_8192], 4, 5 ; t22, t25
-%endif
-
-    ; m4=t16, m5=t17, m9=t18, m8=t19, m0=t20, m1=t21, m13=t22, m12=t23,
-    ; m3=t24, m2=t25, m14=t26, m15=t27, m7=t28, m6=t29, m10=t30, m11=t31
-
-    UNSCRATCH             4, 12, tmpq+11*%%str
-    SUMSUB_BA             w,  0,  6, 5
-    SUMSUB_BA             w,  4,  2, 5
-    UNSCRATCH             5,  8, tmpq+ 5*%%str
-    SCRATCH               4,  8, tmpq+11*%%str
-    SUMSUB_BA             w,  1,  7, 4
-    SUMSUB_BA             w,  5,  3, 4
-    SCRATCH               5, 12, tmpq+ 5*%%str
-
-    VP9_UNPACK_MULSUB_2W_4X   3,  6, 6270, m15137, [pd_8192], 4, 5 ; t20, t27
-    VP9_UNPACK_MULSUB_2W_4X   2,  7, 6270, m15137, [pd_8192], 4, 5 ; t21, t26
-
-    ; m8[s]=t16, m9=t17, m5=t18, m4[s]=t19, m12=t20, m13=t21, m1=t22, m0=t23,
-    ; m15=t24, m14=t25, m2=t26, m3=t27, m11=t28, m10=t29, m6=t30, m7=t31
-
-    UNSCRATCH             5,  9, tmpq+ 9*%%str
-    mova                 m4, [tmpq+23*%%str] ; t16
-%if ARCH_X86_64
-    SUMSUB_BA             w,  1,  5,  9
-    SUMSUB_BA             w,  0,  4,  9
-%else
-    SUMSUB_BADC           w,  1,  5,  0,  4
-%endif
-    mova    [tmpq+29*%%str], m1     ; t17
-    mova    [tmpq+21*%%str], m0     ; t16
-    UNSCRATCH             0, 10, tmpq+ 1*%%str
-    UNSCRATCH             1, 11, tmpq+ 7*%%str
-%if ARCH_X86_64
-    SUMSUB_BA             w,  2,  0,  9
-    SUMSUB_BA             w,  3,  1,  9
-%else
-    SUMSUB_BADC           w,  2,  0,  3,  1
-%endif
-    mova    [tmpq+ 9*%%str], m2     ; t18
-    mova    [tmpq+13*%%str], m3     ; t19
-    SCRATCH               0, 10, tmpq+23*%%str
-    SCRATCH               1, 11, tmpq+27*%%str
-
-    UNSCRATCH             2, 14, tmpq+15*%%str
-    UNSCRATCH             3, 15, tmpq+17*%%str
-    SUMSUB_BA             w,  6,  2, 0
-    SUMSUB_BA             w,  7,  3, 0
-    SCRATCH               6, 14, tmpq+ 3*%%str
-    SCRATCH               7, 15, tmpq+ 7*%%str
-
-    UNSCRATCH             0,  8, tmpq+11*%%str
-    mova                 m1, [tmpq+25*%%str] ; t19
-    UNSCRATCH             6, 12, tmpq+ 5*%%str
-    UNSCRATCH             7, 13, tmpq+31*%%str
-%if ARCH_X86_64
-    SUMSUB_BA             w,  0,  1,  9
-    SUMSUB_BA             w,  6,  7,  9
-%else
-    SUMSUB_BADC           w,  0,  1,  6,  7
-%endif
-
-    ; m0=t16, m1=t17, m2=t18, m3=t19, m11=t20, m10=t21, m9=t22, m8=t23,
-    ; m7=t24, m6=t25, m5=t26, m4=t27, m12=t28, m13=t29, m14=t30, m15=t31
-
-%if 0; cpuflag(ssse3)
-%if ARCH_X86_64
-    SUMSUB_BA             w,  4,  7,  8
-    SUMSUB_BA             w,  5,  1,  8
-%else
-    SUMSUB_BADC           w,  4,  7,  5,  1
-%endif
-
-    pmulhrsw             m7, [pw_11585x2]
-    pmulhrsw             m4, [pw_11585x2]
-    pmulhrsw             m1, [pw_11585x2]
-    pmulhrsw             m5, [pw_11585x2]
-
-    mova    [tmpq+ 5*%%str], m7     ; t23
-    SCRATCH               1, 13, tmpq+25*%%str
-    UNSCRATCH             7, 10, tmpq+23*%%str
-    UNSCRATCH             1, 11, tmpq+27*%%str
-
-%if ARCH_X86_64
-    SUMSUB_BA             w,  7,  3, 10
-    SUMSUB_BA             w,  1,  2, 10
-%else
-    SUMSUB_BADC           w,  7,  3,  1,  2
-%endif
-
-    pmulhrsw             m3, [pw_11585x2]
-    pmulhrsw             m7, [pw_11585x2]
-    pmulhrsw             m2, [pw_11585x2]
-    pmulhrsw             m1, [pw_11585x2]
-%else
-    SCRATCH               0,  8, tmpq+15*%%str
-    SCRATCH               6,  9, tmpq+17*%%str
-    VP9_UNPACK_MULSUB_2W_4X  7,  4, 11585, 11585, [pd_8192], 0, 6
-    mova    [tmpq+ 5*%%str], m7     ; t23
-    UNSCRATCH             7, 10, tmpq+23*%%str
-    VP9_UNPACK_MULSUB_2W_4X  1,  5, 11585, 11585, [pd_8192], 0, 6
-    SCRATCH               1, 13, tmpq+25*%%str
-    UNSCRATCH             1, 11, tmpq+27*%%str
-    VP9_UNPACK_MULSUB_2W_4X  3,  7, 11585, 11585, [pd_8192], 0, 6
-    VP9_UNPACK_MULSUB_2W_4X  2,  1, 11585, 11585, [pd_8192], 0, 6
-    UNSCRATCH             0,  8, tmpq+15*%%str
-    UNSCRATCH             6,  9, tmpq+17*%%str
-%endif
-
-    ; m0=t16, m1=t17, m2=t18, m3=t19, m4=t20, m5=t21, m6=t22, m7=t23,
-    ; m8=t24, m9=t25, m10=t26, m11=t27, m12=t28, m13=t29, m14=t30, m15=t31
-
-    ; then do final pass to sumsub+store the two halves
-%if %2 == 1
-    mova    [tmpq+17*%%str], m2     ; t20
-    mova    [tmpq+ 1*%%str], m3     ; t21
-%if ARCH_X86_64
-    mova    [tmpq+25*%%str], m13    ; t22
-
-    mova                 m8, [tmpq+ 0*%%str] ; t0
-    mova                 m9, [tmpq+ 4*%%str] ; t1
-    mova                m12, [tmpq+ 8*%%str] ; t2
-    mova                m11, [tmpq+12*%%str] ; t3
-    mova                 m2, [tmpq+16*%%str] ; t4
-    mova                 m3, [tmpq+20*%%str] ; t5
-    mova                m13, [tmpq+24*%%str] ; t6
-
-    SUMSUB_BA             w,  6,  8, 10
-    mova    [tmpq+ 3*%%str], m8              ; t15
-    SUMSUB_BA             w,  0,  9,  8
-    SUMSUB_BA             w, 15, 12,  8
-    SUMSUB_BA             w, 14, 11,  8
-    SUMSUB_BA             w,  1,  2,  8
-    SUMSUB_BA             w,  7,  3,  8
-    SUMSUB_BA             w,  5, 13,  8
-    mova                m10, [tmpq+28*%%str] ; t7
-    SUMSUB_BA             w,  4, 10,  8
-%if cpuflag(avx2)
-    ; the "shitty" about this idct is that the final pass does the outermost
-    ; interleave sumsubs (t0/31, t1/30, etc) but the tN for the 16x16 need
-    ; to be sequential, which means I need to load/store half of the sumsub
-    ; intermediates back to/from memory to get a 16x16 transpose going...
-    ; This would be easier if we had more (e.g. 32) YMM regs here.
-    mova    [tmpq+ 7*%%str], m9
-    mova    [tmpq+11*%%str], m12
-    mova    [tmpq+15*%%str], m11
-    mova    [tmpq+19*%%str], m2
-    mova    [tmpq+23*%%str], m3
-    mova    [tmpq+27*%%str], m13
-    mova    [tmpq+31*%%str], m10
-    mova    [tmpq+12*%%str], m5
-
-    mova                m13, [tmpq+30*%%str] ; t8
-    mova                m12, [tmpq+26*%%str] ; t9
-    mova                m11, [tmpq+22*%%str] ; t10
-    mova                m10, [tmpq+18*%%str] ; t11
-    mova                 m9, [tmpq+17*%%str] ; t20
-    mova                 m8, [tmpq+ 1*%%str] ; t21
-    mova                 m3, [tmpq+25*%%str] ; t22
-    mova                 m2, [tmpq+ 5*%%str] ; t23
-
-    SUMSUB_BA             w,  9, 10, 5
-    SUMSUB_BA             w,  8, 11, 5
-    SUMSUB_BA             w,  3, 12, 5
-    SUMSUB_BA             w,  2, 13, 5
-    mova    [tmpq+ 1*%%str], m10
-    mova    [tmpq+ 5*%%str], m11
-    mova    [tmpq+17*%%str], m12
-    mova    [tmpq+25*%%str], m13
-
-    mova                m13, [tmpq+14*%%str] ; t12
-    mova                m12, [tmpq+10*%%str] ; t13
-    mova                m11, [tmpq+ 9*%%str] ; t18
-    mova                m10, [tmpq+13*%%str] ; t19
-
-    SUMSUB_BA             w, 11, 12, 5
-    SUMSUB_BA             w, 10, 13, 5
-    mova    [tmpq+ 9*%%str], m13
-    mova    [tmpq+13*%%str], m12
-    mova    [tmpq+10*%%str], m10
-    mova    [tmpq+14*%%str], m11
-
-    mova                m13, [tmpq+ 6*%%str] ; t14
-    mova                m12, [tmpq+ 2*%%str] ; t15
-    mova                m11, [tmpq+21*%%str] ; t16
-    mova                m10, [tmpq+29*%%str] ; t17
-    SUMSUB_BA             w, 11, 12, 5
-    SUMSUB_BA             w, 10, 13, 5
-    mova    [tmpq+21*%%str], m12
-    mova    [tmpq+29*%%str], m13
-    mova                m12, [tmpq+10*%%str]
-    mova                m13, [tmpq+14*%%str]
-
-    TRANSPOSE16x16W       6,  0, 15, 14,  1,  7,  5,  4, \
-                          2,  3,  8,  9, 12, 13, 10, 11, \
-            [tmpq+12*%%str], [tmpq+ 8*%%str], 1
-    mova    [tmpq+ 0*%%str], m6
-    mova    [tmpq+ 2*%%str], m0
-    mova    [tmpq+ 4*%%str], m15
-    mova    [tmpq+ 6*%%str], m14
-    mova    [tmpq+10*%%str], m7
-    mova    [tmpq+12*%%str], m5
-    mova    [tmpq+14*%%str], m4
-    mova    [tmpq+16*%%str], m2
-    mova    [tmpq+18*%%str], m3
-    mova    [tmpq+20*%%str], m8
-    mova    [tmpq+22*%%str], m9
-    mova    [tmpq+24*%%str], m12
-    mova    [tmpq+26*%%str], m13
-    mova    [tmpq+28*%%str], m10
-    mova    [tmpq+30*%%str], m11
-
-    mova                 m0, [tmpq+21*%%str]
-    mova                 m1, [tmpq+29*%%str]
-    mova                 m2, [tmpq+13*%%str]
-    mova                 m3, [tmpq+ 9*%%str]
-    mova                 m4, [tmpq+ 1*%%str]
-    mova                 m5, [tmpq+ 5*%%str]
-    mova                 m7, [tmpq+25*%%str]
-    mova                 m8, [tmpq+31*%%str]
-    mova                 m9, [tmpq+27*%%str]
-    mova                m10, [tmpq+23*%%str]
-    mova                m11, [tmpq+19*%%str]
-    mova                m12, [tmpq+15*%%str]
-    mova                m13, [tmpq+11*%%str]
-    mova                m14, [tmpq+ 7*%%str]
-    mova                m15, [tmpq+ 3*%%str]
-    TRANSPOSE16x16W       0,  1,  2,  3,  4,  5,  6,  7, \
-                          8,  9, 10, 11, 12, 13, 14, 15, \
-            [tmpq+17*%%str], [tmpq+ 9*%%str], 1
-    mova    [tmpq+ 1*%%str], m0
-    mova    [tmpq+ 3*%%str], m1
-    mova    [tmpq+ 5*%%str], m2
-    mova    [tmpq+ 7*%%str], m3
-    mova    [tmpq+11*%%str], m5
-    mova    [tmpq+13*%%str], m6
-    mova    [tmpq+15*%%str], m7
-    mova    [tmpq+17*%%str], m8
-    mova    [tmpq+19*%%str], m9
-    mova    [tmpq+21*%%str], m10
-    mova    [tmpq+23*%%str], m11
-    mova    [tmpq+25*%%str], m12
-    mova    [tmpq+27*%%str], m13
-    mova    [tmpq+29*%%str], m14
-    mova    [tmpq+31*%%str], m15
-%else ; !avx2
-    TRANSPOSE8x8W         6, 0, 15, 14, 1, 7, 5, 4, 8
-    mova    [tmpq+ 0*%%str], m6
-    mova    [tmpq+ 4*%%str], m0
-    mova    [tmpq+ 8*%%str], m15
-    mova    [tmpq+12*%%str], m14
-    mova    [tmpq+16*%%str], m1
-    mova    [tmpq+20*%%str], m7
-    mova    [tmpq+24*%%str], m5
-    mova    [tmpq+28*%%str], m4
-
-    mova                  m8, [tmpq+ 3*%%str] ; t15
-    TRANSPOSE8x8W         10, 13, 3, 2, 11, 12, 9, 8, 0
-    mova    [tmpq+ 3*%%str], m10
-    mova    [tmpq+ 7*%%str], m13
-    mova    [tmpq+11*%%str], m3
-    mova    [tmpq+15*%%str], m2
-    mova    [tmpq+19*%%str], m11
-    mova    [tmpq+23*%%str], m12
-    mova    [tmpq+27*%%str], m9
-    mova    [tmpq+31*%%str], m8
-
-    mova                m15, [tmpq+30*%%str] ; t8
-    mova                m14, [tmpq+26*%%str] ; t9
-    mova                m13, [tmpq+22*%%str] ; t10
-    mova                m12, [tmpq+18*%%str] ; t11
-    mova                m11, [tmpq+14*%%str] ; t12
-    mova                m10, [tmpq+10*%%str] ; t13
-    mova                 m9, [tmpq+ 6*%%str] ; t14
-    mova                 m8, [tmpq+ 2*%%str] ; t15
-    mova                 m7, [tmpq+21*%%str] ; t16
-    mova                 m6, [tmpq+29*%%str] ; t17
-    mova                 m5, [tmpq+ 9*%%str] ; t18
-    mova                 m4, [tmpq+13*%%str] ; t19
-    mova                 m3, [tmpq+17*%%str] ; t20
-    mova                 m2, [tmpq+ 1*%%str] ; t21
-    mova                 m1, [tmpq+25*%%str] ; t22
-
-    SUMSUB_BA             w,  7,  8, 0
-    mova    [tmpq+ 2*%%str], m8
-    mova                 m0, [tmpq+ 5*%%str] ; t23
-    SUMSUB_BA             w,  6,  9, 8
-    SUMSUB_BA             w,  5, 10, 8
-    SUMSUB_BA             w,  4, 11, 8
-    SUMSUB_BA             w,  3, 12, 8
-    SUMSUB_BA             w,  2, 13, 8
-    SUMSUB_BA             w,  1, 14, 8
-    SUMSUB_BA             w,  0, 15, 8
-
-    TRANSPOSE8x8W         0, 1, 2, 3, 4, 5, 6, 7, 8
-    mova    [tmpq+ 1*%%str], m0
-    mova    [tmpq+ 5*%%str], m1
-    mova    [tmpq+ 9*%%str], m2
-    mova    [tmpq+13*%%str], m3
-    mova    [tmpq+17*%%str], m4
-    mova    [tmpq+21*%%str], m5
-    mova    [tmpq+25*%%str], m6
-    mova    [tmpq+29*%%str], m7
-
-    mova                 m8, [tmpq+ 2*%%str]
-    TRANSPOSE8x8W         8, 9, 10, 11, 12, 13, 14, 15, 0
-    mova    [tmpq+ 2*%%str], m8
-    mova    [tmpq+ 6*%%str], m9
-    mova    [tmpq+10*%%str], m10
-    mova    [tmpq+14*%%str], m11
-    mova    [tmpq+18*%%str], m12
-    mova    [tmpq+22*%%str], m13
-    mova    [tmpq+26*%%str], m14
-    mova    [tmpq+30*%%str], m15
-%endif ; avx2
-%else
-    mova                 m2, [tmpq+24*%%str] ; t6
-    mova                 m3, [tmpq+28*%%str] ; t7
-    SUMSUB_BADC           w,  5,  2,  4,  3
-    mova    [tmpq+24*%%str], m5
-    mova    [tmpq+23*%%str], m2
-    mova    [tmpq+28*%%str], m4
-    mova    [tmpq+19*%%str], m3
-
-    mova                 m2, [tmpq+16*%%str] ; t4
-    mova                 m3, [tmpq+20*%%str] ; t5
-    SUMSUB_BA             w,  1,  2,  5
-    SUMSUB_BA             w,  7,  3,  5
-    mova    [tmpq+15*%%str], m2
-    mova    [tmpq+11*%%str], m3
-
-    mova                 m2, [tmpq+ 0*%%str] ; t0
-    mova                 m3, [tmpq+ 4*%%str] ; t1
-    SUMSUB_BA             w,  6,  2,  5
-    SUMSUB_BA             w,  0,  3,  5
-    mova    [tmpq+31*%%str], m2
-    mova    [tmpq+27*%%str], m3
-
-    mova                 m2, [tmpq+ 8*%%str] ; t2
-    mova                 m3, [tmpq+12*%%str] ; t3
-    mova                 m5, [tmpq+ 7*%%str]
-    mova                 m4, [tmpq+ 3*%%str]
-    SUMSUB_BADC           w,  5,  2,  4,  3
-    mova    [tmpq+ 7*%%str], m2
-    mova    [tmpq+ 3*%%str], m3
-
-    mova                 m3, [tmpq+28*%%str]
-    TRANSPOSE8x8W         6, 0, 5, 4, 1, 7, 2, 3, [tmpq+24*%%str], [tmpq+16*%%str], 1
-    mova    [tmpq+ 0*%%str], m6
-    mova    [tmpq+ 4*%%str], m0
-    mova    [tmpq+ 8*%%str], m5
-    mova    [tmpq+12*%%str], m4
-    mova    [tmpq+20*%%str], m7
-    mova    [tmpq+24*%%str], m2
-    mova    [tmpq+28*%%str], m3
-
-    mova                 m6, [tmpq+19*%%str]
-    mova                 m0, [tmpq+23*%%str]
-    mova                 m5, [tmpq+11*%%str]
-    mova                 m4, [tmpq+15*%%str]
-    mova                 m1, [tmpq+ 3*%%str]
-    mova                 m7, [tmpq+ 7*%%str]
-    mova                 m3, [tmpq+31*%%str]
-    TRANSPOSE8x8W         6, 0, 5, 4, 1, 7, 2, 3, [tmpq+27*%%str], [tmpq+19*%%str], 1
-    mova    [tmpq+ 3*%%str], m6
-    mova    [tmpq+ 7*%%str], m0
-    mova    [tmpq+11*%%str], m5
-    mova    [tmpq+15*%%str], m4
-    mova    [tmpq+23*%%str], m7
-    mova    [tmpq+27*%%str], m2
-    mova    [tmpq+31*%%str], m3
-
-    mova                 m1, [tmpq+ 6*%%str] ; t14
-    mova                 m0, [tmpq+ 2*%%str] ; t15
-    mova                 m7, [tmpq+21*%%str] ; t16
-    mova                 m6, [tmpq+29*%%str] ; t17
-    SUMSUB_BA             w,  7,  0,  2
-    SUMSUB_BA             w,  6,  1,  2
-    mova    [tmpq+29*%%str], m7
-    mova    [tmpq+ 2*%%str], m0
-    mova    [tmpq+21*%%str], m6
-    mova    [tmpq+ 6*%%str], m1
-
-    mova                 m1, [tmpq+14*%%str] ; t12
-    mova                 m0, [tmpq+10*%%str] ; t13
-    mova                 m5, [tmpq+ 9*%%str] ; t18
-    mova                 m4, [tmpq+13*%%str] ; t19
-    SUMSUB_BA             w,  5,  0,  2
-    SUMSUB_BA             w,  4,  1,  2
-    mova     [tmpq+10*%%str], m0
-    mova     [tmpq+14*%%str], m1
-
-    mova                 m1, [tmpq+22*%%str] ; t10
-    mova                 m0, [tmpq+18*%%str] ; t11
-    mova                 m3, [tmpq+17*%%str] ; t20
-    mova                 m2, [tmpq+ 1*%%str] ; t21
-    SUMSUB_BA             w,  3,  0,  6
-    SUMSUB_BA             w,  2,  1,  6
-    mova     [tmpq+18*%%str], m0
-    mova     [tmpq+22*%%str], m1
-
-    mova                 m7, [tmpq+30*%%str] ; t8
-    mova                 m6, [tmpq+26*%%str] ; t9
-    mova                 m1, [tmpq+25*%%str] ; t22
-    mova                 m0, [tmpq+ 5*%%str] ; t23
-    SUMSUB_BADC           w,  1,  6,  0,  7
-    mova     [tmpq+26*%%str], m6
-    mova     [tmpq+30*%%str], m7
-
-    mova                 m7, [tmpq+29*%%str]
-    TRANSPOSE8x8W         0, 1, 2, 3, 4, 5, 6, 7, [tmpq+21*%%str], [tmpq+17*%%str], 1
-    mova    [tmpq+ 1*%%str], m0
-    mova    [tmpq+ 5*%%str], m1
-    mova    [tmpq+ 9*%%str], m2
-    mova    [tmpq+13*%%str], m3
-    mova    [tmpq+21*%%str], m5
-    mova    [tmpq+25*%%str], m6
-    mova    [tmpq+29*%%str], m7
-
-    mova                 m0, [tmpq+ 2*%%str]
-    mova                 m1, [tmpq+ 6*%%str]
-    mova                 m2, [tmpq+10*%%str]
-    mova                 m3, [tmpq+14*%%str]
-    mova                 m4, [tmpq+18*%%str]
-    mova                 m5, [tmpq+22*%%str]
-    mova                 m7, [tmpq+30*%%str]
-    TRANSPOSE8x8W         0, 1, 2, 3, 4, 5, 6, 7, [tmpq+26*%%str], [tmpq+18*%%str], 1
-    mova    [tmpq+ 2*%%str], m0
-    mova    [tmpq+ 6*%%str], m1
-    mova    [tmpq+10*%%str], m2
-    mova    [tmpq+14*%%str], m3
-    mova    [tmpq+22*%%str], m5
-    mova    [tmpq+26*%%str], m6
-    mova    [tmpq+30*%%str], m7
-%endif
-%else
-    ; t0-7 is in [tmpq+{0,4,8,12,16,20,24,28}*%%str]
-    ; t8-15 is in [tmpq+{2,6,10,14,18,22,26,30}*%%str]
-    ; t16-19 and t23 is in [tmpq+{1,5,9,13,29}*%%str]
-    ; t20-22 is in m4-6
-    ; t24-31 is in m8-15
-
-%if cpuflag(ssse3)
-%define ROUND_REG [pw_512]
-%else
-%define ROUND_REG [pw_32]
-%endif
-
-%macro %%STORE_2X2 7-8 1 ; src[1-4], tmp[1-2], zero, inc_dst_ptrs
-    SUMSUB_BA            w, %4, %1, %5
-    SUMSUB_BA            w, %3, %2, %5
-    VP9_IDCT8_WRITEx2   %4, %3, %5, %6, %7, ROUND_REG, 6
-%if %8 == 1
-    add               dstq, stride2q
-%endif
-    VP9_IDCT8_WRITEx2   %2, %1, %5, %6, %7, ROUND_REG, 6, dst_endq
-%if %8 == 1
-    sub           dst_endq, stride2q
-%endif
-%endmacro
-
-%if ARCH_X86_64
-    pxor               m10, m10
-
-    ; store t0-1 and t30-31
-    mova                m8, [tmpq+ 0*%%str]
-    mova                m9, [tmpq+ 4*%%str]
-    %%STORE_2X2          8,  9,  0,  6, 12, 11, 10
-
-    ; store t2-3 and t28-29
-    mova                m8, [tmpq+ 8*%%str]
-    mova                m9, [tmpq+12*%%str]
-    %%STORE_2X2          8,  9, 14, 15, 12, 11, 10
-
-    ; store t4-5 and t26-27
-    mova                m8, [tmpq+16*%%str]
-    mova                m9, [tmpq+20*%%str]
-    %%STORE_2X2          8,  9,  7,  1, 12, 11, 10
-
-    ; store t6-7 and t24-25
-    mova                m8, [tmpq+24*%%str]
-    mova                m9, [tmpq+28*%%str]
-    %%STORE_2X2          8,  9,  4,  5, 12, 11, 10
-
-    ; store t8-9 and t22-23
-    mova                m8, [tmpq+30*%%str]
-    mova                m9, [tmpq+26*%%str]
-    mova                m0, [tmpq+ 5*%%str]
-    %%STORE_2X2          8,  9, 13,  0, 12, 11, 10
-
-    ; store t10-11 and t20-21
-    mova                m8, [tmpq+22*%%str]
-    mova                m9, [tmpq+18*%%str]
-    %%STORE_2X2          8,  9,  2,  3, 12, 11, 10
-
-    ; store t12-13 and t18-19
-    mova                m8, [tmpq+14*%%str]
-    mova                m9, [tmpq+10*%%str]
-    mova                m5, [tmpq+13*%%str]
-    mova                m4, [tmpq+ 9*%%str]
-    %%STORE_2X2          8,  9,  4,  5, 12, 11, 10
-
-    ; store t14-17
-    mova                m8, [tmpq+ 6*%%str]
-    mova                m9, [tmpq+ 2*%%str]
-    mova                m5, [tmpq+29*%%str]
-    mova                m4, [tmpq+21*%%str]
-    %%STORE_2X2          8,  9,  4,  5, 12, 11, 10, 0
-
-    SWAP                 1, 10 ; zero
-%else
-    mova   [tmpq+ 1*%%str], m1
-    mova   [tmpq+11*%%str], m2
-    mova   [tmpq+15*%%str], m3
-    mova   [tmpq+17*%%str], m4
-    mova   [tmpq+19*%%str], m5
-    pxor                m1, m1
-
-    ; store t0-1 and t30-31
-    mova                m2, [tmpq+ 0*%%str]
-    mova                m3, [tmpq+ 4*%%str]
-    %%STORE_2X2          2,  3,  0,  6, 4, 5, 1
-
-    ; store t2-3 and t28-29
-    mova                m2, [tmpq+ 8*%%str]
-    mova                m3, [tmpq+12*%%str]
-    mova                m0, [tmpq+ 3*%%str]
-    mova                m6, [tmpq+ 7*%%str]
-    %%STORE_2X2          2,  3,  0,  6, 4, 5, 1
-
-    ; store t4-5 and t26-27
-    mova                m2, [tmpq+16*%%str]
-    mova                m3, [tmpq+20*%%str]
-    mova                m0, [tmpq+ 1*%%str]
-    %%STORE_2X2          2,  3,  7,  0, 4, 5, 1
-
-    ; store t6-7 and t24-25
-    mova                m2, [tmpq+24*%%str]
-    mova                m3, [tmpq+28*%%str]
-    mova                m0, [tmpq+17*%%str]
-    mova                m6, [tmpq+19*%%str]
-    %%STORE_2X2          2,  3,  0,  6, 4, 5, 1
-
-    ; store t8-9 and t22-23
-    mova                m2, [tmpq+30*%%str]
-    mova                m3, [tmpq+26*%%str]
-    mova                m0, [tmpq+25*%%str]
-    mova                m6, [tmpq+ 5*%%str]
-    %%STORE_2X2          2,  3,  0,  6, 4, 5, 1
-
-    ; store t10-11 and t20-21
-    mova                m2, [tmpq+22*%%str]
-    mova                m3, [tmpq+18*%%str]
-    mova                m0, [tmpq+11*%%str]
-    mova                m6, [tmpq+15*%%str]
-    %%STORE_2X2          2,  3,  0,  6, 4, 5, 1
-
-    ; store t12-13 and t18-19
-    mova                m2, [tmpq+14*%%str]
-    mova                m3, [tmpq+10*%%str]
-    mova                m6, [tmpq+13*%%str]
-    mova                m0, [tmpq+ 9*%%str]
-    %%STORE_2X2          2,  3,  0,  6, 4, 5, 1
-
-    ; store t14-17
-    mova                m2, [tmpq+ 6*%%str]
-    mova                m3, [tmpq+ 2*%%str]
-    mova                m6, [tmpq+29*%%str]
-    mova                m0, [tmpq+21*%%str]
-    %%STORE_2X2          2,  3,  0,  6, 4, 5, 1, 0
-%endif
-%undef ROUND_REG
-%endif
-%endmacro
-
-%macro VP9_IDCT_IDCT_32x32_ADD_XMM 1
-INIT_XMM %1
-cglobal vp9_idct_idct_32x32_add, 0, 6 + ARCH_X86_64 * 3, 16, 2048, dst, stride, block, eob
-    movifnidn         eobd, dword eobm
-%if cpuflag(ssse3)
-    cmp eobd, 135
-    jg .idctfull
-    cmp eobd, 34
-    jg .idct16x16
-    cmp eobd, 1
-    jg .idct8x8
-%else
-    cmp eobd, 1
-    jg .idctfull
-%endif
-
-    ; dc-only case
-    movifnidn       blockq, blockmp
-    movifnidn         dstq, dstmp
-    movifnidn      strideq, stridemp
-%if cpuflag(ssse3)
-    movd                m0, [blockq]
-    mova                m1, [pw_11585x2]
-    pmulhrsw            m0, m1
-    pmulhrsw            m0, m1
-%else
-    DEFINE_ARGS dst, stride, block, coef
-    movsx            coefd, word [blockq]
-    imul             coefd, 11585
-    add              coefd, 8192
-    sar              coefd, 14
-    imul             coefd, 11585
-    add              coefd, (32 << 14) + 8192
-    sar              coefd, 14 + 6
-    movd                m0, coefd
-%endif
-    SPLATW              m0, m0, q0000
-%if cpuflag(ssse3)
-    pmulhrsw            m0, [pw_512]
-%endif
-    pxor                m5, m5
-    movd          [blockq], m5
-%rep 31
-    VP9_STORE_2XFULL    0, 1, 2, 3, 4, 5, mmsize
-    add               dstq, strideq
-%endrep
-    VP9_STORE_2XFULL    0, 1, 2, 3, 4, 5, mmsize
-    RET
-
-%if ARCH_X86_64
-    DEFINE_ARGS dst_bak, stride, block, cnt, dst, stride30, dst_end, stride2, tmp
-%else
-%define dst_bakq r0mp
-%endif
-%if cpuflag(ssse3)
-.idct8x8:
-%if ARCH_X86_32
-    DEFINE_ARGS block, u1, u2, u3, u4, tmp
-    mov             blockq, r2mp
-%endif
-    mov               tmpq, rsp
-    VP9_IDCT32_1D   blockq, 1, 8
-
-%if ARCH_X86_32
-    DEFINE_ARGS dst, stride, stride30, dst_end, stride2, tmp
-    mov            strideq, r1mp
-%define cntd dword r3m
-%endif
-    mov          stride30q, strideq         ; stride
-    lea           stride2q, [strideq*2]     ; stride*2
-    shl          stride30q, 5               ; stride*32
-    mov               cntd, 4
-    sub          stride30q, stride2q        ; stride*30
-.loop2_8x8:
-    mov               dstq, dst_bakq
-    lea           dst_endq, [dstq+stride30q]
-    VP9_IDCT32_1D     tmpq, 2, 8
-    add           dst_bakq, 8
-    add               tmpq, 16
-    dec               cntd
-    jg .loop2_8x8
-
-    ; at the end of the loop, m7 should still be zero
-    ; use that to zero out block coefficients
-%if ARCH_X86_32
-    DEFINE_ARGS block
-    mov             blockq, r2mp
-%endif
-    ZERO_BLOCK      blockq, 64,  8, m1
-    RET
-
-.idct16x16:
-%if ARCH_X86_32
-    DEFINE_ARGS block, tmp, cnt
-    mov             blockq, r2mp
-%endif
-    mov               cntd, 2
-    mov               tmpq, rsp
-.loop1_16x16:
-    VP9_IDCT32_1D   blockq, 1, 16
-    add             blockq, 16
-    add               tmpq, 512
-    dec               cntd
-    jg .loop1_16x16
-
-%if ARCH_X86_64
-    sub             blockq, 32
-%else
-    DEFINE_ARGS dst, stride, stride30, dst_end, stride2, tmp
-    mov            strideq, r1mp
-%define cntd dword r3m
-%endif
-
-    mov          stride30q, strideq         ; stride
-    lea           stride2q, [strideq*2]     ; stride*2
-    shl          stride30q, 5               ; stride*32
-    mov               cntd, 4
-    mov               tmpq, rsp
-    sub          stride30q, stride2q        ; stride*30
-.loop2_16x16:
-    mov               dstq, dst_bakq
-    lea           dst_endq, [dstq+stride30q]
-    VP9_IDCT32_1D     tmpq, 2, 16
-    add           dst_bakq, 8
-    add               tmpq, 16
-    dec               cntd
-    jg .loop2_16x16
-
-    ; at the end of the loop, m7 should still be zero
-    ; use that to zero out block coefficients
-%if ARCH_X86_32
-    DEFINE_ARGS block
-    mov             blockq, r2mp
-%endif
-    ZERO_BLOCK      blockq, 64, 16, m1
-    RET
-%endif
-
-.idctfull:
-%if ARCH_X86_32
-    DEFINE_ARGS block, tmp, cnt
-    mov             blockq, r2mp
-%endif
-    mov               cntd, 4
-    mov               tmpq, rsp
-.loop1_full:
-    VP9_IDCT32_1D   blockq, 1
-    add             blockq, 16
-    add               tmpq, 512
-    dec               cntd
-    jg .loop1_full
-
-%if ARCH_X86_64
-    sub             blockq, 64
-%else
-    DEFINE_ARGS dst, stride, stride30, dst_end, stride2, tmp
-    mov            strideq, r1mp
-%define cntd dword r3m
-%endif
-
-    mov          stride30q, strideq         ; stride
-    lea           stride2q, [strideq*2]     ; stride*2
-    shl          stride30q, 5               ; stride*32
-    mov               cntd, 4
-    mov               tmpq, rsp
-    sub          stride30q, stride2q        ; stride*30
-.loop2_full:
-    mov               dstq, dst_bakq
-    lea           dst_endq, [dstq+stride30q]
-    VP9_IDCT32_1D     tmpq, 2
-    add           dst_bakq, 8
-    add               tmpq, 16
-    dec               cntd
-    jg .loop2_full
-
-    ; at the end of the loop, m7 should still be zero
-    ; use that to zero out block coefficients
-%if ARCH_X86_32
-    DEFINE_ARGS block
-    mov             blockq, r2mp
-%endif
-    ZERO_BLOCK      blockq, 64, 32, m1
-    RET
-%endmacro
-
-VP9_IDCT_IDCT_32x32_ADD_XMM sse2
-VP9_IDCT_IDCT_32x32_ADD_XMM ssse3
-VP9_IDCT_IDCT_32x32_ADD_XMM avx
-
-; this is almost identical to VP9_STORE_2X, but it does two rows
-; for slightly improved interleaving, and it omits vpermq since the
-; input is DC so all values are identical
-%macro VP9_STORE_YMM_DC_2X2 6 ; reg, tmp1, tmp2, tmp3, tmp4, zero
-    mova               m%2, [dstq]
-    mova               m%4, [dstq+strideq]
-    punpckhbw          m%3, m%2, m%6
-    punpcklbw          m%2, m%6
-    punpckhbw          m%5, m%4, m%6
-    punpcklbw          m%4, m%6
-    paddw              m%3, m%1
-    paddw              m%2, m%1
-    paddw              m%5, m%1
-    paddw              m%4, m%1
-    packuswb           m%2, m%3
-    packuswb           m%4, m%5
-    mova  [dstq+strideq*0], m%2
-    mova  [dstq+strideq*1], m%4
-%endmacro
-
-%if ARCH_X86_64 && HAVE_AVX2_EXTERNAL
-INIT_YMM avx2
-cglobal vp9_idct_idct_32x32_add, 4, 9, 16, 2048, dst, stride, block, eob
-    cmp eobd, 135
-    jg .idctfull
-    cmp eobd, 1
-    jg .idct16x16
-
-    ; dc-only case
-    mova                m1, [pw_11585x2]
-    vpbroadcastw        m0, [blockq]
-    pmulhrsw            m0, m1
-    pmulhrsw            m0, m1
-    pxor                m5, m5
-    pmulhrsw            m0, [pw_512]
-    movd          [blockq], xm5
-
-    DEFINE_ARGS dst, stride, cnt
-    mov               cntd, 16
-.loop_dc:
-    VP9_STORE_YMM_DC_2X2 0, 1, 2, 3, 4, 5
-    lea               dstq, [dstq+2*strideq]
-    dec               cntd
-    jg .loop_dc
-    RET
-
-    DEFINE_ARGS dst_bak, stride, block, cnt, dst, stride30, dst_end, stride2, tmp
-.idct16x16:
-    mov               tmpq, rsp
-    VP9_IDCT32_1D   blockq, 1, 16
-
-    mov          stride30q, strideq         ; stride
-    lea           stride2q, [strideq*2]     ; stride*2
-    shl          stride30q, 5               ; stride*32
-    mov               cntd, 2
-    sub          stride30q, stride2q        ; stride*30
-.loop2_16x16:
-    mov               dstq, dst_bakq
-    lea           dst_endq, [dstq+stride30q]
-    VP9_IDCT32_1D     tmpq, 2, 16
-    add           dst_bakq, 16
-    add               tmpq, 32
-    dec               cntd
-    jg .loop2_16x16
-
-    ; at the end of the loop, m1 should still be zero
-    ; use that to zero out block coefficients
-    ZERO_BLOCK      blockq, 64, 16, m1
-    RET
-
-.idctfull:
-    mov               cntd, 2
-    mov               tmpq, rsp
-.loop1_full:
-    VP9_IDCT32_1D   blockq, 1
-    add             blockq, 32
-    add               tmpq, 1024
-    dec               cntd
-    jg .loop1_full
-
-    sub             blockq, 64
-
-    mov          stride30q, strideq         ; stride
-    lea           stride2q, [strideq*2]     ; stride*2
-    shl          stride30q, 5               ; stride*32
-    mov               cntd, 2
-    mov               tmpq, rsp
-    sub          stride30q, stride2q        ; stride*30
-.loop2_full:
-    mov               dstq, dst_bakq
-    lea           dst_endq, [dstq+stride30q]
-    VP9_IDCT32_1D     tmpq, 2
-    add           dst_bakq, 16
-    add               tmpq, 32
-    dec               cntd
-    jg .loop2_full
-
-    ; at the end of the loop, m1 should still be zero
-    ; use that to zero out block coefficients
-    ZERO_BLOCK      blockq, 64, 32, m1
-    RET
-%endif
diff -uparN ffmpeg-4.1/libavcodec/x86/vp9itxfm_template.asm ffmpeg-y/libavcodec/x86/vp9itxfm_template.asm
--- ffmpeg-4.1/libavcodec/x86/vp9itxfm_template.asm	2018-07-17 17:27:41.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/vp9itxfm_template.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,142 +0,0 @@
-;******************************************************************************
-;* VP9 IDCT SIMD optimizations
-;*
-;* Copyright (C) 2013 Clément Bœsch <u pkh me>
-;* Copyright (C) 2013 Ronald S. Bultje <rsbultje gmail com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%macro VP9_IWHT4_1D 0
-    SWAP                 1, 2, 3
-    paddw               m0, m2
-    psubw               m3, m1
-    psubw               m4, m0, m3
-    psraw               m4, 1
-    psubw               m5, m4, m1
-    SWAP                 5, 1
-    psubw               m4, m2
-    SWAP                 4, 2
-    psubw               m0, m1
-    paddw               m3, m2
-    SWAP                 3, 2, 1
-%endmacro
-
-; (a*x + b*y + round) >> shift
-%macro VP9_MULSUB_2W_2X 5 ; dst1, dst2/src, round, coefs1, coefs2
-    pmaddwd            m%1, m%2, %4
-    pmaddwd            m%2,  %5
-    paddd              m%1,  %3
-    paddd              m%2,  %3
-    psrad              m%1,  14
-    psrad              m%2,  14
-%endmacro
-
-%macro VP9_MULSUB_2W_4X 7 ; dst1, dst2, coef1, coef2, rnd, tmp1/src, tmp2
-    VP9_MULSUB_2W_2X    %7,  %6,  %5, [pw_m%3_%4], [pw_%4_%3]
-    VP9_MULSUB_2W_2X    %1,  %2,  %5, [pw_m%3_%4], [pw_%4_%3]
-    packssdw           m%1, m%7
-    packssdw           m%2, m%6
-%endmacro
-
-%macro VP9_UNPACK_MULSUB_2W_4X 7-9 ; dst1, dst2, (src1, src2,) coef1, coef2, rnd, tmp1, tmp2
-%if %0 == 7
-    punpckhwd          m%6, m%2, m%1
-    punpcklwd          m%2, m%1
-    VP9_MULSUB_2W_4X   %1, %2, %3, %4, %5, %6, %7
-%else
-    punpckhwd          m%8, m%4, m%3
-    punpcklwd          m%2, m%4, m%3
-    VP9_MULSUB_2W_4X   %1, %2, %5, %6, %7, %8, %9
-%endif
-%endmacro
-
-%macro VP9_IDCT4_1D_FINALIZE 0
-    SUMSUB_BA            w, 3, 2, 4                         ; m3=t3+t0, m2=-t3+t0
-    SUMSUB_BA            w, 1, 0, 4                         ; m1=t2+t1, m0=-t2+t1
-    SWAP                 0, 3, 2                            ; 3102 -> 0123
-%endmacro
-
-%macro VP9_IDCT4_1D 0
-%if cpuflag(ssse3)
-    SUMSUB_BA            w, 2, 0, 4                         ; m2=IN(0)+IN(2) m0=IN(0)-IN(2)
-    pmulhrsw            m2, m6                              ; m2=t0
-    pmulhrsw            m0, m6                              ; m0=t1
-%else ; <= sse2
-    VP9_UNPACK_MULSUB_2W_4X 0, 2, 11585, 11585, m7, 4, 5    ; m0=t1, m1=t0
-%endif
-    VP9_UNPACK_MULSUB_2W_4X 1, 3, 15137, 6270, m7, 4, 5     ; m1=t2, m3=t3
-    VP9_IDCT4_1D_FINALIZE
-%endmacro
-
-%macro VP9_IADST4_1D 0
-    movq2dq           xmm0, m0
-    movq2dq           xmm1, m1
-    movq2dq           xmm2, m2
-    movq2dq           xmm3, m3
-%if cpuflag(ssse3)
-    paddw               m3, m0
-%endif
-    punpcklwd         xmm0, xmm1
-    punpcklwd         xmm2, xmm3
-    pmaddwd           xmm1, xmm0, [pw_5283_13377]
-    pmaddwd           xmm4, xmm0, [pw_9929_13377]
-%if notcpuflag(ssse3)
-    pmaddwd           xmm6, xmm0, [pw_13377_0]
-%endif
-    pmaddwd           xmm0, [pw_15212_m13377]
-    pmaddwd           xmm3, xmm2, [pw_15212_9929]
-%if notcpuflag(ssse3)
-    pmaddwd           xmm7, xmm2, [pw_m13377_13377]
-%endif
-    pmaddwd           xmm2, [pw_m5283_m15212]
-%if cpuflag(ssse3)
-    psubw               m3, m2
-%else
-    paddd             xmm6, xmm7
-%endif
-    paddd             xmm0, xmm2
-    paddd             xmm3, xmm5
-    paddd             xmm2, xmm5
-%if notcpuflag(ssse3)
-    paddd             xmm6, xmm5
-%endif
-    paddd             xmm1, xmm3
-    paddd             xmm0, xmm3
-    paddd             xmm4, xmm2
-    psrad             xmm1, 14
-    psrad             xmm0, 14
-    psrad             xmm4, 14
-%if cpuflag(ssse3)
-    pmulhrsw            m3, [pw_13377x2]        ; out2
-%else
-    psrad             xmm6, 14
-%endif
-    packssdw          xmm0, xmm0
-    packssdw          xmm1, xmm1
-    packssdw          xmm4, xmm4
-%if notcpuflag(ssse3)
-    packssdw          xmm6, xmm6
-%endif
-    movdq2q             m0, xmm0                ; out3
-    movdq2q             m1, xmm1                ; out0
-    movdq2q             m2, xmm4                ; out1
-%if notcpuflag(ssse3)
-    movdq2q             m3, xmm6                ; out2
-%endif
-    SWAP                 0, 1, 2, 3
-%endmacro
diff -uparN ffmpeg-4.1/libavcodec/x86/vp9lpf_16bpp.asm ffmpeg-y/libavcodec/x86/vp9lpf_16bpp.asm
--- ffmpeg-4.1/libavcodec/x86/vp9lpf_16bpp.asm	2018-11-02 02:34:25.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/vp9lpf_16bpp.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,823 +0,0 @@
-;******************************************************************************
-;* VP9 loop filter SIMD optimizations
-;*
-;* Copyright (C) 2015 Ronald S. Bultje <rsbultje@gmail.com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-
-pw_511: times 16 dw 511
-pw_2047: times 16 dw 2047
-pw_16384: times 16 dw 16384
-pw_m512: times 16 dw -512
-pw_m2048: times 16 dw -2048
-
-cextern pw_1
-cextern pw_3
-cextern pw_4
-cextern pw_8
-cextern pw_16
-cextern pw_256
-cextern pw_1023
-cextern pw_4095
-cextern pw_m1
-
-SECTION .text
-
-%macro SCRATCH 3-4
-%if ARCH_X86_64
-    SWAP                %1, %2
-%if %0 == 4
-%define reg_%4 m%2
-%endif
-%else
-    mova              [%3], m%1
-%if %0 == 4
-%define reg_%4 [%3]
-%endif
-%endif
-%endmacro
-
-%macro UNSCRATCH 3-4
-%if ARCH_X86_64
-    SWAP                %1, %2
-%else
-    mova               m%1, [%3]
-%endif
-%if %0 == 4
-%undef reg_%4
-%endif
-%endmacro
-
-%macro PRELOAD 2-3
-%if ARCH_X86_64
-    mova               m%1, [%2]
-%if %0 == 3
-%define reg_%3 m%1
-%endif
-%elif %0 == 3
-%define reg_%3 [%2]
-%endif
-%endmacro
-
-; calculate p or q portion of flat8out
-%macro FLAT8OUT_HALF 0
-    psubw               m4, m0                      ; q4-q0
-    psubw               m5, m0                      ; q5-q0
-    psubw               m6, m0                      ; q6-q0
-    psubw               m7, m0                      ; q7-q0
-    ABS2                m4, m5, m2, m3              ; abs(q4-q0) | abs(q5-q0)
-    ABS2                m6, m7, m2, m3              ; abs(q6-q0) | abs(q7-q0)
-    pcmpgtw             m4, reg_F                   ; abs(q4-q0) > F
-    pcmpgtw             m5, reg_F                   ; abs(q5-q0) > F
-    pcmpgtw             m6, reg_F                   ; abs(q6-q0) > F
-    pcmpgtw             m7, reg_F                   ; abs(q7-q0) > F
-    por                 m5, m4
-    por                 m7, m6
-    por                 m7, m5                      ; !flat8out, q portion
-%endmacro
-
-; calculate p or q portion of flat8in/hev/fm (excluding mb_edge condition)
-%macro FLAT8IN_HALF 1
-%if %1 > 4
-    psubw               m4, m3, m0                  ; q3-q0
-    psubw               m5, m2, m0                  ; q2-q0
-    ABS2                m4, m5, m6, m7              ; abs(q3-q0) | abs(q2-q0)
-    pcmpgtw             m4, reg_F                   ; abs(q3-q0) > F
-    pcmpgtw             m5, reg_F                   ; abs(q2-q0) > F
-%endif
-    psubw               m3, m2                      ; q3-q2
-    psubw               m2, m1                      ; q2-q1
-    ABS2                m3, m2, m6, m7              ; abs(q3-q2) | abs(q2-q1)
-    pcmpgtw             m3, reg_I                   ; abs(q3-q2) > I
-    pcmpgtw             m2, reg_I                   ; abs(q2-q1) > I
-%if %1 > 4
-    por                 m4, m5
-%endif
-    por                 m2, m3
-    psubw               m3, m1, m0                  ; q1-q0
-    ABS1                m3, m5                      ; abs(q1-q0)
-%if %1 > 4
-    pcmpgtw             m6, m3, reg_F               ; abs(q1-q0) > F
-%endif
-    pcmpgtw             m7, m3, reg_H               ; abs(q1-q0) > H
-    pcmpgtw             m3, reg_I                   ; abs(q1-q0) > I
-%if %1 > 4
-    por                 m4, m6
-%endif
-    por                 m2, m3
-%endmacro
-
-; one step in filter_14/filter_6
-;
-; take sum $reg, downshift, apply mask and write into dst
-;
-; if sub2/add1-2 are present, add/sub as appropriate to prepare for the next
-; step's sum $reg. This is omitted for the last row in each filter.
-;
-; if dont_store is set, don't write the result into memory, instead keep the
-; values in register so we can write it out later
-%macro FILTER_STEP 6-10 "", "", "", 0 ; tmp, reg, mask, shift, dst, \
-                                      ; src/sub1, sub2, add1, add2, dont_store
-    psrlw               %1, %2, %4
-    psubw               %1, %6                      ; abs->delta
-%ifnidn %7, ""
-    psubw               %2, %6
-    psubw               %2, %7
-    paddw               %2, %8
-    paddw               %2, %9
-%endif
-    pand                %1, reg_%3                  ; apply mask
-%if %10 == 1
-    paddw               %6, %1                      ; delta->abs
-%else
-    paddw               %1, %6                      ; delta->abs
-    mova              [%5], %1
-%endif
-%endmacro
-
-; FIXME avx2 versions for 16_16 and mix2_{4,8}{4,8}
-
-%macro LOOP_FILTER 3 ; dir[h/v], wd[4/8/16], bpp[10/12]
-
-%if ARCH_X86_64
-%if %2 == 16
-%assign %%num_xmm_regs 16
-%elif %2 == 8
-%assign %%num_xmm_regs 15
-%else ; %2 == 4
-%assign %%num_xmm_regs 14
-%endif ; %2
-%assign %%bak_mem 0
-%else ; ARCH_X86_32
-%assign %%num_xmm_regs 8
-%if %2 == 16
-%assign %%bak_mem 7
-%elif %2 == 8
-%assign %%bak_mem 6
-%else ; %2 == 4
-%assign %%bak_mem 5
-%endif ; %2
-%endif ; ARCH_X86_64/32
-
-%if %2 == 16
-%ifidn %1, v
-%assign %%num_gpr_regs 6
-%else ; %1 == h
-%assign %%num_gpr_regs 5
-%endif ; %1
-%assign %%wd_mem 6
-%else ; %2 == 8/4
-%assign %%num_gpr_regs 5
-%if ARCH_X86_32 && %2 == 8
-%assign %%wd_mem 2
-%else ; ARCH_X86_64 || %2 == 4
-%assign %%wd_mem 0
-%endif ; ARCH_X86_64/32 etc.
-%endif ; %2
-
-%ifidn %1, v
-%assign %%tsp_mem 0
-%elif %2 == 16 ; && %1 == h
-%assign %%tsp_mem 16
-%else ; %1 == h && %1 == 8/4
-%assign %%tsp_mem 8
-%endif ; %1/%2
-
-%assign %%off %%wd_mem
-%assign %%tspoff %%bak_mem+%%wd_mem
-%assign %%stack_mem ((%%bak_mem+%%wd_mem+%%tsp_mem)*mmsize)
-
-%if %3 == 10
-%define %%maxsgn 511
-%define %%minsgn m512
-%define %%maxusgn 1023
-%define %%maxf 4
-%else ; %3 == 12
-%define %%maxsgn 2047
-%define %%minsgn m2048
-%define %%maxusgn 4095
-%define %%maxf 16
-%endif ; %3
-
-cglobal vp9_loop_filter_%1_%2_%3, 5, %%num_gpr_regs, %%num_xmm_regs, %%stack_mem, dst, stride, E, I, H
-    ; prepare E, I and H masks
-    shl                 Ed, %3-8
-    shl                 Id, %3-8
-    shl                 Hd, %3-8
-%if cpuflag(ssse3)
-    mova                m0, [pw_256]
-%endif
-    movd                m1, Ed
-    movd                m2, Id
-    movd                m3, Hd
-%if cpuflag(ssse3)
-    pshufb              m1, m0                      ; E << (bit_depth - 8)
-    pshufb              m2, m0                      ; I << (bit_depth - 8)
-    pshufb              m3, m0                      ; H << (bit_depth - 8)
-%else
-    punpcklwd           m1, m1
-    punpcklwd           m2, m2
-    punpcklwd           m3, m3
-    pshufd              m1, m1, q0000
-    pshufd              m2, m2, q0000
-    pshufd              m3, m3, q0000
-%endif
-    SCRATCH              1,  8, rsp+(%%off+0)*mmsize,  E
-    SCRATCH              2,  9, rsp+(%%off+1)*mmsize,  I
-    SCRATCH              3, 10, rsp+(%%off+2)*mmsize,  H
-%if %2 > 4
-    PRELOAD                 11, pw_ %+ %%maxf, F
-%endif
-
-    ; set up variables to load data
-%ifidn %1, v
-    DEFINE_ARGS dst8, stride, stride3, dst0, dst4, dst12
-    lea           stride3q, [strideq*3]
-    neg            strideq
-%if %2 == 16
-    lea              dst0q, [dst8q+strideq*8]
-%else
-    lea              dst4q, [dst8q+strideq*4]
-%endif
-    neg            strideq
-%if %2 == 16
-    lea             dst12q, [dst8q+strideq*4]
-    lea              dst4q, [dst0q+strideq*4]
-%endif
-
-%if %2 == 16
-%define %%p7 dst0q
-%define %%p6 dst0q+strideq
-%define %%p5 dst0q+strideq*2
-%define %%p4 dst0q+stride3q
-%endif
-%define %%p3 dst4q
-%define %%p2 dst4q+strideq
-%define %%p1 dst4q+strideq*2
-%define %%p0 dst4q+stride3q
-%define %%q0 dst8q
-%define %%q1 dst8q+strideq
-%define %%q2 dst8q+strideq*2
-%define %%q3 dst8q+stride3q
-%if %2 == 16
-%define %%q4 dst12q
-%define %%q5 dst12q+strideq
-%define %%q6 dst12q+strideq*2
-%define %%q7 dst12q+stride3q
-%endif
-%else ; %1 == h
-    DEFINE_ARGS dst0, stride, stride3, dst4
-    lea           stride3q, [strideq*3]
-    lea              dst4q, [dst0q+strideq*4]
-
-%define %%p3 rsp+(%%tspoff+0)*mmsize
-%define %%p2 rsp+(%%tspoff+1)*mmsize
-%define %%p1 rsp+(%%tspoff+2)*mmsize
-%define %%p0 rsp+(%%tspoff+3)*mmsize
-%define %%q0 rsp+(%%tspoff+4)*mmsize
-%define %%q1 rsp+(%%tspoff+5)*mmsize
-%define %%q2 rsp+(%%tspoff+6)*mmsize
-%define %%q3 rsp+(%%tspoff+7)*mmsize
-
-%if %2 < 16
-    movu                m0, [dst0q+strideq*0-8]
-    movu                m1, [dst0q+strideq*1-8]
-    movu                m2, [dst0q+strideq*2-8]
-    movu                m3, [dst0q+stride3q -8]
-    movu                m4, [dst4q+strideq*0-8]
-    movu                m5, [dst4q+strideq*1-8]
-    movu                m6, [dst4q+strideq*2-8]
-    movu                m7, [dst4q+stride3q -8]
-
-%if ARCH_X86_64
-    TRANSPOSE8x8W        0, 1, 2, 3, 4, 5, 6, 7, 12
-%else
-    TRANSPOSE8x8W        0, 1, 2, 3, 4, 5, 6, 7, [%%p0], [%%q0]
-%endif
-
-    mova            [%%p3], m0
-    mova            [%%p2], m1
-    mova            [%%p1], m2
-    mova            [%%p0], m3
-%if ARCH_X86_64
-    mova            [%%q0], m4
-%endif
-    mova            [%%q1], m5
-    mova            [%%q2], m6
-    mova            [%%q3], m7
-
-    ; FIXME investigate if we can _not_ load q0-3 below if h, and adjust register
-    ; order here accordingly
-%else ; %2 == 16
-
-%define %%p7 rsp+(%%tspoff+ 8)*mmsize
-%define %%p6 rsp+(%%tspoff+ 9)*mmsize
-%define %%p5 rsp+(%%tspoff+10)*mmsize
-%define %%p4 rsp+(%%tspoff+11)*mmsize
-%define %%q4 rsp+(%%tspoff+12)*mmsize
-%define %%q5 rsp+(%%tspoff+13)*mmsize
-%define %%q6 rsp+(%%tspoff+14)*mmsize
-%define %%q7 rsp+(%%tspoff+15)*mmsize
-
-    mova                m0, [dst0q+strideq*0-16]
-    mova                m1, [dst0q+strideq*1-16]
-    mova                m2, [dst0q+strideq*2-16]
-    mova                m3, [dst0q+stride3q -16]
-    mova                m4, [dst4q+strideq*0-16]
-    mova                m5, [dst4q+strideq*1-16]
-%if ARCH_X86_64
-    mova                m6, [dst4q+strideq*2-16]
-%endif
-    mova                m7, [dst4q+stride3q -16]
-
-%if ARCH_X86_64
-    TRANSPOSE8x8W        0, 1, 2, 3, 4, 5, 6, 7, 12
-%else
-    TRANSPOSE8x8W        0, 1, 2, 3, 4, 5, 6, 7, [dst4q+strideq*2-16], [%%p3], 1
-%endif
-
-    mova            [%%p7], m0
-    mova            [%%p6], m1
-    mova            [%%p5], m2
-    mova            [%%p4], m3
-%if ARCH_X86_64
-    mova            [%%p3], m4
-%endif
-    mova            [%%p2], m5
-    mova            [%%p1], m6
-    mova            [%%p0], m7
-
-    mova                m0, [dst0q+strideq*0]
-    mova                m1, [dst0q+strideq*1]
-    mova                m2, [dst0q+strideq*2]
-    mova                m3, [dst0q+stride3q ]
-    mova                m4, [dst4q+strideq*0]
-    mova                m5, [dst4q+strideq*1]
-%if ARCH_X86_64
-    mova                m6, [dst4q+strideq*2]
-%endif
-    mova                m7, [dst4q+stride3q ]
-
-%if ARCH_X86_64
-    TRANSPOSE8x8W        0, 1, 2, 3, 4, 5, 6, 7, 12
-%else
-    TRANSPOSE8x8W        0, 1, 2, 3, 4, 5, 6, 7, [dst4q+strideq*2], [%%q4], 1
-%endif
-
-    mova            [%%q0], m0
-    mova            [%%q1], m1
-    mova            [%%q2], m2
-    mova            [%%q3], m3
-%if ARCH_X86_64
-    mova            [%%q4], m4
-%endif
-    mova            [%%q5], m5
-    mova            [%%q6], m6
-    mova            [%%q7], m7
-
-    ; FIXME investigate if we can _not_ load q0|q4-7 below if h, and adjust register
-    ; order here accordingly
-%endif ; %2
-%endif ; %1
-
-    ; load q0|q4-7 data
-    mova                m0, [%%q0]
-%if %2 == 16
-    mova                m4, [%%q4]
-    mova                m5, [%%q5]
-    mova                m6, [%%q6]
-    mova                m7, [%%q7]
-
-    ; flat8out q portion
-    FLAT8OUT_HALF
-    SCRATCH              7, 15, rsp+(%%off+6)*mmsize, F8O
-%endif
-
-    ; load q1-3 data
-    mova                m1, [%%q1]
-    mova                m2, [%%q2]
-    mova                m3, [%%q3]
-
-    ; r6-8|pw_4[m8-11]=reg_E/I/H/F
-    ; r9[m15]=!flatout[q]
-    ; m12-14=free
-    ; m0-3=q0-q3
-    ; m4-7=free
-
-    ; flat8in|fm|hev q portion
-    FLAT8IN_HALF        %2
-    SCRATCH              7, 13, rsp+(%%off+4)*mmsize, HEV
-%if %2 > 4
-    SCRATCH              4, 14, rsp+(%%off+5)*mmsize, F8I
-%endif
-
-    ; r6-8|pw_4[m8-11]=reg_E/I/H/F
-    ; r9[m15]=!flat8out[q]
-    ; r10[m13]=hev[q]
-    ; r11[m14]=!flat8in[q]
-    ; m2=!fm[q]
-    ; m0,1=q0-q1
-    ; m2-7=free
-    ; m12=free
-
-    ; load p0-1
-    mova                m3, [%%p0]
-    mova                m4, [%%p1]
-
-    ; fm mb_edge portion
-    psubw               m5, m3, m0                  ; q0-p0
-    psubw               m6, m4, m1                  ; q1-p1
-%if ARCH_X86_64
-    ABS2                m5, m6, m7, m12             ; abs(q0-p0) | abs(q1-p1)
-%else
-    ABS1                m5, m7                      ; abs(q0-p0)
-    ABS1                m6, m7                      ; abs(q1-p1)
-%endif
-    paddw               m5, m5
-    psraw               m6, 1
-    paddw               m6, m5                      ; abs(q0-p0)*2+(abs(q1-p1)>>1)
-    pcmpgtw             m6, reg_E
-    por                 m2, m6
-    SCRATCH              2, 12, rsp+(%%off+3)*mmsize, FM
-
-    ; r6-8|pw_4[m8-11]=reg_E/I/H/F
-    ; r9[m15]=!flat8out[q]
-    ; r10[m13]=hev[q]
-    ; r11[m14]=!flat8in[q]
-    ; r12[m12]=!fm[q]
-    ; m3-4=q0-1
-    ; m0-2/5-7=free
-
-    ; load p4-7 data
-    SWAP                 3, 0                       ; p0
-    SWAP                 4, 1                       ; p1
-%if %2 == 16
-    mova                m7, [%%p7]
-    mova                m6, [%%p6]
-    mova                m5, [%%p5]
-    mova                m4, [%%p4]
-
-    ; flat8out p portion
-    FLAT8OUT_HALF
-    por                 m7, reg_F8O
-    SCRATCH              7, 15, rsp+(%%off+6)*mmsize, F8O
-%endif
-
-    ; r6-8|pw_4[m8-11]=reg_E/I/H/F
-    ; r9[m15]=!flat8out
-    ; r10[m13]=hev[q]
-    ; r11[m14]=!flat8in[q]
-    ; r12[m12]=!fm[q]
-    ; m0=p0
-    ; m1-7=free
-
-    ; load p2-3 data
-    mova                m2, [%%p2]
-    mova                m3, [%%p3]
-
-    ; flat8in|fm|hev p portion
-    FLAT8IN_HALF        %2
-    por                 m7, reg_HEV
-%if %2 > 4
-    por                 m4, reg_F8I
-%endif
-    por                 m2, reg_FM
-%if %2 > 4
-    por                 m4, m2                      ; !flat8|!fm
-%if %2 == 16
-    por                 m5, m4, reg_F8O             ; !flat16|!fm
-    pandn               m2, m4                      ; filter4_mask
-    pandn               m4, m5                      ; filter8_mask
-    pxor                m5, [pw_m1]                 ; filter16_mask
-    SCRATCH              5, 15, rsp+(%%off+6)*mmsize, F16M
-%else
-    pandn               m2, m4                      ; filter4_mask
-    pxor                m4, [pw_m1]                 ; filter8_mask
-%endif
-    SCRATCH              4, 14, rsp+(%%off+5)*mmsize, F8M
-%else
-    pxor                m2, [pw_m1]                 ; filter4_mask
-%endif
-    SCRATCH              7, 13, rsp+(%%off+4)*mmsize, HEV
-    SCRATCH              2, 12, rsp+(%%off+3)*mmsize, F4M
-
-    ; r9[m15]=filter16_mask
-    ; r10[m13]=hev
-    ; r11[m14]=filter8_mask
-    ; r12[m12]=filter4_mask
-    ; m0,1=p0-p1
-    ; m2-7=free
-    ; m8-11=free
-
-%if %2 > 4
-%if %2 == 16
-    ; filter_14
-    mova                m2, [%%p7]
-    mova                m3, [%%p6]
-    mova                m6, [%%p5]
-    mova                m7, [%%p4]
-    PRELOAD              8, %%p3, P3
-    PRELOAD              9, %%p2, P2
-%endif
-    PRELOAD             10, %%q0, Q0
-    PRELOAD             11, %%q1, Q1
-%if %2 == 16
-    psllw               m4, m2, 3
-    paddw               m5, m3, m3
-    paddw               m4, m6
-    paddw               m5, m7
-    paddw               m4, reg_P3
-    paddw               m5, reg_P2
-    paddw               m4, m1
-    paddw               m5, m0
-    paddw               m4, reg_Q0                  ; q0+p1+p3+p5+p7*8
-    psubw               m5, m2                      ; p0+p2+p4+p6*2-p7
-    paddw               m4, [pw_8]
-    paddw               m5, m4                      ; q0+p0+p1+p2+p3+p4+p5+p6*2+p7*7+8
-
-    ; below, we use r0-5 for storing pre-filter pixels for subsequent subtraction
-    ; at the end of the filter
-
-    mova    [rsp+0*mmsize], m3
-    FILTER_STEP         m4, m5, F16M, 4, %%p6, m3,     m2,             m6,     reg_Q1
-%endif
-    mova                m3, [%%q2]
-%if %2 == 16
-    mova    [rsp+1*mmsize], m6
-    FILTER_STEP         m4, m5, F16M, 4, %%p5, m6,     m2,             m7,     m3
-%endif
-    mova                m6, [%%q3]
-%if %2 == 16
-    mova    [rsp+2*mmsize], m7
-    FILTER_STEP         m4, m5, F16M, 4, %%p4, m7,     m2,             reg_P3, m6
-    mova                m7, [%%q4]
-%if ARCH_X86_64
-    mova    [rsp+3*mmsize], reg_P3
-%else
-    mova                m4, reg_P3
-    mova    [rsp+3*mmsize], m4
-%endif
-    FILTER_STEP         m4, m5, F16M, 4, %%p3, reg_P3, m2,             reg_P2, m7
-    PRELOAD              8, %%q5, Q5
-%if ARCH_X86_64
-    mova    [rsp+4*mmsize], reg_P2
-%else
-    mova                m4, reg_P2
-    mova    [rsp+4*mmsize], m4
-%endif
-    FILTER_STEP         m4, m5, F16M, 4, %%p2, reg_P2, m2,             m1,     reg_Q5
-    PRELOAD              9, %%q6, Q6
-    mova    [rsp+5*mmsize], m1
-    FILTER_STEP         m4, m5, F16M, 4, %%p1, m1,     m2,             m0,     reg_Q6
-    mova                m1, [%%q7]
-    FILTER_STEP         m4, m5, F16M, 4, %%p0, m0,     m2,             reg_Q0, m1,     1
-    FILTER_STEP         m4, m5, F16M, 4, %%q0, reg_Q0, [rsp+0*mmsize], reg_Q1, m1,     ARCH_X86_64
-    FILTER_STEP         m4, m5, F16M, 4, %%q1, reg_Q1, [rsp+1*mmsize], m3,     m1,     ARCH_X86_64
-    FILTER_STEP         m4, m5, F16M, 4, %%q2, m3,     [rsp+2*mmsize], m6,     m1,     1
-    FILTER_STEP         m4, m5, F16M, 4, %%q3, m6,     [rsp+3*mmsize], m7,     m1
-    FILTER_STEP         m4, m5, F16M, 4, %%q4, m7,     [rsp+4*mmsize], reg_Q5, m1
-    FILTER_STEP         m4, m5, F16M, 4, %%q5, reg_Q5, [rsp+5*mmsize], reg_Q6, m1
-    FILTER_STEP         m4, m5, F16M, 4, %%q6, reg_Q6
-
-    mova                m7, [%%p1]
-%else
-    SWAP                 1, 7
-%endif
-
-    mova                m2, [%%p3]
-    mova                m1, [%%p2]
-
-    ; reg_Q0-1 (m10-m11)
-    ; m0=p0
-    ; m1=p2
-    ; m2=p3
-    ; m3=q2
-    ; m4-5=free
-    ; m6=q3
-    ; m7=p1
-    ; m8-9 unused
-
-    ; filter_6
-    psllw               m4, m2, 2
-    paddw               m5, m1, m1
-    paddw               m4, m7
-    psubw               m5, m2
-    paddw               m4, m0
-    paddw               m5, reg_Q0
-    paddw               m4, [pw_4]
-    paddw               m5, m4
-
-%if ARCH_X86_64
-    mova                m8, m1
-    mova                m9, m7
-%else
-    mova    [rsp+0*mmsize], m1
-    mova    [rsp+1*mmsize], m7
-%endif
-%ifidn %1, v
-    FILTER_STEP         m4, m5, F8M, 3, %%p2, m1,     m2,             m7,     reg_Q1
-%else
-    FILTER_STEP         m4, m5, F8M, 3, %%p2, m1,     m2,             m7,     reg_Q1, 1
-%endif
-    FILTER_STEP         m4, m5, F8M, 3, %%p1, m7,     m2,             m0,     m3, 1
-    FILTER_STEP         m4, m5, F8M, 3, %%p0, m0,     m2,             reg_Q0, m6, 1
-%if ARCH_X86_64
-    FILTER_STEP         m4, m5, F8M, 3, %%q0, reg_Q0, m8,             reg_Q1, m6, ARCH_X86_64
-    FILTER_STEP         m4, m5, F8M, 3, %%q1, reg_Q1, m9,             m3,     m6, ARCH_X86_64
-%else
-    FILTER_STEP         m4, m5, F8M, 3, %%q0, reg_Q0, [rsp+0*mmsize], reg_Q1, m6, ARCH_X86_64
-    FILTER_STEP         m4, m5, F8M, 3, %%q1, reg_Q1, [rsp+1*mmsize], m3,     m6, ARCH_X86_64
-%endif
-    FILTER_STEP         m4, m5, F8M, 3, %%q2, m3
-
-    UNSCRATCH            2, 10, %%q0
-    UNSCRATCH            6, 11, %%q1
-%else
-    SWAP                 1, 7
-    mova                m2, [%%q0]
-    mova                m6, [%%q1]
-%endif
-    UNSCRATCH            3, 13, rsp+(%%off+4)*mmsize, HEV
-
-    ; m0=p0
-    ; m1=p2
-    ; m2=q0
-    ; m3=hev_mask
-    ; m4-5=free
-    ; m6=q1
-    ; m7=p1
-
-    ; filter_4
-    psubw               m4, m7, m6              ; p1-q1
-    psubw               m5, m2, m0              ; q0-p0
-    pand                m4, m3
-    pminsw              m4, [pw_ %+ %%maxsgn]
-    pmaxsw              m4, [pw_ %+ %%minsgn]   ; clip_intp2(p1-q1, 9) -> f
-    paddw               m4, m5
-    paddw               m5, m5
-    paddw               m4, m5                  ; 3*(q0-p0)+f
-    pminsw              m4, [pw_ %+ %%maxsgn]
-    pmaxsw              m4, [pw_ %+ %%minsgn]   ; clip_intp2(3*(q0-p0)+f, 9) -> f
-    pand                m4, reg_F4M
-    paddw               m5, m4, [pw_4]
-    paddw               m4, [pw_3]
-    pminsw              m5, [pw_ %+ %%maxsgn]
-    pminsw              m4, [pw_ %+ %%maxsgn]
-    psraw               m5, 3                   ; min_intp2(f+4, 9)>>3 -> f1
-    psraw               m4, 3                   ; min_intp2(f+3, 9)>>3 -> f2
-    psubw               m2, m5                  ; q0-f1
-    paddw               m0, m4                  ; p0+f2
-    pandn               m3, m5                  ; f1 & !hev (for p1/q1 adj)
-    pxor                m4, m4
-    mova                m5, [pw_ %+ %%maxusgn]
-    pmaxsw              m2, m4
-    pmaxsw              m0, m4
-    pminsw              m2, m5
-    pminsw              m0, m5
-%if cpuflag(ssse3)
-    pmulhrsw            m3, [pw_16384]          ; (f1+1)>>1
-%else
-    paddw               m3, [pw_1]
-    psraw               m3, 1
-%endif
-    paddw               m7, m3                  ; p1+f
-    psubw               m6, m3                  ; q1-f
-    pmaxsw              m7, m4
-    pmaxsw              m6, m4
-    pminsw              m7, m5
-    pminsw              m6, m5
-
-    ; store
-%ifidn %1, v
-    mova            [%%p1], m7
-    mova            [%%p0], m0
-    mova            [%%q0], m2
-    mova            [%%q1], m6
-%else ; %1 == h
-%if %2 == 4
-    TRANSPOSE4x4W        7, 0, 2, 6, 1
-    movh   [dst0q+strideq*0-4], m7
-    movhps [dst0q+strideq*1-4], m7
-    movh   [dst0q+strideq*2-4], m0
-    movhps [dst0q+stride3q -4], m0
-    movh   [dst4q+strideq*0-4], m2
-    movhps [dst4q+strideq*1-4], m2
-    movh   [dst4q+strideq*2-4], m6
-    movhps [dst4q+stride3q -4], m6
-%elif %2 == 8
-    mova                m3, [%%p3]
-    mova                m4, [%%q2]
-    mova                m5, [%%q3]
-
-%if ARCH_X86_64
-    TRANSPOSE8x8W        3, 1, 7, 0, 2, 6, 4, 5, 8
-%else
-    TRANSPOSE8x8W        3, 1, 7, 0, 2, 6, 4, 5, [%%q2], [%%q0], 1
-    mova                m2, [%%q0]
-%endif
-
-    movu [dst0q+strideq*0-8], m3
-    movu [dst0q+strideq*1-8], m1
-    movu [dst0q+strideq*2-8], m7
-    movu [dst0q+stride3q -8], m0
-    movu [dst4q+strideq*0-8], m2
-    movu [dst4q+strideq*1-8], m6
-    movu [dst4q+strideq*2-8], m4
-    movu [dst4q+stride3q -8], m5
-%else ; %2 == 16
-    SCRATCH              2, 8, %%q0
-    SCRATCH              6, 9, %%q1
-    mova                m2, [%%p7]
-    mova                m3, [%%p6]
-    mova                m4, [%%p5]
-    mova                m5, [%%p4]
-    mova                m6, [%%p3]
-
-%if ARCH_X86_64
-    TRANSPOSE8x8W        2, 3, 4, 5, 6, 1, 7, 0, 10
-%else
-    mova            [%%p1], m7
-    TRANSPOSE8x8W        2, 3, 4, 5, 6, 1, 7, 0, [%%p1], [dst4q+strideq*0-16], 1
-%endif
-
-    mova [dst0q+strideq*0-16], m2
-    mova [dst0q+strideq*1-16], m3
-    mova [dst0q+strideq*2-16], m4
-    mova [dst0q+stride3q -16], m5
-%if ARCH_X86_64
-    mova [dst4q+strideq*0-16], m6
-%endif
-    mova [dst4q+strideq*1-16], m1
-    mova [dst4q+strideq*2-16], m7
-    mova [dst4q+stride3q -16], m0
-
-    UNSCRATCH            2, 8, %%q0
-    UNSCRATCH            6, 9, %%q1
-    mova                m0, [%%q2]
-    mova                m1, [%%q3]
-    mova                m3, [%%q4]
-    mova                m4, [%%q5]
-%if ARCH_X86_64
-    mova                m5, [%%q6]
-%endif
-    mova                m7, [%%q7]
-
-%if ARCH_X86_64
-    TRANSPOSE8x8W        2, 6, 0, 1, 3, 4, 5, 7, 8
-%else
-    TRANSPOSE8x8W        2, 6, 0, 1, 3, 4, 5, 7, [%%q6], [dst4q+strideq*0], 1
-%endif
-
-    mova [dst0q+strideq*0], m2
-    mova [dst0q+strideq*1], m6
-    mova [dst0q+strideq*2], m0
-    mova [dst0q+stride3q ], m1
-%if ARCH_X86_64
-    mova [dst4q+strideq*0], m3
-%endif
-    mova [dst4q+strideq*1], m4
-    mova [dst4q+strideq*2], m5
-    mova [dst4q+stride3q ], m7
-%endif ; %2
-%endif ; %1
-    RET
-%endmacro
-
-%macro LOOP_FILTER_CPUSETS 3
-INIT_XMM sse2
-LOOP_FILTER %1, %2, %3
-INIT_XMM ssse3
-LOOP_FILTER %1, %2, %3
-INIT_XMM avx
-LOOP_FILTER %1, %2, %3
-%endmacro
-
-%macro LOOP_FILTER_WDSETS 2
-LOOP_FILTER_CPUSETS %1,  4, %2
-LOOP_FILTER_CPUSETS %1,  8, %2
-LOOP_FILTER_CPUSETS %1, 16, %2
-%endmacro
-
-LOOP_FILTER_WDSETS h, 10
-LOOP_FILTER_WDSETS v, 10
-LOOP_FILTER_WDSETS h, 12
-LOOP_FILTER_WDSETS v, 12
diff -uparN ffmpeg-4.1/libavcodec/x86/vp9lpf.asm ffmpeg-y/libavcodec/x86/vp9lpf.asm
--- ffmpeg-4.1/libavcodec/x86/vp9lpf.asm	2018-11-02 02:34:25.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/vp9lpf.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,1211 +0,0 @@
-;******************************************************************************
-;* VP9 loop filter SIMD optimizations
-;*
-;* Copyright (C) 2013-2014 Clément Bœsch <u pkh me>
-;* Copyright (C) 2014 Ronald S. Bultje <rsbultje@gmail.com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-
-cextern pb_3
-cextern pb_80
-
-pb_4:   times 16 db 0x04
-pb_10:  times 16 db 0x10
-pb_40:  times 16 db 0x40
-pb_81:  times 16 db 0x81
-pb_f8:  times 16 db 0xf8
-pb_fe:  times 16 db 0xfe
-pb_ff:  times 16 db 0xff
-
-cextern pw_4
-cextern pw_8
-
-; with mix functions, two 8-bit thresholds are stored in a 16-bit storage,
-; the following mask is used to splat both in the same register
-mask_mix: times 8 db 0
-          times 8 db 1
-
-mask_mix84: times 8 db 0xff
-            times 8 db 0x00
-mask_mix48: times 8 db 0x00
-            times 8 db 0xff
-
-SECTION .text
-
-%macro SCRATCH 3
-%ifdef m8
-    SWAP                %1, %2
-%else
-    mova              [%3], m%1
-%endif
-%endmacro
-
-%macro UNSCRATCH 3
-%ifdef m8
-    SWAP                %1, %2
-%else
-    mova               m%1, [%3]
-%endif
-%endmacro
-
-; %1 = abs(%2-%3)
-%macro ABSSUB 4 ; dst, src1 (RO), src2 (RO), tmp
-%ifdef m8
-    psubusb             %1, %3, %2
-    psubusb             %4, %2, %3
-%else
-    mova                %1, %3
-    mova                %4, %2
-    psubusb             %1, %2
-    psubusb             %4, %3
-%endif
-    por                 %1, %4
-%endmacro
-
-; %1 = %1>%2
-%macro CMP_GT 2-3 ; src/dst, cmp, pb_80
-%if %0 == 3
-    pxor                %1, %3
-%endif
-    pcmpgtb             %1, %2
-%endmacro
-
-; %1 = abs(%2-%3) > %4
-%macro ABSSUB_GT 5-6 [pb_80]; dst, src1, src2, cmp, tmp, [pb_80]
-    ABSSUB              %1, %2, %3, %5      ; dst = abs(src1-src2)
-    CMP_GT              %1, %4, %6          ; dst > cmp
-%endmacro
-
-%macro MASK_APPLY 4 ; %1=new_data/dst %2=old_data %3=mask %4=tmp
-    pand                %1, %3              ; new &= mask
-    pandn               %4, %3, %2          ; tmp = ~mask & old
-    por                 %1, %4              ; new&mask | old&~mask
-%endmacro
-
-%macro UNPACK 4
-%ifdef m8
-    punpck%1bw          %2, %3, %4
-%else
-    mova                %2, %3
-    punpck%1bw          %2, %4
-%endif
-%endmacro
-
-%macro FILTER_SUBx2_ADDx2 11 ; %1=dst %2=h/l %3=cache %4=stack_off %5=sub1 %6=sub2 %7=add1
-                             ; %8=add2 %9=rshift, [unpack], [unpack_is_mem_on_x86_32]
-    psubw               %3, [rsp+%4+%5*mmsize*2]
-    psubw               %3, [rsp+%4+%6*mmsize*2]
-    paddw               %3, [rsp+%4+%7*mmsize*2]
-%ifnidn %10, ""
-%if %11 == 0
-    punpck%2bw          %1, %10, m0
-%else
-    UNPACK          %2, %1, %10, m0
-%endif
-    mova [rsp+%4+%8*mmsize*2], %1
-    paddw               %3, %1
-%else
-    paddw               %3, [rsp+%4+%8*mmsize*2]
-%endif
-    psraw               %1, %3, %9
-%endmacro
-
-; FIXME interleave l/h better (for instruction pairing)
-%macro FILTER_INIT 9 ; tmp1, tmp2, cacheL, cacheH, dstp, stack_off, filterid, mask, source
-    FILTER%7_INIT       %1, l, %3, %6 +      0
-    FILTER%7_INIT       %2, h, %4, %6 + mmsize
-    packuswb            %1, %2
-    MASK_APPLY          %1, %9, %8, %2
-    mova                %5, %1
-%endmacro
-
-
-%macro FILTER_UPDATE 12-16 "", "", "", 0 ; tmp1, tmp2, cacheL, cacheH, dstp, stack_off, -, -, +, +, rshift,
-                                         ; mask, [source], [unpack + src], [unpack_is_mem_on_x86_32]
-; FIXME interleave this properly with the subx2/addx2
-%ifnidn %15, ""
-%if %16 == 0 || ARCH_X86_64
-    mova               %14, %15
-%endif
-%endif
-    FILTER_SUBx2_ADDx2  %1, l, %3, %6 +      0, %7, %8, %9, %10, %11, %14, %16
-    FILTER_SUBx2_ADDx2  %2, h, %4, %6 + mmsize, %7, %8, %9, %10, %11, %14, %16
-    packuswb            %1, %2
-%ifnidn %13, ""
-    MASK_APPLY          %1, %13, %12, %2
-%else
-    MASK_APPLY          %1, %5, %12, %2
-%endif
-    mova                %5, %1
-%endmacro
-
-%macro SRSHIFT3B_2X 4 ; reg1, reg2, [pb_10], tmp
-    mova                %4, [pb_f8]
-    pand                %1, %4
-    pand                %2, %4
-    psrlq               %1, 3
-    psrlq               %2, 3
-    pxor                %1, %3
-    pxor                %2, %3
-    psubb               %1, %3
-    psubb               %2, %3
-%endmacro
-
-%macro EXTRACT_POS_NEG 3 ; i8, neg, pos
-    pxor                %3, %3
-    pxor                %2, %2
-    pcmpgtb             %3, %1                          ; i8 < 0 mask
-    psubb               %2, %1                          ; neg values (only the originally - will be kept)
-    pand                %2, %3                          ; negative values of i8 (but stored as +)
-    pandn               %3, %1                          ; positive values of i8
-%endmacro
-
-; clip_u8(u8 + i8)
-%macro SIGN_ADD 4 ; dst, u8, i8, tmp1
-    EXTRACT_POS_NEG     %3, %4, %1
-    paddusb             %1, %2                          ; add the positives
-    psubusb             %1, %4                          ; sub the negatives
-%endmacro
-
-; clip_u8(u8 - i8)
-%macro SIGN_SUB 4 ; dst, u8, i8, tmp1
-    EXTRACT_POS_NEG     %3, %1, %4
-    paddusb             %1, %2                          ; add the negatives
-    psubusb             %1, %4                          ; sub the positives
-%endmacro
-
-%macro FILTER6_INIT 4 ; %1=dst %2=h/l %3=cache, %4=stack_off
-    UNPACK          %2, %1, rp3, m0                     ; p3: B->W
-    mova [rsp+%4+0*mmsize*2], %1
-    paddw               %3, %1, %1                      ; p3*2
-    paddw               %3, %1                          ; p3*3
-    punpck%2bw          %1, m1,  m0                     ; p2: B->W
-    mova [rsp+%4+1*mmsize*2], %1
-    paddw               %3, %1                          ; p3*3 + p2
-    paddw               %3, %1                          ; p3*3 + p2*2
-    UNPACK          %2, %1, rp1, m0                     ; p1: B->W
-    mova [rsp+%4+2*mmsize*2], %1
-    paddw               %3, %1                          ; p3*3 + p2*2 + p1
-    UNPACK          %2, %1, rp0, m0                     ; p0: B->W
-    mova [rsp+%4+3*mmsize*2], %1
-    paddw               %3, %1                          ; p3*3 + p2*2 + p1 + p0
-    UNPACK          %2, %1, rq0, m0                     ; q0: B->W
-    mova [rsp+%4+4*mmsize*2], %1
-    paddw               %3, %1                          ; p3*3 + p2*2 + p1 + p0 + q0
-    paddw               %3, [pw_4]                      ; p3*3 + p2*2 + p1 + p0 + q0 + 4
-    psraw               %1, %3, 3                       ; (p3*3 + p2*2 + p1 + p0 + q0 + 4) >> 3
-%endmacro
-
-%macro FILTER14_INIT 4 ; %1=dst %2=h/l %3=cache, %4=stack_off
-    punpck%2bw          %1, m2, m0                      ; p7: B->W
-    mova [rsp+%4+ 8*mmsize*2], %1
-    psllw               %3, %1, 3                       ; p7*8
-    psubw               %3, %1                          ; p7*7
-    punpck%2bw          %1, m3, m0                      ; p6: B->W
-    mova [rsp+%4+ 9*mmsize*2], %1
-    paddw               %3, %1                          ; p7*7 + p6
-    paddw               %3, %1                          ; p7*7 + p6*2
-    UNPACK          %2, %1, rp5, m0                     ; p5: B->W
-    mova [rsp+%4+10*mmsize*2], %1
-    paddw               %3, %1                          ; p7*7 + p6*2 + p5
-    UNPACK          %2, %1, rp4, m0                     ; p4: B->W
-    mova [rsp+%4+11*mmsize*2], %1
-    paddw               %3, %1                          ; p7*7 + p6*2 + p5 + p4
-    paddw               %3, [rsp+%4+ 0*mmsize*2]        ; p7*7 + p6*2 + p5 + p4 + p3
-    paddw               %3, [rsp+%4+ 1*mmsize*2]        ; p7*7 + p6*2 + p5 + .. + p2
-    paddw               %3, [rsp+%4+ 2*mmsize*2]        ; p7*7 + p6*2 + p5 + .. + p1
-    paddw               %3, [rsp+%4+ 3*mmsize*2]        ; p7*7 + p6*2 + p5 + .. + p0
-    paddw               %3, [rsp+%4+ 4*mmsize*2]        ; p7*7 + p6*2 + p5 + .. + p0 + q0
-    paddw               %3, [pw_8]                      ; p7*7 + p6*2 + p5 + .. + p0 + q0 + 8
-    psraw               %1, %3, 4                       ; (p7*7 + p6*2 + p5 + .. + p0 + q0 + 8) >> 4
-%endmacro
-
-%macro TRANSPOSE16x16B 17
-    mova %17, m%16
-    SBUTTERFLY bw,  %1,  %2,  %16
-    SBUTTERFLY bw,  %3,  %4,  %16
-    SBUTTERFLY bw,  %5,  %6,  %16
-    SBUTTERFLY bw,  %7,  %8,  %16
-    SBUTTERFLY bw,  %9,  %10, %16
-    SBUTTERFLY bw,  %11, %12, %16
-    SBUTTERFLY bw,  %13, %14, %16
-    mova m%16,  %17
-    mova  %17, m%14
-    SBUTTERFLY bw,  %15, %16, %14
-    SBUTTERFLY wd,  %1,  %3,  %14
-    SBUTTERFLY wd,  %2,  %4,  %14
-    SBUTTERFLY wd,  %5,  %7,  %14
-    SBUTTERFLY wd,  %6,  %8,  %14
-    SBUTTERFLY wd,  %9,  %11, %14
-    SBUTTERFLY wd,  %10, %12, %14
-    SBUTTERFLY wd,  %13, %15, %14
-    mova m%14,  %17
-    mova  %17, m%12
-    SBUTTERFLY wd,  %14, %16, %12
-    SBUTTERFLY dq,  %1,  %5,  %12
-    SBUTTERFLY dq,  %2,  %6,  %12
-    SBUTTERFLY dq,  %3,  %7,  %12
-    SBUTTERFLY dq,  %4,  %8,  %12
-    SBUTTERFLY dq,  %9,  %13, %12
-    SBUTTERFLY dq,  %10, %14, %12
-    SBUTTERFLY dq,  %11, %15, %12
-    mova m%12, %17
-    mova  %17, m%8
-    SBUTTERFLY dq,  %12, %16, %8
-    SBUTTERFLY qdq, %1,  %9,  %8
-    SBUTTERFLY qdq, %2,  %10, %8
-    SBUTTERFLY qdq, %3,  %11, %8
-    SBUTTERFLY qdq, %4,  %12, %8
-    SBUTTERFLY qdq, %5,  %13, %8
-    SBUTTERFLY qdq, %6,  %14, %8
-    SBUTTERFLY qdq, %7,  %15, %8
-    mova m%8, %17
-    mova %17, m%1
-    SBUTTERFLY qdq, %8,  %16, %1
-    mova m%1, %17
-    SWAP %2,  %9
-    SWAP %3,  %5
-    SWAP %4,  %13
-    SWAP %6,  %11
-    SWAP %8,  %15
-    SWAP %12, %14
-%endmacro
-
-%macro TRANSPOSE8x8B 13
-    SBUTTERFLY bw,  %1, %2, %7
-    movdq%10 m%7, %9
-    movdqa %11, m%2
-    SBUTTERFLY bw,  %3, %4, %2
-    SBUTTERFLY bw,  %5, %6, %2
-    SBUTTERFLY bw,  %7, %8, %2
-    SBUTTERFLY wd,  %1, %3, %2
-    movdqa m%2, %11
-    movdqa %11, m%3
-    SBUTTERFLY wd,  %2, %4, %3
-    SBUTTERFLY wd,  %5, %7, %3
-    SBUTTERFLY wd,  %6, %8, %3
-    SBUTTERFLY dq, %1, %5, %3
-    SBUTTERFLY dq, %2, %6, %3
-    movdqa m%3, %11
-    movh   %12, m%2
-    movhps %13, m%2
-    SBUTTERFLY dq, %3, %7, %2
-    SBUTTERFLY dq, %4, %8, %2
-    SWAP %2, %5
-    SWAP %4, %7
-%endmacro
-
-%macro DEFINE_REAL_P7_TO_Q7 0-1 0
-%define P7 dstq  + 4*mstrideq  + %1
-%define P6 dstq  +   mstride3q + %1
-%define P5 dstq  + 2*mstrideq  + %1
-%define P4 dstq  +   mstrideq  + %1
-%define P3 dstq                + %1
-%define P2 dstq  +    strideq  + %1
-%define P1 dstq  + 2* strideq  + %1
-%define P0 dstq  +    stride3q + %1
-%define Q0 dstq  + 4* strideq  + %1
-%define Q1 dst2q +   mstride3q + %1
-%define Q2 dst2q + 2*mstrideq  + %1
-%define Q3 dst2q +   mstrideq  + %1
-%define Q4 dst2q               + %1
-%define Q5 dst2q +    strideq  + %1
-%define Q6 dst2q + 2* strideq  + %1
-%define Q7 dst2q +    stride3q + %1
-%endmacro
-
-%macro DEFINE_TRANSPOSED_P7_TO_Q7 0-1 0
-%define P3 rsp +  0*mmsize + %1
-%define P2 rsp +  1*mmsize + %1
-%define P1 rsp +  2*mmsize + %1
-%define P0 rsp +  3*mmsize + %1
-%define Q0 rsp +  4*mmsize + %1
-%define Q1 rsp +  5*mmsize + %1
-%define Q2 rsp +  6*mmsize + %1
-%define Q3 rsp +  7*mmsize + %1
-%if mmsize == 16
-%define P7 rsp +  8*mmsize + %1
-%define P6 rsp +  9*mmsize + %1
-%define P5 rsp + 10*mmsize + %1
-%define P4 rsp + 11*mmsize + %1
-%define Q4 rsp + 12*mmsize + %1
-%define Q5 rsp + 13*mmsize + %1
-%define Q6 rsp + 14*mmsize + %1
-%define Q7 rsp + 15*mmsize + %1
-%endif
-%endmacro
-
-; ..............AB -> AAAAAAAABBBBBBBB
-%macro SPLATB_MIX 1-2 [mask_mix]
-%if cpuflag(ssse3)
-    pshufb     %1, %2
-%else
-    punpcklbw  %1, %1
-    punpcklwd  %1, %1
-    punpckldq  %1, %1
-%endif
-%endmacro
-
-%macro LOOPFILTER 5 ; %1=v/h %2=size1 %3+%4=stack, %5=mmx/32bit stack only
-%assign %%ext 0
-%if ARCH_X86_32 || mmsize == 8
-%assign %%ext %5
-%endif
-
-%if UNIX64
-cglobal vp9_loop_filter_%1_%2_ %+ mmsize, 5, 9, 16, %3 + %4 + %%ext, dst, stride, E, I, H, mstride, dst2, stride3, mstride3
-%else
-%if WIN64
-cglobal vp9_loop_filter_%1_%2_ %+ mmsize, 4, 8, 16, %3 + %4 + %%ext, dst, stride, E, I, mstride, dst2, stride3, mstride3
-%else
-cglobal vp9_loop_filter_%1_%2_ %+ mmsize, 2, 6, 16, %3 + %4 + %%ext, dst, stride, mstride, dst2, stride3, mstride3
-%define Ed dword r2m
-%define Id dword r3m
-%endif
-%define Hd dword r4m
-%endif
-
-    mov               mstrideq, strideq
-    neg               mstrideq
-
-    lea               stride3q, [strideq*3]
-    lea              mstride3q, [mstrideq*3]
-
-%ifidn %1, h
-%if %2 != 16
-%if mmsize == 16
-%define movx movh
-%else
-%define movx mova
-%endif
-    lea                   dstq, [dstq + 4*strideq - 4]
-%else
-%define movx movu
-    lea                   dstq, [dstq + 4*strideq - 8] ; go from top center (h pos) to center left (v pos)
-%endif
-%else
-    lea                   dstq, [dstq + 4*mstrideq]
-%endif
-    ; FIXME we shouldn't need two dts registers if mmsize == 8
-    lea                  dst2q, [dstq + 8*strideq]
-
-    DEFINE_REAL_P7_TO_Q7
-
-%ifidn %1, h
-    movx                    m0, [P7]
-    movx                    m1, [P6]
-    movx                    m2, [P5]
-    movx                    m3, [P4]
-    movx                    m4, [P3]
-    movx                    m5, [P2]
-%if (ARCH_X86_64 && mmsize == 16) || %2 > 16
-    movx                    m6, [P1]
-%endif
-    movx                    m7, [P0]
-%ifdef m8
-    movx                    m8, [Q0]
-    movx                    m9, [Q1]
-    movx                   m10, [Q2]
-    movx                   m11, [Q3]
-    movx                   m12, [Q4]
-    movx                   m13, [Q5]
-    movx                   m14, [Q6]
-    movx                   m15, [Q7]
-    DEFINE_TRANSPOSED_P7_TO_Q7
-%if %2 == 16
-    TRANSPOSE16x16B 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, [rsp]
-    mova           [P7],  m0
-    mova           [P6],  m1
-    mova           [P5],  m2
-    mova           [P4],  m3
-%else ; %2 == 44/48/84/88
-    ; 8x16 transpose
-    punpcklbw        m0,  m1
-    punpcklbw        m2,  m3
-    punpcklbw        m4,  m5
-    punpcklbw        m6,  m7
-    punpcklbw        m8,  m9
-    punpcklbw       m10, m11
-    punpcklbw       m12, m13
-    punpcklbw       m14, m15
-    TRANSPOSE8x8W     0, 2, 4, 6, 8, 10, 12, 14, 15
-    SWAP              0,  4
-    SWAP              2,  5
-    SWAP              0,  6
-    SWAP              0,  7
-    SWAP             10,  9
-    SWAP             12, 10
-    SWAP             14, 11
-%endif ; %2
-    mova           [P3],  m4
-    mova           [P2],  m5
-    mova           [P1],  m6
-    mova           [P0],  m7
-    mova           [Q0],  m8
-    mova           [Q1],  m9
-    mova           [Q2], m10
-    mova           [Q3], m11
-%if %2 == 16
-    mova           [Q4], m12
-    mova           [Q5], m13
-    mova           [Q6], m14
-    mova           [Q7], m15
-%endif ; %2
-%else ; x86-32
-%if %2 == 16
-    TRANSPOSE8x8B    0, 1, 2, 3, 4, 5, 6, 7, [P1], u, [rsp+%3+%4], [rsp+64], [rsp+80]
-    DEFINE_TRANSPOSED_P7_TO_Q7
-    movh          [P7], m0
-    movh          [P5], m1
-    movh          [P3], m2
-    movh          [P1], m3
-    movh          [Q2], m5
-    movh          [Q4], m6
-    movh          [Q6], m7
-    movhps        [P6], m0
-    movhps        [P4], m1
-    movhps        [P2], m2
-    movhps        [P0], m3
-    movhps        [Q3], m5
-    movhps        [Q5], m6
-    movhps        [Q7], m7
-    DEFINE_REAL_P7_TO_Q7
-    movx                    m0, [Q0]
-    movx                    m1, [Q1]
-    movx                    m2, [Q2]
-    movx                    m3, [Q3]
-    movx                    m4, [Q4]
-    movx                    m5, [Q5]
-    movx                    m7, [Q7]
-    TRANSPOSE8x8B 0, 1, 2, 3, 4, 5, 6, 7, [Q6], u, [rsp+%3+%4], [rsp+72], [rsp+88]
-    DEFINE_TRANSPOSED_P7_TO_Q7 8
-    movh          [P7], m0
-    movh          [P5], m1
-    movh          [P3], m2
-    movh          [P1], m3
-    movh          [Q2], m5
-    movh          [Q4], m6
-    movh          [Q6], m7
-    movhps        [P6], m0
-    movhps        [P4], m1
-    movhps        [P2], m2
-    movhps        [P0], m3
-    movhps        [Q3], m5
-    movhps        [Q5], m6
-    movhps        [Q7], m7
-    DEFINE_TRANSPOSED_P7_TO_Q7
-%elif %2 > 16 ; %2 == 44/48/84/88
-    punpcklbw        m0, m1
-    punpcklbw        m2, m3
-    punpcklbw        m4, m5
-    punpcklbw        m6, m7
-    movx             m1, [Q0]
-    movx             m3, [Q1]
-    movx             m5, [Q2]
-    movx             m7, [Q3]
-    punpcklbw        m1, m3
-    punpcklbw        m5, m7
-    movx             m3, [Q4]
-    movx             m7, [Q5]
-    punpcklbw        m3, m7
-    mova          [rsp], m3
-    movx             m3, [Q6]
-    movx             m7, [Q7]
-    punpcklbw        m3, m7
-    DEFINE_TRANSPOSED_P7_TO_Q7
-    TRANSPOSE8x8W     0, 2, 4, 6, 1, 5, 7, 3, [rsp], [Q0], 1
-    mova           [P3],  m0
-    mova           [P2],  m2
-    mova           [P1],  m4
-    mova           [P0],  m6
-    mova           [Q1],  m5
-    mova           [Q2],  m7
-    mova           [Q3],  m3
-%else ; %2 == 4 || %2 == 8
-    SBUTTERFLY       bw, 0, 1, 6
-    SBUTTERFLY       bw, 2, 3, 6
-    SBUTTERFLY       bw, 4, 5, 6
-    mova [rsp+4*mmsize], m5
-    mova             m6, [P1]
-    SBUTTERFLY       bw, 6, 7, 5
-    DEFINE_TRANSPOSED_P7_TO_Q7
-    TRANSPOSE4x4W     0, 2, 4, 6, 5
-    mova           [P3], m0
-    mova           [P2], m2
-    mova           [P1], m4
-    mova           [P0], m6
-    mova             m5, [rsp+4*mmsize]
-    TRANSPOSE4x4W     1, 3, 5, 7, 0
-    mova           [Q0], m1
-    mova           [Q1], m3
-    mova           [Q2], m5
-    mova           [Q3], m7
-%endif ; %2
-%endif ; x86-32/64
-%endif ; %1 == h
-
-    ; calc fm mask
-%if %2 == 16 || mmsize == 8
-%if cpuflag(ssse3)
-    pxor                m0, m0
-%endif
-    SPLATB_REG          m2, I, m0                       ; I I I I ...
-    SPLATB_REG          m3, E, m0                       ; E E E E ...
-%else
-%if cpuflag(ssse3)
-    mova                m0, [mask_mix]
-%endif
-    movd                m2, Id
-    movd                m3, Ed
-    SPLATB_MIX          m2, m0
-    SPLATB_MIX          m3, m0
-%endif
-    mova                m0, [pb_80]
-    pxor                m2, m0
-    pxor                m3, m0
-%ifdef m8
-%ifidn %1, v
-    mova                m8, [P3]
-    mova                m9, [P2]
-    mova               m10, [P1]
-    mova               m11, [P0]
-    mova               m12, [Q0]
-    mova               m13, [Q1]
-    mova               m14, [Q2]
-    mova               m15, [Q3]
-%else
-    ; In case of horizontal, P3..Q3 are already present in some registers due
-    ; to the previous transpose, so we just swap registers.
-    SWAP                 8,  4, 12
-    SWAP                 9,  5, 13
-    SWAP                10,  6, 14
-    SWAP                11,  7, 15
-%endif
-%define rp3 m8
-%define rp2 m9
-%define rp1 m10
-%define rp0 m11
-%define rq0 m12
-%define rq1 m13
-%define rq2 m14
-%define rq3 m15
-%else
-%define rp3 [P3]
-%define rp2 [P2]
-%define rp1 [P1]
-%define rp0 [P0]
-%define rq0 [Q0]
-%define rq1 [Q1]
-%define rq2 [Q2]
-%define rq3 [Q3]
-%endif
-    ABSSUB_GT           m5, rp3, rp2, m2, m7, m0        ; m5 = abs(p3-p2) <= I
-    ABSSUB_GT           m1, rp2, rp1, m2, m7, m0        ; m1 = abs(p2-p1) <= I
-    por                 m5, m1
-    ABSSUB_GT           m1, rp1, rp0, m2, m7, m0        ; m1 = abs(p1-p0) <= I
-    por                 m5, m1
-    ABSSUB_GT           m1, rq0, rq1, m2, m7, m0        ; m1 = abs(q1-q0) <= I
-    por                 m5, m1
-    ABSSUB_GT           m1, rq1, rq2, m2, m7, m0        ; m1 = abs(q2-q1) <= I
-    por                 m5, m1
-    ABSSUB_GT           m1, rq2, rq3, m2, m7, m0        ; m1 = abs(q3-q2) <= I
-    por                 m5, m1
-    ABSSUB              m1, rp0, rq0, m7                ; abs(p0-q0)
-    paddusb             m1, m1                          ; abs(p0-q0) * 2
-    ABSSUB              m2, rp1, rq1, m7                ; abs(p1-q1)
-    pand                m2, [pb_fe]                     ; drop lsb so shift can work
-    psrlq               m2, 1                           ; abs(p1-q1)/2
-    paddusb             m1, m2                          ; abs(p0-q0)*2 + abs(p1-q1)/2
-    pxor                m1, m0
-    pcmpgtb             m1, m3
-    por                 m1, m5                          ; fm final value
-    SWAP                 1, 3
-    pxor                m3, [pb_ff]
-
-    ; (m3: fm, m8..15: p3 p2 p1 p0 q0 q1 q2 q3)
-    ; calc flat8in (if not 44_16) and hev masks
-%if %2 != 44 && %2 != 4
-    mova                m6, [pb_81]                     ; [1 1 1 1 ...] ^ 0x80
-    ABSSUB_GT           m2, rp3, rp0, m6, m5            ; abs(p3 - p0) <= 1
-%ifdef m8
-    mova                m8, [pb_80]
-%define rb80 m8
-%else
-%define rb80 [pb_80]
-%endif
-    ABSSUB_GT           m1, rp2, rp0, m6, m5, rb80      ; abs(p2 - p0) <= 1
-    por                 m2, m1
-    ABSSUB              m4, rp1, rp0, m5                ; abs(p1 - p0)
-%if %2 <= 16
-%if cpuflag(ssse3)
-    pxor                m0, m0
-%endif
-    SPLATB_REG          m7, H, m0                       ; H H H H ...
-%else
-    movd                m7, Hd
-    SPLATB_MIX          m7
-%endif
-    pxor                m7, rb80
-    pxor                m4, rb80
-    pcmpgtb             m0, m4, m7                      ; abs(p1 - p0) > H (1/2 hev condition)
-    CMP_GT              m4, m6                          ; abs(p1 - p0) <= 1
-    por                 m2, m4                          ; (flat8in)
-    ABSSUB              m4, rq1, rq0, m1                ; abs(q1 - q0)
-    pxor                m4, rb80
-    pcmpgtb             m5, m4, m7                      ; abs(q1 - q0) > H (2/2 hev condition)
-    por                 m0, m5                          ; hev final value
-    CMP_GT              m4, m6                          ; abs(q1 - q0) <= 1
-    por                 m2, m4                          ; (flat8in)
-    ABSSUB_GT           m1, rq2, rq0, m6, m5, rb80      ; abs(q2 - q0) <= 1
-    por                 m2, m1
-    ABSSUB_GT           m1, rq3, rq0, m6, m5, rb80      ; abs(q3 - q0) <= 1
-    por                 m2, m1                          ; flat8in final value
-    pxor                m2, [pb_ff]
-%if %2 == 84 || %2 == 48
-    pand                m2, [mask_mix%2]
-%endif
-%else
-    mova                m6, [pb_80]
-%if %2 == 44
-    movd                m7, Hd
-    SPLATB_MIX          m7
-%else
-%if cpuflag(ssse3)
-    pxor                m0, m0
-%endif
-    SPLATB_REG          m7, H, m0                       ; H H H H ...
-%endif
-    pxor                m7, m6
-    ABSSUB              m4, rp1, rp0, m1                ; abs(p1 - p0)
-    pxor                m4, m6
-    pcmpgtb             m0, m4, m7                      ; abs(p1 - p0) > H (1/2 hev condition)
-    ABSSUB              m4, rq1, rq0, m1                ; abs(q1 - q0)
-    pxor                m4, m6
-    pcmpgtb             m5, m4, m7                      ; abs(q1 - q0) > H (2/2 hev condition)
-    por                 m0, m5                          ; hev final value
-%endif
-
-%if %2 == 16
-    ; (m0: hev, m2: flat8in, m3: fm, m6: pb_81, m9..15: p2 p1 p0 q0 q1 q2 q3)
-    ; calc flat8out mask
-%ifdef m8
-    mova                m8, [P7]
-    mova                m9, [P6]
-%define rp7 m8
-%define rp6 m9
-%else
-%define rp7 [P7]
-%define rp6 [P6]
-%endif
-    ABSSUB_GT           m1, rp7, rp0, m6, m5            ; abs(p7 - p0) <= 1
-    ABSSUB_GT           m7, rp6, rp0, m6, m5            ; abs(p6 - p0) <= 1
-    por                 m1, m7
-%ifdef m8
-    mova                m8, [P5]
-    mova                m9, [P4]
-%define rp5 m8
-%define rp4 m9
-%else
-%define rp5 [P5]
-%define rp4 [P4]
-%endif
-    ABSSUB_GT           m7, rp5, rp0, m6, m5            ; abs(p5 - p0) <= 1
-    por                 m1, m7
-    ABSSUB_GT           m7, rp4, rp0, m6, m5            ; abs(p4 - p0) <= 1
-    por                 m1, m7
-%ifdef m8
-    mova                m14, [Q4]
-    mova                m15, [Q5]
-%define rq4 m14
-%define rq5 m15
-%else
-%define rq4 [Q4]
-%define rq5 [Q5]
-%endif
-    ABSSUB_GT           m7, rq4, rq0, m6, m5            ; abs(q4 - q0) <= 1
-    por                 m1, m7
-    ABSSUB_GT           m7, rq5, rq0, m6, m5            ; abs(q5 - q0) <= 1
-    por                 m1, m7
-%ifdef m8
-    mova                m14, [Q6]
-    mova                m15, [Q7]
-%define rq6 m14
-%define rq7 m15
-%else
-%define rq6 [Q6]
-%define rq7 [Q7]
-%endif
-    ABSSUB_GT           m7, rq6, rq0, m6, m5            ; abs(q4 - q0) <= 1
-    por                 m1, m7
-    ABSSUB_GT           m7, rq7, rq0, m6, m5            ; abs(q5 - q0) <= 1
-    por                 m1, m7                          ; flat8out final value
-    pxor                m1, [pb_ff]
-%endif
-
-    ; if (fm) {
-    ;     if (out && in) filter_14()
-    ;     else if (in)   filter_6()
-    ;     else if (hev)  filter_2()
-    ;     else           filter_4()
-    ; }
-    ;
-    ; f14:                                                                            fm &  out &  in
-    ; f6:  fm & ~f14 & in        => fm & ~(out & in) & in                          => fm & ~out &  in
-    ; f2:  fm & ~f14 & ~f6 & hev => fm & ~(out & in) & ~(~out & in) & hev          => fm &  ~in &  hev
-    ; f4:  fm & ~f14 & ~f6 & ~f2 => fm & ~(out & in) & ~(~out & in) & ~(~in & hev) => fm &  ~in & ~hev
-
-    ; (m0: hev, [m1: flat8out], [m2: flat8in], m3: fm, m8..15: p5 p4 p1 p0 q0 q1 q6 q7)
-    ; filter2()
-%if %2 != 44 && %2 != 4
-    mova                m6, [pb_80]                     ; already in m6 if 44_16
-    SCRATCH              2, 15, rsp+%3+%4
-%if %2 == 16
-    SCRATCH              1,  8, rsp+%3+%4+16
-%endif
-%endif
-    pxor                m2, m6, rq0                     ; q0 ^ 0x80
-    pxor                m4, m6, rp0                     ; p0 ^ 0x80
-    psubsb              m2, m4                          ; (signed) q0 - p0
-    pxor                m4, m6, rp1                     ; p1 ^ 0x80
-    pxor                m5, m6, rq1                     ; q1 ^ 0x80
-    psubsb              m4, m5                          ; (signed) p1 - q1
-    paddsb              m4, m2                          ;   (q0 - p0) + (p1 - q1)
-    paddsb              m4, m2                          ; 2*(q0 - p0) + (p1 - q1)
-    paddsb              m4, m2                          ; 3*(q0 - p0) + (p1 - q1)
-    paddsb              m6, m4, [pb_4]                  ; m6: f1 = clip(f + 4, 127)
-    paddsb              m4, [pb_3]                      ; m4: f2 = clip(f + 3, 127)
-%ifdef m8
-    mova                m14, [pb_10]                    ; will be reused in filter4()
-%define rb10 m14
-%else
-%define rb10 [pb_10]
-%endif
-    SRSHIFT3B_2X        m6, m4, rb10, m7                ; f1 and f2 sign byte shift by 3
-    SIGN_SUB            m7, rq0, m6, m5                 ; m7 = q0 - f1
-    SIGN_ADD            m1, rp0, m4, m5                 ; m1 = p0 + f2
-%if %2 != 44 && %2 != 4
-%ifdef m8
-    pandn               m6, m15, m3                     ;  ~mask(in) & mask(fm)
-%else
-    mova                m6, [rsp+%3+%4]
-    pandn               m6, m3
-%endif
-    pand                m6, m0                          ; (~mask(in) & mask(fm)) & mask(hev)
-%else
-    pand                m6, m3, m0
-%endif
-    MASK_APPLY          m7, rq0, m6, m5                 ; m7 = filter2(q0) & mask / we write it in filter4()
-    MASK_APPLY          m1, rp0, m6, m5                 ; m1 = filter2(p0) & mask / we write it in filter4()
-
-    ; (m0: hev, m1: p0', m2: q0-p0, m3: fm, m7: q0', [m8: flat8out], m10..13: p1 p0 q0 q1, m14: pb_10, [m15: flat8in], )
-    ; filter4()
-    mova                m4, m2
-    paddsb              m2, m4                          ; 2 * (q0 - p0)
-    paddsb              m2, m4                          ; 3 * (q0 - p0)
-    paddsb              m6, m2, [pb_4]                  ; m6:  f1 = clip(f + 4, 127)
-    paddsb              m2, [pb_3]                      ; m2: f2 = clip(f + 3, 127)
-    SRSHIFT3B_2X        m6, m2, rb10, m4                ; f1 and f2 sign byte shift by 3
-%if %2 != 44 && %2 != 4
-%ifdef m8
-    pandn               m5, m15, m3                     ;               ~mask(in) & mask(fm)
-%else
-    mova                m5, [rsp+%3+%4]
-    pandn               m5, m3
-%endif
-    pandn               m0, m5                          ; ~mask(hev) & (~mask(in) & mask(fm))
-%else
-    pandn               m0, m3
-%endif
-    SIGN_SUB            m5, rq0, m6, m4                 ; q0 - f1
-    MASK_APPLY          m5, m7, m0, m4                  ; filter4(q0) & mask
-    mova                [Q0], m5
-    SIGN_ADD            m7, rp0, m2, m4                 ; p0 + f2
-    MASK_APPLY          m7, m1, m0, m4                  ; filter4(p0) & mask
-    mova                [P0], m7
-    paddb               m6, [pb_80]                     ;
-    pxor                m1, m1                          ;   f=(f1+1)>>1
-    pavgb               m6, m1                          ;
-    psubb               m6, [pb_40]                     ;
-    SIGN_ADD            m1, rp1, m6, m2                 ; p1 + f
-    SIGN_SUB            m4, rq1, m6, m2                 ; q1 - f
-    MASK_APPLY          m1, rp1, m0, m2                 ; m1 = filter4(p1)
-    MASK_APPLY          m4, rq1, m0, m2                 ; m4 = filter4(q1)
-    mova                [P1], m1
-    mova                [Q1], m4
-
-%if %2 != 44 && %2 != 4
-    UNSCRATCH            2, 15, rsp+%3+%4
-%endif
-
-    ; ([m1: flat8out], m2: flat8in, m3: fm, m10..13: p1 p0 q0 q1)
-    ; filter6()
-%if %2 != 44 && %2 != 4
-    pxor                m0, m0
-%if %2 != 16
-    pand                m3, m2
-%else
-    pand                m2, m3                          ;               mask(fm) & mask(in)
-%ifdef m8
-    pandn               m3, m8, m2                      ; ~mask(out) & (mask(fm) & mask(in))
-%else
-    mova                m3, [rsp+%3+%4+16]
-    pandn               m3, m2
-%endif
-%endif
-%ifdef m8
-    mova               m14, [P3]
-    mova                m9, [Q3]
-%define rp3 m14
-%define rq3 m9
-%else
-%define rp3 [P3]
-%define rq3 [Q3]
-%endif
-    mova                m1, [P2]
-    FILTER_INIT         m4, m5, m6, m7, [P2], %4, 6,             m3,  m1             ; [p2]
-    mova                m1, [Q2]
-    FILTER_UPDATE       m4, m5, m6, m7, [P1], %4, 0, 1, 2, 5, 3, m3,  "", rq1, "", 1 ; [p1] -p3 -p2 +p1 +q1
-    FILTER_UPDATE       m4, m5, m6, m7, [P0], %4, 0, 2, 3, 6, 3, m3,  "", m1         ; [p0] -p3 -p1 +p0 +q2
-    FILTER_UPDATE       m4, m5, m6, m7, [Q0], %4, 0, 3, 4, 7, 3, m3,  "", rq3, "", 1 ; [q0] -p3 -p0 +q0 +q3
-    FILTER_UPDATE       m4, m5, m6, m7, [Q1], %4, 1, 4, 5, 7, 3, m3,  ""             ; [q1] -p2 -q0 +q1 +q3
-    FILTER_UPDATE       m4, m5, m6, m7, [Q2], %4, 2, 5, 6, 7, 3, m3,  m1             ; [q2] -p1 -q1 +q2 +q3
-%endif
-
-%if %2 == 16
-    UNSCRATCH            1,  8, rsp+%3+%4+16
-%endif
-
-    ; (m0: 0, [m1: flat8out], m2: fm & flat8in, m8..15: q2 q3 p1 p0 q0 q1 p3 p2)
-    ; filter14()
-    ;
-    ;                            m2  m3  m8  m9 m14 m15 m10 m11 m12 m13
-    ;
-    ;                                    q2  q3  p3  p2  p1  p0  q0  q1
-    ; p6  -7                     p7  p6  p5  p4   .   .   .   .   .
-    ; p5  -6  -p7 -p6 +p5 +q1     .   .   .                           .
-    ; p4  -5  -p7 -p5 +p4 +q2     .       .   .                      q2
-    ; p3  -4  -p7 -p4 +p3 +q3     .           .   .                  q3
-    ; p2  -3  -p7 -p3 +p2 +q4     .               .   .              q4
-    ; p1  -2  -p7 -p2 +p1 +q5     .                   .   .          q5
-    ; p0  -1  -p7 -p1 +p0 +q6     .                       .   .      q6
-    ; q0  +0  -p7 -p0 +q0 +q7     .                           .   .  q7
-    ; q1  +1  -p6 -q0 +q1 +q7    q1   .                           .   .
-    ; q2  +2  -p5 -q1 +q2 +q7     .  q2   .                           .
-    ; q3  +3  -p4 -q2 +q3 +q7         .  q3   .                       .
-    ; q4  +4  -p3 -q3 +q4 +q7             .  q4   .                   .
-    ; q5  +5  -p2 -q4 +q5 +q7                 .  q5   .               .
-    ; q6  +6  -p1 -q5 +q6 +q7                     .  q6   .           .
-
-%if %2 == 16
-    pand            m1, m2                                                              ; mask(out) & (mask(fm) & mask(in))
-    mova            m2, [P7]
-    mova            m3, [P6]
-%ifdef m8
-    mova            m8, [P5]
-    mova            m9, [P4]
-%define rp5 m8
-%define rp4 m9
-%define rp5s m8
-%define rp4s m9
-%define rp3s m14
-%define rq4 m8
-%define rq5 m9
-%define rq6 m14
-%define rq7 m15
-%define rq4s m8
-%define rq5s m9
-%define rq6s m14
-%else
-%define rp5 [P5]
-%define rp4 [P4]
-%define rp5s ""
-%define rp4s ""
-%define rp3s ""
-%define rq4 [Q4]
-%define rq5 [Q5]
-%define rq6 [Q6]
-%define rq7 [Q7]
-%define rq4s ""
-%define rq5s ""
-%define rq6s ""
-%endif
-    FILTER_INIT     m4, m5, m6, m7, [P6], %4, 14,                m1,  m3            ; [p6]
-    FILTER_UPDATE   m4, m5, m6, m7, [P5], %4,  8,  9, 10,  5, 4, m1, rp5s           ; [p5] -p7 -p6 +p5 +q1
-    FILTER_UPDATE   m4, m5, m6, m7, [P4], %4,  8, 10, 11,  6, 4, m1, rp4s           ; [p4] -p7 -p5 +p4 +q2
-    FILTER_UPDATE   m4, m5, m6, m7, [P3], %4,  8, 11,  0,  7, 4, m1, rp3s           ; [p3] -p7 -p4 +p3 +q3
-    FILTER_UPDATE   m4, m5, m6, m7, [P2], %4,  8,  0,  1, 12, 4, m1,  "", rq4, [Q4], 1 ; [p2] -p7 -p3 +p2 +q4
-    FILTER_UPDATE   m4, m5, m6, m7, [P1], %4,  8,  1,  2, 13, 4, m1,  "", rq5, [Q5], 1 ; [p1] -p7 -p2 +p1 +q5
-    FILTER_UPDATE   m4, m5, m6, m7, [P0], %4,  8,  2,  3, 14, 4, m1,  "", rq6, [Q6], 1 ; [p0] -p7 -p1 +p0 +q6
-    FILTER_UPDATE   m4, m5, m6, m7, [Q0], %4,  8,  3,  4, 15, 4, m1,  "", rq7, [Q7], 1 ; [q0] -p7 -p0 +q0 +q7
-    FILTER_UPDATE   m4, m5, m6, m7, [Q1], %4,  9,  4,  5, 15, 4, m1,  ""            ; [q1] -p6 -q0 +q1 +q7
-    FILTER_UPDATE   m4, m5, m6, m7, [Q2], %4, 10,  5,  6, 15, 4, m1,  ""            ; [q2] -p5 -q1 +q2 +q7
-    FILTER_UPDATE   m4, m5, m6, m7, [Q3], %4, 11,  6,  7, 15, 4, m1,  ""            ; [q3] -p4 -q2 +q3 +q7
-    FILTER_UPDATE   m4, m5, m6, m7, [Q4], %4,  0,  7, 12, 15, 4, m1, rq4s           ; [q4] -p3 -q3 +q4 +q7
-    FILTER_UPDATE   m4, m5, m6, m7, [Q5], %4,  1, 12, 13, 15, 4, m1, rq5s           ; [q5] -p2 -q4 +q5 +q7
-    FILTER_UPDATE   m4, m5, m6, m7, [Q6], %4,  2, 13, 14, 15, 4, m1, rq6s           ; [q6] -p1 -q5 +q6 +q7
-%endif
-
-%ifidn %1, h
-%if %2 == 16
-    mova                    m0, [P7]
-    mova                    m1, [P6]
-    mova                    m2, [P5]
-    mova                    m3, [P4]
-    mova                    m4, [P3]
-    mova                    m5, [P2]
-%if ARCH_X86_64
-    mova                    m6, [P1]
-%endif
-    mova                    m7, [P0]
-%if ARCH_X86_64
-    mova                    m8, [Q0]
-    mova                    m9, [Q1]
-    mova                   m10, [Q2]
-    mova                   m11, [Q3]
-    mova                   m12, [Q4]
-    mova                   m13, [Q5]
-    mova                   m14, [Q6]
-    mova                   m15, [Q7]
-    TRANSPOSE16x16B 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, [rsp]
-    DEFINE_REAL_P7_TO_Q7
-    movu  [P7],  m0
-    movu  [P6],  m1
-    movu  [P5],  m2
-    movu  [P4],  m3
-    movu  [P3],  m4
-    movu  [P2],  m5
-    movu  [P1],  m6
-    movu  [P0],  m7
-    movu  [Q0],  m8
-    movu  [Q1],  m9
-    movu  [Q2], m10
-    movu  [Q3], m11
-    movu  [Q4], m12
-    movu  [Q5], m13
-    movu  [Q6], m14
-    movu  [Q7], m15
-%else
-    DEFINE_REAL_P7_TO_Q7
-    TRANSPOSE8x8B 0, 1, 2, 3, 4, 5, 6, 7, [rsp+32], a, [rsp+%3+%4], [Q0], [Q1]
-    movh   [P7],  m0
-    movh   [P5],  m1
-    movh   [P3],  m2
-    movh   [P1],  m3
-    movh   [Q2],  m5
-    movh   [Q4],  m6
-    movh   [Q6],  m7
-    movhps [P6],  m0
-    movhps [P4],  m1
-    movhps [P2],  m2
-    movhps [P0],  m3
-    movhps [Q3],  m5
-    movhps [Q5],  m6
-    movhps [Q7],  m7
-    DEFINE_TRANSPOSED_P7_TO_Q7
-    mova                    m0, [Q0]
-    mova                    m1, [Q1]
-    mova                    m2, [Q2]
-    mova                    m3, [Q3]
-    mova                    m4, [Q4]
-    mova                    m5, [Q5]
-    mova                    m7, [Q7]
-    DEFINE_REAL_P7_TO_Q7 8
-    TRANSPOSE8x8B 0, 1, 2, 3, 4, 5, 6, 7, [rsp+224], a, [rsp+%3+%4], [Q0], [Q1]
-    movh   [P7],  m0
-    movh   [P5],  m1
-    movh   [P3],  m2
-    movh   [P1],  m3
-    movh   [Q2],  m5
-    movh   [Q4],  m6
-    movh   [Q6],  m7
-    movhps [P6],  m0
-    movhps [P4],  m1
-    movhps [P2],  m2
-    movhps [P0],  m3
-    movhps [Q3],  m5
-    movhps [Q5],  m6
-    movhps [Q7],  m7
-%endif
-%elif %2 == 44 || %2 == 4
-    SWAP 0, 1   ; m0 = p1
-    SWAP 1, 7   ; m1 = p0
-    SWAP 2, 5   ; m2 = q0
-    SWAP 3, 4   ; m3 = q1
-    DEFINE_REAL_P7_TO_Q7 2
-    SBUTTERFLY  bw, 0, 1, 4
-    SBUTTERFLY  bw, 2, 3, 4
-    SBUTTERFLY  wd, 0, 2, 4
-    SBUTTERFLY  wd, 1, 3, 4
-%if mmsize == 16
-    movd  [P7], m0
-    movd  [P3], m2
-    movd  [Q0], m1
-    movd  [Q4], m3
-    psrldq  m0, 4
-    psrldq  m1, 4
-    psrldq  m2, 4
-    psrldq  m3, 4
-    movd  [P6], m0
-    movd  [P2], m2
-    movd  [Q1], m1
-    movd  [Q5], m3
-    psrldq  m0, 4
-    psrldq  m1, 4
-    psrldq  m2, 4
-    psrldq  m3, 4
-    movd  [P5], m0
-    movd  [P1], m2
-    movd  [Q2], m1
-    movd  [Q6], m3
-    psrldq  m0, 4
-    psrldq  m1, 4
-    psrldq  m2, 4
-    psrldq  m3, 4
-    movd  [P4], m0
-    movd  [P0], m2
-    movd  [Q3], m1
-    movd  [Q7], m3
-%else
-    movd  [P7], m0
-    movd  [P5], m2
-    movd  [P3], m1
-    movd  [P1], m3
-    psrlq   m0, 32
-    psrlq   m2, 32
-    psrlq   m1, 32
-    psrlq   m3, 32
-    movd  [P6], m0
-    movd  [P4], m2
-    movd  [P2], m1
-    movd  [P0], m3
-%endif
-%else
-    ; the following code do a transpose of 8 full lines to 16 half
-    ; lines (high part). It is inlined to avoid the need of a staging area
-    mova                    m0, [P3]
-    mova                    m1, [P2]
-    mova                    m2, [P1]
-    mova                    m3, [P0]
-    mova                    m4, [Q0]
-    mova                    m5, [Q1]
-%ifdef m8
-    mova                    m6, [Q2]
-%endif
-    mova                    m7, [Q3]
-    DEFINE_REAL_P7_TO_Q7
-%ifdef m8
-    SBUTTERFLY  bw,  0,  1, 8
-    SBUTTERFLY  bw,  2,  3, 8
-    SBUTTERFLY  bw,  4,  5, 8
-    SBUTTERFLY  bw,  6,  7, 8
-    SBUTTERFLY  wd,  0,  2, 8
-    SBUTTERFLY  wd,  1,  3, 8
-    SBUTTERFLY  wd,  4,  6, 8
-    SBUTTERFLY  wd,  5,  7, 8
-    SBUTTERFLY  dq,  0,  4, 8
-    SBUTTERFLY  dq,  1,  5, 8
-    SBUTTERFLY  dq,  2,  6, 8
-    SBUTTERFLY  dq,  3,  7, 8
-%else
-    SBUTTERFLY  bw,  0,  1, 6
-    mova [rsp+mmsize*4], m1
-    mova        m6, [rsp+mmsize*6]
-    SBUTTERFLY  bw,  2,  3, 1
-    SBUTTERFLY  bw,  4,  5, 1
-    SBUTTERFLY  bw,  6,  7, 1
-    SBUTTERFLY  wd,  0,  2, 1
-    mova [rsp+mmsize*6], m2
-    mova        m1, [rsp+mmsize*4]
-    SBUTTERFLY  wd,  1,  3, 2
-    SBUTTERFLY  wd,  4,  6, 2
-    SBUTTERFLY  wd,  5,  7, 2
-    SBUTTERFLY  dq,  0,  4, 2
-    SBUTTERFLY  dq,  1,  5, 2
-%if mmsize == 16
-    movh      [Q0], m1
-    movhps    [Q1], m1
-%else
-    mova      [P3], m1
-%endif
-    mova        m2, [rsp+mmsize*6]
-    SBUTTERFLY  dq,  2,  6, 1
-    SBUTTERFLY  dq,  3,  7, 1
-%endif
-    SWAP         3, 6
-    SWAP         1, 4
-%if mmsize == 16
-    movh      [P7], m0
-    movhps    [P6], m0
-    movh      [P5], m1
-    movhps    [P4], m1
-    movh      [P3], m2
-    movhps    [P2], m2
-    movh      [P1], m3
-    movhps    [P0], m3
-%ifdef m8
-    movh      [Q0], m4
-    movhps    [Q1], m4
-%endif
-    movh      [Q2], m5
-    movhps    [Q3], m5
-    movh      [Q4], m6
-    movhps    [Q5], m6
-    movh      [Q6], m7
-    movhps    [Q7], m7
-%else
-    mova      [P7], m0
-    mova      [P6], m1
-    mova      [P5], m2
-    mova      [P4], m3
-    mova      [P2], m5
-    mova      [P1], m6
-    mova      [P0], m7
-%endif
-%endif
-%endif
-
-    RET
-%endmacro
-
-%macro LPF_16_VH 5
-INIT_XMM %5
-LOOPFILTER v, %1, %2,  0, %4
-LOOPFILTER h, %1, %2, %3, %4
-%endmacro
-
-%macro LPF_16_VH_ALL_OPTS 4
-LPF_16_VH %1, %2, %3, %4, sse2
-LPF_16_VH %1, %2, %3, %4, ssse3
-LPF_16_VH %1, %2, %3, %4, avx
-%endmacro
-
-LPF_16_VH_ALL_OPTS 16, 512, 256, 32
-LPF_16_VH_ALL_OPTS 44,   0, 128,  0
-LPF_16_VH_ALL_OPTS 48, 256, 128, 16
-LPF_16_VH_ALL_OPTS 84, 256, 128, 16
-LPF_16_VH_ALL_OPTS 88, 256, 128, 16
-
-INIT_MMX mmxext
-LOOPFILTER v, 4,   0,  0, 0
-LOOPFILTER h, 4,   0, 64, 0
-LOOPFILTER v, 8, 128,  0, 8
-LOOPFILTER h, 8, 128, 64, 8
diff -uparN ffmpeg-4.1/libavcodec/x86/vp9mc_16bpp.asm ffmpeg-y/libavcodec/x86/vp9mc_16bpp.asm
--- ffmpeg-4.1/libavcodec/x86/vp9mc_16bpp.asm	2018-07-17 17:27:41.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/vp9mc_16bpp.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,431 +0,0 @@
-;******************************************************************************
-;* VP9 MC SIMD optimizations
-;*
-;* Copyright (c) 2015 Ronald S. Bultje <rsbultje gmail com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA 32
-
-pd_64: times 8 dd 64
-
-cextern pw_1023
-cextern pw_4095
-
-SECTION .text
-
-%macro filter_h4_fn 1-2 12
-cglobal vp9_%1_8tap_1d_h_4_10, 6, 6, %2, dst, dstride, src, sstride, h, filtery
-    mova        m5, [pw_1023]
-.body:
-%if notcpuflag(sse4) && ARCH_X86_64
-    pxor       m11, m11
-%endif
-    mova        m6, [pd_64]
-    mova        m7, [filteryq+ 0]
-%if ARCH_X86_64 && mmsize > 8
-    mova        m8, [filteryq+32]
-    mova        m9, [filteryq+64]
-    mova       m10, [filteryq+96]
-%endif
-.loop:
-    movh        m0, [srcq-6]
-    movh        m1, [srcq-4]
-    movh        m2, [srcq-2]
-    movh        m3, [srcq+0]
-    movh        m4, [srcq+2]
-    punpcklwd   m0, m1
-    punpcklwd   m2, m3
-    pmaddwd     m0, m7
-%if ARCH_X86_64 && mmsize > 8
-    pmaddwd     m2, m8
-%else
-    pmaddwd     m2, [filteryq+32]
-%endif
-    movu        m1, [srcq+4]
-    movu        m3, [srcq+6]
-    paddd       m0, m2
-    movu        m2, [srcq+8]
-    add       srcq, sstrideq
-    punpcklwd   m4, m1
-    punpcklwd   m3, m2
-%if ARCH_X86_64 && mmsize > 8
-    pmaddwd     m4, m9
-    pmaddwd     m3, m10
-%else
-    pmaddwd     m4, [filteryq+64]
-    pmaddwd     m3, [filteryq+96]
-%endif
-    paddd       m0, m4
-    paddd       m0, m3
-    paddd       m0, m6
-    psrad       m0, 7
-%if cpuflag(sse4)
-    packusdw    m0, m0
-%else
-    packssdw    m0, m0
-%endif
-%ifidn %1, avg
-    movh        m1, [dstq]
-%endif
-    pminsw      m0, m5
-%if notcpuflag(sse4)
-%if ARCH_X86_64
-    pmaxsw      m0, m11
-%else
-    pxor        m2, m2
-    pmaxsw      m0, m2
-%endif
-%endif
-%ifidn %1, avg
-    pavgw       m0, m1
-%endif
-    movh    [dstq], m0
-    add       dstq, dstrideq
-    dec         hd
-    jg .loop
-    RET
-
-cglobal vp9_%1_8tap_1d_h_4_12, 6, 6, %2, dst, dstride, src, sstride, h, filtery
-    mova        m5, [pw_4095]
-    jmp mangle(private_prefix %+ _ %+ vp9_%1_8tap_1d_h_4_10 %+ SUFFIX).body
-%endmacro
-
-INIT_XMM sse2
-filter_h4_fn put
-filter_h4_fn avg
-
-%macro filter_h_fn 1-2 12
-%assign %%px mmsize/2
-cglobal vp9_%1_8tap_1d_h_ %+ %%px %+ _10, 6, 6, %2, dst, dstride, src, sstride, h, filtery
-    mova        m5, [pw_1023]
-.body:
-%if notcpuflag(sse4) && ARCH_X86_64
-    pxor       m11, m11
-%endif
-    mova        m6, [pd_64]
-    mova        m7, [filteryq+ 0]
-%if ARCH_X86_64 && mmsize > 8
-    mova        m8, [filteryq+32]
-    mova        m9, [filteryq+64]
-    mova       m10, [filteryq+96]
-%endif
-.loop:
-    movu        m0, [srcq-6]
-    movu        m1, [srcq-4]
-    movu        m2, [srcq-2]
-    movu        m3, [srcq+0]
-    movu        m4, [srcq+2]
-    pmaddwd     m0, m7
-    pmaddwd     m1, m7
-%if ARCH_X86_64 && mmsize > 8
-    pmaddwd     m2, m8
-    pmaddwd     m3, m8
-    pmaddwd     m4, m9
-%else
-    pmaddwd     m2, [filteryq+32]
-    pmaddwd     m3, [filteryq+32]
-    pmaddwd     m4, [filteryq+64]
-%endif
-    paddd       m0, m2
-    paddd       m1, m3
-    paddd       m0, m4
-    movu        m2, [srcq+4]
-    movu        m3, [srcq+6]
-    movu        m4, [srcq+8]
-    add       srcq, sstrideq
-%if ARCH_X86_64 && mmsize > 8
-    pmaddwd     m2, m9
-    pmaddwd     m3, m10
-    pmaddwd     m4, m10
-%else
-    pmaddwd     m2, [filteryq+64]
-    pmaddwd     m3, [filteryq+96]
-    pmaddwd     m4, [filteryq+96]
-%endif
-    paddd       m1, m2
-    paddd       m0, m3
-    paddd       m1, m4
-    paddd       m0, m6
-    paddd       m1, m6
-    psrad       m0, 7
-    psrad       m1, 7
-%if cpuflag(sse4)
-    packusdw    m0, m0
-    packusdw    m1, m1
-%else
-    packssdw    m0, m0
-    packssdw    m1, m1
-%endif
-    punpcklwd   m0, m1
-    pminsw      m0, m5
-%if notcpuflag(sse4)
-%if ARCH_X86_64
-    pmaxsw      m0, m11
-%else
-    pxor        m2, m2
-    pmaxsw      m0, m2
-%endif
-%endif
-%ifidn %1, avg
-    pavgw       m0, [dstq]
-%endif
-    mova    [dstq], m0
-    add       dstq, dstrideq
-    dec         hd
-    jg .loop
-    RET
-
-cglobal vp9_%1_8tap_1d_h_ %+ %%px %+ _12, 6, 6, %2, dst, dstride, src, sstride, h, filtery
-    mova        m5, [pw_4095]
-    jmp mangle(private_prefix %+ _ %+ vp9_%1_8tap_1d_h_ %+ %%px %+ _10 %+ SUFFIX).body
-%endmacro
-
-INIT_XMM sse2
-filter_h_fn put
-filter_h_fn avg
-%if HAVE_AVX2_EXTERNAL
-INIT_YMM avx2
-filter_h_fn put
-filter_h_fn avg
-%endif
-
-%macro filter_v4_fn 1-2 12
-%if ARCH_X86_64
-cglobal vp9_%1_8tap_1d_v_4_10, 6, 8, %2, dst, dstride, src, sstride, h, filtery, src4, sstride3
-%else
-cglobal vp9_%1_8tap_1d_v_4_10, 4, 7, %2, dst, dstride, src, sstride, filtery, src4, sstride3
-    mov   filteryq, r5mp
-%define hd r4mp
-%endif
-    mova        m5, [pw_1023]
-.body:
-%if notcpuflag(sse4) && ARCH_X86_64
-    pxor       m11, m11
-%endif
-    mova        m6, [pd_64]
-    lea  sstride3q, [sstrideq*3]
-    lea      src4q, [srcq+sstrideq]
-    sub       srcq, sstride3q
-    mova        m7, [filteryq+  0]
-%if ARCH_X86_64 && mmsize > 8
-    mova        m8, [filteryq+ 32]
-    mova        m9, [filteryq+ 64]
-    mova       m10, [filteryq+ 96]
-%endif
-.loop:
-    ; FIXME maybe reuse loads from previous rows, or just
-    ; more generally unroll this to prevent multiple loads of
-    ; the same data?
-    movh        m0, [srcq]
-    movh        m1, [srcq+sstrideq]
-    movh        m2, [srcq+sstrideq*2]
-    movh        m3, [srcq+sstride3q]
-    add       srcq, sstrideq
-    movh        m4, [src4q]
-    punpcklwd   m0, m1
-    punpcklwd   m2, m3
-    pmaddwd     m0, m7
-%if ARCH_X86_64 && mmsize > 8
-    pmaddwd     m2, m8
-%else
-    pmaddwd     m2, [filteryq+ 32]
-%endif
-    movh        m1, [src4q+sstrideq]
-    movh        m3, [src4q+sstrideq*2]
-    paddd       m0, m2
-    movh        m2, [src4q+sstride3q]
-    add      src4q, sstrideq
-    punpcklwd   m4, m1
-    punpcklwd   m3, m2
-%if ARCH_X86_64 && mmsize > 8
-    pmaddwd     m4, m9
-    pmaddwd     m3, m10
-%else
-    pmaddwd     m4, [filteryq+ 64]
-    pmaddwd     m3, [filteryq+ 96]
-%endif
-    paddd       m0, m4
-    paddd       m0, m3
-    paddd       m0, m6
-    psrad       m0, 7
-%if cpuflag(sse4)
-    packusdw    m0, m0
-%else
-    packssdw    m0, m0
-%endif
-%ifidn %1, avg
-    movh        m1, [dstq]
-%endif
-    pminsw      m0, m5
-%if notcpuflag(sse4)
-%if ARCH_X86_64
-    pmaxsw      m0, m11
-%else
-    pxor        m2, m2
-    pmaxsw      m0, m2
-%endif
-%endif
-%ifidn %1, avg
-    pavgw       m0, m1
-%endif
-    movh    [dstq], m0
-    add       dstq, dstrideq
-    dec         hd
-    jg .loop
-    RET
-
-%if ARCH_X86_64
-cglobal vp9_%1_8tap_1d_v_4_12, 6, 8, %2, dst, dstride, src, sstride, h, filtery, src4, sstride3
-%else
-cglobal vp9_%1_8tap_1d_v_4_12, 4, 7, %2, dst, dstride, src, sstride, filtery, src4, sstride3
-    mov   filteryq, r5mp
-%endif
-    mova        m5, [pw_4095]
-    jmp mangle(private_prefix %+ _ %+ vp9_%1_8tap_1d_v_4_10 %+ SUFFIX).body
-%endmacro
-
-INIT_XMM sse2
-filter_v4_fn put
-filter_v4_fn avg
-
-%macro filter_v_fn 1-2 13
-%assign %%px mmsize/2
-%if ARCH_X86_64
-cglobal vp9_%1_8tap_1d_v_ %+ %%px %+ _10, 6, 8, %2, dst, dstride, src, sstride, h, filtery, src4, sstride3
-%else
-cglobal vp9_%1_8tap_1d_v_ %+ %%px %+ _10, 4, 7, %2, dst, dstride, src, sstride, filtery, src4, sstride3
-    mov   filteryq, r5mp
-%define hd r4mp
-%endif
-    mova        m5, [pw_1023]
-.body:
-%if notcpuflag(sse4) && ARCH_X86_64
-    pxor       m12, m12
-%endif
-%if ARCH_X86_64
-    mova       m11, [pd_64]
-%endif
-    lea  sstride3q, [sstrideq*3]
-    lea      src4q, [srcq+sstrideq]
-    sub       srcq, sstride3q
-    mova        m7, [filteryq+  0]
-%if ARCH_X86_64 && mmsize > 8
-    mova        m8, [filteryq+ 32]
-    mova        m9, [filteryq+ 64]
-    mova       m10, [filteryq+ 96]
-%endif
-.loop:
-    ; FIXME maybe reuse loads from previous rows, or just
-    ; more generally unroll this to prevent multiple loads of
-    ; the same data?
-    movu        m0, [srcq]
-    movu        m1, [srcq+sstrideq]
-    movu        m2, [srcq+sstrideq*2]
-    movu        m3, [srcq+sstride3q]
-    add       srcq, sstrideq
-    movu        m4, [src4q]
-    SBUTTERFLY  wd, 0, 1, 6
-    SBUTTERFLY  wd, 2, 3, 6
-    pmaddwd     m0, m7
-    pmaddwd     m1, m7
-%if ARCH_X86_64 && mmsize > 8
-    pmaddwd     m2, m8
-    pmaddwd     m3, m8
-%else
-    pmaddwd     m2, [filteryq+ 32]
-    pmaddwd     m3, [filteryq+ 32]
-%endif
-    paddd       m0, m2
-    paddd       m1, m3
-    movu        m2, [src4q+sstrideq]
-    movu        m3, [src4q+sstrideq*2]
-    SBUTTERFLY  wd, 4, 2, 6
-%if ARCH_X86_64 && mmsize > 8
-    pmaddwd     m4, m9
-    pmaddwd     m2, m9
-%else
-    pmaddwd     m4, [filteryq+ 64]
-    pmaddwd     m2, [filteryq+ 64]
-%endif
-    paddd       m0, m4
-    paddd       m1, m2
-    movu        m4, [src4q+sstride3q]
-    add      src4q, sstrideq
-    SBUTTERFLY  wd, 3, 4, 6
-%if ARCH_X86_64 && mmsize > 8
-    pmaddwd     m3, m10
-    pmaddwd     m4, m10
-%else
-    pmaddwd     m3, [filteryq+ 96]
-    pmaddwd     m4, [filteryq+ 96]
-%endif
-    paddd       m0, m3
-    paddd       m1, m4
-%if ARCH_X86_64
-    paddd       m0, m11
-    paddd       m1, m11
-%else
-    paddd       m0, [pd_64]
-    paddd       m1, [pd_64]
-%endif
-    psrad       m0, 7
-    psrad       m1, 7
-%if cpuflag(sse4)
-    packusdw    m0, m1
-%else
-    packssdw    m0, m1
-%endif
-    pminsw      m0, m5
-%if notcpuflag(sse4)
-%if ARCH_X86_64
-    pmaxsw      m0, m12
-%else
-    pxor        m2, m2
-    pmaxsw      m0, m2
-%endif
-%endif
-%ifidn %1, avg
-    pavgw       m0, [dstq]
-%endif
-    mova    [dstq], m0
-    add       dstq, dstrideq
-    dec         hd
-    jg .loop
-    RET
-
-%if ARCH_X86_64
-cglobal vp9_%1_8tap_1d_v_ %+ %%px %+ _12, 6, 8, %2, dst, dstride, src, sstride, h, filtery, src4, sstride3
-%else
-cglobal vp9_%1_8tap_1d_v_ %+ %%px %+ _12, 4, 7, %2, dst, dstride, src, sstride, filtery, src4, sstride3
-    mov   filteryq, r5mp
-%endif
-    mova        m5, [pw_4095]
-    jmp mangle(private_prefix %+ _ %+ vp9_%1_8tap_1d_v_ %+ %%px %+ _10 %+ SUFFIX).body
-%endmacro
-
-INIT_XMM sse2
-filter_v_fn put
-filter_v_fn avg
-%if HAVE_AVX2_EXTERNAL
-INIT_YMM avx2
-filter_v_fn put
-filter_v_fn avg
-%endif
diff -uparN ffmpeg-4.1/libavcodec/x86/vp9mc.asm ffmpeg-y/libavcodec/x86/vp9mc.asm
--- ffmpeg-4.1/libavcodec/x86/vp9mc.asm	2018-11-02 02:34:25.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/vp9mc.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,675 +0,0 @@
-;******************************************************************************
-;* VP9 motion compensation SIMD optimizations
-;*
-;* Copyright (c) 2013 Ronald S. Bultje <rsbultje gmail com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA 32
-
-cextern pw_256
-cextern pw_64
-
-%macro F8_SSSE3_TAPS 8
-times 16 db %1, %2
-times 16 db %3, %4
-times 16 db %5, %6
-times 16 db %7, %8
-%endmacro
-
-%macro F8_SSE2_TAPS 8
-times 8 dw %1
-times 8 dw %2
-times 8 dw %3
-times 8 dw %4
-times 8 dw %5
-times 8 dw %6
-times 8 dw %7
-times 8 dw %8
-%endmacro
-
-%macro F8_16BPP_TAPS 8
-times 8 dw %1, %2
-times 8 dw %3, %4
-times 8 dw %5, %6
-times 8 dw %7, %8
-%endmacro
-
-%macro FILTER 1
-const filters_%1 ; smooth
-                    F8_TAPS -3, -1,  32,  64,  38,   1, -3,  0
-                    F8_TAPS -2, -2,  29,  63,  41,   2, -3,  0
-                    F8_TAPS -2, -2,  26,  63,  43,   4, -4,  0
-                    F8_TAPS -2, -3,  24,  62,  46,   5, -4,  0
-                    F8_TAPS -2, -3,  21,  60,  49,   7, -4,  0
-                    F8_TAPS -1, -4,  18,  59,  51,   9, -4,  0
-                    F8_TAPS -1, -4,  16,  57,  53,  12, -4, -1
-                    F8_TAPS -1, -4,  14,  55,  55,  14, -4, -1
-                    F8_TAPS -1, -4,  12,  53,  57,  16, -4, -1
-                    F8_TAPS  0, -4,   9,  51,  59,  18, -4, -1
-                    F8_TAPS  0, -4,   7,  49,  60,  21, -3, -2
-                    F8_TAPS  0, -4,   5,  46,  62,  24, -3, -2
-                    F8_TAPS  0, -4,   4,  43,  63,  26, -2, -2
-                    F8_TAPS  0, -3,   2,  41,  63,  29, -2, -2
-                    F8_TAPS  0, -3,   1,  38,  64,  32, -1, -3
-                    ; regular
-                    F8_TAPS  0,  1,  -5, 126,   8,  -3,  1,  0
-                    F8_TAPS -1,  3, -10, 122,  18,  -6,  2,  0
-                    F8_TAPS -1,  4, -13, 118,  27,  -9,  3, -1
-                    F8_TAPS -1,  4, -16, 112,  37, -11,  4, -1
-                    F8_TAPS -1,  5, -18, 105,  48, -14,  4, -1
-                    F8_TAPS -1,  5, -19,  97,  58, -16,  5, -1
-                    F8_TAPS -1,  6, -19,  88,  68, -18,  5, -1
-                    F8_TAPS -1,  6, -19,  78,  78, -19,  6, -1
-                    F8_TAPS -1,  5, -18,  68,  88, -19,  6, -1
-                    F8_TAPS -1,  5, -16,  58,  97, -19,  5, -1
-                    F8_TAPS -1,  4, -14,  48, 105, -18,  5, -1
-                    F8_TAPS -1,  4, -11,  37, 112, -16,  4, -1
-                    F8_TAPS -1,  3,  -9,  27, 118, -13,  4, -1
-                    F8_TAPS  0,  2,  -6,  18, 122, -10,  3, -1
-                    F8_TAPS  0,  1,  -3,   8, 126,  -5,  1,  0
-                    ; sharp
-                    F8_TAPS -1,  3,  -7, 127,   8,  -3,  1,  0
-                    F8_TAPS -2,  5, -13, 125,  17,  -6,  3, -1
-                    F8_TAPS -3,  7, -17, 121,  27, -10,  5, -2
-                    F8_TAPS -4,  9, -20, 115,  37, -13,  6, -2
-                    F8_TAPS -4, 10, -23, 108,  48, -16,  8, -3
-                    F8_TAPS -4, 10, -24, 100,  59, -19,  9, -3
-                    F8_TAPS -4, 11, -24,  90,  70, -21, 10, -4
-                    F8_TAPS -4, 11, -23,  80,  80, -23, 11, -4
-                    F8_TAPS -4, 10, -21,  70,  90, -24, 11, -4
-                    F8_TAPS -3,  9, -19,  59, 100, -24, 10, -4
-                    F8_TAPS -3,  8, -16,  48, 108, -23, 10, -4
-                    F8_TAPS -2,  6, -13,  37, 115, -20,  9, -4
-                    F8_TAPS -2,  5, -10,  27, 121, -17,  7, -3
-                    F8_TAPS -1,  3,  -6,  17, 125, -13,  5, -2
-                    F8_TAPS  0,  1,  -3,   8, 127,  -7,  3, -1
-%endmacro
-
-%define F8_TAPS F8_SSSE3_TAPS
-; int8_t ff_filters_ssse3[3][15][4][32]
-FILTER ssse3
-%define F8_TAPS F8_SSE2_TAPS
-; int16_t ff_filters_sse2[3][15][8][8]
-FILTER sse2
-%define F8_TAPS F8_16BPP_TAPS
-; int16_t ff_filters_16bpp[3][15][4][16]
-FILTER 16bpp
-
-SECTION .text
-
-%macro filter_sse2_h_fn 1
-%assign %%px mmsize/2
-cglobal vp9_%1_8tap_1d_h_ %+ %%px %+ _8, 6, 6, 15, dst, dstride, src, sstride, h, filtery
-    pxor        m5, m5
-    mova        m6, [pw_64]
-    mova        m7, [filteryq+  0]
-%if ARCH_X86_64 && mmsize > 8
-    mova        m8, [filteryq+ 16]
-    mova        m9, [filteryq+ 32]
-    mova       m10, [filteryq+ 48]
-    mova       m11, [filteryq+ 64]
-    mova       m12, [filteryq+ 80]
-    mova       m13, [filteryq+ 96]
-    mova       m14, [filteryq+112]
-%endif
-.loop:
-    movh        m0, [srcq-3]
-    movh        m1, [srcq-2]
-    movh        m2, [srcq-1]
-    movh        m3, [srcq+0]
-    movh        m4, [srcq+1]
-    punpcklbw   m0, m5
-    punpcklbw   m1, m5
-    punpcklbw   m2, m5
-    punpcklbw   m3, m5
-    punpcklbw   m4, m5
-    pmullw      m0, m7
-%if ARCH_X86_64 && mmsize > 8
-    pmullw      m1, m8
-    pmullw      m2, m9
-    pmullw      m3, m10
-    pmullw      m4, m11
-%else
-    pmullw      m1, [filteryq+ 16]
-    pmullw      m2, [filteryq+ 32]
-    pmullw      m3, [filteryq+ 48]
-    pmullw      m4, [filteryq+ 64]
-%endif
-    paddw       m0, m1
-    paddw       m2, m3
-    paddw       m0, m4
-    movh        m1, [srcq+2]
-    movh        m3, [srcq+3]
-    movh        m4, [srcq+4]
-    add       srcq, sstrideq
-    punpcklbw   m1, m5
-    punpcklbw   m3, m5
-    punpcklbw   m4, m5
-%if ARCH_X86_64 && mmsize > 8
-    pmullw      m1, m12
-    pmullw      m3, m13
-    pmullw      m4, m14
-%else
-    pmullw      m1, [filteryq+ 80]
-    pmullw      m3, [filteryq+ 96]
-    pmullw      m4, [filteryq+112]
-%endif
-    paddw       m0, m1
-    paddw       m3, m4
-    paddw       m0, m6
-    paddw       m2, m3
-    paddsw      m0, m2
-    psraw       m0, 7
-%ifidn %1, avg
-    movh        m1, [dstq]
-%endif
-    packuswb    m0, m0
-%ifidn %1, avg
-    pavgb       m0, m1
-%endif
-    movh    [dstq], m0
-    add       dstq, dstrideq
-    dec         hd
-    jg .loop
-    RET
-%endmacro
-
-INIT_MMX mmxext
-filter_sse2_h_fn put
-filter_sse2_h_fn avg
-
-INIT_XMM sse2
-filter_sse2_h_fn put
-filter_sse2_h_fn avg
-
-%macro filter_h_fn 1
-%assign %%px mmsize/2
-cglobal vp9_%1_8tap_1d_h_ %+ %%px %+ _8, 6, 6, 11, dst, dstride, src, sstride, h, filtery
-    mova        m6, [pw_256]
-    mova        m7, [filteryq+ 0]
-%if ARCH_X86_64 && mmsize > 8
-    mova        m8, [filteryq+32]
-    mova        m9, [filteryq+64]
-    mova       m10, [filteryq+96]
-%endif
-.loop:
-    movh        m0, [srcq-3]
-    movh        m1, [srcq-2]
-    movh        m2, [srcq-1]
-    movh        m3, [srcq+0]
-    movh        m4, [srcq+1]
-    movh        m5, [srcq+2]
-    punpcklbw   m0, m1
-    punpcklbw   m2, m3
-    movh        m1, [srcq+3]
-    movh        m3, [srcq+4]
-    add       srcq, sstrideq
-    punpcklbw   m4, m5
-    punpcklbw   m1, m3
-    pmaddubsw   m0, m7
-%if ARCH_X86_64 && mmsize > 8
-    pmaddubsw   m2, m8
-    pmaddubsw   m4, m9
-    pmaddubsw   m1, m10
-%else
-    pmaddubsw   m2, [filteryq+32]
-    pmaddubsw   m4, [filteryq+64]
-    pmaddubsw   m1, [filteryq+96]
-%endif
-    paddw       m0, m4
-    paddw       m2, m1
-    paddsw      m0, m2
-    pmulhrsw    m0, m6
-%ifidn %1, avg
-    movh        m1, [dstq]
-%endif
-    packuswb    m0, m0
-%ifidn %1, avg
-    pavgb       m0, m1
-%endif
-    movh    [dstq], m0
-    add       dstq, dstrideq
-    dec         hd
-    jg .loop
-    RET
-%endmacro
-
-INIT_MMX ssse3
-filter_h_fn put
-filter_h_fn avg
-
-INIT_XMM ssse3
-filter_h_fn put
-filter_h_fn avg
-
-%if ARCH_X86_64
-%macro filter_hx2_fn 1
-%assign %%px mmsize
-cglobal vp9_%1_8tap_1d_h_ %+ %%px %+ _8, 6, 6, 14, dst, dstride, src, sstride, h, filtery
-    mova       m13, [pw_256]
-    mova        m8, [filteryq+ 0]
-    mova        m9, [filteryq+32]
-    mova       m10, [filteryq+64]
-    mova       m11, [filteryq+96]
-.loop:
-    movu        m0, [srcq-3]
-    movu        m1, [srcq-2]
-    movu        m2, [srcq-1]
-    movu        m3, [srcq+0]
-    movu        m4, [srcq+1]
-    movu        m5, [srcq+2]
-    movu        m6, [srcq+3]
-    movu        m7, [srcq+4]
-    add       srcq, sstrideq
-    SBUTTERFLY  bw, 0, 1, 12
-    SBUTTERFLY  bw, 2, 3, 12
-    SBUTTERFLY  bw, 4, 5, 12
-    SBUTTERFLY  bw, 6, 7, 12
-    pmaddubsw   m0, m8
-    pmaddubsw   m1, m8
-    pmaddubsw   m2, m9
-    pmaddubsw   m3, m9
-    pmaddubsw   m4, m10
-    pmaddubsw   m5, m10
-    pmaddubsw   m6, m11
-    pmaddubsw   m7, m11
-    paddw       m0, m4
-    paddw       m1, m5
-    paddw       m2, m6
-    paddw       m3, m7
-    paddsw      m0, m2
-    paddsw      m1, m3
-    pmulhrsw    m0, m13
-    pmulhrsw    m1, m13
-    packuswb    m0, m1
-%ifidn %1, avg
-    pavgb       m0, [dstq]
-%endif
-    mova    [dstq], m0
-    add       dstq, dstrideq
-    dec         hd
-    jg .loop
-    RET
-%endmacro
-
-INIT_XMM ssse3
-filter_hx2_fn put
-filter_hx2_fn avg
-
-%if HAVE_AVX2_EXTERNAL
-INIT_YMM avx2
-filter_hx2_fn put
-filter_hx2_fn avg
-%endif
-
-%endif ; ARCH_X86_64
-
-%macro filter_sse2_v_fn 1
-%assign %%px mmsize/2
-%if ARCH_X86_64
-cglobal vp9_%1_8tap_1d_v_ %+ %%px %+ _8, 6, 8, 15, dst, dstride, src, sstride, h, filtery, src4, sstride3
-%else
-cglobal vp9_%1_8tap_1d_v_ %+ %%px %+ _8, 4, 7, 15, dst, dstride, src, sstride, filtery, src4, sstride3
-    mov   filteryq, r5mp
-%define hd r4mp
-%endif
-    pxor        m5, m5
-    mova        m6, [pw_64]
-    lea  sstride3q, [sstrideq*3]
-    lea      src4q, [srcq+sstrideq]
-    sub       srcq, sstride3q
-    mova        m7, [filteryq+  0]
-%if ARCH_X86_64 && mmsize > 8
-    mova        m8, [filteryq+ 16]
-    mova        m9, [filteryq+ 32]
-    mova       m10, [filteryq+ 48]
-    mova       m11, [filteryq+ 64]
-    mova       m12, [filteryq+ 80]
-    mova       m13, [filteryq+ 96]
-    mova       m14, [filteryq+112]
-%endif
-.loop:
-    ; FIXME maybe reuse loads from previous rows, or just
-    ; more generally unroll this to prevent multiple loads of
-    ; the same data?
-    movh        m0, [srcq]
-    movh        m1, [srcq+sstrideq]
-    movh        m2, [srcq+sstrideq*2]
-    movh        m3, [srcq+sstride3q]
-    add       srcq, sstrideq
-    movh        m4, [src4q]
-    punpcklbw   m0, m5
-    punpcklbw   m1, m5
-    punpcklbw   m2, m5
-    punpcklbw   m3, m5
-    punpcklbw   m4, m5
-    pmullw      m0, m7
-%if ARCH_X86_64 && mmsize > 8
-    pmullw      m1, m8
-    pmullw      m2, m9
-    pmullw      m3, m10
-    pmullw      m4, m11
-%else
-    pmullw      m1, [filteryq+ 16]
-    pmullw      m2, [filteryq+ 32]
-    pmullw      m3, [filteryq+ 48]
-    pmullw      m4, [filteryq+ 64]
-%endif
-    paddw       m0, m1
-    paddw       m2, m3
-    paddw       m0, m4
-    movh        m1, [src4q+sstrideq]
-    movh        m3, [src4q+sstrideq*2]
-    movh        m4, [src4q+sstride3q]
-    add      src4q, sstrideq
-    punpcklbw   m1, m5
-    punpcklbw   m3, m5
-    punpcklbw   m4, m5
-%if ARCH_X86_64 && mmsize > 8
-    pmullw      m1, m12
-    pmullw      m3, m13
-    pmullw      m4, m14
-%else
-    pmullw      m1, [filteryq+ 80]
-    pmullw      m3, [filteryq+ 96]
-    pmullw      m4, [filteryq+112]
-%endif
-    paddw       m0, m1
-    paddw       m3, m4
-    paddw       m0, m6
-    paddw       m2, m3
-    paddsw      m0, m2
-    psraw       m0, 7
-%ifidn %1, avg
-    movh        m1, [dstq]
-%endif
-    packuswb    m0, m0
-%ifidn %1, avg
-    pavgb       m0, m1
-%endif
-    movh    [dstq], m0
-    add       dstq, dstrideq
-    dec         hd
-    jg .loop
-    RET
-%endmacro
-
-INIT_MMX mmxext
-filter_sse2_v_fn put
-filter_sse2_v_fn avg
-
-INIT_XMM sse2
-filter_sse2_v_fn put
-filter_sse2_v_fn avg
-
-%macro filter_v_fn 1
-%assign %%px mmsize/2
-%if ARCH_X86_64
-cglobal vp9_%1_8tap_1d_v_ %+ %%px %+ _8, 6, 8, 11, dst, dstride, src, sstride, h, filtery, src4, sstride3
-%else
-cglobal vp9_%1_8tap_1d_v_ %+ %%px %+ _8, 4, 7, 11, dst, dstride, src, sstride, filtery, src4, sstride3
-    mov   filteryq, r5mp
-%define hd r4mp
-%endif
-    mova        m6, [pw_256]
-    lea  sstride3q, [sstrideq*3]
-    lea      src4q, [srcq+sstrideq]
-    sub       srcq, sstride3q
-    mova        m7, [filteryq+ 0]
-%if ARCH_X86_64 && mmsize > 8
-    mova        m8, [filteryq+32]
-    mova        m9, [filteryq+64]
-    mova       m10, [filteryq+96]
-%endif
-.loop:
-    ; FIXME maybe reuse loads from previous rows, or just more generally
-    ; unroll this to prevent multiple loads of the same data?
-    movh        m0, [srcq]
-    movh        m1, [srcq+sstrideq]
-    movh        m2, [srcq+sstrideq*2]
-    movh        m3, [srcq+sstride3q]
-    movh        m4, [src4q]
-    movh        m5, [src4q+sstrideq]
-    punpcklbw   m0, m1
-    punpcklbw   m2, m3
-    movh        m1, [src4q+sstrideq*2]
-    movh        m3, [src4q+sstride3q]
-    add       srcq, sstrideq
-    add      src4q, sstrideq
-    punpcklbw   m4, m5
-    punpcklbw   m1, m3
-    pmaddubsw   m0, m7
-%if ARCH_X86_64 && mmsize > 8
-    pmaddubsw   m2, m8
-    pmaddubsw   m4, m9
-    pmaddubsw   m1, m10
-%else
-    pmaddubsw   m2, [filteryq+32]
-    pmaddubsw   m4, [filteryq+64]
-    pmaddubsw   m1, [filteryq+96]
-%endif
-    paddw       m0, m4
-    paddw       m2, m1
-    paddsw      m0, m2
-    pmulhrsw    m0, m6
-%ifidn %1, avg
-    movh        m1, [dstq]
-%endif
-    packuswb    m0, m0
-%ifidn %1, avg
-    pavgb       m0, m1
-%endif
-    movh    [dstq], m0
-    add       dstq, dstrideq
-    dec         hd
-    jg .loop
-    RET
-%endmacro
-
-INIT_MMX ssse3
-filter_v_fn put
-filter_v_fn avg
-
-INIT_XMM ssse3
-filter_v_fn put
-filter_v_fn avg
-
-%if ARCH_X86_64
-
-%macro filter_vx2_fn 1
-%assign %%px mmsize
-cglobal vp9_%1_8tap_1d_v_ %+ %%px %+ _8, 6, 8, 14, dst, dstride, src, sstride, h, filtery, src4, sstride3
-    mova       m13, [pw_256]
-    lea  sstride3q, [sstrideq*3]
-    lea      src4q, [srcq+sstrideq]
-    sub       srcq, sstride3q
-    mova        m8, [filteryq+ 0]
-    mova        m9, [filteryq+32]
-    mova       m10, [filteryq+64]
-    mova       m11, [filteryq+96]
-.loop:
-    ; FIXME maybe reuse loads from previous rows, or just
-    ; more generally unroll this to prevent multiple loads of
-    ; the same data?
-    movu        m0, [srcq]
-    movu        m1, [srcq+sstrideq]
-    movu        m2, [srcq+sstrideq*2]
-    movu        m3, [srcq+sstride3q]
-    movu        m4, [src4q]
-    movu        m5, [src4q+sstrideq]
-    movu        m6, [src4q+sstrideq*2]
-    movu        m7, [src4q+sstride3q]
-    add       srcq, sstrideq
-    add      src4q, sstrideq
-    SBUTTERFLY  bw, 0, 1, 12
-    SBUTTERFLY  bw, 2, 3, 12
-    SBUTTERFLY  bw, 4, 5, 12
-    SBUTTERFLY  bw, 6, 7, 12
-    pmaddubsw   m0, m8
-    pmaddubsw   m1, m8
-    pmaddubsw   m2, m9
-    pmaddubsw   m3, m9
-    pmaddubsw   m4, m10
-    pmaddubsw   m5, m10
-    pmaddubsw   m6, m11
-    pmaddubsw   m7, m11
-    paddw       m0, m4
-    paddw       m1, m5
-    paddw       m2, m6
-    paddw       m3, m7
-    paddsw      m0, m2
-    paddsw      m1, m3
-    pmulhrsw    m0, m13
-    pmulhrsw    m1, m13
-    packuswb    m0, m1
-%ifidn %1, avg
-    pavgb       m0, [dstq]
-%endif
-    mova    [dstq], m0
-    add       dstq, dstrideq
-    dec         hd
-    jg .loop
-    RET
-%endmacro
-
-INIT_XMM ssse3
-filter_vx2_fn put
-filter_vx2_fn avg
-
-%if HAVE_AVX2_EXTERNAL
-INIT_YMM avx2
-filter_vx2_fn put
-filter_vx2_fn avg
-%endif
-
-%endif ; ARCH_X86_64
-
-%macro fpel_fn 6-8 0, 4
-%if %2 == 4
-%define %%srcfn movh
-%define %%dstfn movh
-%else
-%define %%srcfn movu
-%define %%dstfn mova
-%endif
-
-%if %7 == 8
-%define %%pavg pavgb
-%define %%szsuf _8
-%elif %7 == 16
-%define %%pavg pavgw
-%define %%szsuf _16
-%else
-%define %%szsuf
-%endif
-
-%if %2 <= mmsize
-cglobal vp9_%1%2 %+ %%szsuf, 5, 7, 4, dst, dstride, src, sstride, h, dstride3, sstride3
-    lea  sstride3q, [sstrideq*3]
-    lea  dstride3q, [dstrideq*3]
-%else
-cglobal vp9_%1%2 %+ %%szsuf, 5, 5, %8, dst, dstride, src, sstride, h
-%endif
-.loop:
-    %%srcfn     m0, [srcq]
-    %%srcfn     m1, [srcq+s%3]
-    %%srcfn     m2, [srcq+s%4]
-    %%srcfn     m3, [srcq+s%5]
-%if %2/mmsize == 8
-    %%srcfn     m4, [srcq+mmsize*4]
-    %%srcfn     m5, [srcq+mmsize*5]
-    %%srcfn     m6, [srcq+mmsize*6]
-    %%srcfn     m7, [srcq+mmsize*7]
-%endif
-    lea       srcq, [srcq+sstrideq*%6]
-%ifidn %1, avg
-    %%pavg      m0, [dstq]
-    %%pavg      m1, [dstq+d%3]
-    %%pavg      m2, [dstq+d%4]
-    %%pavg      m3, [dstq+d%5]
-%if %2/mmsize == 8
-    %%pavg      m4, [dstq+mmsize*4]
-    %%pavg      m5, [dstq+mmsize*5]
-    %%pavg      m6, [dstq+mmsize*6]
-    %%pavg      m7, [dstq+mmsize*7]
-%endif
-%endif
-    %%dstfn [dstq], m0
-    %%dstfn [dstq+d%3], m1
-    %%dstfn [dstq+d%4], m2
-    %%dstfn [dstq+d%5], m3
-%if %2/mmsize == 8
-    %%dstfn [dstq+mmsize*4], m4
-    %%dstfn [dstq+mmsize*5], m5
-    %%dstfn [dstq+mmsize*6], m6
-    %%dstfn [dstq+mmsize*7], m7
-%endif
-    lea       dstq, [dstq+dstrideq*%6]
-    sub         hd, %6
-    jnz .loop
-    RET
-%endmacro
-
-%define d16 16
-%define s16 16
-%define d32 32
-%define s32 32
-INIT_MMX mmx
-fpel_fn put, 4,  strideq, strideq*2, stride3q, 4
-fpel_fn put, 8,  strideq, strideq*2, stride3q, 4
-INIT_MMX mmxext
-fpel_fn avg, 4,  strideq, strideq*2, stride3q, 4, 8
-fpel_fn avg, 8,  strideq, strideq*2, stride3q, 4, 8
-INIT_XMM sse
-fpel_fn put, 16, strideq, strideq*2, stride3q, 4
-fpel_fn put, 32, mmsize,  strideq,   strideq+mmsize, 2
-fpel_fn put, 64, mmsize,  mmsize*2,  mmsize*3, 1
-fpel_fn put, 128, mmsize, mmsize*2,  mmsize*3, 1, 0, 8
-INIT_XMM sse2
-fpel_fn avg, 16, strideq, strideq*2, stride3q, 4, 8
-fpel_fn avg, 32, mmsize,  strideq,   strideq+mmsize, 2, 8
-fpel_fn avg, 64, mmsize,  mmsize*2,  mmsize*3, 1, 8
-INIT_YMM avx
-fpel_fn put, 32, strideq, strideq*2, stride3q, 4
-fpel_fn put, 64, mmsize,  strideq,   strideq+mmsize, 2
-fpel_fn put, 128, mmsize, mmsize*2,     mmsize*3, 1
-%if HAVE_AVX2_EXTERNAL
-INIT_YMM avx2
-fpel_fn avg, 32, strideq, strideq*2, stride3q, 4, 8
-fpel_fn avg, 64, mmsize,  strideq,   strideq+mmsize, 2, 8
-%endif
-INIT_MMX mmxext
-fpel_fn avg,  8,  strideq, strideq*2, stride3q, 4, 16
-INIT_XMM sse2
-fpel_fn avg,  16, strideq, strideq*2, stride3q, 4, 16
-fpel_fn avg,  32, mmsize,  strideq,   strideq+mmsize, 2, 16
-fpel_fn avg,  64, mmsize,  mmsize*2,  mmsize*3, 1, 16
-fpel_fn avg, 128, mmsize,  mmsize*2,  mmsize*3, 1, 16, 8
-%if HAVE_AVX2_EXTERNAL
-INIT_YMM avx2
-fpel_fn avg,  32, strideq, strideq*2, stride3q, 4, 16
-fpel_fn avg,  64, mmsize,  strideq,   strideq+mmsize, 2, 16
-fpel_fn avg, 128, mmsize,  mmsize*2,  mmsize*3, 1, 16
-%endif
-%undef s16
-%undef d16
-%undef s32
-%undef d32
diff -uparN ffmpeg-4.1/libavcodec/x86/xvididct.asm ffmpeg-y/libavcodec/x86/xvididct.asm
--- ffmpeg-4.1/libavcodec/x86/xvididct.asm	2018-01-01 06:35:48.000000000 +0800
+++ ffmpeg-y/libavcodec/x86/xvididct.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,983 +0,0 @@
-; XVID MPEG-4 VIDEO CODEC
-;
-; Conversion from gcc syntax to x264asm syntax with modifications
-; by Christophe Gisquet <christophe.gisquet@gmail.com>
-;
-; ===========     SSE2 inverse discrete cosine transform     ===========
-;
-; Copyright(C) 2003 Pascal Massimino <skal@planet-d.net>
-;
-; Conversion to gcc syntax with modifications
-; by Alexander Strange <astrange@ithinksw.com>
-;
-; Originally from dct/x86_asm/fdct_sse2_skal.asm in Xvid.
-;
-; Vertical pass is an implementation of the scheme:
-;  Loeffler C., Ligtenberg A., and Moschytz C.S.:
-;  Practical Fast 1D DCT Algorithm with Eleven Multiplications,
-;  Proc. ICASSP 1989, 988-991.
-;
-; Horizontal pass is a double 4x4 vector/matrix multiplication,
-; (see also Intel's Application Note 922:
-;  http://developer.intel.com/vtune/cbts/strmsimd/922down.htm
-;  Copyright (C) 1999 Intel Corporation)
-;
-; More details at http://skal.planet-d.net/coding/dct.html
-;
-; =======     MMX and XMM forward discrete cosine transform     =======
-;
-; Copyright(C) 2001 Peter Ross <pross@xvid.org>
-;
-; Originally provided by Intel at AP-922
-; http://developer.intel.com/vtune/cbts/strmsimd/922down.htm
-; (See more app notes at http://developer.intel.com/vtune/cbts/strmsimd/appnotes.htm)
-; but in a limited edition.
-; New macro implements a column part for precise iDCT
-; The routine precision now satisfies IEEE standard 1180-1990.
-;
-; Copyright(C) 2000-2001 Peter Gubanov <peter@elecard.net.ru>
-; Rounding trick Copyright(C) 2000 Michel Lespinasse <walken@zoy.org>
-;
-; http://www.elecard.com/peter/idct.html
-; http://www.linuxvideo.org/mpeg2dec/
-;
-; These examples contain code fragments for first stage iDCT 8x8
-; (for rows) and first stage DCT 8x8 (for columns)
-;
-; conversion to gcc syntax by Michael Niedermayer
-;
-; ======================================================================
-;
-; This file is part of FFmpeg.
-;
-; FFmpeg is free software; you can redistribute it and/or
-; modify it under the terms of the GNU Lesser General Public
-; License as published by the Free Software Foundation; either
-; version 2.1 of the License, or (at your option) any later version.
-;
-; FFmpeg is distributed in the hope that it will be useful,
-; but WITHOUT ANY WARRANTY; without even the implied warranty of
-; MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-; Lesser General Public License for more details.
-;
-; You should have received a copy of the GNU Lesser General Public License
-; along with FFmpeg; if not, write to the Free Software Foundation,
-; Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-; Similar to tg_1_16 in MMX code
-tan1:   times 8 dw 13036
-tan2:   times 8 dw 27146
-tan3:   times 8 dw 43790
-sqrt2:  times 8 dw 23170
-
-; SSE2 tables
-iTab1:  dw 0x4000, 0x539f, 0xc000, 0xac61, 0x4000, 0xdd5d, 0x4000, 0xdd5d
-        dw 0x4000, 0x22a3, 0x4000, 0x22a3, 0xc000, 0x539f, 0x4000, 0xac61
-        dw 0x3249, 0x11a8, 0x4b42, 0xee58, 0x11a8, 0x4b42, 0x11a8, 0xcdb7
-        dw 0x58c5, 0x4b42, 0xa73b, 0xcdb7, 0x3249, 0xa73b, 0x4b42, 0xa73b
-iTab2:  dw 0x58c5, 0x73fc, 0xa73b, 0x8c04, 0x58c5, 0xcff5, 0x58c5, 0xcff5
-        dw 0x58c5, 0x300b, 0x58c5, 0x300b, 0xa73b, 0x73fc, 0x58c5, 0x8c04
-        dw 0x45bf, 0x187e, 0x6862, 0xe782, 0x187e, 0x6862, 0x187e, 0xba41
-        dw 0x7b21, 0x6862, 0x84df, 0xba41, 0x45bf, 0x84df, 0x6862, 0x84df
-iTab3:  dw 0x539f, 0x6d41, 0xac61, 0x92bf, 0x539f, 0xd2bf, 0x539f, 0xd2bf
-        dw 0x539f, 0x2d41, 0x539f, 0x2d41, 0xac61, 0x6d41, 0x539f, 0x92bf
-        dw 0x41b3, 0x1712, 0x6254, 0xe8ee, 0x1712, 0x6254, 0x1712, 0xbe4d
-        dw 0x73fc, 0x6254, 0x8c04, 0xbe4d, 0x41b3, 0x8c04, 0x6254, 0x8c04
-iTab4:  dw 0x4b42, 0x6254, 0xb4be, 0x9dac, 0x4b42, 0xd746, 0x4b42, 0xd746
-        dw 0x4b42, 0x28ba, 0x4b42, 0x28ba, 0xb4be, 0x6254, 0x4b42, 0x9dac
-        dw 0x3b21, 0x14c3, 0x587e, 0xeb3d, 0x14c3, 0x587e, 0x14c3, 0xc4df
-        dw 0x6862, 0x587e, 0x979e, 0xc4df, 0x3b21, 0x979e, 0x587e, 0x979e
-
-%if ARCH_X86_32
-; -----------------------------------------------------------------------------
-;
-; The first stage iDCT 8x8 - inverse DCTs of rows
-;
-; -----------------------------------------------------------------------------
-; The 8-point inverse DCT direct algorithm
-; -----------------------------------------------------------------------------
-;
-; static const short w[32] = {
-;     FIX(cos_4_16),  FIX(cos_2_16),  FIX(cos_4_16),  FIX(cos_6_16),
-;     FIX(cos_4_16),  FIX(cos_6_16), -FIX(cos_4_16), -FIX(cos_2_16),
-;     FIX(cos_4_16), -FIX(cos_6_16), -FIX(cos_4_16),  FIX(cos_2_16),
-;     FIX(cos_4_16), -FIX(cos_2_16),  FIX(cos_4_16), -FIX(cos_6_16),
-;     FIX(cos_1_16),  FIX(cos_3_16),  FIX(cos_5_16),  FIX(cos_7_16),
-;     FIX(cos_3_16), -FIX(cos_7_16), -FIX(cos_1_16), -FIX(cos_5_16),
-;     FIX(cos_5_16), -FIX(cos_1_16),  FIX(cos_7_16),  FIX(cos_3_16),
-;     FIX(cos_7_16), -FIX(cos_5_16),  FIX(cos_3_16), -FIX(cos_1_16) };
-;
-; #define DCT_8_INV_ROW(x, y)
-; {
-;     int a0, a1, a2, a3, b0, b1, b2, b3;
-;
-;     a0 = x[0] * w[0]  + x[2] * w[1]  + x[4] * w[2]  + x[6] * w[3];
-;     a1 = x[0] * w[4]  + x[2] * w[5]  + x[4] * w[6]  + x[6] * w[7];
-;     a2 = x[0] * w[8]  + x[2] * w[9]  + x[4] * w[10] + x[6] * w[11];
-;     a3 = x[0] * w[12] + x[2] * w[13] + x[4] * w[14] + x[6] * w[15];
-;     b0 = x[1] * w[16] + x[3] * w[17] + x[5] * w[18] + x[7] * w[19];
-;     b1 = x[1] * w[20] + x[3] * w[21] + x[5] * w[22] + x[7] * w[23];
-;     b2 = x[1] * w[24] + x[3] * w[25] + x[5] * w[26] + x[7] * w[27];
-;     b3 = x[1] * w[28] + x[3] * w[29] + x[5] * w[30] + x[7] * w[31];
-;
-;     y[0] = SHIFT_ROUND(a0 + b0);
-;     y[1] = SHIFT_ROUND(a1 + b1);
-;     y[2] = SHIFT_ROUND(a2 + b2);
-;     y[3] = SHIFT_ROUND(a3 + b3);
-;     y[4] = SHIFT_ROUND(a3 - b3);
-;     y[5] = SHIFT_ROUND(a2 - b2);
-;     y[6] = SHIFT_ROUND(a1 - b1);
-;     y[7] = SHIFT_ROUND(a0 - b0);
-; }
-;
-; -----------------------------------------------------------------------------
-;
-; In this implementation the outputs of the iDCT-1D are multiplied
-;     for rows 0,4 - by cos_4_16,
-;     for rows 1,7 - by cos_1_16,
-;     for rows 2,6 - by cos_2_16,
-;     for rows 3,5 - by cos_3_16
-; and are shifted to the left for better accuracy.
-;
-; For the constants used,
-;     FIX(float_const) = (short) (float_const * (1 << 15) + 0.5)
-;
-; -----------------------------------------------------------------------------
-
-; -----------------------------------------------------------------------------
-; Tables for mmx processors
-; -----------------------------------------------------------------------------
-
-; Table for rows 0,4 - constants are multiplied by cos_4_16
-tab_i_04_mmx: dw  16384,  16384,  16384, -16384
-              dw  21407,   8867,   8867, -21407 ; w07 w05 w03 w01
-              dw  16384, -16384,  16384,  16384 ; w14 w12 w10 w08
-              dw  -8867,  21407, -21407,  -8867 ; w15 w13 w11 w09
-              dw  22725,  12873,  19266, -22725 ; w22 w20 w18 w16
-              dw  19266,   4520,  -4520, -12873 ; w23 w21 w19 w17
-              dw  12873,   4520,   4520,  19266 ; w30 w28 w26 w24
-              dw -22725,  19266, -12873, -22725 ; w31 w29 w27 w25
-; Table for rows 1,7 - constants are multiplied by cos_1_16
-              dw  22725,  22725,  22725, -22725 ; movq-> w06 w04 w02 w00
-              dw  29692,  12299,  12299, -29692 ; w07 w05 w03 w01
-              dw  22725, -22725,  22725,  22725 ; w14 w12 w10 w08
-              dw -12299,  29692, -29692, -12299 ; w15 w13 w11 w09
-              dw  31521,  17855,  26722, -31521 ; w22 w20 w18 w16
-              dw  26722,   6270,  -6270, -17855 ; w23 w21 w19 w17
-              dw  17855,   6270,   6270,  26722 ; w30 w28 w26 w24
-              dw -31521,  26722, -17855, -31521 ; w31 w29 w27 w25
-; Table for rows 2,6 - constants are multiplied by cos_2_16
-              dw  21407,  21407,  21407, -21407 ; movq-> w06 w04 w02 w00
-              dw  27969,  11585,  11585, -27969 ; w07 w05 w03 w01
-              dw  21407, -21407,  21407,  21407 ; w14 w12 w10 w08
-              dw -11585,  27969, -27969, -11585 ; w15 w13 w11 w09
-              dw  29692,  16819,  25172, -29692 ; w22 w20 w18 w16
-              dw  25172,   5906,  -5906, -16819 ; w23 w21 w19 w17
-              dw  16819,   5906,   5906,  25172 ; w30 w28 w26 w24
-              dw -29692,  25172, -16819, -29692 ; w31 w29 w27 w25
-; Table for rows 3,5 - constants are multiplied by cos_3_16
-              dw  19266,  19266,  19266, -19266 ; movq-> w06 w04 w02 w00
-              dw  25172,  10426,  10426, -25172 ; w07 w05 w03 w01
-              dw  19266, -19266,  19266,  19266 ; w14 w12 w10 w08
-              dw -10426,  25172, -25172, -10426 ; w15 w13 w11 w09
-              dw  26722,  15137,  22654, -26722 ; w22 w20 w18 w16
-              dw  22654,   5315,  -5315, -15137 ; w23 w21 w19 w17
-              dw  15137,   5315,   5315,  22654 ; w30 w28 w26 w24
-              dw -26722,  22654, -15137, -26722 ; w31 w29 w27 w25
-
-; -----------------------------------------------------------------------------
-; Tables for xmm processors
-; -----------------------------------------------------------------------------
-
-; %3 for rows 0,4 - constants are multiplied by cos_4_16
-tab_i_04_xmm: dw  16384,  21407,  16384,   8867 ; movq-> w05 w04 w01 w00
-              dw  16384,   8867, -16384, -21407 ; w07 w06 w03 w02
-              dw  16384,  -8867,  16384, -21407 ; w13 w12 w09 w08
-              dw -16384,  21407,  16384,  -8867 ; w15 w14 w11 w10
-              dw  22725,  19266,  19266,  -4520 ; w21 w20 w17 w16
-              dw  12873,   4520, -22725, -12873 ; w23 w22 w19 w18
-              dw  12873, -22725,   4520, -12873 ; w29 w28 w25 w24
-              dw   4520,  19266,  19266, -22725 ; w31 w30 w27 w26
-; %3 for rows 1,7 - constants are multiplied by cos_1_16
-              dw  22725,  29692,  22725,  12299 ; movq-> w05 w04 w01 w00
-              dw  22725,  12299, -22725, -29692 ; w07 w06 w03 w02
-              dw  22725, -12299,  22725, -29692 ; w13 w12 w09 w08
-              dw -22725,  29692,  22725, -12299 ; w15 w14 w11 w10
-              dw  31521,  26722,  26722,  -6270 ; w21 w20 w17 w16
-              dw  17855,   6270, -31521, -17855 ; w23 w22 w19 w18
-              dw  17855, -31521,   6270, -17855 ; w29 w28 w25 w24
-              dw   6270,  26722,  26722, -31521 ; w31 w30 w27 w26
-; %3 for rows 2,6 - constants are multiplied by cos_2_16
-              dw  21407,  27969,  21407,  11585 ; movq-> w05 w04 w01 w00
-              dw  21407,  11585, -21407, -27969 ; w07 w06 w03 w02
-              dw  21407, -11585,  21407, -27969 ; w13 w12 w09 w08
-              dw -21407,  27969,  21407, -11585 ; w15 w14 w11 w10
-              dw  29692,  25172,  25172,  -5906 ; w21 w20 w17 w16
-              dw  16819,   5906, -29692, -16819 ; w23 w22 w19 w18
-              dw  16819, -29692,   5906, -16819 ; w29 w28 w25 w24
-              dw   5906,  25172,  25172, -29692 ; w31 w30 w27 w26
-; %3 for rows 3,5 - constants are multiplied by cos_3_16
-              dw  19266,  25172,  19266,  10426 ; movq-> w05 w04 w01 w00
-              dw  19266,  10426, -19266, -25172 ; w07 w06 w03 w02
-              dw  19266, -10426,  19266, -25172 ; w13 w12 w09 w08
-              dw -19266,  25172,  19266, -10426 ; w15 w14 w11 w10
-              dw  26722,  22654,  22654,  -5315 ; w21 w20 w17 w16
-              dw  15137,   5315, -26722, -15137 ; w23 w22 w19 w18
-              dw  15137, -26722,   5315, -15137 ; w29 w28 w25 w24
-              dw   5315,  22654,  22654, -26722 ; w31 w30 w27 w26
-%endif ; ~ARCH_X86_32
-
-; Similar to rounder_0 in MMX code
-; 4 first similar, then: 4*8->6*16  5*8->4*16  6/7*8->5*16
-walkenIdctRounders: times 4 dd 65536
-                    times 4 dd  3597
-                    times 4 dd  2260
-                    times 4 dd  1203
-                    times 4 dd   120
-                    times 4 dd   512
-                    times 2 dd     0
-
-pb_127: times 8 db 127
-
-SECTION .text
-
-; Temporary storage before the column pass
-%define ROW1 xmm6
-%define ROW3 xmm4
-%define ROW5 xmm5
-%define ROW7 xmm7
-
-%macro CLEAR_ODD 1
-    pxor      %1, %1
-%endmacro
-%macro PUT_ODD 1
-    pshufhw   %1, xmm2, 0x1B
-%endmacro
-
-%macro MOV32 2
-%if ARCH_X86_32
-    movdqa    %2, %1
-%endif
-%endmacro
-
-%macro CLEAR_EVEN 1
-%if ARCH_X86_64
-    CLEAR_ODD %1
-%endif
-%endmacro
-
-%macro PUT_EVEN 1
-%if ARCH_X86_64
-    PUT_ODD   %1
-%else
-    pshufhw xmm2, xmm2, 0x1B
-    movdqa    %1, xmm2
-%endif
-%endmacro
-
-%if ARCH_X86_64
-%define ROW0  xmm8
-%define REG0  ROW0
-%define ROW2  xmm9
-%define REG2  ROW2
-%define ROW4  xmm10
-%define REG4  ROW4
-%define ROW6  xmm11
-%define REG6  ROW6
-%define XMMS  xmm12
-%define SREG2 REG2
-%define TAN3  xmm13
-%define TAN1  xmm14
-%else
-%define ROW0  [BLOCK + 0*16]
-%define REG0  xmm4
-%define ROW2  [BLOCK + 2*16]
-%define REG2  xmm4
-%define ROW4  [BLOCK + 4*16]
-%define REG4  xmm6
-%define ROW6  [BLOCK + 6*16]
-%define REG6  xmm6
-%define XMMS  xmm2
-%define SREG2 xmm7
-%define TAN3  xmm0
-%define TAN1  xmm2
-%endif
-
-%macro JZ  2
-    test      %1, %1
-    jz       .%2
-%endmacro
-
-%macro JNZ  2
-    test      %1, %1
-    jnz      .%2
-%endmacro
-
-%macro TEST_ONE_ROW 4 ; src, reg, clear, arg
-    %3        %4
-    movq     mm1, [%1]
-    por      mm1, [%1 + 8]
-    paddusb  mm1, mm0
-    pmovmskb  %2, mm1
-%endmacro
-
-;row1, row2, reg1, reg2, clear1, arg1, clear2, arg2
-%macro  TEST_TWO_ROWS  8
-    %5         %6
-    %7         %8
-    movq      mm1, [%1 + 0]
-    por       mm1, [%1 + 8]
-    movq      mm2, [%2 + 0]
-    por       mm2, [%2 + 8]
-    paddusb   mm1, mm0
-    paddusb   mm2, mm0
-    pmovmskb   %3, mm1
-    pmovmskb   %4, mm2
-%endmacro
-
-; IDCT pass on rows.
-%macro iMTX_MULT   4-5 ; src, table, put, arg, rounder
-    movdqa       xmm3, [%1]
-    movdqa       xmm0, xmm3
-    pshufd       xmm1, xmm3, 0x11 ; 4602
-    punpcklqdq   xmm0, xmm0       ; 0246
-    pmaddwd      xmm0, [%2]
-    pmaddwd      xmm1, [%2+16]
-    pshufd       xmm2, xmm3, 0xBB ; 5713
-    punpckhqdq   xmm3, xmm3       ; 1357
-    pmaddwd      xmm2, [%2+32]
-    pmaddwd      xmm3, [%2+48]
-    paddd        xmm0, xmm1
-    paddd        xmm2, xmm3
-%if %0 == 5
-    paddd        xmm0, [walkenIdctRounders+%5]
-%endif
-    movdqa       xmm3, xmm2
-    paddd        xmm2, xmm0
-    psubd        xmm0, xmm3
-    psrad        xmm2, 11
-    psrad        xmm0, 11
-    packssdw     xmm2, xmm0
-    %3           %4
-%endmacro
-
-%macro iLLM_HEAD 0
-    movdqa   TAN3, [tan3]
-    movdqa   TAN1, [tan1]
-%endmacro
-
-%macro FIRST_HALF 2  ; %1=dct  %2=type(normal,add,put)
-    psraw    xmm5, 6
-    psraw    REG0, 6
-    psraw    TAN3, 6
-    psraw    xmm3, 6
-    ; dct coeffs must still be written for AC prediction
-%if %2 == 0
-    movdqa   [%1+1*16], TAN3
-    movdqa   [%1+2*16], xmm3
-    movdqa   [%1+5*16], REG0
-    movdqa   [%1+6*16], xmm5
-%else
-    ; Must now load args as gprs are no longer used for masks
-    ; DEST is set to where address of dest was loaded
-    %if ARCH_X86_32
-        %if %2 == 2 ; Not enough xmms, store
-    movdqa   [%1+1*16], TAN3
-    movdqa   [%1+2*16], xmm3
-    movdqa   [%1+5*16], REG0
-    movdqa   [%1+6*16], xmm5
-        %endif
-    %xdefine DEST r2q ; BLOCK is r0, stride r1
-    movifnidn DEST, destm
-    movifnidn strideq, stridem
-    %else
-    %xdefine DEST r0q
-    %endif
-    lea      r3q, [3*strideq]
-    %if %2 == 1
-    packuswb TAN3, xmm3
-    packuswb xmm5, REG0
-    movq     [DEST + strideq], TAN3
-    movhps   [DEST + 2*strideq], TAN3
-    ; REG0 and TAN3 are now available (and likely used in second half)
-    %endif
-%endif
-%endmacro
-
-%macro SECOND_HALF 6 ; %1=dct  %2=type(normal,add,put) 3-6: xmms
-    psraw    %3, 6
-    psraw    %4, 6
-    psraw    %5, 6
-    psraw    %6, 6
-    ; dct coeffs must still be written for AC prediction
-%if %2 == 0
-    movdqa   [%1+0*16], %3
-    movdqa   [%1+3*16], %5
-    movdqa   [%1+4*16], %6
-    movdqa   [%1+7*16], %4
-%elif %2 == 1
-    packuswb %3, %5
-    packuswb %6, %4
-    ; address of dest may have been loaded
-    movq     [DEST], %3
-    movhps   [DEST + r3q], %3
-    lea      DEST, [DEST + 4*strideq]
-    movq     [DEST], %6
-    movhps   [DEST + r3q], %6
-    ; and now write remainder of first half
-    movq     [DEST + 2*strideq], xmm5
-    movhps   [DEST + strideq], xmm5
-%elif %2 == 2
-    pxor        xmm0, xmm0
-    %if ARCH_X86_32
-    ; free: m3 REG0=m4 m5
-    ; input: m1, m7, m2, m6
-    movq        xmm3, [DEST+0*strideq]
-    movq        xmm4, [DEST+1*strideq]
-    punpcklbw   xmm3, xmm0
-    punpcklbw   xmm4, xmm0
-    paddsw      xmm3, %3
-    paddsw      xmm4, [%1 + 1*16]
-    movq          %3, [DEST+2*strideq]
-    movq        xmm5, [DEST+      r3q]
-    punpcklbw     %3, xmm0
-    punpcklbw   xmm5, xmm0
-    paddsw        %3, [%1 + 2*16]
-    paddsw      xmm5, %5
-    packuswb    xmm3, xmm4
-    packuswb      %3, xmm5
-    movq    [DEST+0*strideq], xmm3
-    movhps  [DEST+1*strideq], xmm3
-    movq    [DEST+2*strideq], %3
-    movhps  [DEST+      r3q], %3
-    lea         DEST, [DEST+4*strideq]
-    movq        xmm3, [DEST+0*strideq]
-    movq        xmm4, [DEST+1*strideq]
-    movq          %3, [DEST+2*strideq]
-    movq        xmm5, [DEST+      r3q]
-    punpcklbw   xmm3, xmm0
-    punpcklbw   xmm4, xmm0
-    punpcklbw     %3, xmm0
-    punpcklbw   xmm5, xmm0
-    paddsw      xmm3, %6
-    paddsw      xmm4, [%1 + 5*16]
-    paddsw        %3, [%1 + 6*16]
-    paddsw      xmm5, %4
-    packuswb    xmm3, xmm4
-    packuswb      %3, xmm5
-    movq    [DEST+0*strideq], xmm3
-    movhps  [DEST+1*strideq], xmm3
-    movq    [DEST+2*strideq], %3
-    movhps  [DEST+      r3q], %3
-    %else
-    ; l1:TAN3=m13  l2:m3  l5:REG0=m8 l6=m5
-    ; input: m1, m7/SREG2=m9, TAN1=m14, REG4=m10
-    movq        xmm2, [DEST+0*strideq]
-    movq        xmm4, [DEST+1*strideq]
-    movq       xmm12, [DEST+2*strideq]
-    movq       xmm11, [DEST+      r3q]
-    punpcklbw   xmm2, xmm0
-    punpcklbw   xmm4, xmm0
-    punpcklbw  xmm12, xmm0
-    punpcklbw  xmm11, xmm0
-    paddsw      xmm2, %3
-    paddsw      xmm4, TAN3
-    paddsw     xmm12, xmm3
-    paddsw     xmm11, %5
-    packuswb    xmm2, xmm4
-    packuswb   xmm12, xmm11
-    movq    [DEST+0*strideq], xmm2
-    movhps  [DEST+1*strideq], xmm2
-    movq    [DEST+2*strideq], xmm12
-    movhps  [DEST+      r3q], xmm12
-    lea         DEST, [DEST+4*strideq]
-    movq        xmm2, [DEST+0*strideq]
-    movq        xmm4, [DEST+1*strideq]
-    movq       xmm12, [DEST+2*strideq]
-    movq       xmm11, [DEST+      r3q]
-    punpcklbw   xmm2, xmm0
-    punpcklbw   xmm4, xmm0
-    punpcklbw  xmm12, xmm0
-    punpcklbw  xmm11, xmm0
-    paddsw      xmm2, %6
-    paddsw      xmm4, REG0
-    paddsw     xmm12, xmm5
-    paddsw     xmm11, %4
-    packuswb    xmm2, xmm4
-    packuswb   xmm12, xmm11
-    movq    [DEST+0*strideq], xmm2
-    movhps  [DEST+1*strideq], xmm2
-    movq    [DEST+2*strideq], xmm12
-    movhps  [DEST+      r3q], xmm12
-    %endif
-%endif
-%endmacro
-
-
-; IDCT pass on columns.
-%macro iLLM_PASS  2  ; %1=dct  %2=type(normal,add,put)
-    movdqa   xmm1, TAN3
-    movdqa   xmm3, TAN1
-    pmulhw   TAN3, xmm4
-    pmulhw   xmm1, xmm5
-    paddsw   TAN3, xmm4
-    paddsw   xmm1, xmm5
-    psubsw   TAN3, xmm5
-    paddsw   xmm1, xmm4
-    pmulhw   xmm3, xmm7
-    pmulhw   TAN1, xmm6
-    paddsw   xmm3, xmm6
-    psubsw   TAN1, xmm7
-    movdqa   xmm7, xmm3
-    movdqa   xmm6, TAN1
-    psubsw   xmm3, xmm1
-    psubsw   TAN1, TAN3
-    paddsw   xmm1, xmm7
-    paddsw   TAN3, xmm6
-    movdqa   xmm6, xmm3
-    psubsw   xmm3, TAN3
-    paddsw   TAN3, xmm6
-    movdqa   xmm4, [sqrt2]
-    pmulhw   xmm3, xmm4
-    pmulhw   TAN3, xmm4
-    paddsw   TAN3, TAN3
-    paddsw   xmm3, xmm3
-    movdqa   xmm7, [tan2]
-    MOV32    ROW2, REG2
-    MOV32    ROW6, REG6
-    movdqa   xmm5, xmm7
-    pmulhw   xmm7, REG6
-    pmulhw   xmm5, REG2
-    paddsw   xmm7, REG2
-    psubsw   xmm5, REG6
-    MOV32    ROW0, REG0
-    MOV32    ROW4, REG4
-    MOV32    TAN1, [BLOCK]
-    movdqa   XMMS, REG0
-    psubsw   REG0, REG4
-    paddsw   REG4, XMMS
-    movdqa   XMMS, REG4
-    psubsw   REG4, xmm7
-    paddsw   xmm7, XMMS
-    movdqa   XMMS, REG0
-    psubsw   REG0, xmm5
-    paddsw   xmm5, XMMS
-    movdqa   XMMS, xmm5
-    psubsw   xmm5, TAN3
-    paddsw   TAN3, XMMS
-    movdqa   XMMS, REG0
-    psubsw   REG0, xmm3
-    paddsw   xmm3, XMMS
-    MOV32    [BLOCK], TAN1
-
-    FIRST_HALF %1, %2
-
-    movdqa   xmm0, xmm7
-    movdqa   xmm4, REG4
-    psubsw   xmm7, xmm1
-    psubsw   REG4, TAN1
-    paddsw   xmm1, xmm0
-    paddsw   TAN1, xmm4
-
-    SECOND_HALF %1, %2, xmm1, xmm7, TAN1, REG4
-%endmacro
-
-; IDCT pass on columns, assuming rows 4-7 are zero
-%macro iLLM_PASS_SPARSE   2 ; %1=dct   %2=type(normal,put,add)
-    pmulhw   TAN3, xmm4
-    paddsw   TAN3, xmm4
-    movdqa   xmm3, xmm6
-    pmulhw   TAN1, xmm6
-    movdqa   xmm1, xmm4
-    psubsw   xmm3, xmm1
-    paddsw   xmm1, xmm6
-    movdqa   xmm6, TAN1
-    psubsw   TAN1, TAN3
-    paddsw   TAN3, xmm6
-    movdqa   xmm6, xmm3
-    psubsw   xmm3, TAN3
-    paddsw   TAN3, xmm6
-    movdqa   xmm4, [sqrt2]
-    pmulhw   xmm3, xmm4
-    pmulhw   TAN3, xmm4
-    paddsw   TAN3, TAN3
-    paddsw   xmm3, xmm3
-    movdqa   xmm5, [tan2]
-    MOV32    ROW2, SREG2
-    pmulhw   xmm5, SREG2
-    MOV32    ROW0, REG0
-    movdqa   xmm6, REG0
-    psubsw   xmm6, SREG2
-    paddsw  SREG2, REG0
-    MOV32    TAN1, [BLOCK]
-    movdqa   XMMS, REG0
-    psubsw   REG0, xmm5
-    paddsw   xmm5, XMMS
-    movdqa   XMMS, xmm5
-    psubsw   xmm5, TAN3
-    paddsw   TAN3, XMMS
-    movdqa   XMMS, REG0
-    psubsw   REG0, xmm3
-    paddsw   xmm3, XMMS
-    MOV32    [BLOCK], TAN1
-
-    FIRST_HALF %1, %2
-
-    movdqa   xmm0, SREG2
-    movdqa   xmm4, xmm6
-    psubsw  SREG2, xmm1
-    psubsw   xmm6, TAN1
-    paddsw   xmm1, xmm0
-    paddsw   TAN1, xmm4
-
-    SECOND_HALF %1, %2, xmm1, SREG2, TAN1, xmm6
-%endmacro
-
-%macro IDCT_SSE2 1 ; 0=normal  1=put  2=add
-%if %1 == 0 || ARCH_X86_32
-    %define GPR0  r1d
-    %define GPR1  r2d
-    %define GPR2  r3d
-    %define GPR3  r4d
-    %define NUM_GPRS 5
-%else
-    %define GPR0  r3d
-    %define GPR1  r4d
-    %define GPR2  r5d
-    %define GPR3  r6d
-    %define NUM_GPRS 7
-%endif
-%if %1 == 0
-cglobal xvid_idct, 1, NUM_GPRS, 8+7*ARCH_X86_64, block
-%xdefine BLOCK blockq
-%else
-    %if %1 == 1
-cglobal xvid_idct_put, 0, NUM_GPRS, 8+7*ARCH_X86_64, dest, stride, block
-    %else
-cglobal xvid_idct_add, 0, NUM_GPRS, 8+7*ARCH_X86_64, dest, stride, block
-    %endif
-    %if ARCH_X86_64
-    %xdefine BLOCK blockq
-    %else
-    mov    r0q, blockm
-    %xdefine BLOCK r0q
-    %endif
-%endif
-    movq           mm0, [pb_127]
-    iMTX_MULT      BLOCK + 0*16, iTab1, PUT_EVEN, ROW0, 0*16
-    iMTX_MULT      BLOCK + 1*16, iTab2, PUT_ODD, ROW1,  1*16
-    iMTX_MULT      BLOCK + 2*16, iTab3, PUT_EVEN, ROW2, 2*16
-
-    TEST_TWO_ROWS  BLOCK + 3*16, BLOCK + 4*16, GPR0, GPR1, CLEAR_ODD, ROW3, CLEAR_EVEN, ROW4 ; a, c
-    JZ   GPR0, col1
-    iMTX_MULT      BLOCK + 3*16, iTab4, PUT_ODD, ROW3,  3*16
-.col1:
-    TEST_TWO_ROWS  BLOCK + 5*16, BLOCK + 6*16, GPR0, GPR2, CLEAR_ODD, ROW5, CLEAR_EVEN, ROW6 ; a, d
-    TEST_ONE_ROW   BLOCK + 7*16, GPR3, CLEAR_ODD, ROW7 ; esi
-
-    iLLM_HEAD
-    JNZ  GPR1, 2
-    JNZ  GPR0, 3
-    JNZ  GPR2, 4
-    JNZ  GPR3, 5
-    iLLM_PASS_SPARSE BLOCK, %1
-    jmp .6
-.2:
-    iMTX_MULT     BLOCK + 4*16, iTab1, PUT_EVEN, ROW4
-.3:
-    iMTX_MULT     BLOCK + 5*16, iTab4, PUT_ODD, ROW5,  4*16
-    JZ   GPR2, col2
-.4:
-    iMTX_MULT     BLOCK + 6*16, iTab3, PUT_EVEN, ROW6, 5*16
-.col2:
-    JZ   GPR3, col3
-.5:
-    iMTX_MULT     BLOCK + 7*16, iTab2, PUT_ODD, ROW7,  5*16
-.col3:
-%if ARCH_X86_32
-    iLLM_HEAD
-%endif
-    iLLM_PASS     BLOCK, %1
-.6:
-    RET
-%endmacro
-
-INIT_XMM sse2
-IDCT_SSE2 0
-IDCT_SSE2 1
-IDCT_SSE2 2
-
-%if ARCH_X86_32
-
-; %1=offset  %2=tab_offset
-; %3=rnd_offset where 4*8->6*16  5*8->4*16  6/7*8->5*16
-%macro DCT_8_INV_ROW  3
-    movq       mm0, [r0+16*%1+0]  ; 0 ; x3 x2 x1 x0
-    movq       mm1, [r0+16*%1+8]  ; 1 ; x7 x6 x5 x4
-    movq       mm2, mm0       ; 2 ; x3 x2 x1 x0
-    movq       mm3, [%2+ 0]   ; 3 ; w06 w04 w02 w00
-%if cpuflag(mmxext)
-    pshufw     mm0, mm0, 0x88 ; x2 x0 x2 x0
-    movq       mm4, [%2+ 8]   ; 4 ; w07 w06 w03 w02
-    movq       mm5, mm1       ; 5 ; x7 x6 x5 x4
-    pmaddwd    mm3, mm0       ; x2*w05+x0*w04 x2*w01+x0*w00
-    movq       mm6, [%2+32]   ; 6 ; w21 w20 w17 w16
-    pshufw     mm1, mm1, 0x88 ; x6 x4 x6 x4
-    pmaddwd    mm4, mm1       ; x6*w07+x4*w06 x6*w03+x4*w02
-    movq       mm7, [%2+40]   ; 7; w23 w22 w19 w18
-    pshufw     mm2, mm2, 0xdd ; x3 x1 x3 x1
-    pmaddwd    mm6, mm2       ; x3*w21+x1*w20 x3*w17+x1*w16
-    pshufw     mm5, mm5, 0xdd ; x7 x5 x7 x5
-    pmaddwd    mm7, mm5       ; x7*w23+x5*w22 x7*w19+x5*w18
-    paddd      mm3, [walkenIdctRounders + %3]      ; +%3
-    pmaddwd    mm0, [%2+16]   ; x2*w13+x0*w12 x2*w09+x0*w08
-    paddd      mm3, mm4       ; 4 ; a1=sum(even1) a0=sum(even0)
-    pmaddwd    mm1, [%2+24]   ; x6*w15+x4*w14 x6*w11+x4*w10
-    movq       mm4, mm3       ; 4 ; a1 a0
-    pmaddwd    mm2, [%2+48]   ; x3*w29+x1*w28 x3*w25+x1*w24
-    paddd      mm6, mm7       ; 7 ; b1=sum(odd1) b0=sum(odd0)
-    pmaddwd    mm5, [%2+56]   ; x7*w31+x5*w30 x7*w27+x5*w26
-    paddd      mm3, mm6       ; a1+b1 a0+b0
-    paddd      mm0, [walkenIdctRounders + %3]      ; +%3
-    psrad      mm3, 11        ; y1=a1+b1 y0=a0+b0
-    paddd      mm0, mm1       ; 1 ; a3=sum(even3) a2=sum(even2)
-    psubd      mm4, mm6       ; 6 ; a1-b1 a0-b0
-    movq       mm7, mm0       ; 7 ; a3 a2
-    paddd      mm2, mm5       ; 5 ; b3=sum(odd3) b2=sum(odd2)
-    paddd      mm0, mm2       ; a3+b3 a2+b2
-    psrad      mm4, 11        ; y6=a1-b1 y7=a0-b0
-    psubd      mm7, mm2       ; 2 ; a3-b3 a2-b2
-    psrad      mm0, 11        ; y3=a3+b3 y2=a2+b2
-    psrad      mm7, 11        ; y4=a3-b3 y5=a2-b2
-    packssdw   mm3, mm0       ; 0 ; y3 y2 y1 y0
-    packssdw   mm7, mm4       ; 4 ; y6 y7 y4 y5
-    movq  [r0+16*%1+0], mm3       ; 3 ; save y3 y2 y1 y0
-    pshufw     mm7, mm7, 0xb1 ; y7 y6 y5 y4
-%else
-    punpcklwd  mm0, mm1       ; x5 x1 x4 x0
-    movq       mm5, mm0       ; 5 ; x5 x1 x4 x0
-    punpckldq  mm0, mm0       ; x4 x0 x4 x0
-    movq       mm4, [%2+ 8]   ; 4 ; w07 w05 w03 w01
-    punpckhwd  mm2, mm1       ; 1 ; x7 x3 x6 x2
-    pmaddwd    mm3, mm0       ; x4*w06+x0*w04 x4*w02+x0*w00
-    movq       mm6, mm2       ; 6 ; x7 x3 x6 x2
-    movq       mm1, [%2+32]   ; 1 ; w22 w20 w18 w16
-    punpckldq  mm2, mm2       ; x6 x2 x6 x2
-    pmaddwd    mm4, mm2       ; x6*w07+x2*w05 x6*w03+x2*w01
-    punpckhdq  mm5, mm5       ; x5 x1 x5 x1
-    pmaddwd    mm0, [%2+16]   ; x4*w14+x0*w12 x4*w10+x0*w08
-    punpckhdq  mm6, mm6       ; x7 x3 x7 x3
-    movq       mm7, [%2+40]   ; 7 ; w23 w21 w19 w17
-    pmaddwd    mm1, mm5       ; x5*w22+x1*w20 x5*w18+x1*w16
-    paddd      mm3, [walkenIdctRounders + %3]     ; +%3
-    pmaddwd    mm7, mm6       ; x7*w23+x3*w21 x7*w19+x3*w17
-    pmaddwd    mm2, [%2+24]   ; x6*w15+x2*w13 x6*w11+x2*w09
-    paddd      mm3, mm4       ; 4 ; a1=sum(even1) a0=sum(even0)
-    pmaddwd    mm5, [%2+48]   ; x5*w30+x1*w28 x5*w26+x1*w24
-    movq       mm4, mm3       ; 4 ; a1 a0
-    pmaddwd    mm6, [%2+56]   ; x7*w31+x3*w29 x7*w27+x3*w25
-    paddd      mm1, mm7       ; 7 ; b1=sum(odd1) b0=sum(odd0)
-    paddd      mm0, [walkenIdctRounders + %3]     ; +%3
-    psubd      mm3, mm1       ; a1-b1 a0-b0
-    psrad      mm3, 11        ; y6=a1-b1 y7=a0-b0
-    paddd      mm1, mm4       ; 4 ; a1+b1 a0+b0
-    paddd      mm0, mm2       ; 2 ; a3=sum(even3) a2=sum(even2)
-    psrad      mm1, 11        ; y1=a1+b1 y0=a0+b0
-    paddd      mm5, mm6       ; 6 ; b3=sum(odd3) b2=sum(odd2)
-    movq       mm4, mm0       ; 4 ; a3 a2
-    paddd      mm0, mm5       ; a3+b3 a2+b2
-    psubd      mm4, mm5       ; 5 ; a3-b3 a2-b2
-    psrad      mm0, 11        ; y3=a3+b3 y2=a2+b2
-    psrad      mm4, 11        ; y4=a3-b3 y5=a2-b2
-    packssdw   mm1, mm0       ; 0 ; y3 y2 y1 y0
-    packssdw   mm4, mm3       ; 3 ; y6 y7 y4 y5
-    movq       mm7, mm4       ; 7 ; y6 y7 y4 y5
-    psrld      mm4, 16        ; 0 y6 0 y4
-    pslld      mm7, 16        ; y7 0 y5 0
-    movq  [r0+16*%1+0], mm1   ; 1 ; save y3 y2 y1 y0
-    por        mm7, mm4       ; 4 ; y7 y6 y5 y4
-%endif
-    movq  [r0+16*%1+8], mm7   ; 7 ; save y7 y6 y5 y4
-%endmacro
-
-; -----------------------------------------------------------------------------
-;
-; The first stage DCT 8x8 - forward DCTs of columns
-;
-; The %2puts are multiplied
-; for rows 0,4 - on cos_4_16,
-; for rows 1,7 - on cos_1_16,
-; for rows 2,6 - on cos_2_16,
-; for rows 3,5 - on cos_3_16
-; and are shifted to the left for rise of accuracy
-;
-; -----------------------------------------------------------------------------
-;
-; The 8-point scaled forward DCT algorithm (26a8m)
-;
-; -----------------------------------------------------------------------------
-;
-;#define DCT_8_FRW_COL(x, y)
-; {
-;     short t0, t1, t2, t3, t4, t5, t6, t7;
-;     short tp03, tm03, tp12, tm12, tp65, tm65;
-;     short tp465, tm465, tp765, tm765;
-;
-;     t0 = LEFT_SHIFT(x[0] + x[7]);
-;     t1 = LEFT_SHIFT(x[1] + x[6]);
-;     t2 = LEFT_SHIFT(x[2] + x[5]);
-;     t3 = LEFT_SHIFT(x[3] + x[4]);
-;     t4 = LEFT_SHIFT(x[3] - x[4]);
-;     t5 = LEFT_SHIFT(x[2] - x[5]);
-;     t6 = LEFT_SHIFT(x[1] - x[6]);
-;     t7 = LEFT_SHIFT(x[0] - x[7]);
-;
-;     tp03 = t0 + t3;
-;     tm03 = t0 - t3;
-;     tp12 = t1 + t2;
-;     tm12 = t1 - t2;
-;
-;     y[0] = tp03 + tp12;
-;     y[4] = tp03 - tp12;
-;
-;     y[2] = tm03 + tm12 * tg_2_16;
-;     y[6] = tm03 * tg_2_16 - tm12;
-;
-;     tp65 = (t6 + t5) * cos_4_16;
-;     tm65 = (t6 - t5) * cos_4_16;
-;
-;     tp765 = t7 + tp65;
-;     tm765 = t7 - tp65;
-;     tp465 = t4 + tm65;
-;     tm465 = t4 - tm65;
-;
-;     y[1] = tp765 + tp465 * tg_1_16;
-;     y[7] = tp765 * tg_1_16 - tp465;
-;     y[5] = tm765 * tg_3_16 + tm465;
-;     y[3] = tm765 - tm465 * tg_3_16;
-; }
-;
-; -----------------------------------------------------------------------------
-
-; -----------------------------------------------------------------------------
-; DCT_8_INV_COL_4  INP,OUT
-; -----------------------------------------------------------------------------
-%macro DCT_8_INV_COL 1
-    movq        mm0, [tan3]
-    movq        mm3, [%1+16*3]
-    movq        mm1, mm0 ; tg_3_16
-    movq        mm5, [%1+16*5]
-    pmulhw      mm0, mm3 ; x3*(tg_3_16-1)
-    movq        mm4, [tan1]
-    pmulhw      mm1, mm5 ; x5*(tg_3_16-1)
-    movq        mm7, [%1+16*7]
-    movq        mm2, mm4 ; tg_1_16
-    movq        mm6, [%1+16*1]
-    pmulhw      mm4, mm7 ; x7*tg_1_16
-    paddsw      mm0, mm3 ; x3*tg_3_16
-    pmulhw      mm2, mm6 ; x1*tg_1_16
-    paddsw      mm1, mm3 ; x3+x5*(tg_3_16-1)
-    psubsw      mm0, mm5 ; x3*tg_3_16-x5 = tm35
-    movq        mm3, [sqrt2]
-    paddsw      mm1, mm5 ; x3+x5*tg_3_16 = tp35
-    paddsw      mm4, mm6 ; x1+tg_1_16*x7 = tp17
-    psubsw      mm2, mm7 ; x1*tg_1_16-x7 = tm17
-    movq        mm5, mm4 ; tp17
-    movq        mm6, mm2 ; tm17
-    paddsw      mm5, mm1 ; tp17+tp35 = b0
-    psubsw      mm6, mm0 ; tm17-tm35 = b3
-    psubsw      mm4, mm1 ; tp17-tp35 = t1
-    paddsw      mm2, mm0 ; tm17+tm35 = t2
-    movq        mm7, [tan2]
-    movq        mm1, mm4 ; t1
-    movq  [%1+3*16], mm5 ; save b0
-    paddsw      mm1, mm2 ; t1+t2
-    movq  [%1+5*16], mm6 ; save b3
-    psubsw      mm4, mm2 ; t1-t2
-    movq        mm5, [%1+2*16]
-    movq        mm0, mm7 ; tg_2_16
-    movq        mm6, [%1+6*16]
-    pmulhw      mm0, mm5 ; x2*tg_2_16
-    pmulhw      mm7, mm6 ; x6*tg_2_16
-    pmulhw      mm1, mm3 ; ocos_4_16*(t1+t2) = b1/2
-    movq        mm2, [%1+0*16]
-    pmulhw      mm4, mm3 ; ocos_4_16*(t1-t2) = b2/2
-    psubsw      mm0, mm6 ; t2*tg_2_16-x6 = tm26
-    movq        mm3, mm2 ; x0
-    movq        mm6, [%1+4*16]
-    paddsw      mm7, mm5 ; x2+x6*tg_2_16 = tp26
-    paddsw      mm2, mm6 ; x0+x4 = tp04
-    psubsw      mm3, mm6 ; x0-x4 = tm04
-    movq        mm5, mm2 ; tp04
-    movq        mm6, mm3 ; tm04
-    psubsw      mm2, mm7 ; tp04-tp26 = a3
-    paddsw      mm3, mm0 ; tm04+tm26 = a1
-    paddsw      mm1, mm1 ; b1
-    paddsw      mm4, mm4 ; b2
-    paddsw      mm5, mm7 ; tp04+tp26 = a0
-    psubsw      mm6, mm0 ; tm04-tm26 = a2
-    movq        mm7, mm3 ; a1
-    movq        mm0, mm6 ; a2
-    paddsw      mm3, mm1 ; a1+b1
-    paddsw      mm6, mm4 ; a2+b2
-    psraw       mm3, 6   ; dst1
-    psubsw      mm7, mm1 ; a1-b1
-    psraw       mm6, 6   ; dst2
-    psubsw      mm0, mm4 ; a2-b2
-    movq        mm1, [%1+3*16] ; load b0
-    psraw       mm7, 6   ; dst6
-    movq        mm4, mm5 ; a0
-    psraw       mm0, 6   ; dst5
-    movq  [%1+1*16], mm3
-    paddsw      mm5, mm1 ; a0+b0
-    movq  [%1+2*16], mm6
-    psubsw      mm4, mm1 ; a0-b0
-    movq        mm3, [%1+5*16] ; load b3
-    psraw       mm5, 6   ; dst0
-    movq        mm6, mm2 ; a3
-    psraw       mm4, 6   ; dst7
-    movq  [%1+5*16], mm0
-    paddsw      mm2, mm3 ; a3+b3
-    movq  [%1+6*16], mm7
-    psubsw      mm6, mm3 ; a3-b3
-    movq  [%1+0*16], mm5
-    psraw       mm2, 6   ; dst3
-    movq  [%1+7*16], mm4
-    psraw       mm6, 6   ; dst4
-    movq  [%1+3*16], mm2
-    movq  [%1+4*16], mm6
-%endmacro
-
-%macro XVID_IDCT_MMX 0
-cglobal xvid_idct, 1, 1, 0, block
-%if cpuflag(mmxext)
-%define TAB tab_i_04_xmm
-%else
-%define TAB tab_i_04_mmx
-%endif
-    ; Process each row - beware of rounder offset
-    DCT_8_INV_ROW  0, TAB + 64 * 0, 0*16
-    DCT_8_INV_ROW  1, TAB + 64 * 1, 1*16
-    DCT_8_INV_ROW  2, TAB + 64 * 2, 2*16
-    DCT_8_INV_ROW  3, TAB + 64 * 3, 3*16
-    DCT_8_INV_ROW  4, TAB + 64 * 0, 6*16
-    DCT_8_INV_ROW  5, TAB + 64 * 3, 4*16
-    DCT_8_INV_ROW  6, TAB + 64 * 2, 5*16
-    DCT_8_INV_ROW  7, TAB + 64 * 1, 5*16
-
-    ; Process the columns (4 at a time)
-    DCT_8_INV_COL  r0+0
-    DCT_8_INV_COL  r0+8
-
-    RET
-%endmacro
-
-INIT_MMX mmx
-XVID_IDCT_MMX
-INIT_MMX mmxext
-XVID_IDCT_MMX
-
-%endif ; ~ARCH_X86_32
diff -uparN ffmpeg-4.1/libavdevice/indev_list.c ffmpeg-y/libavdevice/indev_list.c
--- ffmpeg-4.1/libavdevice/indev_list.c	1970-01-01 08:00:00.000000000 +0800
+++ ffmpeg-y/libavdevice/indev_list.c	2019-06-29 11:49:36.853017668 +0800
@@ -0,0 +1,2 @@
+static const AVInputFormat * const indev_list[] = {
+    NULL };
diff -uparN ffmpeg-4.1/libavdevice/outdev_list.c ffmpeg-y/libavdevice/outdev_list.c
--- ffmpeg-4.1/libavdevice/outdev_list.c	1970-01-01 08:00:00.000000000 +0800
+++ ffmpeg-y/libavdevice/outdev_list.c	2019-06-29 11:49:36.853017668 +0800
@@ -0,0 +1,2 @@
+static const AVOutputFormat * const outdev_list[] = {
+    NULL };
diff -uparN ffmpeg-4.1/libavfilter/filter_list.c ffmpeg-y/libavfilter/filter_list.c
--- ffmpeg-4.1/libavfilter/filter_list.c	1970-01-01 08:00:00.000000000 +0800
+++ ffmpeg-y/libavfilter/filter_list.c	2019-06-29 11:49:36.897017669 +0800
@@ -0,0 +1,6 @@
+static const AVFilter * const filter_list[] = {
+    &ff_asrc_abuffer,
+    &ff_vsrc_buffer,
+    &ff_asink_abuffer,
+    &ff_vsink_buffer,
+    NULL };
diff -uparN ffmpeg-4.1/libavfilter/x86/af_afir.asm ffmpeg-y/libavfilter/x86/af_afir.asm
--- ffmpeg-4.1/libavfilter/x86/af_afir.asm	2018-11-02 02:34:26.000000000 +0800
+++ ffmpeg-y/libavfilter/x86/af_afir.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,60 +0,0 @@
-;*****************************************************************************
-;* x86-optimized functions for afir filter
-;* Copyright (c) 2017 Paul B Mahol
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION .text
-
-;------------------------------------------------------------------------------
-; void ff_fcmul_add(float *sum, const float *t, const float *c, int len)
-;------------------------------------------------------------------------------
-
-INIT_XMM sse3
-cglobal fcmul_add, 4,4,6, sum, t, c, len
-    shl       lend, 3
-    add       lend, mmsize*2
-    add         tq, lenq
-    add         cq, lenq
-    add       sumq, lenq
-    neg       lenq
-ALIGN 16
-.loop:
-    movsldup  m0, [tq + lenq]
-    movsldup  m3, [tq + lenq+mmsize]
-    movaps    m1, [cq + lenq]
-    movaps    m4, [cq + lenq+mmsize]
-    mulps     m0, m1
-    mulps     m3, m4
-    shufps    m1, m1, 0xb1
-    shufps    m4, m4, 0xb1
-    movshdup  m2, [tq + lenq]
-    movshdup  m5, [tq + lenq+mmsize]
-    mulps     m2, m1
-    mulps     m5, m4
-    addsubps  m0, m2
-    addsubps  m3, m5
-    addps     m0, [sumq + lenq]
-    addps     m3, [sumq + lenq+mmsize]
-    movaps    [sumq + lenq], m0
-    movaps    [sumq + lenq+mmsize], m3
-    add       lenq, mmsize*2
-    jl .loop
-    REP_RET
diff -uparN ffmpeg-4.1/libavfilter/x86/af_volume.asm ffmpeg-y/libavfilter/x86/af_volume.asm
--- ffmpeg-4.1/libavfilter/x86/af_volume.asm	2018-01-01 06:35:48.000000000 +0800
+++ ffmpeg-y/libavfilter/x86/af_volume.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,140 +0,0 @@
-;*****************************************************************************
-;* x86-optimized functions for volume filter
-;* Copyright (c) 2012 Justin Ruggles <justin.ruggles@gmail.com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA 32
-
-pd_1_256:     times 4 dq 0x3F70000000000000
-pd_int32_max: times 4 dq 0x41DFFFFFFFC00000
-pw_1:         times 8 dw 1
-pw_128:       times 8 dw 128
-pq_128:       times 2 dq 128
-
-SECTION .text
-
-;------------------------------------------------------------------------------
-; void ff_scale_samples_s16(uint8_t *dst, const uint8_t *src, int len,
-;                           int volume)
-;------------------------------------------------------------------------------
-
-INIT_XMM sse2
-cglobal scale_samples_s16, 4,4,4, dst, src, len, volume
-    movd        m0, volumem
-    pshuflw     m0, m0, 0
-    punpcklwd   m0, [pw_1]
-    mova        m1, [pw_128]
-    lea       lenq, [lend*2-mmsize]
-.loop:
-    ; dst[i] = av_clip_int16((src[i] * volume + 128) >> 8);
-    mova        m2, [srcq+lenq]
-    punpcklwd   m3, m2, m1
-    punpckhwd   m2, m1
-    pmaddwd     m3, m0
-    pmaddwd     m2, m0
-    psrad       m3, 8
-    psrad       m2, 8
-    packssdw    m3, m2
-    mova  [dstq+lenq], m3
-    sub       lenq, mmsize
-    jge .loop
-    REP_RET
-
-;------------------------------------------------------------------------------
-; void ff_scale_samples_s32(uint8_t *dst, const uint8_t *src, int len,
-;                           int volume)
-;------------------------------------------------------------------------------
-
-%macro SCALE_SAMPLES_S32 0
-cglobal scale_samples_s32, 4,4,4, dst, src, len, volume
-%if ARCH_X86_32 && cpuflag(avx)
-    vbroadcastss   xmm2, volumem
-%else
-    movd           xmm2, volumed
-    pshufd         xmm2, xmm2, 0
-%endif
-    CVTDQ2PD         m2, xmm2
-    mulpd            m2, m2, [pd_1_256]
-    mova             m3, [pd_int32_max]
-    lea            lenq, [lend*4-mmsize]
-.loop:
-    CVTDQ2PD         m0, [srcq+lenq         ]
-    CVTDQ2PD         m1, [srcq+lenq+mmsize/2]
-    mulpd            m0, m0, m2
-    mulpd            m1, m1, m2
-    minpd            m0, m0, m3
-    minpd            m1, m1, m3
-    cvtpd2dq       xmm0, m0
-    cvtpd2dq       xmm1, m1
-%if cpuflag(avx)
-    vmovdqa [dstq+lenq         ], xmm0
-    vmovdqa [dstq+lenq+mmsize/2], xmm1
-%else
-    movq    [dstq+lenq         ], xmm0
-    movq    [dstq+lenq+mmsize/2], xmm1
-%endif
-    sub            lenq, mmsize
-    jge .loop
-    REP_RET
-%endmacro
-
-INIT_XMM sse2
-%define CVTDQ2PD cvtdq2pd
-SCALE_SAMPLES_S32
-%if HAVE_AVX_EXTERNAL
-%define CVTDQ2PD vcvtdq2pd
-INIT_YMM avx
-SCALE_SAMPLES_S32
-%endif
-%undef CVTDQ2PD
-
-; NOTE: This is not bit-identical with the C version because it clips to
-;       [-INT_MAX, INT_MAX] instead of [INT_MIN, INT_MAX]
-
-INIT_XMM ssse3, atom
-cglobal scale_samples_s32, 4,4,8, dst, src, len, volume
-    movd        m4, volumem
-    pshufd      m4, m4, 0
-    mova        m5, [pq_128]
-    pxor        m6, m6
-    lea       lenq, [lend*4-mmsize]
-.loop:
-    ; src[i] = av_clipl_int32((src[i] * volume + 128) >> 8);
-    mova        m7, [srcq+lenq]
-    pabsd       m3, m7
-    pshufd      m0, m3, q0100
-    pshufd      m1, m3, q0302
-    pmuludq     m0, m4
-    pmuludq     m1, m4
-    paddq       m0, m5
-    paddq       m1, m5
-    psrlq       m0, 7
-    psrlq       m1, 7
-    shufps      m2, m0, m1, q3131
-    shufps      m0, m0, m1, q2020
-    pcmpgtd     m2, m6
-    por         m0, m2
-    psrld       m0, 1
-    psignd      m0, m7
-    mova  [dstq+lenq], m0
-    sub       lenq, mmsize
-    jge .loop
-    REP_RET
diff -uparN ffmpeg-4.1/libavfilter/x86/avf_showcqt.asm ffmpeg-y/libavfilter/x86/avf_showcqt.asm
--- ffmpeg-4.1/libavfilter/x86/avf_showcqt.asm	2018-11-02 02:34:26.000000000 +0800
+++ ffmpeg-y/libavfilter/x86/avf_showcqt.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,192 +0,0 @@
-;*****************************************************************************
-;* x86-optimized functions for showcqt filter
-;*
-;* Copyright (C) 2016 Muhammad Faiz <mfcc64@gmail.com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-%if ARCH_X86_64
-%define pointer resq
-%else
-%define pointer resd
-%endif
-
-struc Coeffs
-    .val:   pointer 1
-    .start: resd 1
-    .len:   resd 1
-    .sizeof:
-endstruc
-
-%macro CQT_CALC 9
-; %1 = a_re, %2 = a_im, %3 = b_re, %4 = b_im
-; %5 = m_re, %6 = m_im, %7 = tmp, %8 = coeffval, %9 = coeffsq_offset
-    mov     id, xd
-    add     id, [coeffsq + Coeffs.start + %9]
-    movaps  m%5, [srcq + 8 * iq]
-    movaps  m%7, [srcq + 8 * iq + mmsize]
-    shufps  m%6, m%5, m%7, q3131
-    shufps  m%5, m%5, m%7, q2020
-    sub     id, fft_lend
-    FMULADD_PS m%2, m%6, m%8, m%2, m%6
-    neg     id
-    FMULADD_PS m%1, m%5, m%8, m%1, m%5
-    movups  m%5, [srcq + 8 * iq - mmsize + 8]
-    movups  m%7, [srcq + 8 * iq - 2*mmsize + 8]
-    %if mmsize == 32
-    vperm2f128 m%5, m%5, m%5, 1
-    vperm2f128 m%7, m%7, m%7, 1
-    %endif
-    shufps  m%6, m%5, m%7, q1313
-    shufps  m%5, m%5, m%7, q0202
-    FMULADD_PS m%4, m%6, m%8, m%4, m%6
-    FMULADD_PS m%3, m%5, m%8, m%3, m%5
-%endmacro ; CQT_CALC
-
-%macro CQT_SEPARATE 6 ; a_re, a_im, b_re, b_im, tmp, tmp2
-    addps   m%5, m%4, m%2
-    subps   m%6, m%3, m%1
-    addps   m%1, m%1, m%3
-    subps   m%2, m%2, m%4
-    HADDPS  m%5, m%6, m%3
-    HADDPS  m%1, m%2, m%3
-    HADDPS  m%1, m%5, m%2
-    %if mmsize == 32
-    vextractf128 xmm%2, m%1, 1
-    addps   xmm%1, xmm%2
-    %endif
-%endmacro ; CQT_SEPARATE
-
-%macro DECLARE_CQT_CALC 0
-; ff_showcqt_cqt_calc_*(dst, src, coeffs, len, fft_len)
-%if ARCH_X86_64
-cglobal showcqt_cqt_calc, 5, 10, 12, dst, src, coeffs, len, fft_len, x, coeffs_val, coeffs_val2, i, coeffs_len
-    align   16
-    .loop_k:
-        mov     xd, [coeffsq + Coeffs.len]
-        xorps   m0, m0, m0
-        movaps  m1, m0
-        movaps  m2, m0
-        mov     coeffs_lend, [coeffsq + Coeffs.len + Coeffs.sizeof]
-        movaps  m3, m0
-        movaps  m8, m0
-        cmp     coeffs_lend, xd
-        movaps  m9, m0
-        movaps  m10, m0
-        movaps  m11, m0
-        cmova   coeffs_lend, xd
-        xor     xd, xd
-        test    coeffs_lend, coeffs_lend
-        jz      .check_loop_b
-        mov     coeffs_valq, [coeffsq + Coeffs.val]
-        mov     coeffs_val2q, [coeffsq + Coeffs.val + Coeffs.sizeof]
-        align   16
-        .loop_ab:
-            movaps  m7, [coeffs_valq + 4 * xq]
-            CQT_CALC 0, 1, 2, 3, 4, 5, 6, 7, 0
-            movaps  m7, [coeffs_val2q + 4 * xq]
-            CQT_CALC 8, 9, 10, 11, 4, 5, 6, 7, Coeffs.sizeof
-            add     xd, mmsize/4
-            cmp     xd, coeffs_lend
-            jb      .loop_ab
-        .check_loop_b:
-        cmp     xd, [coeffsq + Coeffs.len + Coeffs.sizeof]
-        jae     .check_loop_a
-        align   16
-        .loop_b:
-            movaps  m7, [coeffs_val2q + 4 * xq]
-            CQT_CALC 8, 9, 10, 11, 4, 5, 6, 7, Coeffs.sizeof
-            add     xd, mmsize/4
-            cmp     xd, [coeffsq + Coeffs.len + Coeffs.sizeof]
-            jb      .loop_b
-        .loop_end:
-        CQT_SEPARATE 0, 1, 2, 3, 4, 5
-        CQT_SEPARATE 8, 9, 10, 11, 4, 5
-        mulps   xmm0, xmm0
-        mulps   xmm8, xmm8
-        HADDPS  xmm0, xmm8, xmm1
-        movaps  [dstq], xmm0
-        sub     lend, 2
-        lea     dstq, [dstq + 16]
-        lea     coeffsq, [coeffsq + 2*Coeffs.sizeof]
-        jnz     .loop_k
-        REP_RET
-        align   16
-        .check_loop_a:
-        cmp     xd, [coeffsq + Coeffs.len]
-        jae     .loop_end
-        align   16
-        .loop_a:
-            movaps  m7, [coeffs_valq + 4 * xq]
-            CQT_CALC 0, 1, 2, 3, 4, 5, 6, 7, 0
-            add     xd, mmsize/4
-            cmp     xd, [coeffsq + Coeffs.len]
-            jb      .loop_a
-        jmp     .loop_end
-%else
-cglobal showcqt_cqt_calc, 4, 7, 8, dst, src, coeffs, len, x, coeffs_val, i
-%define fft_lend r4m
-    align   16
-    .loop_k:
-        mov     xd, [coeffsq + Coeffs.len]
-        xorps   m0, m0, m0
-        movaps  m1, m0
-        movaps  m2, m0
-        movaps  m3, m0
-        test    xd, xd
-        jz      .store
-        mov     coeffs_valq, [coeffsq + Coeffs.val]
-        xor     xd, xd
-        align   16
-        .loop_x:
-            movaps  m7, [coeffs_valq + 4 * xq]
-            CQT_CALC 0, 1, 2, 3, 4, 5, 6, 7, 0
-            add     xd, mmsize/4
-            cmp     xd, [coeffsq + Coeffs.len]
-            jb      .loop_x
-        CQT_SEPARATE 0, 1, 2, 3, 4, 5
-        mulps   xmm0, xmm0
-        HADDPS  xmm0, xmm0, xmm1
-        .store:
-        movlps  [dstq], xmm0
-        sub     lend, 1
-        lea     dstq, [dstq + 8]
-        lea     coeffsq, [coeffsq + Coeffs.sizeof]
-        jnz     .loop_k
-        REP_RET
-%endif ; ARCH_X86_64
-%endmacro ; DECLARE_CQT_CALC
-
-INIT_XMM sse
-DECLARE_CQT_CALC
-INIT_XMM sse3
-DECLARE_CQT_CALC
-%if HAVE_AVX_EXTERNAL
-INIT_YMM avx
-DECLARE_CQT_CALC
-%endif
-%if HAVE_FMA3_EXTERNAL
-INIT_YMM fma3
-DECLARE_CQT_CALC
-%endif
-%if HAVE_FMA4_EXTERNAL
-INIT_XMM fma4
-DECLARE_CQT_CALC
-%endif
diff -uparN ffmpeg-4.1/libavfilter/x86/colorspacedsp.asm ffmpeg-y/libavfilter/x86/colorspacedsp.asm
--- ffmpeg-4.1/libavfilter/x86/colorspacedsp.asm	2018-11-02 02:34:26.000000000 +0800
+++ ffmpeg-y/libavfilter/x86/colorspacedsp.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,1097 +0,0 @@
-;*****************************************************************************
-;* x86-optimized functions for colorspace filter
-;*
-;* Copyright (C) 2016 Ronald S. Bultje <rsbultje@gmail.com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-
-pw_1: times 8 dw 1
-pw_2: times 8 dw 2
-pw_4: times 8 dw 4
-pw_8: times 8 dw 8
-pw_16: times 8 dw 16
-pw_64: times 8 dw 64
-pw_128: times 8 dw 128
-pw_256: times 8 dw 256
-pw_512: times 8 dw 512
-pw_1023: times 8 dw 1023
-pw_1024: times 8 dw 1024
-pw_2048: times 8 dw 2048
-pw_4095: times 8 dw 4095
-pw_8192: times 8 dw 8192
-pw_16384: times 8 dw 16384
-
-pd_1: times 4 dd 1
-pd_2: times 4 dd 2
-pd_128: times 4 dd 128
-pd_512: times 4 dd 512
-pd_2048: times 4 dd 2048
-pd_8192: times 4 dd 8192
-pd_32768: times 4 dd 32768
-pd_131072: times 4 dd 131072
-
-SECTION .text
-
-; void ff_yuv2yuv_420p8to8_sse2(uint8_t *yuv_out[3], ptrdiff_t yuv_out_stride[3],
-;                               uint8_t *yuv_in[3], ptrdiff_t yuv_in_stride[3],
-;                               int w, int h, const int16_t yuv2yuv_coeffs[3][3][8],
-;                               const int16_t yuv_offset[2][8])
-
-%if ARCH_X86_64
-%macro YUV2YUV_FN 4 ; in_bitdepth, out_bitdepth, log2_chroma_w (horiz), log2_chroma_h (vert)
-
-%assign %%sh (14 + %1 - %2)
-%assign %%rnd (1 << (%%sh - 1))
-%assign %%uvinoff (128 << (%1 - 8))
-%assign %%uvoutoff (128 << (%2 - 8))
-%if %3 == 0
-%assign %%ss 444
-%elif %4 == 0
-%assign %%ss 422
-%else ; %4 == 1
-%assign %%ss 420
-%endif ; %3/%4
-%if %2 != 8
-%assign %%maxval (1 << %2) - 1
-%endif ; %2 != 8
-
-%assign %%ypsh %%sh - 1
-%if %%ypsh > 14
-%assign %%yoffsh %%ypsh - 13
-%assign %%ypsh 14
-%else
-%assign %%yoffsh 1
-%endif
-%assign %%yprnd (1 << (%%yoffsh - 1))
-%assign %%ypmul (1 << %%ypsh)
-
-cglobal yuv2yuv_ %+ %%ss %+ p%1to%2, 8, 14, 16, 0 - (4 * mmsize), \
-                                     yo, yos, yi, yis, w, h, c, yoff, ui, vi, uo, vo
-%if %3 == 1
-    inc             wd
-    sar             wd, 1
-%if %4 == 1
-    inc             hd
-    sar             hd, 1
-%endif ; %4 == 1
-%endif ; %3 == 1
-    mov [rsp+3*mmsize+0], wd
-    mov [rsp+3*mmsize+4], hd
-
-    mova           m10, [cq]
-    pxor           m11, m11
-    mova           m12, [pd_ %+ %%uvoutoff]
-    pslld          m12, %%sh
-    paddd          m12, [pd_ %+ %%rnd]
-    mova           m13, [pw_ %+ %%uvinoff]
-    mova           m14, [yoffq+ 0]      ; y_off_in
-    mova           m15, [yoffq+16]      ; y_off_out
-%if %%yoffsh != 0
-    psllw          m15, %%yoffsh
-%endif
-    paddw          m15, [pw_ %+ %%yprnd]
-    punpcklwd      m10, m15
-    mova           m15, [pw_ %+ %%ypmul]
-    movh            m0, [cq+1*16]       ; cyu
-    movh            m1, [cq+2*16]       ; cyv
-    movh            m2, [cq+4*16]       ; cuu
-    movh            m3, [cq+5*16]       ; cuv
-    movh            m4, [cq+7*16]       ; cvu
-    movh            m5, [cq+8*16]       ; cvv
-    punpcklwd       m0, m1
-    punpcklwd       m2, m3
-    punpcklwd       m4, m5
-    mova [rsp+0*mmsize], m0
-    mova [rsp+1*mmsize], m2
-    mova [rsp+2*mmsize], m4
-
-    DEFINE_ARGS yo, yos, yi, yis, ui, vi, uo, vo, uis, vis, uos, vos, x, tmp
-
-    mov            uiq, [yiq+gprsize*1]
-    mov            viq, [yiq+gprsize*2]
-    mov            yiq, [yiq+gprsize*0]
-    mov            uoq, [yoq+gprsize*1]
-    mov            voq, [yoq+gprsize*2]
-    mov            yoq, [yoq+gprsize*0]
-    mov           uisq, [yisq+gprsize*1]
-    mov           visq, [yisq+gprsize*2]
-    mov           yisq, [yisq+gprsize*0]
-    mov           uosq, [yosq+gprsize*1]
-    mov           vosq, [yosq+gprsize*2]
-    mov           yosq, [yosq+gprsize*0]
-
-.loop_v:
-    xor             xq, xq
-
-.loop_h:
-%if %4 == 1
-    lea           tmpq, [yiq+yisq]
-%endif ; %4 == 1
-%if %1 == 8
-    movu            m0, [yiq+xq*(1<<%3)]        ; y00/01
-%if %4 == 1
-    movu            m2, [tmpq+xq*2]             ; y10/11
-%endif ; %4 == 1
-%if %3 == 1
-    movh            m4, [uiq+xq]                ; u
-    movh            m5, [viq+xq]                ; v
-%else ; %3 != 1
-    movu            m4, [uiq+xq]                ; u
-    movu            m5, [viq+xq]                ; v
-%endif ; %3 ==/!= 1
-    punpckhbw       m1, m0, m11
-    punpcklbw       m0, m11
-%if %4 == 1
-    punpckhbw       m3, m2, m11
-    punpcklbw       m2, m11
-%endif ; %4 == 1
-%if %3 == 0
-    punpckhbw       m2, m4, m11
-    punpckhbw       m3, m5, m11
-%endif ; %3 == 0
-    punpcklbw       m4, m11
-    punpcklbw       m5, m11
-%else ; %1 != 8
-    movu            m0, [yiq+xq*(2<<%3)]        ; y00/01
-    movu            m1, [yiq+xq*(2<<%3)+mmsize] ; y00/01
-%if %4 == 1
-    movu            m2, [tmpq+xq*4]             ; y10/11
-    movu            m3, [tmpq+xq*4+mmsize]      ; y10/11
-%endif ; %4 == 1
-    movu            m4, [uiq+xq*2]              ; u
-    movu            m5, [viq+xq*2]              ; v
-%if %3 == 0
-    movu            m2, [uiq+xq*2+mmsize]
-    movu            m3, [viq+xq*2+mmsize]
-%endif ; %3 == 0
-%endif ; %1 ==/!= 8
-    psubw           m0, m14
-    psubw           m1, m14
-%if %4 == 1
-    psubw           m2, m14
-    psubw           m3, m14
-%endif ; %4 == 1
-    psubw           m4, m13
-    psubw           m5, m13
-%if %3 == 0
-    psubw           m2, m13
-    psubw           m3, m13
-%endif ; %3 == 0
-
-    SBUTTERFLY   wd, 4, 5, 6
-    pmaddwd         m6, m4, [rsp+1*mmsize]
-    pmaddwd         m7, m5, [rsp+1*mmsize]
-%if %3 == 0
-    SBUTTERFLY   wd, 2, 3, 8
-    pmaddwd         m8, m2, [rsp+1*mmsize]
-    pmaddwd         m9, m3, [rsp+1*mmsize]
-%else ; %3 != 0
-    pmaddwd         m8, m4, [rsp+2*mmsize]
-    pmaddwd         m9, m5, [rsp+2*mmsize]
-%endif
-    paddd           m6, m12
-    paddd           m7, m12
-    paddd           m8, m12
-    paddd           m9, m12
-    psrad           m6, %%sh
-    psrad           m7, %%sh
-    psrad           m8, %%sh
-    psrad           m9, %%sh
-    packssdw        m6, m7
-    packssdw        m8, m9
-%if %2 == 8
-    packuswb        m6, m8
-%if %3 == 0
-    movu      [uoq+xq], m6
-%else ; %3 != 0
-    movh      [uoq+xq], m6
-    movhps    [voq+xq], m6
-%endif ; %3 ==/!= 0
-%else ; %2 != 8
-    CLIPW           m6, m11, [pw_ %+ %%maxval]
-    CLIPW           m8, m11, [pw_ %+ %%maxval]
-    movu    [uoq+xq*2], m6
-%if %3 == 0
-    movu    [uoq+xq*2+mmsize], m8
-%else ; %3 != 0
-    movu    [voq+xq*2], m8
-%endif ; %3 ==/!= 0
-%endif ; %2 ==/!= 8
-
-%if %3 == 0
-    pmaddwd         m6, m4, [rsp+2*mmsize]
-    pmaddwd         m7, m5, [rsp+2*mmsize]
-    pmaddwd         m8, m2, [rsp+2*mmsize]
-    pmaddwd         m9, m3, [rsp+2*mmsize]
-    paddd           m6, m12
-    paddd           m7, m12
-    paddd           m8, m12
-    paddd           m9, m12
-    psrad           m6, %%sh
-    psrad           m7, %%sh
-    psrad           m8, %%sh
-    psrad           m9, %%sh
-    packssdw        m6, m7
-    packssdw        m8, m9
-%if %2 == 8
-    packuswb        m6, m8
-    movu      [voq+xq], m6
-%else ; %2 != 8
-    CLIPW           m6, m11, [pw_ %+ %%maxval]
-    CLIPW           m8, m11, [pw_ %+ %%maxval]
-    movu    [voq+xq*2], m6
-    movu    [voq+xq*2+mmsize], m8
-%endif ; %2 ==/!= 8
-%endif ; %3 == 0
-
-    pmaddwd         m4, [rsp+0*mmsize]
-    pmaddwd         m5, [rsp+0*mmsize]          ; uv_val
-%if %3 == 0
-    pmaddwd         m2, [rsp+0*mmsize]
-    pmaddwd         m3, [rsp+0*mmsize]
-%endif ; %3 == 0
-
-    ; unpack y pixels with m15 (shifted round + offset), then multiply
-    ; by m10, add uv pixels, and we're done!
-%if %3 == 1
-    punpckhdq       m8, m4, m4
-    punpckldq       m4, m4
-    punpckhdq       m9, m5, m5
-    punpckldq       m5, m5
-%else ; %3 != 1
-    SWAP             8, 5, 2
-    SWAP             3, 9
-%endif ; %3 ==/!= 1
-%if %4 == 1
-    punpckhwd       m6, m2, m15
-    punpcklwd       m2, m15
-    punpckhwd       m7, m3, m15
-    punpcklwd       m3, m15
-    pmaddwd         m2, m10
-    pmaddwd         m6, m10
-    pmaddwd         m3, m10
-    pmaddwd         m7, m10
-    paddd           m2, m4
-    paddd           m6, m8
-    paddd           m3, m5
-    paddd           m7, m9
-    psrad           m2, %%sh
-    psrad           m6, %%sh
-    psrad           m3, %%sh
-    psrad           m7, %%sh
-    packssdw        m2, m6
-    packssdw        m3, m7
-
-    lea           tmpq, [yoq+yosq]
-%if %2 == 8
-    packuswb        m2, m3
-    movu   [tmpq+xq*2], m2
-%else ; %2 != 8
-    CLIPW           m2, m11, [pw_ %+ %%maxval]
-    CLIPW           m3, m11, [pw_ %+ %%maxval]
-    movu   [tmpq+xq*4], m2
-    movu [tmpq+xq*4+mmsize], m3
-%endif ; %2 ==/!= 8
-%endif ; %4 == 1
-
-    punpckhwd       m6, m0, m15
-    punpcklwd       m0, m15
-    punpckhwd       m7, m1, m15
-    punpcklwd       m1, m15
-    pmaddwd         m0, m10
-    pmaddwd         m6, m10
-    pmaddwd         m1, m10
-    pmaddwd         m7, m10
-    paddd           m0, m4
-    paddd           m6, m8
-    paddd           m1, m5
-    paddd           m7, m9
-    psrad           m0, %%sh
-    psrad           m6, %%sh
-    psrad           m1, %%sh
-    psrad           m7, %%sh
-    packssdw        m0, m6
-    packssdw        m1, m7
-
-%if %2 == 8
-    packuswb        m0, m1
-    movu    [yoq+xq*(1<<%3)], m0
-%else ; %2 != 8
-    CLIPW           m0, m11, [pw_ %+ %%maxval]
-    CLIPW           m1, m11, [pw_ %+ %%maxval]
-    movu  [yoq+xq*(2<<%3)], m0
-    movu [yoq+xq*(2<<%3)+mmsize], m1
-%endif ; %2 ==/!= 8
-
-    add             xq, mmsize >> %3
-    cmp             xd, dword [rsp+3*mmsize+0]
-    jl .loop_h
-
-%if %4 == 1
-    lea            yiq, [yiq+yisq*2]
-    lea            yoq, [yoq+yosq*2]
-%else ; %4 != 1
-    add            yiq, yisq
-    add            yoq, yosq
-%endif ; %4 ==/!= 1
-    add            uiq, uisq
-    add            viq, visq
-    add            uoq, uosq
-    add            voq, vosq
-    dec dword [rsp+3*mmsize+4]
-    jg .loop_v
-
-    RET
-%endmacro
-
-%macro YUV2YUV_FNS 2 ; ss_w, ss_h
-YUV2YUV_FN  8,  8, %1, %2
-YUV2YUV_FN 10,  8, %1, %2
-YUV2YUV_FN 12,  8, %1, %2
-YUV2YUV_FN  8, 10, %1, %2
-YUV2YUV_FN 10, 10, %1, %2
-YUV2YUV_FN 12, 10, %1, %2
-YUV2YUV_FN  8, 12, %1, %2
-YUV2YUV_FN 10, 12, %1, %2
-YUV2YUV_FN 12, 12, %1, %2
-%endmacro
-
-INIT_XMM sse2
-YUV2YUV_FNS 0, 0
-YUV2YUV_FNS 1, 0
-YUV2YUV_FNS 1, 1
-
-; void ff_yuv2rgb_420p8_sse2(int16_t *rgb[3], ptrdiff_t rgb_stride,
-;                            uint8_t *yuv[3], ptrdiff_t yuv_stride[3],
-;                            int w, int h, const int16_t yuv2rgb_coeffs[3][3][8],
-;                            const int16_t yuv_offset[8])
-%macro YUV2RGB_FN 3 ; depth, log2_chroma_w (horiz), log2_chroma_h (vert)
-%assign %%sh (%1 - 1)
-%assign %%rnd (1 << (%%sh - 1))
-%assign %%uvoff (1 << (%1 - 1))
-%if %2 == 0
-%assign %%ss 444
-%elif %3 == 0
-%assign %%ss 422
-%else ; %3 == 1
-%assign %%ss 420
-%endif ; %2/%3
-
-cglobal yuv2rgb_ %+ %%ss %+ p%1, 8, 14, 16, 0 - 8 * mmsize, \
-                                rgb, rgbs, yuv, yuvs, ww, h, c, yoff
-%if %2 == 1
-    inc            wwd
-    sar            wwd, 1
-%endif ; %2 == 1
-%if %3 == 1
-    inc             hd
-    sar             hd, 1
-%endif ; %3 == 1
-    pxor           m11, m11
-    mova           m15, [yoffq]                 ; yoff
-    movh           m14, [cq+  0]                ; cy
-    movh           m10, [cq+ 32]                ; crv
-    movh           m13, [cq+112]                ; cbu
-    movh           m12, [cq+ 64]                ; cgu
-    movh            m9, [cq+ 80]                ; cgv
-    punpcklwd      m14, [pw_ %+ %%rnd]          ; cy, rnd
-    punpcklwd      m13, m11                     ; cbu, 0
-    punpcklwd      m11, m10                     ; 0, crv
-    punpcklwd      m12, m9                      ; cgu, cgv
-    mova [rsp+0*mmsize], m11
-    mova [rsp+1*mmsize], m12
-    mova [rsp+2*mmsize], m13
-    mova [rsp+3*mmsize], m14
-    pxor           m14, m14
-
-    DEFINE_ARGS r, rgbs, y, ys, ww, h, g, b, u, v, us, vs, x, tmp
-
-    mov             gq, [rq+1*gprsize]
-    mov             bq, [rq+2*gprsize]
-    mov             rq, [rq+0*gprsize]
-    mov             uq, [yq+1*gprsize]
-    mov             vq, [yq+2*gprsize]
-    mov             yq, [yq+0*gprsize]
-    mov            usq, [ysq+1*gprsize]
-    mov            vsq, [ysq+2*gprsize]
-    mov            ysq, [ysq+0*gprsize]
-
-.loop_v:
-    xor             xq, xq
-
-.loop_h:
-%if %3 == 1
-    lea           tmpq, [yq+ysq]
-%endif ; %3 == 1
-%if %1 == 8
-    movu            m0, [yq+xq*(1<<%2)]
-%if %3 == 1
-    movu            m2, [tmpq+xq*2]
-%endif ; %3 == 1
-%if %2 == 1
-    movh            m4, [uq+xq]
-    movh            m5, [vq+xq]
-%else ; %2 != 1
-    movu            m4, [uq+xq]
-    movu            m5, [vq+xq]
-%endif ; %2 ==/!= 1
-    punpckhbw       m1, m0, m14
-    punpcklbw       m0, m14
-%if %3 == 1
-    punpckhbw       m3, m2, m14
-    punpcklbw       m2, m14
-%endif ; %3 == 1
-%if %2 == 0
-    punpckhbw       m2, m4, m14
-    punpckhbw       m3, m5, m14
-%endif ; %2 == 0
-    punpcklbw       m4, m14
-    punpcklbw       m5, m14
-%else ; %1 != 8
-    movu            m0, [yq+xq*(2<<%2)]
-    movu            m1, [yq+xq*(2<<%2)+mmsize]
-%if %3 == 1
-    movu            m2, [tmpq+xq*4]
-    movu            m3, [tmpq+xq*4+mmsize]
-%endif ; %3 == 1
-    movu            m4, [uq+xq*2]
-    movu            m5, [vq+xq*2]
-%if %2 == 0
-    movu            m2, [uq+xq*2+mmsize]
-    movu            m3, [vq+xq*2+mmsize]
-%endif ; %2 == 0
-%endif ; %1 ==/!= 8
-    psubw           m0, m15
-    psubw           m1, m15
-%if %3 == 1
-    psubw           m2, m15
-    psubw           m3, m15
-%endif ; %3 == 1
-    psubw           m4, [pw_ %+ %%uvoff]
-    psubw           m5, [pw_ %+ %%uvoff]
-    SBUTTERFLY   wd, 4, 5, 6
-%if %2 == 0
-    psubw           m2, [pw_ %+ %%uvoff]
-    psubw           m3, [pw_ %+ %%uvoff]
-    SBUTTERFLY   wd, 2, 3, 6
-%endif ; %2 == 0
-
-    ; calculate y+rnd full-resolution [0-3,6-9]
-    punpckhwd       m6, m0, [pw_1]              ; y, 1
-    punpcklwd       m0, [pw_1]                  ; y, 1
-    punpckhwd       m7, m1, [pw_1]              ; y, 1
-    punpcklwd       m1, [pw_1]                  ; y, 1
-    pmaddwd         m0, [rsp+3*mmsize]
-    pmaddwd         m6, [rsp+3*mmsize]
-    pmaddwd         m1, [rsp+3*mmsize]
-    pmaddwd         m7, [rsp+3*mmsize]
-%if %3 == 1
-    punpckhwd       m8, m2, [pw_1]              ; y, 1
-    punpcklwd       m2, [pw_1]                  ; y, 1
-    punpckhwd       m9, m3, [pw_1]              ; y, 1
-    punpcklwd       m3, [pw_1]                  ; y, 1
-    pmaddwd         m2, [rsp+3*mmsize]
-    pmaddwd         m8, [rsp+3*mmsize]
-    pmaddwd         m3, [rsp+3*mmsize]
-    pmaddwd         m9, [rsp+3*mmsize]
-    mova [rsp+4*mmsize], m2
-    mova [rsp+5*mmsize], m8
-    mova [rsp+6*mmsize], m3
-    mova [rsp+7*mmsize], m9
-%endif ; %3 == 1
-
-    ; calculate r offsets (un-subsampled, then duplicate)
-    pmaddwd        m10, m4, [rsp+0*mmsize]
-%if %2 == 1
-    pmaddwd        m12, m5, [rsp+0*mmsize]
-    punpckhdq      m11, m10, m10
-    punpckldq      m10, m10
-    punpckhdq      m13, m12, m12
-    punpckldq      m12, m12
-%else ; %2 != 1
-    pmaddwd        m11, m5, [rsp+0*mmsize]
-    pmaddwd        m12, m2, [rsp+0*mmsize]
-    pmaddwd        m13, m3, [rsp+0*mmsize]
-%endif ; %2 ==/!= 1
-%if %3 == 1
-    paddd           m2, m10, [rsp+4*mmsize]
-    paddd           m3, m11, [rsp+5*mmsize]
-    paddd           m8, m12, [rsp+6*mmsize]
-    paddd           m9, m13, [rsp+7*mmsize]
-%endif
-    paddd          m10, m0
-    paddd          m11, m6
-    paddd          m12, m1
-    paddd          m13, m7
-%if %3 == 1
-    psrad           m2, %%sh
-    psrad           m3, %%sh
-    psrad           m8, %%sh
-    psrad           m9, %%sh
-%endif ; %3 == 1
-    psrad          m10, %%sh
-    psrad          m11, %%sh
-    psrad          m12, %%sh
-    psrad          m13, %%sh
-%if %3 == 1
-    lea           tmpq, [rq+rgbsq*2]
-    packssdw        m2, m3
-    packssdw        m8, m9
-    mova [tmpq+xq*4], m2
-    mova [tmpq+xq*4+mmsize], m8
-%endif ; %3 == 1
-    packssdw       m10, m11
-    packssdw       m12, m13
-    mova   [rq+xq*(2 << %2)], m10
-    mova   [rq+xq*(2 << %2)+mmsize], m12
-
-    ; calculate g offsets (un-subsampled, then duplicate)
-    pmaddwd        m10, m4, [rsp+1*mmsize]
-%if %2 == 1
-    pmaddwd        m12, m5, [rsp+1*mmsize]
-    punpckhdq      m11, m10, m10
-    punpckldq      m10, m10
-    punpckhdq      m13, m12, m12
-    punpckldq      m12, m12
-%else ; %2 != 1
-    pmaddwd        m11, m5, [rsp+1*mmsize]
-    pmaddwd        m12, m2, [rsp+1*mmsize]
-    pmaddwd        m13, m3, [rsp+1*mmsize]
-%endif ; %2 ==/!= 1
-%if %3 == 1
-    paddd           m2, m10, [rsp+4*mmsize]
-    paddd           m3, m11, [rsp+5*mmsize]
-    paddd           m8, m12, [rsp+6*mmsize]
-    paddd           m9, m13, [rsp+7*mmsize]
-%endif ; %3 == 1
-    paddd          m10, m0
-    paddd          m11, m6
-    paddd          m12, m1
-    paddd          m13, m7
-%if %3 == 1
-    psrad           m2, %%sh
-    psrad           m3, %%sh
-    psrad           m8, %%sh
-    psrad           m9, %%sh
-%endif ; %3 == 1
-    psrad          m10, %%sh
-    psrad          m11, %%sh
-    psrad          m12, %%sh
-    psrad          m13, %%sh
-%if %3 == 1
-    lea           tmpq, [gq+rgbsq*2]
-    packssdw        m2, m3
-    packssdw        m8, m9
-    mova [tmpq+xq*4], m2
-    mova [tmpq+xq*4+mmsize], m8
-%endif ; %3 == 1
-    packssdw       m10, m11
-    packssdw       m12, m13
-    mova   [gq+xq*(2 << %2)], m10
-    mova   [gq+xq*(2 << %2)+mmsize], m12
-
-    ; calculate b offsets (un-subsampled, then duplicate)
-    pmaddwd         m4, [rsp+2*mmsize]
-    pmaddwd         m5, [rsp+2*mmsize]
-%if %2 == 1
-    punpckhdq       m2, m4, m4
-    punpckldq       m4, m4
-    punpckhdq       m3, m5, m5
-    punpckldq       m5, m5
-%else ; %2 != 1
-    pmaddwd         m2, [rsp+2*mmsize]
-    pmaddwd         m3, [rsp+2*mmsize]
-    SWAP             2, 5
-%endif ; %2 ==/!= 1
-    paddd           m0, m4
-    paddd           m6, m2
-    paddd           m1, m5
-    paddd           m7, m3
-%if %3 == 1
-    paddd           m4, [rsp+4*mmsize]
-    paddd           m2, [rsp+5*mmsize]
-    paddd           m5, [rsp+6*mmsize]
-    paddd           m3, [rsp+7*mmsize]
-%endif ; %3 == 1
-    psrad           m0, %%sh
-    psrad           m6, %%sh
-    psrad           m1, %%sh
-    psrad           m7, %%sh
-%if %3 == 1
-    psrad           m4, %%sh
-    psrad           m2, %%sh
-    psrad           m5, %%sh
-    psrad           m3, %%sh
-%endif ; %3 == 1
-    packssdw        m0, m6
-    packssdw        m1, m7
-    movu   [bq+xq*(2 << %2)], m0
-    movu   [bq+xq*(2 << %2)+mmsize], m1
-%if %3 == 1
-    lea           tmpq, [bq+rgbsq*2]
-    packssdw        m4, m2
-    packssdw        m5, m3
-    movu [tmpq+xq*4], m4
-    movu [tmpq+xq*4+mmsize], m5
-%endif ; %3 == 1
-
-    add             xd, mmsize >> %2
-    cmp             xd, wwd
-    jl .loop_h
-
-    lea             rq, [rq+rgbsq*(2 << %3)]
-    lea             gq, [gq+rgbsq*(2 << %3)]
-    lea             bq, [bq+rgbsq*(2 << %3)]
-%if %3 == 1
-    lea             yq, [yq+ysq*2]
-%else ; %3 != 0
-    add             yq, ysq
-%endif ; %3 ==/!= 1
-    add             uq, usq
-    add             vq, vsq
-    dec             hd
-    jg .loop_v
-
-    RET
-%endmacro
-
-%macro YUV2RGB_FNS 2
-YUV2RGB_FN  8, %1, %2
-YUV2RGB_FN 10, %1, %2
-YUV2RGB_FN 12, %1, %2
-%endmacro
-
-INIT_XMM sse2
-YUV2RGB_FNS 0, 0
-YUV2RGB_FNS 1, 0
-YUV2RGB_FNS 1, 1
-
-%macro RGB2YUV_FN 3 ; depth, log2_chroma_w (horiz), log2_chroma_h (vert)
-%assign %%sh 29 - %1
-%assign %%rnd (1 << (%%sh - 15))
-%assign %%uvrnd ((128 << (%1 - 8)) << (%%sh - 14))
-%if %1 != 8
-%assign %%maxval ((1 << %1) - 1)
-%endif ; %1 != 8
-%if %2 == 0
-%assign %%ss 444
-%elif %3 == 0
-%assign %%ss 422
-%else ; %3 == 1
-%assign %%ss 420
-%endif ; %2/%3
-
-cglobal rgb2yuv_ %+ %%ss %+ p%1, 8, 14, 16, 0 - 6 * mmsize, \
-                                 yuv, yuvs, rgb, rgbs, ww, h, c, off
-%if %2 == 1
-    inc            wwd
-    sar            wwd, 1
-%endif ; %2 == 1
-%if %3 == 1
-    inc             hd
-    sar             hd, 1
-%endif ; %3 == 1
-
-    ; prepare coeffs
-    movh            m8, [offq]
-    movh            m9, [pw_ %+ %%uvrnd]
-    psllw           m8, %%sh - 14
-    paddw           m9, [pw_ %+ %%rnd]
-    paddw           m8, [pw_ %+ %%rnd]
-    movh            m0, [cq+  0]
-    movh            m1, [cq+ 16]
-    movh            m2, [cq+ 32]
-    movh            m3, [cq+ 48]
-    movh            m4, [cq+ 64]
-    movh            m5, [cq+ 80]
-    movh            m6, [cq+112]
-    movh            m7, [cq+128]
-    punpcklwd       m0, m1
-    punpcklwd       m2, m8
-    punpcklwd       m3, m4
-    punpcklwd       m4, m5, m9
-    punpcklwd       m5, m6
-    punpcklwd       m7, m9
-
-    mova [rsp+0*mmsize], m0                 ; cry, cgy
-    mova [rsp+1*mmsize], m2                 ; cby, off + rnd
-    mova [rsp+2*mmsize], m3                 ; cru, cgu
-    mova [rsp+3*mmsize], m4                 ; cburv, uvoff + rnd
-    mova [rsp+4*mmsize], m5                 ; cburv, cgv
-    mova [rsp+5*mmsize], m7                 ; cbv, uvoff + rnd
-
-
-    DEFINE_ARGS y, ys, r, rgbs, ww, h, u, v, us, vs, g, b, tmp, x
-    mov             gq, [rq+gprsize*1]
-    mov             bq, [rq+gprsize*2]
-    mov             rq, [rq+gprsize*0]
-    mov             uq, [yq+gprsize*1]
-    mov             vq, [yq+gprsize*2]
-    mov             yq, [yq+gprsize*0]
-    mov            usq, [ysq+gprsize*1]
-    mov            vsq, [ysq+gprsize*2]
-    mov            ysq, [ysq+gprsize*0]
-
-    pxor           m15, m15
-.loop_v:
-    xor             xd, xd
-
-.loop_h:
-    ; top line y
-    mova            m0, [rq+xq*(2<<%2)]
-    mova            m3, [rq+xq*(2<<%2)+mmsize]
-    mova            m1, [gq+xq*(2<<%2)]
-    mova            m4, [gq+xq*(2<<%2)+mmsize]
-    mova            m2, [bq+xq*(2<<%2)]
-    mova            m5, [bq+xq*(2<<%2)+mmsize]
-
-    punpcklwd       m6, m0, m1
-    punpckhwd       m7, m0, m1
-    punpcklwd       m8, m3, m4
-    punpckhwd       m9, m3, m4
-    punpcklwd      m10, m2, [pw_16384]
-    punpckhwd      m11, m2, [pw_16384]
-    punpcklwd      m12, m5, [pw_16384]
-    punpckhwd      m13, m5, [pw_16384]
-
-    pmaddwd         m6, [rsp+0*mmsize]
-    pmaddwd         m7, [rsp+0*mmsize]
-    pmaddwd         m8, [rsp+0*mmsize]
-    pmaddwd         m9, [rsp+0*mmsize]
-    pmaddwd        m10, [rsp+1*mmsize]
-    pmaddwd        m11, [rsp+1*mmsize]
-    pmaddwd        m12, [rsp+1*mmsize]
-    pmaddwd        m13, [rsp+1*mmsize]
-    paddd           m6, m10
-    paddd           m7, m11
-    paddd           m8, m12
-    paddd           m9, m13
-    psrad           m6, %%sh
-    psrad           m7, %%sh
-    psrad           m8, %%sh
-    psrad           m9, %%sh
-    packssdw        m6, m7
-    packssdw        m8, m9
-%if %1 == 8
-    packuswb        m6, m8
-    movu [yq+xq*(1<<%2)], m6
-%else
-    CLIPW           m6, m15, [pw_ %+ %%maxval]
-    CLIPW           m8, m15, [pw_ %+ %%maxval]
-    movu [yq+xq*(2<<%2)], m6
-    movu [yq+xq*(2<<%2)+mmsize], m8
-%endif
-
-%if %2 == 1
-    ; subsampling cached data
-    pmaddwd         m0, [pw_1]
-    pmaddwd         m1, [pw_1]
-    pmaddwd         m2, [pw_1]
-    pmaddwd         m3, [pw_1]
-    pmaddwd         m4, [pw_1]
-    pmaddwd         m5, [pw_1]
-
-%if %3 == 1
-    ; bottom line y, r/g portion only
-    lea           tmpq, [rgbsq+xq*2]
-    mova            m6, [rq+tmpq*2]
-    mova            m9, [rq+tmpq*2+mmsize]
-    mova            m7, [gq+tmpq*2]
-    mova           m10, [gq+tmpq*2+mmsize]
-    mova            m8, [bq+tmpq*2]
-    mova           m11, [bq+tmpq*2+mmsize]
-
-    punpcklwd      m12, m6, m7
-    punpckhwd      m13, m6, m7
-    punpcklwd      m14, m9, m10
-    punpckhwd      m15, m9, m10
-
-    ; release two more registers
-    pmaddwd         m6, [pw_1]
-    pmaddwd         m7, [pw_1]
-    pmaddwd         m9, [pw_1]
-    pmaddwd        m10, [pw_1]
-    paddd           m0, m6
-    paddd           m3, m9
-    paddd           m1, m7
-    paddd           m4, m10
-
-    ; bottom line y, b/rnd portion only
-    punpcklwd       m6, m8,  [pw_16384]
-    punpckhwd       m7, m8,  [pw_16384]
-    punpcklwd       m9, m11, [pw_16384]
-    punpckhwd      m10, m11, [pw_16384]
-
-    pmaddwd        m12, [rsp+0*mmsize]
-    pmaddwd        m13, [rsp+0*mmsize]
-    pmaddwd        m14, [rsp+0*mmsize]
-    pmaddwd        m15, [rsp+0*mmsize]
-    pmaddwd         m6, [rsp+1*mmsize]
-    pmaddwd         m7, [rsp+1*mmsize]
-    pmaddwd         m9, [rsp+1*mmsize]
-    pmaddwd        m10, [rsp+1*mmsize]
-    paddd          m12, m6
-    paddd          m13, m7
-    paddd          m14, m9
-    paddd          m15, m10
-    psrad          m12, %%sh
-    psrad          m13, %%sh
-    psrad          m14, %%sh
-    psrad          m15, %%sh
-    packssdw       m12, m13
-    packssdw       m14, m15
-    lea           tmpq, [yq+ysq]
-%if %1 == 8
-    packuswb       m12, m14
-    movu   [tmpq+xq*2], m12
-%else
-    pxor           m15, m15
-    CLIPW          m12, m15, [pw_ %+ %%maxval]
-    CLIPW          m14, m15, [pw_ %+ %%maxval]
-    movu   [tmpq+xq*4], m12
-    movu [tmpq+xq*4+mmsize], m14
-%endif
-
-    ; complete subsampling of r/g/b pixels for u/v
-    pmaddwd         m8, [pw_1]
-    pmaddwd        m11, [pw_1]
-    paddd           m2, m8
-    paddd           m5, m11
-    paddd           m0, [pd_2]
-    paddd           m1, [pd_2]
-    paddd           m2, [pd_2]
-    paddd           m3, [pd_2]
-    paddd           m4, [pd_2]
-    paddd           m5, [pd_2]
-    psrad           m0, 2
-    psrad           m1, 2
-    psrad           m2, 2
-    psrad           m3, 2
-    psrad           m4, 2
-    psrad           m5, 2
-%else ; %3 != 1
-    paddd           m0, [pd_1]
-    paddd           m1, [pd_1]
-    paddd           m2, [pd_1]
-    paddd           m3, [pd_1]
-    paddd           m4, [pd_1]
-    paddd           m5, [pd_1]
-    psrad           m0, 1
-    psrad           m1, 1
-    psrad           m2, 1
-    psrad           m3, 1
-    psrad           m4, 1
-    psrad           m5, 1
-%endif ; %3 ==/!= 1
-    packssdw        m0, m3
-    packssdw        m1, m4
-    packssdw        m2, m5
-%endif ; %2 == 1
-
-    ; convert u/v pixels
-    SBUTTERFLY   wd, 0, 1, 6
-    punpckhwd       m6, m2, [pw_16384]
-    punpcklwd       m2, [pw_16384]
-
-    pmaddwd         m7, m0, [rsp+2*mmsize]
-    pmaddwd         m8, m1, [rsp+2*mmsize]
-    pmaddwd         m9, m2, [rsp+3*mmsize]
-    pmaddwd        m10, m6, [rsp+3*mmsize]
-    pmaddwd         m0, [rsp+4*mmsize]
-    pmaddwd         m1, [rsp+4*mmsize]
-    pmaddwd         m2, [rsp+5*mmsize]
-    pmaddwd         m6, [rsp+5*mmsize]
-    paddd           m7, m9
-    paddd           m8, m10
-    paddd           m0, m2
-    paddd           m1, m6
-    psrad           m7, %%sh
-    psrad           m8, %%sh
-    psrad           m0, %%sh
-    psrad           m1, %%sh
-    packssdw        m7, m8
-    packssdw        m0, m1
-%if %2 == 1
-%if %1 == 8
-    packuswb        m7, m0
-    movh       [uq+xq], m7
-    movhps     [vq+xq], m7
-%else
-    CLIPW           m7, m15, [pw_ %+ %%maxval]
-    CLIPW           m0, m15, [pw_ %+ %%maxval]
-    movu     [uq+xq*2], m7
-    movu     [vq+xq*2], m0
-%endif
-%else ; %2 != 1
-    ; second set of u/v pixels
-    SBUTTERFLY   wd, 3, 4, 6
-    punpckhwd       m6, m5, [pw_16384]
-    punpcklwd       m5, [pw_16384]
-
-    pmaddwd         m8, m3, [rsp+2*mmsize]
-    pmaddwd         m9, m4, [rsp+2*mmsize]
-    pmaddwd        m10, m5, [rsp+3*mmsize]
-    pmaddwd        m11, m6, [rsp+3*mmsize]
-    pmaddwd         m3, [rsp+4*mmsize]
-    pmaddwd         m4, [rsp+4*mmsize]
-    pmaddwd         m5, [rsp+5*mmsize]
-    pmaddwd         m6, [rsp+5*mmsize]
-    paddd           m8, m10
-    paddd           m9, m11
-    paddd           m3, m5
-    paddd           m4, m6
-    psrad           m8, %%sh
-    psrad           m9, %%sh
-    psrad           m3, %%sh
-    psrad           m4, %%sh
-    packssdw        m8, m9
-    packssdw        m3, m4
-
-%if %1 == 8
-    packuswb        m7, m8
-    packuswb        m0, m3
-    movu       [uq+xq], m7
-    movu       [vq+xq], m0
-%else
-    CLIPW           m7, m15, [pw_ %+ %%maxval]
-    CLIPW           m0, m15, [pw_ %+ %%maxval]
-    CLIPW           m8, m15, [pw_ %+ %%maxval]
-    CLIPW           m3, m15, [pw_ %+ %%maxval]
-    movu     [uq+xq*2], m7
-    movu [uq+xq*2+mmsize], m8
-    movu     [vq+xq*2], m0
-    movu [vq+xq*2+mmsize], m3
-%endif
-%endif ; %2 ==/!= 1
-
-    add             xq, mmsize >> %2
-    cmp             xd, wwd
-    jl .loop_h
-
-%if %3 == 0
-    add             yq, ysq
-%else ; %3 != 0
-    lea             yq, [yq+ysq*2]
-%endif ; %3 ==/!= 0
-    add             uq, usq
-    add             vq, vsq
-    lea             rq, [rq+rgbsq*(2<<%3)]
-    lea             gq, [gq+rgbsq*(2<<%3)]
-    lea             bq, [bq+rgbsq*(2<<%3)]
-    dec             hd
-    jg .loop_v
-
-    RET
-%endmacro
-
-%macro RGB2YUV_FNS 2
-RGB2YUV_FN  8, %1, %2
-RGB2YUV_FN 10, %1, %2
-RGB2YUV_FN 12, %1, %2
-%endmacro
-
-INIT_XMM sse2
-RGB2YUV_FNS 0, 0
-RGB2YUV_FNS 1, 0
-RGB2YUV_FNS 1, 1
-
-; void ff_multiply3x3_sse2(int16_t *data[3], ptrdiff_t stride,
-;                          int w, int h, const int16_t coeff[3][3][8])
-INIT_XMM sse2
-cglobal multiply3x3, 5, 7, 16, data, stride, ww, h, c
-    movh            m0, [cq+  0]
-    movh            m1, [cq+ 32]
-    movh            m2, [cq+ 48]
-    movh            m3, [cq+ 80]
-    movh            m4, [cq+ 96]
-    movh            m5, [cq+128]
-    punpcklwd       m0, [cq+ 16]
-    punpcklwd       m1, [pw_8192]
-    punpcklwd       m2, [cq+ 64]
-    punpcklwd       m3, [pw_8192]
-    punpcklwd       m4, [cq+112]
-    punpcklwd       m5, [pw_8192]
-
-    DEFINE_ARGS data0, stride, ww, h, data1, data2, x
-    shl        strideq, 1
-    mov         data1q, [data0q+gprsize*1]
-    mov         data2q, [data0q+gprsize*2]
-    mov         data0q, [data0q+gprsize*0]
-
-.loop_v:
-    xor             xd, xd
-
-.loop_h:
-    mova            m6, [data0q+xq*2]
-    mova            m7, [data1q+xq*2]
-    mova            m8, [data2q+xq*2]
-    SBUTTERFLY   wd, 6, 7, 9
-    punpckhwd       m9, m8, [pw_1]
-    punpcklwd       m8, [pw_1]
-
-    pmaddwd        m10, m6, m0
-    pmaddwd        m11, m7, m0
-    pmaddwd        m12, m8, m1
-    pmaddwd        m13, m9, m1
-    paddd          m10, m12
-    paddd          m11, m13
-    psrad          m10, 14
-    psrad          m11, 14
-
-    pmaddwd        m12, m6, m2
-    pmaddwd        m13, m7, m2
-    pmaddwd        m14, m8, m3
-    pmaddwd        m15, m9, m3
-    paddd          m12, m14
-    paddd          m13, m15
-    psrad          m12, 14
-    psrad          m13, 14
-
-    pmaddwd         m6, m4
-    pmaddwd         m7, m4
-    pmaddwd         m8, m5
-    pmaddwd         m9, m5
-    paddd           m6, m8
-    paddd           m7, m9
-    psrad           m6, 14
-    psrad           m7, 14
-
-    packssdw       m10, m11
-    packssdw       m12, m13
-    packssdw        m6, m7
-
-    mova [data0q+xq*2], m10
-    mova [data1q+xq*2], m12
-    mova [data2q+xq*2], m6
-
-    add             xd, mmsize / 2
-    cmp             xd, wwd
-    jl .loop_h
-
-    add         data0q, strideq
-    add         data1q, strideq
-    add         data2q, strideq
-    dec             hd
-    jg .loop_v
-
-    RET
-%endif
diff -uparN ffmpeg-4.1/libavfilter/x86/vf_blend.asm ffmpeg-y/libavfilter/x86/vf_blend.asm
--- ffmpeg-4.1/libavfilter/x86/vf_blend.asm	2018-11-03 08:17:29.000000000 +0800
+++ ffmpeg-y/libavfilter/x86/vf_blend.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,498 +0,0 @@
-;*****************************************************************************
-;* x86-optimized functions for blend filter
-;*
-;* Copyright (C) 2015 Paul B Mahol
-;* Copyright (C) 2018 Henrik Gramner
-;* Copyright (C) 2018 Jokyo Images
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-
-ps_255: times 4 dd 255.0
-pd_32768 : times 4 dd 32768
-pd_65535 : times 4 dd 65535
-pw_1:   times 8 dw 1
-pw_128: times 8 dw 128
-pw_255: times 8 dw 255
-pb_127: times 16 db 127
-pb_128: times 16 db 128
-pb_255: times 16 db 255
-
-SECTION .text
-
-%macro BLEND_INIT 2-3
-%if ARCH_X86_64
-cglobal blend_%1, 6, 9, %2, top, top_linesize, bottom, bottom_linesize, dst, dst_linesize, width, end, x
-    mov    widthd, dword widthm
-    %if %0 == 3; is 16 bit
-        add    widthq, widthq ; doesn't compile on x86_32
-    %endif
-%else
-cglobal blend_%1, 5, 7, %2, top, top_linesize, bottom, bottom_linesize, dst, end, x
-%define dst_linesizeq r5mp
-%define widthq r6mp
-%endif
-    mov      endd, dword r7m
-    add      topq, widthq
-    add   bottomq, widthq
-    add      dstq, widthq
-    neg    widthq
-%endmacro
-
-%macro BLEND_END 0
-    add          topq, top_linesizeq
-    add       bottomq, bottom_linesizeq
-    add          dstq, dst_linesizeq
-    sub          endd, 1
-    jg .nextrow
-REP_RET
-%endmacro
-
-%macro BLEND_SIMPLE 2-3
-BLEND_INIT %1, 2, %3
-.nextrow:
-    mov        xq, widthq
-
-    .loop:
-        movu            m0, [topq + xq]
-        movu            m1, [bottomq + xq]
-        p%2             m0, m1
-        mova   [dstq + xq], m0
-        add             xq, mmsize
-    jl .loop
-BLEND_END
-%endmacro
-
-; %1 name , %2 src (b or w), %3 inter (w or d), %4 (1 if 16bit, not set if 8 bit)
-%macro GRAINEXTRACT 3-4
-BLEND_INIT %1, 6, %4
-    pxor           m4, m4
-%if %0 == 4 ; 16 bit
-    VBROADCASTI128 m5, [pd_32768]
-%else
-    VBROADCASTI128 m5, [pw_128]
-%endif
-.nextrow:
-    mov        xq, widthq
-    .loop:
-        movu           m1, [topq + xq]
-        movu           m3, [bottomq + xq]
-
-        punpckl%2%3      m0, m1, m4
-        punpckh%2%3      m1, m4
-        punpckl%2%3      m2, m3, m4
-        punpckh%2%3      m3, m4
-
-        padd%3          m0, m5
-        padd%3          m1, m5
-        psub%3          m0, m2
-        psub%3          m1, m3
-
-        packus%3%2       m0, m1
-
-        mova  [dstq + xq], m0
-        add            xq, mmsize
-    jl .loop
-BLEND_END
-%endmacro
-
-%macro MULTIPLY 3 ; a, b, pw_1
-    pmullw          %1, %2               ; xxxxxxxx  a * b
-    paddw           %1, %3
-    psrlw           %2, %1, 8
-    paddw           %1, %2
-    psrlw           %1, 8                ; 00xx00xx  a * b / 255
-%endmacro
-
-%macro SCREEN 4   ; a, b, pw_1, pw_255
-    pxor            %1, %4               ; 00xx00xx  255 - a
-    pxor            %2, %4
-    MULTIPLY        %1, %2, %3
-    pxor            %1, %4               ; 00xx00xx  255 - x / 255
-%endmacro
-
-%macro BLEND_MULTIPLY 0
-BLEND_INIT multiply, 6
-    pxor       m4, m4
-    VBROADCASTI128       m5, [pw_1]
-.nextrow:
-    mov        xq, widthq
-
-    .loop:
-        movu           m1, [topq + xq]
-        movu           m3, [bottomq + xq]
-        punpcklbw      m0, m1, m4
-        punpckhbw      m1, m4
-        punpcklbw      m2, m3, m4
-        punpckhbw      m3, m4
-
-        MULTIPLY        m0, m2, m5
-        MULTIPLY        m1, m3, m5
-
-        packuswb       m0, m1
-        mova  [dstq + xq], m0
-        add            xq, mmsize
-    jl .loop
-BLEND_END
-%endmacro
-
-%macro BLEND_SCREEN 0
-BLEND_INIT screen, 7
-    pxor       m4, m4
-
-    VBROADCASTI128       m5, [pw_1]
-    VBROADCASTI128       m6, [pw_255]
-.nextrow:
-    mov        xq, widthq
-
-    .loop:
-        movu           m1, [topq + xq]
-        movu           m3, [bottomq + xq]
-        punpcklbw      m0, m1, m4
-        punpckhbw      m1, m4
-        punpcklbw      m2, m3, m4
-        punpckhbw      m3, m4
-
-        SCREEN          m0, m2, m5, m6
-        SCREEN          m1, m3, m5, m6
-
-        packuswb       m0, m1
-        mova  [dstq + xq], m0
-        add            xq, mmsize
-    jl .loop
-BLEND_END
-%endmacro
-
-;%1 name, %2 (b or w), %3 (set if 16 bit)
-%macro AVERAGE 2-3
-BLEND_INIT %1, 3, %3
-    pcmpeqb        m2, m2
-
-.nextrow:
-    mov        xq, widthq
-
-.loop:
-    movu           m0, [topq + xq]
-    movu           m1, [bottomq + xq]
-    pxor           m0, m2
-    pxor           m1, m2
-    pavg%2         m0, m1
-    pxor           m0, m2
-    mova  [dstq + xq], m0
-    add            xq, mmsize
-    jl .loop
-BLEND_END
-%endmacro
-
-; %1 name , %2 src (b or w), %3 inter (w or d), %4 (1 if 16bit, not set if 8 bit)
-%macro GRAINMERGE 3-4
-BLEND_INIT %1, 6, %4
-    pxor       m4, m4
-%if %0 == 4 ; 16 bit
-    VBROADCASTI128       m5, [pd_32768]
-%else
-    VBROADCASTI128       m5, [pw_128]
-%endif
-.nextrow:
-    mov        xq, widthq
-
-    .loop:
-        movu           m1, [topq + xq]
-        movu           m3, [bottomq + xq]
-
-        punpckl%2%3    m0, m1, m4
-        punpckh%2%3    m1, m4
-        punpckl%2%3    m2, m3, m4
-        punpckh%2%3    m3, m4
-
-        padd%3         m0, m2
-        padd%3         m1, m3
-        psub%3         m0, m5
-        psub%3         m1, m5
-
-        packus%3%2     m0, m1
-
-        mova  [dstq + xq], m0
-        add            xq, mmsize
-    jl .loop
-BLEND_END
-%endmacro
-
-%macro HARDMIX 0
-BLEND_INIT hardmix, 5
-    VBROADCASTI128       m2, [pb_255]
-    VBROADCASTI128       m3, [pb_128]
-    VBROADCASTI128       m4, [pb_127]
-.nextrow:
-    mov        xq, widthq
-
-    .loop:
-        movu            m0, [topq + xq]
-        movu            m1, [bottomq + xq]
-        pxor            m1, m4
-        pxor            m0, m3
-        pcmpgtb         m1, m0
-        pxor            m1, m2
-        mova   [dstq + xq], m1
-        add             xq, mmsize
-    jl .loop
-BLEND_END
-%endmacro
-
-%macro DIVIDE 0
-BLEND_INIT divide, 4
-    pxor       m2, m2
-    mova       m3, [ps_255]
-.nextrow:
-    mov        xq, widthq
-
-    .loop:
-        movd            m0, [topq + xq]      ; 000000xx
-        movd            m1, [bottomq + xq]
-        punpcklbw       m0, m2               ; 00000x0x
-        punpcklbw       m1, m2
-        punpcklwd       m0, m2               ; 000x000x
-        punpcklwd       m1, m2
-
-        cvtdq2ps        m0, m0
-        cvtdq2ps        m1, m1
-        divps           m0, m1               ; a / b
-        mulps           m0, m3               ; a / b * 255
-        minps           m0, m3
-        cvttps2dq       m0, m0
-
-        packssdw        m0, m0               ; 00000x0x
-        packuswb        m0, m0               ; 000000xx
-        movd   [dstq + xq], m0
-        add             xq, mmsize / 4
-
-    jl .loop
-BLEND_END
-%endmacro
-
-%macro PHOENIX 2-3
-; %1 name, %2 b or w, %3 (opt) 1 if 16 bit
-BLEND_INIT %1, 4, %3
-    VBROADCASTI128       m3, [pb_255]
-.nextrow:
-    mov        xq, widthq
-
-    .loop:
-        movu            m0, [topq + xq]
-        movu            m1, [bottomq + xq]
-        mova            m2, m0
-        pminu%2         m0, m1
-        pmaxu%2         m1, m2
-        mova            m2, m3
-        psubus%2        m2, m1
-        paddus%2        m2, m0
-        mova   [dstq + xq], m2
-        add             xq, mmsize
-    jl .loop
-BLEND_END
-%endmacro
-
-; %1 name , %2 src (b or w), %3 inter (w or d), %4 (1 if 16bit, not set if 8 bit)
-%macro DIFFERENCE 3-4
-BLEND_INIT %1, 5, %4
-    pxor       m2, m2
-.nextrow:
-    mov        xq, widthq
-
-    .loop:
-        movu            m0, [topq + xq]
-        movu            m1, [bottomq + xq]
-        punpckh%2%3     m3, m0, m2
-        punpckl%2%3     m0, m2
-        punpckh%2%3     m4, m1, m2
-        punpckl%2%3     m1, m2
-        psub%3          m0, m1
-        psub%3          m3, m4
-%if %0 == 4; 16 bit
-        pabsd           m0, m0
-        pabsd           m3, m3
-%else
-        ABS2            m0, m3, m1, m4
-%endif
-        packus%3%2      m0, m3
-        mova   [dstq + xq], m0
-        add             xq, mmsize
-    jl .loop
-BLEND_END
-%endmacro
-
-; %1 name , %2 src (b or w), %3 inter (w or d), %4 (1 if 16bit, not set if 8 bit)
-%macro EXTREMITY 3-4
-BLEND_INIT %1, 8, %4
-    pxor       m2, m2
-%if %0 == 4; 16 bit
-    VBROADCASTI128       m4, [pd_65535]
-%else
-    VBROADCASTI128       m4, [pw_255]
-%endif
-.nextrow:
-    mov        xq, widthq
-
-    .loop:
-        movu            m0, [topq + xq]
-        movu            m1, [bottomq + xq]
-        punpckh%2%3     m5, m0, m2
-        punpckl%2%3     m0, m2
-        punpckh%2%3     m6, m1, m2
-        punpckl%2%3     m1, m2
-        psub%3          m3, m4, m0
-        psub%3          m7, m4, m5
-        psub%3          m3, m1
-        psub%3          m7, m6
-%if %0 == 4; 16 bit
-        pabsd           m3, m3
-        pabsd           m7, m7
-%else
-        ABS2            m3, m7, m1, m6
-%endif
-        packus%3%2      m3, m7
-        mova   [dstq + xq], m3
-        add             xq, mmsize
-    jl .loop
-BLEND_END
-%endmacro
-
-%macro NEGATION 3-4
-BLEND_INIT %1, 8, %4
-    pxor       m2, m2
-%if %0 == 4; 16 bit
-    VBROADCASTI128       m4, [pd_65535]
-%else
-    VBROADCASTI128       m4, [pw_255]
-%endif
-.nextrow:
-    mov        xq, widthq
-
-    .loop:
-        movu            m0, [topq + xq]
-        movu            m1, [bottomq + xq]
-        punpckh%2%3     m5, m0, m2
-        punpckl%2%3     m0, m2
-        punpckh%2%3     m6, m1, m2
-        punpckl%2%3     m1, m2
-        psub%3          m3, m4, m0
-        psub%3          m7, m4, m5
-        psub%3          m3, m1
-        psub%3          m7, m6
-%if %0 == 4; 16 bit
-        pabsd           m3, m3
-        pabsd           m7, m7
-%else
-        ABS2            m3, m7, m1, m6
-%endif
-        psub%3          m0, m4, m3
-        psub%3          m1, m4, m7
-        packus%3%2      m0, m1
-        mova   [dstq + xq], m0
-        add             xq, mmsize
-    jl .loop
-BLEND_END
-%endmacro
-
-INIT_XMM sse2
-BLEND_SIMPLE xor,      xor
-BLEND_SIMPLE or,       or
-BLEND_SIMPLE and,      and
-BLEND_SIMPLE addition, addusb
-BLEND_SIMPLE subtract, subusb
-BLEND_SIMPLE darken,   minub
-BLEND_SIMPLE lighten,  maxub
-GRAINEXTRACT grainextract, b, w
-BLEND_MULTIPLY
-BLEND_SCREEN
-AVERAGE       average,    b
-GRAINMERGE    grainmerge, b, w
-HARDMIX
-PHOENIX phoenix, b
-DIFFERENCE difference, b, w
-DIVIDE
-EXTREMITY extremity, b, w
-NEGATION negation, b, w
-
-%if ARCH_X86_64
-BLEND_SIMPLE addition_16, addusw, 1
-BLEND_SIMPLE and_16,      and,    1
-BLEND_SIMPLE or_16,       or,     1
-BLEND_SIMPLE subtract_16, subusw, 1
-BLEND_SIMPLE xor_16,      xor,    1
-AVERAGE      average_16,  w,      1
-%endif
-
-INIT_XMM ssse3
-DIFFERENCE difference, b, w
-EXTREMITY extremity, b, w
-NEGATION negation, b, w
-
-INIT_XMM sse4
-%if ARCH_X86_64
-BLEND_SIMPLE darken_16,   minuw, 1
-BLEND_SIMPLE lighten_16,  maxuw, 1
-GRAINEXTRACT grainextract_16, w, d, 1
-GRAINMERGE   grainmerge_16, w, d, 1
-PHOENIX      phoenix_16,      w, 1
-DIFFERENCE   difference_16, w, d, 1
-EXTREMITY    extremity_16, w, d, 1
-NEGATION     negation_16, w, d, 1
-%endif
-
-%if HAVE_AVX2_EXTERNAL
-INIT_YMM avx2
-BLEND_SIMPLE xor,      xor
-BLEND_SIMPLE or,       or
-BLEND_SIMPLE and,      and
-BLEND_SIMPLE addition, addusb
-BLEND_SIMPLE subtract, subusb
-BLEND_SIMPLE darken,   minub
-BLEND_SIMPLE lighten,  maxub
-GRAINEXTRACT grainextract, b, w
-BLEND_MULTIPLY
-BLEND_SCREEN
-AVERAGE    average,    b
-GRAINMERGE grainmerge, b, w
-HARDMIX
-PHOENIX phoenix, b
-
-DIFFERENCE difference, b, w
-EXTREMITY extremity, b, w
-NEGATION negation, b, w
-
-%if ARCH_X86_64
-BLEND_SIMPLE addition_16, addusw, 1
-BLEND_SIMPLE and_16,      and,    1
-BLEND_SIMPLE darken_16,   minuw,  1
-BLEND_SIMPLE lighten_16,  maxuw,  1
-BLEND_SIMPLE or_16,       or,     1
-BLEND_SIMPLE subtract_16, subusw, 1
-BLEND_SIMPLE xor_16,      xor,    1
-GRAINEXTRACT grainextract_16, w, d, 1
-AVERAGE      average_16,  w,      1
-GRAINMERGE   grainmerge_16, w, d, 1
-PHOENIX      phoenix_16,       w, 1
-DIFFERENCE   difference_16, w, d, 1
-EXTREMITY    extremity_16, w, d, 1
-NEGATION     negation_16, w, d, 1
-%endif
-%endif
diff -uparN ffmpeg-4.1/libavfilter/x86/vf_bwdif.asm ffmpeg-y/libavfilter/x86/vf_bwdif.asm
--- ffmpeg-4.1/libavfilter/x86/vf_bwdif.asm	2018-11-02 02:34:26.000000000 +0800
+++ ffmpeg-y/libavfilter/x86/vf_bwdif.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,270 +0,0 @@
-;*****************************************************************************
-;* x86-optimized functions for bwdif filter
-;*
-;* Copyright (C) 2016 Thomas Mundt <loudmax@yahoo.de>
-;*
-;* Based on yadif simd code
-;* Copyright (C) 2006 Michael Niedermayer <michaelni@gmx.at>
-;*               2013 Daniel Kang <daniel.d.kang@gmail.com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-
-pw_coefhf:  times 4 dw  1016, 5570
-pw_coefhf1: times 8 dw -3801
-pw_coefsp:  times 4 dw  5077, -981
-pw_splfdif: times 4 dw  -768,  768
-
-SECTION .text
-
-%macro LOAD8 2
-    movh         %1, %2
-    punpcklbw    %1, m7
-%endmacro
-
-%macro LOAD12 2
-    movu         %1, %2
-%endmacro
-
-%macro DISP8 0
-    packuswb     m2, m2
-    movh     [dstq], m2
-%endmacro
-
-%macro DISP12 0
-    CLIPW        m2, m7, m12
-    movu     [dstq], m2
-%endmacro
-
-%macro FILTER 5
-    pxor         m7, m7
-.loop%1:
-    LOAD%4       m0, [curq+t0*%5]
-    LOAD%4       m1, [curq+t1*%5]
-    LOAD%4       m2, [%2]
-    LOAD%4       m3, [%3]
-    mova         m4, m3
-    paddw        m3, m2
-    psubw        m2, m4
-    ABS1         m2, m4
-    mova         m8, m3
-    mova         m9, m2
-    LOAD%4       m3, [prevq+t0*%5]
-    LOAD%4       m4, [prevq+t1*%5]
-    psubw        m3, m0
-    psubw        m4, m1
-    ABS2         m3, m4, m5, m6
-    paddw        m3, m4
-    psrlw        m2, 1
-    psrlw        m3, 1
-    pmaxsw       m2, m3
-    LOAD%4       m3, [nextq+t0*%5]
-    LOAD%4       m4, [nextq+t1*%5]
-    psubw        m3, m0
-    psubw        m4, m1
-    ABS2         m3, m4, m5, m6
-    paddw        m3, m4
-    psrlw        m3, 1
-    pmaxsw       m2, m3
-
-    LOAD%4       m3, [%2+t0*2*%5]
-    LOAD%4       m4, [%3+t0*2*%5]
-    LOAD%4       m5, [%2+t1*2*%5]
-    LOAD%4       m6, [%3+t1*2*%5]
-    paddw        m3, m4
-    paddw        m5, m6
-    mova         m6, m3
-    paddw        m6, m5
-    mova        m10, m6
-    psrlw        m3, 1
-    psrlw        m5, 1
-    psubw        m3, m0
-    psubw        m5, m1
-    mova         m6, m3
-    pminsw       m3, m5
-    pmaxsw       m5, m6
-    mova         m4, m8
-    psraw        m4, 1
-    mova         m6, m4
-    psubw        m6, m0
-    psubw        m4, m1
-    pmaxsw       m3, m6
-    pminsw       m5, m6
-    pmaxsw       m3, m4
-    pminsw       m5, m4
-    mova         m6, m7
-    psubw        m6, m3
-    pmaxsw       m6, m5
-    mova         m3, m2
-    pcmpgtw      m3, m7
-    pand         m6, m3
-    pmaxsw       m2, m6
-    mova        m11, m2
-
-    LOAD%4       m2, [%2+t0*4*%5]
-    LOAD%4       m3, [%3+t0*4*%5]
-    LOAD%4       m4, [%2+t1*4*%5]
-    LOAD%4       m5, [%3+t1*4*%5]
-    paddw        m2, m3
-    paddw        m4, m5
-    paddw        m2, m4
-    mova         m3, m2
-    punpcklwd    m2, m8
-    punpckhwd    m3, m8
-    pmaddwd      m2, [pw_coefhf]
-    pmaddwd      m3, [pw_coefhf]
-    mova         m4, m10
-    mova         m6, m4
-    pmullw       m4, [pw_coefhf1]
-    pmulhw       m6, [pw_coefhf1]
-    mova         m5, m4
-    punpcklwd    m4, m6
-    punpckhwd    m5, m6
-    paddd        m2, m4
-    paddd        m3, m5
-    psrad        m2, 2
-    psrad        m3, 2
-
-    mova         m4, m0
-    paddw        m0, m1
-%if ARCH_X86_64
-    LOAD%4       m5, [curq+t2*%5]
-    LOAD%4       m6, [curq+t3*%5]
-%else
-    mov          r4, prefs3mp
-    mov          r5, mrefs3mp
-    LOAD%4       m5, [curq+t0*%5]
-    LOAD%4       m6, [curq+t1*%5]
-    mov          r4, prefsmp
-    mov          r5, mrefsmp
-%endif
-    paddw        m6, m5
-    psubw        m1, m4
-    ABS1         m1, m4
-    pcmpgtw      m1, m9
-    mova         m4, m1
-    punpcklwd    m1, m4
-    punpckhwd    m4, m4
-    pand         m2, m1
-    pand         m3, m4
-    mova         m5, [pw_splfdif]
-    mova         m7, m5
-    pand         m5, m1
-    pand         m7, m4
-    paddw        m5, [pw_coefsp]
-    paddw        m7, [pw_coefsp]
-    mova         m4, m0
-    punpcklwd    m0, m6
-    punpckhwd    m4, m6
-    pmaddwd      m0, m5
-    pmaddwd      m4, m7
-    paddd        m2, m0
-    paddd        m3, m4
-    psrad        m2, 13
-    psrad        m3, 13
-    packssdw     m2, m3
-
-    mova         m4, m8
-    psraw        m4, 1
-    mova         m0, m11
-    mova         m3, m4
-    psubw        m4, m0
-    paddw        m3, m0
-    CLIPW        m2, m4, m3
-    pxor         m7, m7
-    DISP%4
-
-    add        dstq, STEP
-    add       prevq, STEP
-    add        curq, STEP
-    add       nextq, STEP
-    sub    DWORD wm, mmsize/2
-    jg .loop%1
-%endmacro
-
-%macro PROC 2
-%if ARCH_X86_64
-    movsxd       r5, DWORD prefsm
-    movsxd       r6, DWORD mrefsm
-    movsxd       r7, DWORD prefs3m
-    movsxd       r8, DWORD mrefs3m
-    DECLARE_REG_TMP 5, 6, 7, 8
-%else
-    %define m8  [rsp+ 0]
-    %define m9  [rsp+16]
-    %define m10 [rsp+32]
-    %define m11 [rsp+48]
-    mov          r4, prefsmp
-    mov          r5, mrefsmp
-    DECLARE_REG_TMP 4, 5
-%endif
-    cmp DWORD paritym, 0
-    je .parity0
-    FILTER 1, prevq, curq, %1, %2
-    jmp .ret
-.parity0:
-    FILTER 0, curq, nextq, %1, %2
-.ret:
-    RET
-%endmacro
-
-%macro BWDIF 0
-%if ARCH_X86_64
-cglobal bwdif_filter_line, 4, 9, 12, 0, dst, prev, cur, next, w, prefs, \
-                                        mrefs, prefs2, mrefs2, prefs3, mrefs3, \
-                                        prefs4, mrefs4, parity, clip_max
-%else
-cglobal bwdif_filter_line, 4, 6, 8, 64, dst, prev, cur, next, w, prefs, \
-                                        mrefs, prefs2, mrefs2, prefs3, mrefs3, \
-                                        prefs4, mrefs4, parity, clip_max
-%endif
-    %define STEP mmsize/2
-    PROC 8, 1
-
-%if ARCH_X86_64
-cglobal bwdif_filter_line_12bit, 4, 9, 13, 0, dst, prev, cur, next, w, \
-                                              prefs, mrefs, prefs2, mrefs2, \
-                                              prefs3, mrefs3, prefs4, \
-                                              mrefs4, parity, clip_max
-    movd        m12, DWORD clip_maxm
-    SPLATW      m12, m12, 0
-%else
-cglobal bwdif_filter_line_12bit, 4, 6, 8, 80, dst, prev, cur, next, w, \
-                                              prefs, mrefs, prefs2, mrefs2, \
-                                              prefs3, mrefs3, prefs4, \
-                                              mrefs4, parity, clip_max
-    %define m12 [rsp+64]
-    movd         m0, DWORD clip_maxm
-    SPLATW       m0, m0, 0
-    mova        m12, m0
-%endif
-    %define STEP mmsize
-    PROC 12, 2
-%endmacro
-
-INIT_XMM ssse3
-BWDIF
-INIT_XMM sse2
-BWDIF
-%if ARCH_X86_32
-INIT_MMX mmxext
-BWDIF
-%endif
diff -uparN ffmpeg-4.1/libavfilter/x86/vf_framerate.asm ffmpeg-y/libavfilter/x86/vf_framerate.asm
--- ffmpeg-4.1/libavfilter/x86/vf_framerate.asm	2018-11-03 08:17:29.000000000 +0800
+++ ffmpeg-y/libavfilter/x86/vf_framerate.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,134 +0,0 @@
-;*****************************************************************************
-;* x86-optimized functions for framerate filter
-;*
-;* Copyright (C) 2018 Marton Balint
-;*
-;* Based on vf_blend.asm, Copyright (C) 2015 Paul B Mahol
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION .text
-
-
-%macro XSPLAT 3
-%if cpuflag(avx2)
-    vpbroadcast%3  %1, %2
-%else
-    movd           %1, %2
-%ifidn %3, d
-    SPLATD         %1
-%else
-    SPLATW         %1, %1
-%endif
-%endif
-%endmacro
-
-
-%macro BLEND_INIT 0-1
-%if ARCH_X86_64
-cglobal blend_frames%1, 6, 9, 5, src1, src1_linesize, src2, src2_linesize, dst, dst_linesize, width, end, x
-    mov    widthd, dword widthm
-%else
-cglobal blend_frames%1, 5, 7, 5, src1, src1_linesize, src2, src2_linesize, dst, end, x
-%define dst_linesizeq r5mp
-%define widthq r6mp
-%endif
-    mov      endd, dword r7m
-    add     src1q, widthq
-    add     src2q, widthq
-    add      dstq, widthq
-    neg    widthq
-%endmacro
-
-
-%macro BLEND_LOOP 4
-.nextrow:
-    mov        xq, widthq
-
-    .loop:
-        movu            m0, [src1q + xq]
-        movu            m1, [src2q + xq]
-        SBUTTERFLY    %1%2, 0, 1, 4         ; aAbBcCdD
-                                            ; eEfFgGhH
-        pmadd%3         m0, m2
-        pmadd%3         m1, m2
-
-        padd%2          m0, m3
-        padd%2          m1, m3
-        psrl%2          m0, %4              ; 0A0B0C0D
-        psrl%2          m1, %4              ; 0E0F0G0H
-
-        packus%2%1      m0, m1              ; ABCDEFGH
-        movu   [dstq + xq], m0
-        add             xq, mmsize
-    jl .loop
-    add     src1q, src1_linesizeq
-    add     src2q, src2_linesizeq
-    add      dstq, dst_linesizeq
-    sub      endd, 1
-    jg .nextrow
-REP_RET
-%endmacro
-
-
-%macro BLEND_FRAMES 0
-    BLEND_INIT
-
-    XSPLAT     m2, r8m, w                   ; factor1
-    XSPLAT     m3, r9m, w                   ; factor2
-
-    psllw      m3, 8
-    por        m2, m3                       ; interleaved factors
-
-    XSPLAT     m3, r10m, w                  ; half
-
-    BLEND_LOOP  b, w, ubsw, 7
-%endmacro
-
-
-%macro BLEND_FRAMES16 0
-    BLEND_INIT 16
-
-    XSPLAT     m2, r8m, d                   ; factor1
-    XSPLAT     m3, r9m, d                   ; factor2
-
-    pslld      m3, 16
-    por        m2, m3                       ; interleaved factors
-
-    XSPLAT     m3, r10m, d                  ; half
-
-    BLEND_LOOP  w, d, wd, 15
-%endmacro
-
-
-INIT_XMM ssse3
-BLEND_FRAMES
-
-INIT_XMM sse4
-BLEND_FRAMES16
-
-
-%if HAVE_AVX2_EXTERNAL
-
-INIT_YMM avx2
-BLEND_FRAMES
-BLEND_FRAMES16
-
-%endif
diff -uparN ffmpeg-4.1/libavfilter/x86/vf_fspp.asm ffmpeg-y/libavfilter/x86/vf_fspp.asm
--- ffmpeg-4.1/libavfilter/x86/vf_fspp.asm	2018-01-01 06:35:48.000000000 +0800
+++ ffmpeg-y/libavfilter/x86/vf_fspp.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,727 +0,0 @@
-;*****************************************************************************
-;* x86-optimized functions for fspp filter
-;*
-;* Copyright (c) 2003 Michael Niedermayer <michaelni@gmx.at>
-;* Copyright (C) 2005 Nikolaj Poroshin <porosh3@psu.ru>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or modify
-;* it under the terms of the GNU General Public License as published by
-;* the Free Software Foundation; either version 2 of the License, or
-;* (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
-;* GNU General Public License for more details.
-;*
-;* You should have received a copy of the GNU General Public License along
-;* with FFmpeg; if not, write to the Free Software Foundation, Inc.,
-;* 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-
-pb_dither: db 0,  48,  12,  60,   3,  51,  15,  63, 32,  16,  44,  28,  35,  19,  47,  31, \
-              8,  56,   4,  52,  11,  59,   7,  55, 40,  24,  36,  20,  43,  27,  39,  23, \
-              2,  50,  14,  62,   1,  49,  13,  61, 34,  18,  46,  30,  33,  17,  45,  29, \
-             10,  58,   6,  54,   9,  57,   5,  53, 42,  26,  38,  22,  41,  25,  37,  21
-pw_187E: times 4 dw 0x187E ; FIX64(0.382683433, 14)
-pw_22A3: times 4 dw 0x22A3 ; FIX64(1.082392200, 13)
-pw_2D41: times 4 dw 0x2D41 ; FIX64(1.414213562, 13)
-pw_539F: times 4 dw 0x539F ; FIX64(1.306562965, 14)
-pw_5A82: times 4 dw 0x5A82 ; FIX64(1.414213562, 14)
-pw_3B21: times 4 dw 0x3B21 ; FIX64(1.847759065, 13)
-pw_AC62: times 4 dw 0xAC62 ; FIX64(-2.613125930, 13)
-pw_3642: times 4 dw 0x3642 ; FIX64(0.847759065, 14)
-pw_2441: times 4 dw 0x2441 ; FIX64(0.566454497, 14)
-pw_0CBB: times 4 dw 0x0CBB ; FIX64(0.198912367, 14)
-pw_4:    times 4 dw 4
-pw_2:    times 4 dw 2
-
-SECTION .text
-
-%define DCTSIZE 8
-
-INIT_MMX mmx
-
-;void ff_store_slice_mmx(uint8_t *dst, int16_t *src,
-;                        ptrdiff_t dst_stride, ptrdiff_t src_stride,
-;                        ptrdiff_t width, ptrdiff_t height, ptrdiff_t log2_scale)
-%if ARCH_X86_64
-cglobal store_slice, 7, 9, 0, dst, src, dst_stride, src_stride, width, dither_height, dither, tmp, tmp2
-%else
-cglobal store_slice, 2, 7, 0, dst, src, width, dither_height, dither, tmp, tmp2
-%define dst_strideq r2m
-%define src_strideq r3m
-    mov       widthq, r4m
-    mov       dither_heightq, r5m
-    mov       ditherq, r6m ; log2_scale
-%endif
-    add       widthq, 7
-    mov       tmpq, src_strideq
-    and       widthq, ~7
-    sub       dst_strideq, widthq
-    movd      m5, ditherd ; log2_scale
-    xor       ditherq, -1 ; log2_scale
-    mov       tmp2q, tmpq
-    add       ditherq, 7 ; log2_scale
-    neg       tmpq
-    sub       tmp2q, widthq
-    movd      m2, ditherd ; log2_scale
-    add       tmp2q, tmp2q
-    lea       ditherq, [pb_dither]
-    mov       src_strideq, tmp2q
-    shl       tmpq, 4
-    lea       dither_heightq, [ditherq+dither_heightq*8]
-    pxor      m7, m7
-
-.loop_height:
-    movq      m3, [ditherq]
-    movq      m4, m3
-    punpcklbw m3, m7
-    punpckhbw m4, m7
-    mov       tmp2q, widthq
-    psraw     m3, m5
-    psraw     m4, m5
-
-.loop_width:
-    movq      [srcq+tmpq], m7
-    movq      m0, [srcq]
-    movq      m1, [srcq+8]
-    movq      [srcq+tmpq+8], m7
-    paddw     m0, m3
-    paddw     m1, m4
-    movq      [srcq], m7
-    psraw     m0, m2
-    psraw     m1, m2
-    movq      [srcq+8], m7
-    packuswb  m0, m1
-    add       srcq, 16
-    movq      [dstq], m0
-    add       dstq, 8
-    sub       tmp2q, 8
-    jg .loop_width
-
-    add       srcq, src_strideq
-    add       ditherq, 8
-    add       dstq, dst_strideq
-    cmp       ditherq, dither_heightq
-    jl .loop_height
-    RET
-
-;void ff_store_slice2_mmx(uint8_t *dst, int16_t *src,
-;                         ptrdiff_t dst_stride, ptrdiff_t src_stride,
-;                         ptrdiff_t width, ptrdiff_t height, ptrdiff_t log2_scale)
-%if ARCH_X86_64
-cglobal store_slice2, 7, 9, 0, dst, src, dst_stride, src_stride, width, dither_height, dither, tmp, tmp2
-%else
-cglobal store_slice2, 0, 7, 0, dst, src, width, dither_height, dither, tmp, tmp2
-%define dst_strideq r2m
-%define src_strideq r3m
-    mov       dstq, dstm
-    mov       srcq, srcm
-    mov       widthq, r4m
-    mov       dither_heightq, r5m
-    mov       ditherq, r6m ; log2_scale
-%endif
-    add       widthq, 7
-    mov       tmpq, src_strideq
-    and       widthq, ~7
-    sub       dst_strideq, widthq
-    movd      m5, ditherd ; log2_scale
-    xor       ditherq, -1 ; log2_scale
-    mov       tmp2q, tmpq
-    add       ditherq, 7 ; log2_scale
-    sub       tmp2q, widthq
-    movd      m2, ditherd ; log2_scale
-    add       tmp2q, tmp2q
-    lea       ditherq, [pb_dither]
-    mov       src_strideq, tmp2q
-    shl       tmpq, 5
-    lea       dither_heightq, [ditherq+dither_heightq*8]
-    pxor      m7, m7
-
-.loop_height:
-    movq      m3, [ditherq]
-    movq      m4, m3
-    punpcklbw m3, m7
-    punpckhbw m4, m7
-    mov       tmp2q,widthq
-    psraw     m3, m5
-    psraw     m4, m5
-
-.loop_width:
-    movq      m0, [srcq]
-    movq      m1, [srcq+8]
-    paddw     m0, m3
-    paddw     m0, [srcq+tmpq]
-    paddw     m1, m4
-    movq      m6, [srcq+tmpq+8]
-    movq      [srcq+tmpq], m7
-    psraw     m0, m2
-    paddw     m1, m6
-    movq      [srcq+tmpq+8], m7
-    psraw     m1, m2
-    packuswb  m0, m1
-    movq      [dstq], m0
-    add       srcq, 16
-    add       dstq, 8
-    sub       tmp2q, 8
-    jg .loop_width
-
-    add       srcq, src_strideq
-    add       ditherq, 8
-    add       dstq, dst_strideq
-    cmp       ditherq, dither_heightq
-    jl .loop_height
-    RET
-
-;void ff_mul_thrmat_mmx(int16_t *thr_adr_noq, int16_t *thr_adr, int q);
-cglobal mul_thrmat, 3, 3, 0, thrn, thr, q
-    movd      m7, qd
-    movq      m0, [thrnq]
-    punpcklwd m7, m7
-    movq      m1, [thrnq+8]
-    punpckldq m7, m7
-    pmullw    m0, m7
-    movq      m2, [thrnq+8*2]
-    pmullw    m1, m7
-    movq      m3, [thrnq+8*3]
-    pmullw    m2, m7
-    movq      [thrq], m0
-    movq      m4, [thrnq+8*4]
-    pmullw    m3, m7
-    movq      [thrq+8], m1
-    movq      m5, [thrnq+8*5]
-    pmullw    m4, m7
-    movq      [thrq+8*2], m2
-    movq      m6, [thrnq+8*6]
-    pmullw    m5, m7
-    movq      [thrq+8*3], m3
-    movq      m0, [thrnq+8*7]
-    pmullw    m6, m7
-    movq      [thrq+8*4], m4
-    movq      m1, [thrnq+8*7+8]
-    pmullw    m0, m7
-    movq      [thrq+8*5], m5
-    movq      m2, [thrnq+8*7+8*2]
-    pmullw    m1, m7
-    movq      [thrq+8*6], m6
-    movq      m3, [thrnq+8*7+8*3]
-    pmullw    m2, m7
-    movq      [thrq+8*7], m0
-    movq      m4, [thrnq+8*7+8*4]
-    pmullw    m3, m7
-    movq      [thrq+8*7+8], m1
-    movq      m5, [thrnq+8*7+8*5]
-    pmullw    m4, m7
-    movq      [thrq+8*7+8*2], m2
-    movq      m6, [thrnq+8*7+8*6]
-    pmullw    m5, m7
-    movq      [thrq+8*7+8*3], m3
-    movq      m0, [thrnq+14*8]
-    pmullw    m6, m7
-    movq      [thrq+8*7+8*4], m4
-    movq      m1, [thrnq+14*8+8]
-    pmullw    m0, m7
-    movq      [thrq+8*7+8*5], m5
-    pmullw    m1, m7
-    movq      [thrq+8*7+8*6], m6
-    movq      [thrq+14*8], m0
-    movq      [thrq+14*8+8], m1
-    RET
-
-%macro COLUMN_FDCT 1-3 0, 0
-    movq      m1, [srcq+DCTSIZE*0*2]
-    movq      m7, [srcq+DCTSIZE*3*2]
-    movq      m0, m1
-    paddw     m1, [srcq+DCTSIZE*7*2]
-    movq      m3, m7
-    paddw     m7, [srcq+DCTSIZE*4*2]
-    movq      m5, m1
-    movq      m6, [srcq+DCTSIZE*1*2]
-    psubw     m1, m7
-    movq      m2, [srcq+DCTSIZE*2*2]
-    movq      m4, m6
-    paddw     m6, [srcq+DCTSIZE*6*2]
-    paddw     m5, m7
-    paddw     m2, [srcq+DCTSIZE*5*2]
-    movq      m7, m6
-    paddw     m6, m2
-    psubw     m7, m2
-    movq      m2, m5
-    paddw     m5, m6
-    psubw     m2, m6
-    paddw     m7, m1
-    movq      m6, [thrq+4*16+%2]
-    psllw     m7, 2
-    psubw     m5, [thrq+%2]
-    psubw     m2, m6
-    paddusw   m5, [thrq+%2]
-    paddusw   m2, m6
-    pmulhw    m7, [pw_2D41]
-    paddw     m5, [thrq+%2]
-    paddw     m2, m6
-    psubusw   m5, [thrq+%2]
-    psubusw   m2, m6
-    paddw     m5, [pw_2]
-    movq      m6, m2
-    paddw     m2, m5
-    psubw     m5, m6
-    movq      m6, m1
-    paddw     m1, m7
-    psubw     m1, [thrq+2*16+%2]
-    psubw     m6, m7
-    movq      m7, [thrq+6*16+%2]
-    psraw     m5, 2
-    paddusw   m1, [thrq+2*16+%2]
-    psubw     m6, m7
-    paddw     m1, [thrq+2*16+%2]
-    paddusw   m6, m7
-    psubusw   m1, [thrq+2*16+%2]
-    paddw     m6, m7
-    psubw     m3, [srcq+DCTSIZE*4*2]
-    psubusw   m6, m7
-    movq      m7, m1
-    psraw     m2, 2
-    psubw     m4, [srcq+DCTSIZE*6*2]
-    psubw     m1, m6
-    psubw     m0, [srcq+DCTSIZE*7*2]
-    paddw     m6, m7
-    psraw     m6, 2
-    movq      m7, m2
-    pmulhw    m1, [pw_5A82]
-    paddw     m2, m6
-    movq      [rsp], m2
-    psubw     m7, m6
-    movq      m2, [srcq+DCTSIZE*2*2]
-    psubw     m1, m6
-    psubw     m2, [srcq+DCTSIZE*5*2]
-    movq      m6, m5
-    movq      [rsp+8*3], m7
-    paddw     m3, m2
-    paddw     m2, m4
-    paddw     m4, m0
-    movq      m7, m3
-    psubw     m3, m4
-    psllw     m3, 2
-    psllw     m7, 2
-    pmulhw    m3, [pw_187E]
-    psllw     m4, 2
-    pmulhw    m7, [pw_22A3]
-    psllw     m2, 2
-    pmulhw    m4, [pw_539F]
-    paddw     m5, m1
-    pmulhw    m2, [pw_2D41]
-    psubw     m6, m1
-    paddw     m7, m3
-    movq      [rsp+8], m5
-    paddw     m4, m3
-    movq      m3, [thrq+3*16+%2]
-    movq      m1, m0
-    movq      [rsp+8*2], m6
-    psubw     m1, m2
-    paddw     m0, m2
-    movq      m5, m1
-    movq      m2, [thrq+5*16+%2]
-    psubw     m1, m7
-    paddw     m5, m7
-    psubw     m1, m3
-    movq      m7, [thrq+16+%2]
-    psubw     m5, m2
-    movq      m6, m0
-    paddw     m0, m4
-    paddusw   m1, m3
-    psubw     m6, m4
-    movq      m4, [thrq+7*16+%2]
-    psubw     m0, m7
-    psubw     m6, m4
-    paddusw   m5, m2
-    paddusw   m6, m4
-    paddw     m1, m3
-    paddw     m5, m2
-    paddw     m6, m4
-    psubusw   m1, m3
-    psubusw   m5, m2
-    psubusw   m6, m4
-    movq      m4, m1
-    por       m4, m5
-    paddusw   m0, m7
-    por       m4, m6
-    paddw     m0, m7
-    packssdw  m4, m4
-    psubusw   m0, m7
-    movd      tmpd, m4
-    or        tmpd, tmpd
-    jnz %1
-    movq      m4, [rsp]
-    movq      m1, m0
-    pmulhw    m0, [pw_3642]
-    movq      m2, m1
-    movq      m5, [outq+DCTSIZE*0*2]
-    movq      m3, m2
-    pmulhw    m1, [pw_2441]
-    paddw     m5, m4
-    movq      m6, [rsp+8]
-    psraw     m3, 2
-    pmulhw    m2, [pw_0CBB]
-    psubw     m4, m3
-    movq      m7, [outq+DCTSIZE*1*2]
-    paddw     m5, m3
-    movq      [outq+DCTSIZE*7*2], m4
-    paddw     m7, m6
-    movq      m3, [rsp+8*2]
-    psubw     m6, m0
-    movq      m4, [outq+DCTSIZE*2*2]
-    paddw     m7, m0
-    movq      [outq], m5
-    paddw     m4, m3
-    movq      [outq+DCTSIZE*6*2], m6
-    psubw     m3, m1
-    movq      m5, [outq+DCTSIZE*5*2]
-    paddw     m4, m1
-    movq      m6, [outq+DCTSIZE*3*2]
-    paddw     m5, m3
-    movq      m0, [rsp+8*3]
-    add       srcq, 8+%3
-    movq      [outq+DCTSIZE*1*2], m7
-    paddw     m6, m0
-    movq      [outq+DCTSIZE*2*2], m4
-    psubw     m0, m2
-    movq      m7, [outq+DCTSIZE*4*2]
-    paddw     m6, m2
-    movq      [outq+DCTSIZE*5*2], m5
-    paddw     m7, m0
-    movq      [outq+DCTSIZE*3*2], m6
-    movq      [outq+DCTSIZE*4*2], m7
-    add       outq, 8+%3
-%endmacro
-
-%macro COLUMN_IDCT 0-1 0
-    movq      m3, m5
-    psubw     m5, m1
-    psllw     m5, 1
-    paddw     m3, m1
-    movq      m2, m0
-    psubw     m0, m6
-    movq      m1, m5
-    psllw     m0, 1
-    pmulhw    m1, [pw_AC62]
-    paddw     m5, m0
-    pmulhw    m5, [pw_3B21]
-    paddw     m2, m6
-    pmulhw    m0, [pw_22A3]
-    movq      m7, m2
-    movq      m4, [rsp]
-    psubw     m2, m3
-    psllw     m2, 1
-    paddw     m7, m3
-    pmulhw    m2, [pw_2D41]
-    movq      m6, m4
-    psraw     m7, 2
-    paddw     m4, [outq]
-    psubw     m6, m7
-    movq      m3, [rsp+8]
-    paddw     m4, m7
-    movq      [outq+DCTSIZE*7*2], m6
-    paddw     m1, m5
-    movq      [outq], m4
-    psubw     m1, m7
-    movq      m7, [rsp+8*2]
-    psubw     m0, m5
-    movq      m6, [rsp+8*3]
-    movq      m5, m3
-    paddw     m3, [outq+DCTSIZE*1*2]
-    psubw     m5, m1
-    psubw     m2, m1
-    paddw     m3, m1
-    movq      [outq+DCTSIZE*6*2], m5
-    movq      m4, m7
-    paddw     m7, [outq+DCTSIZE*2*2]
-    psubw     m4, m2
-    paddw     m4, [outq+DCTSIZE*5*2]
-    paddw     m7, m2
-    movq      [outq+DCTSIZE*1*2], m3
-    paddw     m0, m2
-    movq      [outq+DCTSIZE*2*2], m7
-    movq      m1, m6
-    paddw     m6, [outq+DCTSIZE*4*2]
-    psubw     m1, m0
-    paddw     m1, [outq+DCTSIZE*3*2]
-    paddw     m6, m0
-    movq      [outq+DCTSIZE*5*2], m4
-    add       srcq, 8+%1
-    movq      [outq+DCTSIZE*4*2], m6
-    movq      [outq+DCTSIZE*3*2], m1
-    add       outq, 8+%1
-%endmacro
-
-;void ff_column_fidct_mmx(int16_t *thr_adr, int16_t *data, int16_t *output, int cnt);
-cglobal column_fidct, 4, 5, 0, 32, thr, src, out, cnt, tmp
-.fdct1:
-    COLUMN_FDCT .idct1
-    jmp .fdct2
-
-.idct1:
-    COLUMN_IDCT
-
-.fdct2:
-    COLUMN_FDCT .idct2, 8, 16
-    sub    cntd, 2
-    jg .fdct1
-    RET
-
-.idct2:
-    COLUMN_IDCT 16
-    sub    cntd, 2
-    jg .fdct1
-    RET
-
-;void ff_row_idct_mmx(int16_t *workspace, int16_t *output_adr, ptrdiff_t output_stride, int cnt);
-cglobal row_idct, 4, 5, 0, 16, src, dst, stride, cnt, stride3
-    add       strideq, strideq
-    lea       stride3q, [strideq+strideq*2]
-.loop:
-    movq      m0, [srcq+DCTSIZE*0*2]
-    movq      m1, [srcq+DCTSIZE*1*2]
-    movq      m4, m0
-    movq      m2, [srcq+DCTSIZE*2*2]
-    punpcklwd m0, m1
-    movq      m3, [srcq+DCTSIZE*3*2]
-    punpckhwd m4, m1
-    movq      m7, m2
-    punpcklwd m2, m3
-    movq      m6, m0
-    punpckldq m0, m2
-    punpckhdq m6, m2
-    movq      m5, m0
-    punpckhwd m7, m3
-    psubw     m0, m6
-    pmulhw    m0, [pw_5A82]
-    movq      m2, m4
-    punpckldq m4, m7
-    paddw     m5, m6
-    punpckhdq m2, m7
-    movq      m1, m4
-    psllw     m0, 2
-    paddw     m4, m2
-    movq      m3, [srcq+DCTSIZE*0*2+8]
-    psubw     m1, m2
-    movq      m2, [srcq+DCTSIZE*1*2+8]
-    psubw     m0, m5
-    movq      m6, m4
-    paddw     m4, m5
-    psubw     m6, m5
-    movq      m7, m1
-    movq      m5, [srcq+DCTSIZE*2*2+8]
-    paddw     m1, m0
-    movq      [rsp], m4
-    movq      m4, m3
-    movq      [rsp+8], m6
-    punpcklwd m3, m2
-    movq      m6, [srcq+DCTSIZE*3*2+8]
-    punpckhwd m4, m2
-    movq      m2, m5
-    punpcklwd m5, m6
-    psubw     m7, m0
-    punpckhwd m2, m6
-    movq      m0, m3
-    punpckldq m3, m5
-    punpckhdq m0, m5
-    movq      m5, m4
-    movq      m6, m3
-    punpckldq m4, m2
-    psubw     m3, m0
-    punpckhdq m5, m2
-    paddw     m6, m0
-    movq      m2, m4
-    movq      m0, m3
-    psubw     m4, m5
-    pmulhw    m0, [pw_AC62]
-    paddw     m3, m4
-    pmulhw    m3, [pw_3B21]
-    paddw     m2, m5
-    pmulhw    m4, [pw_22A3]
-    movq      m5, m2
-    psubw     m2, m6
-    paddw     m5, m6
-    pmulhw    m2, [pw_2D41]
-    paddw     m0, m3
-    psllw     m0, 3
-    psubw     m4, m3
-    movq      m6, [rsp]
-    movq      m3, m1
-    psllw     m4, 3
-    psubw     m0, m5
-    psllw     m2, 3
-    paddw     m1, m0
-    psubw     m2, m0
-    psubw     m3, m0
-    paddw     m4, m2
-    movq      m0, m7
-    paddw     m7, m2
-    psubw     m0, m2
-    movq      m2, [pw_4]
-    psubw     m6, m5
-    paddw     m5, [rsp]
-    paddw     m1, m2
-    paddw     m5, m2
-    psraw     m1, 3
-    paddw     m7, m2
-    psraw     m5, 3
-    paddw     m5, [dstq]
-    psraw     m7, 3
-    paddw     m1, [dstq+strideq*1]
-    paddw     m0, m2
-    paddw     m7, [dstq+strideq*2]
-    paddw     m3, m2
-    movq      [dstq], m5
-    paddw     m6, m2
-    movq      [dstq+strideq*1], m1
-    psraw     m0, 3
-    movq      [dstq+strideq*2], m7
-    add       dstq, stride3q
-    movq      m5, [rsp+8]
-    psraw     m3, 3
-    paddw     m0, [dstq+strideq*2]
-    psubw     m5, m4
-    paddw     m3, [dstq+stride3q*1]
-    psraw     m6, 3
-    paddw     m4, [rsp+8]
-    paddw     m5, m2
-    paddw     m6, [dstq+strideq*4]
-    paddw     m4, m2
-    movq      [dstq+strideq*2], m0
-    psraw     m5, 3
-    paddw     m5, [dstq]
-    psraw     m4, 3
-    paddw     m4, [dstq+strideq*1]
-    add       srcq, DCTSIZE*2*4
-    movq      [dstq+stride3q*1], m3
-    movq      [dstq+strideq*4], m6
-    movq      [dstq], m5
-    movq      [dstq+strideq*1], m4
-    sub       dstq, stride3q
-    add       dstq, 8
-    dec       r3d
-    jnz .loop
-    RET
-
-;void ff_row_fdct_mmx(int16_t *data, const uint8_t *pixels, ptrdiff_t line_size, int cnt);
-cglobal row_fdct, 4, 5, 0, 16, src, pix, stride, cnt, stride3
-    lea       stride3q, [strideq+strideq*2]
-.loop:
-    movd      m0, [pixq]
-    pxor      m7, m7
-    movd      m1, [pixq+strideq*1]
-    punpcklbw m0, m7
-    movd      m2, [pixq+strideq*2]
-    punpcklbw m1, m7
-    punpcklbw m2, m7
-    add       pixq,stride3q
-    movq      m5, m0
-    movd      m3, [pixq+strideq*4]
-    movq      m6, m1
-    movd      m4, [pixq+stride3q*1]
-    punpcklbw m3, m7
-    psubw     m5, m3
-    punpcklbw m4, m7
-    paddw     m0, m3
-    psubw     m6, m4
-    movd      m3, [pixq+strideq*2]
-    paddw     m1, m4
-    movq      [rsp], m5
-    punpcklbw m3, m7
-    movq      [rsp+8], m6
-    movq      m4, m2
-    movd      m5, [pixq]
-    paddw     m2, m3
-    movd      m6, [pixq+strideq*1]
-    punpcklbw m5, m7
-    psubw     m4, m3
-    punpcklbw m6, m7
-    movq      m3, m5
-    paddw     m5, m6
-    psubw     m3, m6
-    movq      m6, m0
-    movq      m7, m1
-    psubw     m0, m5
-    psubw     m1, m2
-    paddw     m7, m2
-    paddw     m1, m0
-    movq      m2, m7
-    psllw     m1, 2
-    paddw     m6, m5
-    pmulhw    m1, [pw_2D41]
-    paddw     m7, m6
-    psubw     m6, m2
-    movq      m5, m0
-    movq      m2, m7
-    punpcklwd m7, m6
-    paddw     m0, m1
-    punpckhwd m2, m6
-    psubw     m5, m1
-    movq      m6, m0
-    movq      m1, [rsp+8]
-    punpcklwd m0, m5
-    punpckhwd m6, m5
-    movq      m5, m0
-    punpckldq m0, m7
-    paddw     m3, m4
-    punpckhdq m5, m7
-    movq      m7, m6
-    movq      [srcq+DCTSIZE*0*2], m0
-    punpckldq m6, m2
-    movq      [srcq+DCTSIZE*1*2], m5
-    punpckhdq m7, m2
-    movq      [srcq+DCTSIZE*2*2], m6
-    paddw     m4, m1
-    movq      [srcq+DCTSIZE*3*2], m7
-    psllw     m3, 2
-    movq      m2, [rsp]
-    psllw     m4, 2
-    pmulhw    m4, [pw_2D41]
-    paddw     m1, m2
-    psllw     m1, 2
-    movq      m0, m3
-    pmulhw    m0, [pw_22A3]
-    psubw     m3, m1
-    pmulhw    m3, [pw_187E]
-    movq      m5, m2
-    pmulhw    m1, [pw_539F]
-    psubw     m2, m4
-    paddw     m5, m4
-    movq      m6, m2
-    paddw     m0, m3
-    movq      m7, m5
-    paddw     m2, m0
-    psubw     m6, m0
-    movq      m4, m2
-    paddw     m1, m3
-    punpcklwd m2, m6
-    paddw     m5, m1
-    punpckhwd m4, m6
-    psubw     m7, m1
-    movq      m6, m5
-    punpcklwd m5, m7
-    punpckhwd m6, m7
-    movq      m7, m2
-    punpckldq m2, m5
-    sub       pixq, stride3q
-    punpckhdq m7, m5
-    movq      m5, m4
-    movq      [srcq+DCTSIZE*0*2+8], m2
-    punpckldq m4, m6
-    movq      [srcq+DCTSIZE*1*2+8], m7
-    punpckhdq m5, m6
-    movq      [srcq+DCTSIZE*2*2+8], m4
-    add       pixq, 4
-    movq      [srcq+DCTSIZE*3*2+8], m5
-    add       srcq, DCTSIZE*4*2
-    dec       cntd
-    jnz .loop
-    RET
diff -uparN ffmpeg-4.1/libavfilter/x86/vf_gradfun.asm ffmpeg-y/libavfilter/x86/vf_gradfun.asm
--- ffmpeg-4.1/libavfilter/x86/vf_gradfun.asm	2016-03-29 10:25:27.000000000 +0800
+++ ffmpeg-y/libavfilter/x86/vf_gradfun.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,110 +0,0 @@
-;******************************************************************************
-;* x86-optimized functions for gradfun filter
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-
-pw_7f: times 8 dw 0x7F
-pw_ff: times 8 dw 0xFF
-
-SECTION .text
-
-%macro FILTER_LINE 1
-    movh       m0, [r2+r0]
-    movh       m1, [r3+r0]
-    punpcklbw  m0, m7
-    punpcklwd  m1, m1
-    psllw      m0, 7
-    psubw      m1, m0
-    PABSW      m2, m1
-    pmulhuw    m2, m5
-    psubw      m2, m6
-    pminsw     m2, m7
-    pmullw     m2, m2
-    psllw      m1, 2
-    paddw      m0, %1
-    pmulhw     m1, m2
-    paddw      m0, m1
-    psraw      m0, 7
-    packuswb   m0, m0
-    movh  [r1+r0], m0
-%endmacro
-
-INIT_MMX mmxext
-cglobal gradfun_filter_line, 6, 6
-    movh      m5, r4d
-    pxor      m7, m7
-    pshufw    m5, m5,0
-    mova      m6, [pw_7f]
-    mova      m3, [r5]
-    mova      m4, [r5+8]
-.loop:
-    FILTER_LINE m3
-    add       r0, 4
-    jge .end
-    FILTER_LINE m4
-    add       r0, 4
-    jl .loop
-.end:
-    REP_RET
-
-INIT_XMM ssse3
-cglobal gradfun_filter_line, 6, 6, 8
-    movd       m5, r4d
-    pxor       m7, m7
-    pshuflw    m5, m5, 0
-    mova       m6, [pw_7f]
-    punpcklqdq m5, m5
-    mova       m4, [r5]
-.loop:
-    FILTER_LINE m4
-    add        r0, 8
-    jl .loop
-    REP_RET
-
-%macro BLUR_LINE 1
-cglobal gradfun_blur_line_%1, 6, 6, 8
-    mova        m7, [pw_ff]
-.loop:
-    %1          m0, [r4+r0]
-    %1          m1, [r5+r0]
-    mova        m2, m0
-    mova        m3, m1
-    psrlw       m0, 8
-    psrlw       m1, 8
-    pand        m2, m7
-    pand        m3, m7
-    paddw       m0, m1
-    paddw       m2, m3
-    paddw       m0, m2
-    paddw       m0, [r2+r0]
-    mova        m1, [r1+r0]
-    mova   [r1+r0], m0
-    psubw       m0, m1
-    mova   [r3+r0], m0
-    add         r0, 16
-    jl .loop
-    REP_RET
-%endmacro
-
-INIT_XMM sse2
-BLUR_LINE movdqa
-BLUR_LINE movdqu
diff -uparN ffmpeg-4.1/libavfilter/x86/vf_hflip.asm ffmpeg-y/libavfilter/x86/vf_hflip.asm
--- ffmpeg-4.1/libavfilter/x86/vf_hflip.asm	2018-11-03 08:17:29.000000000 +0800
+++ ffmpeg-y/libavfilter/x86/vf_hflip.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,90 +0,0 @@
-;*****************************************************************************
-;* x86-optimized functions for hflip filter
-;*
-;* Copyright (C) 2017 Paul B Mahol
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;*****************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-
-pb_flip_byte:  db 15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,0
-pb_flip_short: db 14,15,12,13,10,11,8,9,6,7,4,5,2,3,0,1
-
-SECTION .text
-
-;%1 byte or short, %2 b or w, %3 size in byte (1 for byte, 2 for short)
-%macro HFLIP 3
-cglobal hflip_%1, 3, 5, 3, src, dst, w, r, x
-    VBROADCASTI128    m0, [pb_flip_%1]
-    xor               xq, xq
-%if %3 == 1
-    movsxdifnidn wq, wd
-%else ; short
-    add     wd, wd
-%endif
-    mov     rq, wq
-    and     rq, 2 * mmsize - 1
-    cmp     wq, 2 * mmsize
-    jl .loop1
-    sub     wq, rq
-
-    .loop0:
-        neg     xq
-%if mmsize == 32
-        vpermq  m1, [srcq + xq -     mmsize + %3], 0x4e; flip each lane at load
-        vpermq  m2, [srcq + xq - 2 * mmsize + %3], 0x4e; flip each lane at load
-%else
-        movu    m1, [srcq + xq -     mmsize + %3]
-        movu    m2, [srcq + xq - 2 * mmsize + %3]
-%endif
-        pshufb  m1, m0
-        pshufb  m2, m0
-        neg     xq
-        movu    [dstq + xq         ], m1
-        movu    [dstq + xq + mmsize], m2
-        add     xq, mmsize * 2
-        cmp     xq, wq
-        jl .loop0
-
-    cmp    rq, 0
-    je .end
-    add    wq, rq
-
-    .loop1:
-        neg    xq
-        mov    r%2, [srcq + xq]
-        neg    xq
-        mov    [dstq + xq], r%2
-        add    xq, %3
-        cmp    xq, wq
-        jl .loop1
-    .end:
-        RET
-%endmacro
-
-INIT_XMM ssse3
-HFLIP byte, b, 1
-HFLIP short, w, 2
-
-%if HAVE_AVX2_EXTERNAL
-INIT_YMM avx2
-HFLIP byte, b, 1
-HFLIP short, w, 2
-%endif
diff -uparN ffmpeg-4.1/libavfilter/x86/vf_hqdn3d.asm ffmpeg-y/libavfilter/x86/vf_hqdn3d.asm
--- ffmpeg-4.1/libavfilter/x86/vf_hqdn3d.asm	2016-03-29 10:25:27.000000000 +0800
+++ ffmpeg-y/libavfilter/x86/vf_hqdn3d.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,106 +0,0 @@
-;******************************************************************************
-;* Copyright (c) 2012 Loren Merritt
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION .text
-
-%macro LOWPASS 3 ; prevsample, cursample, lut
-    sub    %1q, %2q
-%if lut_bits != 8
-    sar    %1q, 8-lut_bits
-%endif
-    movsx  %1q, word [%3q+%1q*2]
-    add    %1q, %2q
-%endmacro
-
-%macro LOAD 3 ; dstreg, x, bitdepth
-%if %3 == 8
-    movzx  %1, byte [srcq+%2]
-%else
-    movzx  %1, word [srcq+(%2)*2]
-%endif
-%if %3 != 16
-    shl    %1, 16-%3
-    add    %1, (1<<(15-%3))-1
-%endif
-%endmacro
-
-%macro HQDN3D_ROW 1 ; bitdepth
-%if ARCH_X86_64
-cglobal hqdn3d_row_%1_x86, 7,10,0, src, dst, lineant, frameant, width, spatial, temporal, pixelant, t0, t1
-%else
-cglobal hqdn3d_row_%1_x86, 7,7,0, src, dst, lineant, frameant, width, spatial, temporal
-%endif
-    %assign bytedepth (%1+7)>>3
-    %assign lut_bits 4+4*(%1/16)
-    dec    widthq
-    lea    srcq, [srcq+widthq*bytedepth]
-    lea    dstq, [dstq+widthq*bytedepth]
-    lea    frameantq, [frameantq+widthq*2]
-    lea    lineantq,  [lineantq+widthq*2]
-    neg    widthq
-    %define xq widthq
-%if ARCH_X86_32
-    mov    dstmp, dstq
-    mov    srcmp, srcq
-    mov    frameantmp, frameantq
-    mov    lineantmp,  lineantq
-    %define dstq r0
-    %define frameantq r0
-    %define lineantq  r0
-    %define pixelantq r1
-    %define pixelantd r1d
-    DECLARE_REG_TMP 2,3
-%endif
-    LOAD   pixelantd, xq, %1
-ALIGN 16
-.loop:
-    movifnidn srcq, srcmp
-    LOAD      t0d, xq+1, %1 ; skip on the last iteration to avoid overread
-.loop2:
-    movifnidn lineantq, lineantmp
-    movzx     t1d, word [lineantq+xq*2]
-    LOWPASS   t1, pixelant, spatial
-    mov       [lineantq+xq*2], t1w
-    LOWPASS   pixelant, t0, spatial
-    movifnidn frameantq, frameantmp
-    movzx     t0d, word [frameantq+xq*2]
-    LOWPASS   t0, t1, temporal
-    mov       [frameantq+xq*2], t0w
-    movifnidn dstq, dstmp
-%if %1 != 16
-    shr    t0d, 16-%1 ; could eliminate this by storing from t0h, but only with some contraints on register allocation
-%endif
-%if %1 == 8
-    mov    [dstq+xq], t0b
-%else
-    mov    [dstq+xq*2], t0w
-%endif
-    inc    xq
-    jl .loop
-    je .loop2
-    REP_RET
-%endmacro ; HQDN3D_ROW
-
-HQDN3D_ROW 8
-HQDN3D_ROW 9
-HQDN3D_ROW 10
-HQDN3D_ROW 16
diff -uparN ffmpeg-4.1/libavfilter/x86/vf_idet.asm ffmpeg-y/libavfilter/x86/vf_idet.asm
--- ffmpeg-4.1/libavfilter/x86/vf_idet.asm	2018-01-01 06:35:48.000000000 +0800
+++ ffmpeg-y/libavfilter/x86/vf_idet.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,170 +0,0 @@
-;*****************************************************************************
-;* x86-optimized functions for idet filter
-;*
-;* Copyright (C) 2014 Pascal Massimino (pascal.massimino@gmail.com)
-;* Copyright (c) 2014 Neil Birkbeck (birkbeck@google.com)
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION .text
-
-; Implementation that does 8-bytes at a time using single-word operations.
-%macro IDET_FILTER_LINE 1
-INIT_MMX %1
-cglobal idet_filter_line, 4, 5, 0, a, b, c, width, index
-    xor       indexq, indexq
-%define   m_zero m2
-%define   m_sum  m5
-    pxor      m_sum, m_sum
-    pxor      m_zero, m_zero
-
-.loop:
-    movu      m0, [aq + indexq*1]
-    punpckhbw m1, m0, m_zero
-    punpcklbw m0, m_zero
-
-    movu      m3, [cq + indexq*1]
-    punpckhbw m4, m3, m_zero
-    punpcklbw m3, m_zero
-
-    paddsw    m1, m4
-    paddsw    m0, m3
-
-    movu      m3, [bq + indexq*1]
-    punpckhbw m4, m3, m_zero
-    punpcklbw m3, m_zero
-
-    paddw     m4, m4
-    paddw     m3, m3
-    psubsw    m1, m4
-    psubsw    m0, m3
-
-    ABS2      m1, m0, m4, m3
-
-    paddw     m0, m1
-    punpckhwd m1, m0, m_zero
-    punpcklwd m0, m_zero
-
-    paddd     m0, m1
-    paddd     m_sum, m0
-
-    add       indexq, 0x8
-    CMP       widthd, indexd
-    jg        .loop
-
-    HADDD     m_sum, m0
-    movd      eax, m_sum
-    RET
-%endmacro
-
-%if ARCH_X86_32
-IDET_FILTER_LINE mmxext
-IDET_FILTER_LINE mmx
-%endif
-
-;******************************************************************************
-; 16bit implementation that does 4/8-pixels at a time
-
-%macro PABS_DIFF_WD 3    ; a, b, junk   , output=a
-  psubusw   %3, %2, %1
-  psubusw   %1, %2
-  por       %1, %3
-
-  mova      %2, %1
-  punpcklwd %1, m_zero
-  punpckhwd %2, m_zero
-  paddd     %1, %2
-%endmacro
-
-%macro IDET_FILTER_LINE_16BIT 1   ; %1=increment (4 or 8 words)
-cglobal idet_filter_line_16bit, 4, 5, 8, a, b, c, width, index
-    xor       indexq, indexq
-%define m_zero m1
-%define m_sum  m0
-    pxor      m_sum, m_sum
-    pxor      m_zero, m_zero
-
-.loop_16bit:
-    movu      m2, [bq + indexq * 2]  ; B
-    movu      m3, [aq + indexq * 2]  ; A
-    mova      m6, m2
-    psubusw   m5, m2, m3             ; ba
-
-    movu      m4, [cq + indexq * 2]  ; C
-    add       indexq, %1
-    psubusw   m3, m2                 ; ab
-    CMP       indexd, widthd
-
-    psubusw   m6, m4                 ; bc
-    psubusw   m4, m2                 ; cb
-
-    PABS_DIFF_WD   m3, m6, m7        ; |ab - bc|
-    PABS_DIFF_WD   m5, m4, m7        ; |ba - cb|
-    paddd          m_sum, m3
-    paddd          m_sum, m5
-    jl        .loop_16bit
-
-    HADDD     m_sum, m2
-    movd      eax, m_sum
-    RET
-%endmacro
-
-INIT_XMM sse2
-IDET_FILTER_LINE_16BIT 8
-%if ARCH_X86_32
-INIT_MMX mmx
-IDET_FILTER_LINE_16BIT 4
-%endif
-
-;******************************************************************************
-; SSE2 8-bit implementation that does 16-bytes at a time:
-
-INIT_XMM sse2
-cglobal idet_filter_line, 4, 6, 7, a, b, c, width, index, total
-    xor       indexq, indexq
-    pxor      m0, m0
-    pxor      m1, m1
-
-.sse2_loop:
-    movu      m2, [bq + indexq*1]  ; B
-    movu      m3, [aq + indexq*1]  ; A
-    mova      m6, m2
-    mova      m4, m3
-    psubusb   m5, m2, m3           ; ba
-
-    movu      m3, [cq + indexq*1]  ; C
-    add       indexq, 0x10
-    psubusb   m4, m2               ; ab
-    CMP       indexd, widthd
-
-    psubusb   m6, m3               ; bc
-    psubusb   m3, m2               ; cb
-
-    psadbw    m4, m6               ; |ab - bc|
-    paddq     m0, m4
-    psadbw    m5, m3               ; |ba - cb|
-    paddq     m1, m5
-    jl       .sse2_loop
-
-    paddq     m0, m1
-    movhlps   m1, m0
-    paddq     m0, m1
-    movd      eax, m0
-    RET
diff -uparN ffmpeg-4.1/libavfilter/x86/vf_interlace.asm ffmpeg-y/libavfilter/x86/vf_interlace.asm
--- ffmpeg-4.1/libavfilter/x86/vf_interlace.asm	2018-11-03 08:17:29.000000000 +0800
+++ ffmpeg-y/libavfilter/x86/vf_interlace.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,226 +0,0 @@
-;*****************************************************************************
-;* x86-optimized functions for interlace filter
-;*
-;* Copyright (C) 2014 Kieran Kunhya <kierank@obe.tv>
-;* Copyright (c) 2014 Michael Niedermayer <michaelni@gmx.at>
-;* Copyright (c) 2017 Thomas Mundt <tmundt75@gmail.com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or modify
-;* it under the terms of the GNU General Public License as published by
-;* the Free Software Foundation; either version 2 of the License, or
-;* (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
-;* GNU General Public License for more details.
-;*
-;* You should have received a copy of the GNU General Public License along
-;* with FFmpeg; if not, write to the Free Software Foundation, Inc.,
-;* 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-
-pw_4: times 8 dw 4
-
-SECTION .text
-
-%macro LOWPASS 1
-    add dstq, hq
-    add srcq, hq
-    add mrefq, srcq
-    add prefq, srcq
-    neg hq
-
-    pcmpeq%1 m6, m6
-
-    test hq, mmsize
-    je .loop
-
-    ;process 1 * mmsize
-    movu m0, [mrefq+hq]
-    pavg%1 m0, [prefq+hq]
-    pxor m0, m6
-    pxor m2, m6, [srcq+hq]
-    pavg%1 m0, m2
-    pxor m0, m6
-    mova [dstq+hq], m0
-    add hq, mmsize
-    jge .end
-
-.loop:
-    movu m0, [mrefq+hq]
-    movu m1, [mrefq+hq+mmsize]
-    pavg%1 m0, [prefq+hq]
-    pavg%1 m1, [prefq+hq+mmsize]
-    pxor m0, m6
-    pxor m1, m6
-    pxor m2, m6, [srcq+hq]
-    pxor m3, m6, [srcq+hq+mmsize]
-    pavg%1 m0, m2
-    pavg%1 m1, m3
-    pxor m0, m6
-    pxor m1, m6
-    mova [dstq+hq], m0
-    mova [dstq+hq+mmsize], m1
-
-    add hq, 2*mmsize
-    jl .loop
-
-.end:
-    REP_RET
-%endmacro
-
-%macro LOWPASS_LINE 0
-cglobal lowpass_line, 5, 5, 7, dst, h, src, mref, pref
-    LOWPASS b
-
-cglobal lowpass_line_16, 5, 5, 7, dst, h, src, mref, pref
-    shl hq, 1
-    LOWPASS w
-%endmacro
-
-%macro LOWPASS_LINE_COMPLEX 0
-cglobal lowpass_line_complex, 5, 5, 8, dst, h, src, mref, pref
-    pxor m7, m7
-.loop:
-    movu m0, [srcq+mrefq]
-    movu m2, [srcq+prefq]
-    mova m1, m0
-    mova m3, m2
-    punpcklbw m0, m7
-    punpcklbw m2, m7
-    punpckhbw m1, m7
-    punpckhbw m3, m7
-    paddw m0, m2
-    paddw m1, m3
-    mova m6, m0
-    mova m5, m1
-    movu m2, [srcq]
-    mova m3, m2
-    punpcklbw m2, m7
-    punpckhbw m3, m7
-    paddw m0, m2
-    paddw m1, m3
-    psllw m2, 1
-    psllw m3, 1
-    paddw m0, m2
-    paddw m1, m3
-    psllw m0, 1
-    psllw m1, 1
-    pcmpgtw m6, m2
-    pcmpgtw m5, m3
-    packsswb m6, m5
-    movu m2, [srcq+mrefq*2]
-    movu m4, [srcq+prefq*2]
-    mova m3, m2
-    mova m5, m4
-    punpcklbw m2, m7
-    punpcklbw m4, m7
-    punpckhbw m3, m7
-    punpckhbw m5, m7
-    paddw m2, m4
-    paddw m3, m5
-    paddw m0, [pw_4]
-    paddw m1, [pw_4]
-    psubusw m0, m2
-    psubusw m1, m3
-    psrlw m0, 3
-    psrlw m1, 3
-    packuswb m0, m1
-    mova m1, m0
-    movu m2, [srcq]
-    pmaxub m0, m2
-    pminub m1, m2
-    pand m0, m6
-    pandn m6, m1
-    por m0, m6
-    mova [dstq], m0
-
-    add dstq, mmsize
-    add srcq, mmsize
-    sub hd, mmsize
-    jg .loop
-REP_RET
-
-cglobal lowpass_line_complex_12, 5, 5, 8, 16, dst, h, src, mref, pref, clip_max
-    movd m7, DWORD clip_maxm
-    SPLATW m7, m7, 0
-    movu [rsp], m7
-.loop:
-    movu m0, [srcq+mrefq]
-    movu m1, [srcq+mrefq+mmsize]
-    movu m2, [srcq+prefq]
-    movu m3, [srcq+prefq+mmsize]
-    paddw m0, m2
-    paddw m1, m3
-    mova m6, m0
-    mova m7, m1
-    movu m2, [srcq]
-    movu m3, [srcq+mmsize]
-    paddw m0, m2
-    paddw m1, m3
-    psllw m2, 1
-    psllw m3, 1
-    paddw m0, m2
-    paddw m1, m3
-    psllw m0, 1
-    psllw m1, 1
-    pcmpgtw m6, m2
-    pcmpgtw m7, m3
-    movu m2, [srcq+2*mrefq]
-    movu m3, [srcq+2*mrefq+mmsize]
-    movu m4, [srcq+2*prefq]
-    movu m5, [srcq+2*prefq+mmsize]
-    paddw m2, m4
-    paddw m3, m5
-    paddw m0, [pw_4]
-    paddw m1, [pw_4]
-    psubusw m0, m2
-    psubusw m1, m3
-    psrlw m0, 3
-    psrlw m1, 3
-    pminsw m0, [rsp]
-    pminsw m1, [rsp]
-    mova m2, m0
-    mova m3, m1
-    movu m4, [srcq]
-    pmaxsw m0, m4
-    pminsw m2, m4
-    movu m4, [srcq + mmsize]
-    pmaxsw m1, m4
-    pminsw m3, m4
-    pand m0, m6
-    pand m1, m7
-    pandn m6, m2
-    pandn m7, m3
-    por m0, m6
-    por m1, m7
-    mova [dstq], m0
-    mova [dstq+mmsize], m1
-
-    add dstq, 2*mmsize
-    add srcq, 2*mmsize
-    sub hd, mmsize
-    jg .loop
-REP_RET
-%endmacro
-
-INIT_XMM sse2
-LOWPASS_LINE
-
-INIT_XMM avx
-LOWPASS_LINE
-
-%if HAVE_AVX2_EXTERNAL
-INIT_YMM avx2
-LOWPASS_LINE
-%endif
-
-INIT_XMM sse2
-LOWPASS_LINE_COMPLEX
diff -uparN ffmpeg-4.1/libavfilter/x86/vf_limiter.asm ffmpeg-y/libavfilter/x86/vf_limiter.asm
--- ffmpeg-4.1/libavfilter/x86/vf_limiter.asm	2018-11-02 02:34:26.000000000 +0800
+++ ffmpeg-y/libavfilter/x86/vf_limiter.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,80 +0,0 @@
-;*****************************************************************************
-;* x86-optimized functions for limiter filter
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION .text
-
-INIT_XMM sse2
-
-cglobal limiter_8bit, 6, 7, 3, src, dst, slinesize, dlinesize, w, h, x
-    movsxdifnidn wq, wd
-    add        srcq, wq
-    add        dstq, wq
-    neg          wq
-    movd         m1, r6m
-    punpcklbw    m1, m1
-    SPLATW       m1, m1
-    movd         m2, r7m
-    punpcklbw    m2, m2
-    SPLATW       m2, m2
-.nextrow:
-    mov          xq, wq
-
-    .loop:
-        movu           m0, [srcq + xq]
-        CLIPUB         m0, m1, m2
-        mova    [dstq+xq], m0
-        add            xq, mmsize
-    jl .loop
-
-    add        srcq, slinesizeq
-    add        dstq, dlinesizeq
-    sub        hd, 1
-    jg .nextrow
-    RET
-
-INIT_XMM sse4
-
-cglobal limiter_16bit, 6, 7, 3, src, dst, slinesize, dlinesize, w, h, x
-    shl          wd, 1
-    add        srcq, wq
-    add        dstq, wq
-    neg          wq
-    movd         m1, r6m
-    SPLATW       m1, m1
-    movd         m2, r7m
-    SPLATW       m2, m2
-.nextrow:
-    mov          xq, wq
-
-    .loop:
-        movu           m0, [srcq + xq]
-        pmaxuw         m0, m1
-        pminuw         m0, m2
-        mova    [dstq+xq], m0
-        add            xq, mmsize
-    jl .loop
-
-    add        srcq, slinesizeq
-    add        dstq, dlinesizeq
-    sub        hd, 1
-    jg .nextrow
-    RET
diff -uparN ffmpeg-4.1/libavfilter/x86/vf_maskedmerge.asm ffmpeg-y/libavfilter/x86/vf_maskedmerge.asm
--- ffmpeg-4.1/libavfilter/x86/vf_maskedmerge.asm	2018-07-17 17:27:42.000000000 +0800
+++ ffmpeg-y/libavfilter/x86/vf_maskedmerge.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,81 +0,0 @@
-;*****************************************************************************
-;* x86-optimized functions for maskedmerge filter
-;*
-;* Copyright (C) 2015 Paul B Mahol
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;*****************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-
-pw_128: times 8 dw 128
-pw_256: times 8 dw 256
-
-SECTION .text
-
-INIT_XMM sse2
-%if ARCH_X86_64
-cglobal maskedmerge8, 8, 11, 7, bsrc, osrc, msrc, dst, blinesize, olinesize, mlinesize, dlinesize, w, h, x
-    mov         wd, dword wm
-    mov         hd, dword hm
-%else
-cglobal maskedmerge8, 5, 7, 7, bsrc, osrc, msrc, dst, blinesize, w, x
-    mov         wd, r8m
-%define olinesizeq r5mp
-%define mlinesizeq r6mp
-%define dlinesizeq r7mp
-%define hd r9mp
-%endif
-    mova        m4, [pw_256]
-    mova        m5, [pw_128]
-    pxor        m6, m6
-    add      bsrcq, wq
-    add      osrcq, wq
-    add      msrcq, wq
-    add       dstq, wq
-    neg         wq
-.nextrow:
-    mov         xq, wq
-
-    .loop:
-        movh            m0, [bsrcq + xq]
-        movh            m1, [osrcq + xq]
-        movh            m3, [msrcq + xq]
-        mova            m2, m4
-        punpcklbw       m0, m6
-        punpcklbw       m1, m6
-        punpcklbw       m3, m6
-        psubw           m2, m3
-        pmullw          m2, m0
-        pmullw          m1, m3
-        paddw           m1, m2
-        paddw           m1, m5
-        psrlw           m1, 8
-        packuswb        m1, m1
-        movh   [dstq + xq], m1
-        add             xq, mmsize / 2
-    jl .loop
-
-    add         bsrcq, blinesizeq
-    add         osrcq, olinesizeq
-    add         msrcq, mlinesizeq
-    add          dstq, dlinesizeq
-    sub         hd, 1
-    jg .nextrow
-REP_RET
diff -uparN ffmpeg-4.1/libavfilter/x86/vf_overlay.asm ffmpeg-y/libavfilter/x86/vf_overlay.asm
--- ffmpeg-4.1/libavfilter/x86/vf_overlay.asm	2018-11-06 07:22:26.000000000 +0800
+++ ffmpeg-y/libavfilter/x86/vf_overlay.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,144 +0,0 @@
-;*****************************************************************************
-;* x86-optimized functions for overlay filter
-;*
-;* Copyright (C) 2018 Paul B Mahol
-;* Copyright (C) 2018 Henrik Gramner
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;*****************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-
-pb_1:     times 16 db 1
-pw_128:   times  8 dw 128
-pw_255:   times  8 dw 255
-pw_257:   times  8 dw 257
-
-SECTION .text
-
-INIT_XMM sse4
-cglobal overlay_row_44, 5, 7, 6, 0, d, da, s, a, w, r, x
-    xor          xq, xq
-    movsxdifnidn wq, wd
-    mov          rq, wq
-    and          rq, mmsize/2 - 1
-    cmp          wq, mmsize/2
-    jl .end
-    sub          wq, rq
-    mova         m3, [pw_255]
-    mova         m4, [pw_128]
-    mova         m5, [pw_257]
-    .loop:
-        pmovzxbw    m0, [sq+xq]
-        pmovzxbw    m2, [aq+xq]
-        pmovzxbw    m1, [dq+xq]
-        pmullw      m0, m2
-        pxor        m2, m3
-        pmullw      m1, m2
-        paddw       m0, m4
-        paddw       m0, m1
-        pmulhuw     m0, m5
-        packuswb    m0, m0
-        movq   [dq+xq], m0
-        add         xq, mmsize/2
-        cmp         xq, wq
-        jl .loop
-
-    .end:
-    mov    eax, xd
-    RET
-
-INIT_XMM sse4
-cglobal overlay_row_22, 5, 7, 6, 0, d, da, s, a, w, r, x
-    xor          xq, xq
-    movsxdifnidn wq, wd
-    sub          wq, 1
-    mov          rq, wq
-    and          rq, mmsize/2 - 1
-    cmp          wq, mmsize/2
-    jl .end
-    sub          wq, rq
-    mova         m3, [pw_255]
-    mova         m4, [pw_128]
-    mova         m5, [pw_257]
-    .loop:
-        pmovzxbw    m0, [sq+xq]
-        movu        m1, [aq+2*xq]
-        pandn       m2, m3, m1
-        psllw       m1, 8
-        pavgw       m2, m1
-        pavgw       m2, m1
-        psrlw       m2, 8
-        pmovzxbw    m1, [dq+xq]
-        pmullw      m0, m2
-        pxor        m2, m3
-        pmullw      m1, m2
-        paddw       m0, m4
-        paddw       m0, m1
-        pmulhuw     m0, m5
-        packuswb    m0, m0
-        movq   [dq+xq], m0
-        add         xq, mmsize/2
-        cmp         xq, wq
-        jl .loop
-
-    .end:
-    mov    eax, xd
-    RET
-
-INIT_XMM sse4
-cglobal overlay_row_20, 6, 7, 7, 0, d, da, s, a, w, r, x
-    mov         daq, aq
-    add         daq, rmp
-    xor          xq, xq
-    movsxdifnidn wq, wd
-    sub          wq, 1
-    mov          rq, wq
-    and          rq, mmsize/2 - 1
-    cmp          wq, mmsize/2
-    jl .end
-    sub          wq, rq
-    mova         m3, [pw_255]
-    mova         m4, [pw_128]
-    mova         m5, [pw_257]
-    mova         m6, [pb_1]
-    .loop:
-        pmovzxbw    m0, [sq+xq]
-        movu        m2, [aq+2*xq]
-        movu        m1, [daq+2*xq]
-        pmaddubsw   m2, m6
-        pmaddubsw   m1, m6
-        paddw       m2, m1
-        psrlw       m2, 2
-        pmovzxbw    m1, [dq+xq]
-        pmullw      m0, m2
-        pxor        m2, m3
-        pmullw      m1, m2
-        paddw       m0, m4
-        paddw       m0, m1
-        pmulhuw     m0, m5
-        packuswb    m0, m0
-        movq   [dq+xq], m0
-        add         xq, mmsize/2
-        cmp         xq, wq
-        jl .loop
-
-    .end:
-    mov    eax, xd
-    RET
diff -uparN ffmpeg-4.1/libavfilter/x86/vf_pp7.asm ffmpeg-y/libavfilter/x86/vf_pp7.asm
--- ffmpeg-4.1/libavfilter/x86/vf_pp7.asm	2018-01-01 06:35:48.000000000 +0800
+++ ffmpeg-y/libavfilter/x86/vf_pp7.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,57 +0,0 @@
-;*****************************************************************************
-;* x86-optimized functions for pp7 filter
-;*
-;* Copyright (c) 2005 Michael Niedermayer <michaelni@gmx.at>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or modify
-;* it under the terms of the GNU General Public License as published by
-;* the Free Software Foundation; either version 2 of the License, or
-;* (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
-;* GNU General Public License for more details.
-;*
-;* You should have received a copy of the GNU General Public License along
-;* with FFmpeg; if not, write to the Free Software Foundation, Inc.,
-;* 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION .text
-
-INIT_MMX mmx
-
-;void ff_pp7_dctB_mmx(int16_t *dst, int16_t *src)
-cglobal pp7_dctB, 2, 2, 0, dst, src
-    movq   m0, [srcq]
-    movq   m1, [srcq+mmsize*1]
-    paddw  m0, [srcq+mmsize*6]
-    paddw  m1, [srcq+mmsize*5]
-    movq   m2, [srcq+mmsize*2]
-    movq   m3, [srcq+mmsize*3]
-    paddw  m2, [srcq+mmsize*4]
-    paddw  m3, m3
-    movq   m4, m3
-    psubw  m3, m0
-    paddw  m4, m0
-    movq   m0, m2
-    psubw  m2, m1
-    paddw  m0, m1
-    movq   m1, m4
-    psubw  m4, m0
-    paddw  m1, m0
-    movq   m0, m3
-    psubw  m3, m2
-    psubw  m3, m2
-    paddw  m2, m0
-    paddw  m2, m0
-    movq   [dstq], m1
-    movq   [dstq+mmsize*2], m4
-    movq   [dstq+mmsize*1], m2
-    movq   [dstq+mmsize*3], m3
-    RET
diff -uparN ffmpeg-4.1/libavfilter/x86/vf_psnr.asm ffmpeg-y/libavfilter/x86/vf_psnr.asm
--- ffmpeg-4.1/libavfilter/x86/vf_psnr.asm	2018-07-17 17:27:42.000000000 +0800
+++ ffmpeg-y/libavfilter/x86/vf_psnr.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,140 +0,0 @@
-;*****************************************************************************
-;* x86-optimized functions for psnr filter
-;*
-;* Copyright (C) 2015 Ronald S. Bultje <rsbultje@gmail.com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION .text
-
-%macro SSE_LINE_FN 2 ; 8 or 16, byte or word
-INIT_XMM sse2
-%if ARCH_X86_32
-%if %1 == 8
-cglobal sse_line_%1 %+ bit, 0, 6, 8, res, buf, w, px1, px2, ref
-%else
-cglobal sse_line_%1 %+ bit, 0, 7, 8, res, buf, reshigh, w, px1, px2, ref
-%endif
-    mov       bufq, r0mp
-    mov       refq, r1mp
-    mov         wd, r2m
-%else
-cglobal sse_line_%1 %+ bit, 3, 5, 8, buf, ref, w, px1, px2
-%endif
-    pxor        m6, m6
-    pxor        m7, m7
-    sub         wd, mmsize*2
-    jl .end
-
-.loop:
-    movu        m0, [bufq+mmsize*0]
-    movu        m1, [bufq+mmsize*1]
-    movu        m2, [refq+mmsize*0]
-    movu        m3, [refq+mmsize*1]
-%if %1 == 8
-    add       bufq, mmsize*2
-    add       refq, mmsize*2
-    psubusb     m4, m0, m2
-    psubusb     m5, m1, m3
-    psubusb     m2, m0
-    psubusb     m3, m1
-    por         m2, m4
-    por         m3, m5
-    punpcklbw   m0, m2, m6
-    punpcklbw   m1, m3, m6
-    punpckhbw   m2, m6
-    punpckhbw   m3, m6
-%else
-    psubw       m0, m2
-    psubw       m1, m3
-    movu        m2, [bufq+mmsize*2]
-    movu        m3, [bufq+mmsize*3]
-    movu        m4, [refq+mmsize*2]
-    movu        m5, [refq+mmsize*3]
-    psubw       m2, m4
-    psubw       m3, m5
-    add       bufq, mmsize*4
-    add       refq, mmsize*4
-%endif
-    pmaddwd     m0, m0
-    pmaddwd     m1, m1
-    pmaddwd     m2, m2
-    pmaddwd     m3, m3
-    paddd       m0, m1
-    paddd       m2, m3
-%if %1 == 8
-    paddd       m7, m0
-    paddd       m7, m2
-%else
-    paddd       m0, m2
-    punpckldq   m2, m0, m6
-    punpckhdq   m0, m6
-    paddq       m7, m0
-    paddq       m7, m2
-%endif
-    sub         wd, mmsize*2
-    jge .loop
-
-.end:
-    add         wd, mmsize*2
-    movhlps     m0, m7
-%if %1 == 8
-    paddd       m7, m0
-    pshufd      m0, m7, 1
-    paddd       m7, m0
-    movd       eax, m7
-%else
-    paddq       m7, m0
-%if ARCH_X86_32
-    movd       eax, m7
-    psrldq      m7, 4
-    movd       edx, m7
-%else
-    movq       rax, m7
-%endif
-%endif
-
-    ; deal with cases where w % 32 != 0
-    test        wd, wd
-    jz .end_scalar
-.loop_scalar:
-    movzx     px1d, %2 [bufq+wq*(%1/8)-(%1/8)]
-    movzx     px2d, %2 [refq+wq*(%1/8)-(%1/8)]
-    sub       px1d, px2d
-    imul      px1d, px1d
-%if %1 == 8
-    add        eax, px1d
-%elif ARCH_X86_64
-    add        rax, px1q
-%else
-    add        eax, px1d
-    adc        edx, 0
-%endif
-    dec         wd
-    jg .loop_scalar
-
-.end_scalar:
-    ; for %1=8, no need to zero edx on x86-32, since edx=wd, which is zero
-    RET
-%endmacro
-
-INIT_XMM sse2
-SSE_LINE_FN  8, byte
-SSE_LINE_FN 16, word
diff -uparN ffmpeg-4.1/libavfilter/x86/vf_pullup.asm ffmpeg-y/libavfilter/x86/vf_pullup.asm
--- ffmpeg-4.1/libavfilter/x86/vf_pullup.asm	2018-01-01 06:35:48.000000000 +0800
+++ ffmpeg-y/libavfilter/x86/vf_pullup.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,178 +0,0 @@
-;*****************************************************************************
-;* x86-optimized functions for pullup filter
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or modify
-;* it under the terms of the GNU General Public License as published by
-;* the Free Software Foundation; either version 2 of the License, or
-;* (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
-;* GNU General Public License for more details.
-;*
-;* You should have received a copy of the GNU General Public License along
-;* with FFmpeg; if not, write to the Free Software Foundation, Inc.,
-;* 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION .text
-
-INIT_MMX mmx
-cglobal pullup_filter_diff, 3, 5, 8, first, second, size
-    mov        r3, 4
-    pxor       m4, m4
-    pxor       m7, m7
-
-.loop:
-    movq       m0, [firstq]
-    movq       m2, [firstq]
-    add        firstq, sizeq
-    movq       m1, [secondq]
-    add        secondq, sizeq
-    psubusb    m2, m1
-    psubusb    m1, m0
-    movq       m0, m2
-    movq       m3, m1
-    punpcklbw  m0, m7
-    punpcklbw  m1, m7
-    punpckhbw  m2, m7
-    punpckhbw  m3, m7
-    paddw      m4, m0
-    paddw      m4, m1
-    paddw      m4, m2
-    paddw      m4, m3
-
-    dec        r3
-    jnz .loop
-
-    movq       m3, m4
-    punpcklwd  m4, m7
-    punpckhwd  m3, m7
-    paddd      m3, m4
-    movd      eax, m3
-    psrlq      m3, 32
-    movd      r4d, m3
-    add       eax, r4d
-    RET
-
-INIT_MMX mmx
-cglobal pullup_filter_comb, 3, 5, 8, first, second, size
-    mov        r3, 4
-    pxor       m6, m6
-    pxor       m7, m7
-    sub        secondq, sizeq
-
-.loop:
-    movq       m0, [firstq]
-    movq       m1, [secondq]
-    punpcklbw  m0, m7
-    movq       m2, [secondq+sizeq]
-    punpcklbw  m1, m7
-    punpcklbw  m2, m7
-    paddw      m0, m0
-    paddw      m1, m2
-    movq       m2, m0
-    psubusw    m0, m1
-    psubusw    m1, m2
-    paddw      m6, m0
-    paddw      m6, m1
-
-    movq       m0, [firstq]
-    movq       m1, [secondq]
-    punpckhbw  m0, m7
-    movq       m2, [secondq+sizeq]
-    punpckhbw  m1, m7
-    punpckhbw  m2, m7
-    paddw      m0, m0
-    paddw      m1, m2
-    movq       m2, m0
-    psubusw    m0, m1
-    psubusw    m1, m2
-    paddw      m6, m0
-    paddw      m6, m1
-
-    movq       m0, [secondq+sizeq]
-    movq       m1, [firstq]
-    punpcklbw  m0, m7
-    movq       m2, [firstq+sizeq]
-    punpcklbw  m1, m7
-    punpcklbw  m2, m7
-    paddw      m0, m0
-    paddw      m1, m2
-    movq       m2, m0
-    psubusw    m0, m1
-    psubusw    m1, m2
-    paddw      m6, m0
-    paddw      m6, m1
-
-    movq       m0, [secondq+sizeq]
-    movq       m1, [firstq]
-    punpckhbw  m0, m7
-    movq       m2, [firstq+sizeq]
-    punpckhbw  m1, m7
-    punpckhbw  m2, m7
-    paddw      m0, m0
-    paddw      m1, m2
-    movq       m2, m0
-    psubusw    m0, m1
-    psubusw    m1, m2
-    paddw      m6, m0
-    paddw      m6, m1
-
-    add        firstq, sizeq
-    add        secondq, sizeq
-    dec        r3
-    jnz .loop
-
-    movq       m5, m6
-    punpcklwd  m6, m7
-    punpckhwd  m5, m7
-    paddd      m5, m6
-    movd      eax, m5
-    psrlq      m5, 32
-    movd      r4d, m5
-    add       eax, r4d
-    RET
-
-INIT_MMX mmx
-cglobal pullup_filter_var, 3, 5, 8, first, second, size
-    mov        r3, 3
-    pxor       m4, m4
-    pxor       m7, m7
-
-.loop:
-    movq       m0, [firstq]
-    movq       m2, [firstq]
-    movq       m1, [firstq+sizeq]
-    add        firstq, sizeq
-    psubusb    m2, m1
-    psubusb    m1, m0
-    movq       m0, m2
-    movq       m3, m1
-    punpcklbw  m0, m7
-    punpcklbw  m1, m7
-    punpckhbw  m2, m7
-    punpckhbw  m3, m7
-    paddw      m4, m0
-    paddw      m4, m1
-    paddw      m4, m2
-    paddw      m4, m3
-
-    dec        r3
-    jnz .loop
-
-    movq       m3, m4
-    punpcklwd  m4, m7
-    punpckhwd  m3, m7
-    paddd      m3, m4
-    movd      eax, m3
-    psrlq      m3, 32
-    movd      r4d, m3
-    add       eax, r4d
-    shl       eax, 2
-    RET
diff -uparN ffmpeg-4.1/libavfilter/x86/vf_removegrain.asm ffmpeg-y/libavfilter/x86/vf_removegrain.asm
--- ffmpeg-4.1/libavfilter/x86/vf_removegrain.asm	2018-01-01 06:35:48.000000000 +0800
+++ ffmpeg-y/libavfilter/x86/vf_removegrain.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,1218 +0,0 @@
-;*****************************************************************************
-;* x86-optimized functions for removegrain filter
-;*
-;* Copyright (C) 2015 James Darnley
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or modify
-;* it under the terms of the GNU General Public License as published by
-;* the Free Software Foundation; either version 2 of the License, or
-;* (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
-;* GNU General Public License for more details.
-;*
-;* You should have received a copy of the GNU General Public License along
-;* with FFmpeg; if not, write to the Free Software Foundation, Inc.,
-;* 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
-;*****************************************************************************
-
-; column: -1  0 +1
-; row -1: a1 a2 a3
-; row  0: a4  c a5
-; row +1: a6 a7 a8
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA 32
-
-pw_4:    times 16 dw 4
-pw_8:    times 16 dw 8
-pw_div9: times 16 dw ((1<<16)+4)/9
-
-SECTION .text
-
-;*** Preprocessor helpers
-
-%define a1 srcq+stride_n-1
-%define a2 srcq+stride_n
-%define a3 srcq+stride_n+1
-%define a4 srcq-1
-%define c  srcq
-%define a5 srcq+1
-%define a6 srcq+stride_p-1
-%define a7 srcq+stride_p
-%define a8 srcq+stride_p+1
-
-; %1 dest simd register
-; %2 source memory location
-; %3 zero location (simd register/memory)
-%macro LOAD 3
-    movh %1, %2
-    punpcklbw %1, %3
-%endmacro
-
-%macro LOAD_SQUARE 0
-    movu m1, [a1]
-    movu m2, [a2]
-    movu m3, [a3]
-    movu m4, [a4]
-    movu m0, [c]
-    movu m5, [a5]
-    movu m6, [a6]
-    movu m7, [a7]
-    movu m8, [a8]
-%endmacro
-
-; %1 zero location (simd register/memory)
-%macro LOAD_SQUARE_16 1
-    LOAD m1, [a1], %1
-    LOAD m2, [a2], %1
-    LOAD m3, [a3], %1
-    LOAD m4, [a4], %1
-    LOAD m0, [c], %1
-    LOAD m5, [a5], %1
-    LOAD m6, [a6], %1
-    LOAD m7, [a7], %1
-    LOAD m8, [a8], %1
-%endmacro
-
-; %1 data type
-; %2 simd register to hold maximums
-; %3 simd register to hold minimums
-; %4 temp location (simd register/memory)
-%macro SORT_PAIR 4
-    mova   %4, %2
-    pmin%1 %2, %3
-    pmax%1 %3, %4
-%endmacro
-
-%macro SORT_AXIS 0
-    SORT_PAIR ub, m1, m8, m9
-    SORT_PAIR ub, m2, m7, m10
-    SORT_PAIR ub, m3, m6, m11
-    SORT_PAIR ub, m4, m5, m12
-%endmacro
-
-
-%macro SORT_AXIS_16 0
-    SORT_PAIR sw, m1, m8, m9
-    SORT_PAIR sw, m2, m7, m10
-    SORT_PAIR sw, m3, m6, m11
-    SORT_PAIR sw, m4, m5, m12
-%endmacro
-
-; The loop doesn't need to do all the iterations.  It could stop when the right
-; pixels are in the right registers.
-%macro SORT_SQUARE 0
-    %assign k 7
-    %rep 7
-        %assign i 1
-        %assign j 2
-        %rep k
-            SORT_PAIR ub, m %+ i , m %+ j , m9
-            %assign i i+1
-            %assign j j+1
-        %endrep
-        %assign k k-1
-    %endrep
-%endmacro
-
-; %1 dest simd register
-; %2 source (simd register/memory)
-; %3 temp simd register
-%macro ABS_DIFF 3
-    mova %3, %2
-    psubusb %3, %1
-    psubusb %1, %2
-    por %1, %3
-%endmacro
-
-; %1 dest simd register
-; %2 source (simd register/memory)
-; %3 temp simd register
-%macro ABS_DIFF_W 3
-    mova %3, %2
-    psubusw %3, %1
-    psubusw %1, %2
-    por %1, %3
-%endmacro
-
-; %1 simd register that holds the "false" values and will hold the result
-; %2 simd register that holds the "true" values
-; %3 location (simd register/memory) that hold the mask
-%macro BLEND 3
-%if cpuflag(avx2)
-    vpblendvb %1, %1, %2, %3
-%else
-    pand      %2, %3
-    pandn     %3, %1
-    por       %3, %2
-    SWAP      %1, %3
-%endif
-%endmacro
-
-; Functions
-
-INIT_XMM sse2
-cglobal rg_fl_mode_1, 4, 5, 3, 0, dst, src, stride, pixels
-    mov r4q, strideq
-    neg r4q
-    %define stride_p strideq
-    %define stride_n r4q
-
-    .loop:
-        movu m0, [a1]
-        mova m1, m0
-
-        movu m2, [a2]
-        pmaxub m0, m2
-        pminub m1, m2
-
-        movu m2, [a3]
-        pmaxub m0, m2
-        pminub m1, m2
-
-        movu m2, [a4]
-        pmaxub m0, m2
-        pminub m1, m2
-
-        movu m2, [a5]
-        pmaxub m0, m2
-        pminub m1, m2
-
-        movu m2, [a6]
-        pmaxub m0, m2
-        pminub m1, m2
-
-        movu m2, [a7]
-        pmaxub m0, m2
-        pminub m1, m2
-
-        movu m2, [a8]
-        pmaxub m0, m2
-        pminub m1, m2
-
-        movu m2, [c]
-        pminub m2, m0
-        pmaxub m2, m1
-
-        movu [dstq], m2
-        add srcq, mmsize
-        add dstq, mmsize
-        sub pixelsd, mmsize
-    jg .loop
-RET
-
-%if ARCH_X86_64
-cglobal rg_fl_mode_2, 4, 5, 10, 0, dst, src, stride, pixels
-    mov r4q, strideq
-    neg r4q
-    %define stride_p strideq
-    %define stride_n r4q
-
-    .loop:
-        LOAD_SQUARE
-        SORT_SQUARE
-
-        CLIPUB m0, m2, m7
-
-        movu [dstq], m0
-        add srcq, mmsize
-        add dstq, mmsize
-        sub pixelsd, mmsize
-    jg .loop
-RET
-
-cglobal rg_fl_mode_3, 4, 5, 10, 0, dst, src, stride, pixels
-    mov r4q, strideq
-    neg r4q
-    %define stride_p strideq
-    %define stride_n r4q
-
-    .loop:
-        LOAD_SQUARE
-        SORT_SQUARE
-
-        CLIPUB m0, m3, m6
-
-        movu [dstq], m0
-        add srcq, mmsize
-        add dstq, mmsize
-        sub pixelsd, mmsize
-    jg .loop
-RET
-
-cglobal rg_fl_mode_4, 4, 5, 10, 0, dst, src, stride, pixels
-    mov r4q, strideq
-    neg r4q
-    %define stride_p strideq
-    %define stride_n r4q
-
-    .loop:
-        LOAD_SQUARE
-        SORT_SQUARE
-
-        CLIPUB m0, m4, m5
-
-        movu [dstq], m0
-        add srcq, mmsize
-        add dstq, mmsize
-        sub pixelsd, mmsize
-    jg .loop
-RET
-
-cglobal rg_fl_mode_5, 4, 5, 13, 0, dst, src, stride, pixels
-    mov r4q, strideq
-    neg r4q
-    %define stride_p strideq
-    %define stride_n r4q
-
-    .loop:
-        LOAD_SQUARE
-        SORT_AXIS
-
-        mova m9, m0
-        mova m10, m0
-        mova m11, m0
-        mova m12, m0
-
-        CLIPUB m9, m1, m8
-        CLIPUB m10, m2, m7
-        CLIPUB m11, m3, m6
-        CLIPUB m12, m4, m5
-
-        mova m8, m9  ; clip1
-        mova m7, m10 ; clip2
-        mova m6, m11 ; clip3
-        mova m5, m12 ; clip4
-
-        ABS_DIFF m9, m0, m1  ; c1
-        ABS_DIFF m10, m0, m2 ; c2
-        ABS_DIFF m11, m0, m3 ; c3
-        ABS_DIFF m12, m0, m4 ; c4
-
-        pminub m9, m10
-        pminub m9, m11
-        pminub m9, m12 ; mindiff
-
-        pcmpeqb m10, m9
-        pcmpeqb m11, m9
-        pcmpeqb m12, m9
-
-        ; Notice the order here: c1, c3, c2, c4
-        BLEND m8, m6, m11
-        BLEND m8, m7, m10
-        BLEND m8, m5, m12
-
-        movu [dstq], m8
-        add srcq, mmsize
-        add dstq, mmsize
-        sub pixelsd, mmsize
-    jg .loop
-RET
-
-cglobal rg_fl_mode_6, 4, 5, 16, 0, dst, src, stride, pixels
-    mov r4q, strideq
-    neg r4q
-    %define stride_p strideq
-    %define stride_n r4q
-
-    ; Some register saving suggestions: the zero can be somewhere other than a
-    ; register, the center pixels could be on the stack.
-
-    pxor m15, m15
-    .loop:
-        LOAD_SQUARE_16 m15
-        SORT_AXIS_16
-
-        mova m9, m0
-        mova m10, m0
-        mova m11, m0
-        mova m12, m0
-        CLIPW m9, m1, m8  ; clip1
-        CLIPW m10, m2, m7 ; clip2
-        CLIPW m11, m3, m6 ; clip3
-        CLIPW m12, m4, m5 ; clip4
-
-        psubw m8, m1 ; d1
-        psubw m7, m2 ; d2
-        psubw m6, m3 ; d3
-        psubw m5, m4 ; d4
-
-        mova m1, m9
-        mova m2, m10
-        mova m3, m11
-        mova m4, m12
-        ABS_DIFF_W m1, m0, m13
-        ABS_DIFF_W m2, m0, m14
-        ABS_DIFF_W m3, m0, m13
-        ABS_DIFF_W m4, m0, m14
-        psllw m1, 1
-        psllw m2, 1
-        psllw m3, 1
-        psllw m4, 1
-        paddw m1, m8 ; c1
-        paddw m2, m7 ; c2
-        paddw m3, m6 ; c3
-        paddw m4, m5 ; c4
-        ; As the differences (d1..d4) can only be positive, there is no need to
-        ; clip to zero.  Also, the maximum positive value is less than 768.
-
-        pminsw m1, m2
-        pminsw m1, m3
-        pminsw m1, m4
-
-        pcmpeqw m2, m1
-        pcmpeqw m3, m1
-        pcmpeqw m4, m1
-
-        BLEND m9, m11, m3
-        BLEND m9, m10, m2
-        BLEND m9, m12, m4
-        packuswb m9, m9
-
-        movh [dstq], m9
-        add srcq, mmsize/2
-        add dstq, mmsize/2
-        sub pixelsd, mmsize/2
-    jg .loop
-RET
-
-; This is just copy-pasted straight from mode 6 with the left shifts removed.
-cglobal rg_fl_mode_7, 4, 5, 16, 0, dst, src, stride, pixels
-    mov r4q, strideq
-    neg r4q
-    %define stride_p strideq
-    %define stride_n r4q
-
-    ; Can this be done without unpacking?
-
-    pxor m15, m15
-    .loop:
-        LOAD_SQUARE_16 m15
-        SORT_AXIS_16
-
-        mova m9, m0
-        mova m10, m0
-        mova m11, m0
-        mova m12, m0
-        CLIPW m9, m1, m8  ; clip1
-        CLIPW m10, m2, m7 ; clip2
-        CLIPW m11, m3, m6 ; clip3
-        CLIPW m12, m4, m5 ; clip4
-
-        psubw m8, m1 ; d1
-        psubw m7, m2 ; d2
-        psubw m6, m3 ; d3
-        psubw m5, m4 ; d4
-
-        mova m1, m9
-        mova m2, m10
-        mova m3, m11
-        mova m4, m12
-        ABS_DIFF_W m1, m0, m13
-        ABS_DIFF_W m2, m0, m14
-        ABS_DIFF_W m3, m0, m13
-        ABS_DIFF_W m4, m0, m14
-        paddw m1, m8 ; c1
-        paddw m2, m7 ; c2
-        paddw m3, m6 ; c3
-        paddw m4, m5 ; c4
-
-        pminsw m1, m2
-        pminsw m1, m3
-        pminsw m1, m4
-
-        pcmpeqw m2, m1
-        pcmpeqw m3, m1
-        pcmpeqw m4, m1
-
-        BLEND m9, m11, m3
-        BLEND m9, m10, m2
-        BLEND m9, m12, m4
-        packuswb m9, m9
-
-        movh [dstq], m9
-        add srcq, mmsize/2
-        add dstq, mmsize/2
-        sub pixelsd, mmsize/2
-    jg .loop
-RET
-
-; This is just copy-pasted straight from mode 6 with a few changes.
-cglobal rg_fl_mode_8, 4, 5, 16, 0, dst, src, stride, pixels
-    mov r4q, strideq
-    neg r4q
-    %define stride_p strideq
-    %define stride_n r4q
-
-    pxor m15, m15
-    .loop:
-        LOAD_SQUARE_16 m15
-        SORT_AXIS_16
-
-        mova m9, m0
-        mova m10, m0
-        mova m11, m0
-        mova m12, m0
-        CLIPW m9, m1, m8  ; clip1
-        CLIPW m10, m2, m7 ; clip2
-        CLIPW m11, m3, m6 ; clip3
-        CLIPW m12, m4, m5 ; clip4
-
-        psubw m8, m1 ; d1
-        psubw m7, m2 ; d2
-        psubw m6, m3 ; d3
-        psubw m5, m4 ; d4
-        psllw m8, 1
-        psllw m7, 1
-        psllw m6, 1
-        psllw m5, 1
-
-        mova m1, m9
-        mova m2, m10
-        mova m3, m11
-        mova m4, m12
-        ABS_DIFF_W m1, m0, m13
-        ABS_DIFF_W m2, m0, m14
-        ABS_DIFF_W m3, m0, m13
-        ABS_DIFF_W m4, m0, m14
-        paddw m1, m8 ; c1
-        paddw m2, m7 ; c1
-        paddw m3, m6 ; c1
-        paddw m4, m5 ; c1
-        ; As the differences (d1..d4) can only be positive, there is no need to
-        ; clip to zero.  Also, the maximum positive value is less than 768.
-
-        pminsw m1, m2
-        pminsw m1, m3
-        pminsw m1, m4
-
-        pcmpeqw m2, m1
-        pcmpeqw m3, m1
-        pcmpeqw m4, m1
-
-        BLEND m9, m11, m3
-        BLEND m9, m10, m2
-        BLEND m9, m12, m4
-        packuswb m9, m9
-
-        movh [dstq], m9
-        add srcq, mmsize/2
-        add dstq, mmsize/2
-        sub pixelsd, mmsize/2
-    jg .loop
-RET
-
-cglobal rg_fl_mode_9, 4, 5, 13, 0, dst, src, stride, pixels
-    mov r4q, strideq
-    neg r4q
-    %define stride_p strideq
-    %define stride_n r4q
-
-    .loop:
-        LOAD_SQUARE
-        SORT_AXIS
-
-        mova m9, m0
-        mova m10, m0
-        mova m11, m0
-        mova m12, m0
-        CLIPUB m9, m1, m8  ; clip1
-        CLIPUB m10, m2, m7 ; clip2
-        CLIPUB m11, m3, m6 ; clip3
-        CLIPUB m12, m4, m5 ; clip4
-
-        psubb m8, m1 ; d1
-        psubb m7, m2 ; d2
-        psubb m6, m3 ; d3
-        psubb m5, m4 ; d4
-
-        pminub m8, m7
-        pminub m8, m6
-        pminub m8, m5
-
-        pcmpeqb m7, m8
-        pcmpeqb m6, m8
-        pcmpeqb m5, m8
-
-        BLEND m9, m11, m6
-        BLEND m9, m10, m7
-        BLEND m9, m12, m5
-
-        movu [dstq], m9
-        add srcq, mmsize
-        add dstq, mmsize
-        sub pixelsd, mmsize
-    jg .loop
-RET
-%endif
-
-cglobal rg_fl_mode_10, 4, 5, 8, 0, dst, src, stride, pixels
-    mov r4q, strideq
-    neg r4q
-    %define stride_p strideq
-    %define stride_n r4q
-
-    .loop:
-        movu m0, [c]
-
-        movu m1, [a4]
-        mova m2, m1
-        ABS_DIFF m1, m0, m7
-
-        movu m3, [a5]       ; load pixel
-        mova m4, m3
-        ABS_DIFF m4, m0, m7 ; absolute difference from center
-        pminub m1, m4       ; mindiff
-        pcmpeqb m4, m1      ; if (difference == mindiff)
-        BLEND m2, m3, m4    ;     return pixel
-
-        movu m5, [a1]
-        mova m6, m5
-        ABS_DIFF m6, m0, m7
-        pminub m1, m6
-        pcmpeqb m6, m1
-        BLEND m2, m5, m6
-
-        movu m3, [a3]
-        mova m4, m3
-        ABS_DIFF m4, m0, m7
-        pminub m1, m4
-        pcmpeqb m4, m1
-        BLEND m2, m3, m4
-
-        movu m5, [a2]
-        mova m6, m5
-        ABS_DIFF m6, m0, m7
-        pminub m1, m6
-        pcmpeqb m6, m1
-        BLEND m2, m5, m6
-
-        movu m3, [a6]
-        mova m4, m3
-        ABS_DIFF m4, m0, m7
-        pminub m1, m4
-        pcmpeqb m4, m1
-        BLEND m2, m3, m4
-
-        movu m5, [a8]
-        mova m6, m5
-        ABS_DIFF m6, m0, m7
-        pminub m1, m6
-        pcmpeqb m6, m1
-        BLEND m2, m5, m6
-
-        movu m3, [a7]
-        mova m4, m3
-        ABS_DIFF m4, m0, m7
-        pminub m1, m4
-        pcmpeqb m4, m1
-        BLEND m2, m3, m4
-
-        movu [dstq], m2
-        add srcq, mmsize
-        add dstq, mmsize
-        sub pixelsd, mmsize
-    jg .loop
-RET
-
-cglobal rg_fl_mode_11_12, 4, 5, 7, 0, dst, src, stride, pixels
-    mov r4q, strideq
-    neg r4q
-    %define stride_p strideq
-    %define stride_n r4q
-
-    pxor m0, m0
-    .loop:
-        LOAD m1, [c], m0
-        LOAD m2, [a2], m0
-        LOAD m3, [a4], m0
-        LOAD m4, [a5], m0
-        LOAD m5, [a7], m0
-
-        psllw m1, 2
-        paddw m2, m3
-        paddw m4, m5
-        paddw m2, m4
-        psllw m2, 1
-
-        LOAD m3, [a1], m0
-        LOAD m4, [a3], m0
-        LOAD m5, [a6], m0
-        LOAD m6, [a8], m0
-        paddw m1, m2
-        paddw m3, m4
-        paddw m5, m6
-        paddw m1, m3
-        paddw m1, m5
-
-        paddw m1, [pw_8]
-        psraw m1, 4
-
-        packuswb m1, m1
-
-        movh [dstq], m1
-        add srcq, mmsize/2
-        add dstq, mmsize/2
-        sub pixelsd, mmsize/2
-    jg .loop
-RET
-
-cglobal rg_fl_mode_13_14, 4, 5, 8, 0, dst, src, stride, pixels
-    mov r4q, strideq
-    neg r4q
-    %define stride_p strideq
-    %define stride_n r4q
-
-    .loop:
-        movu m1, [a1]
-        movu m2, [a8]
-        mova m0, m1
-        pavgb m1, m2
-        ABS_DIFF m0, m2, m6
-
-        movu m3, [a3]
-        movu m4, [a6]
-        mova m5, m3
-        pavgb m3, m4
-        ABS_DIFF m5, m4, m7
-        pminub m0, m5
-        pcmpeqb m5, m0
-        BLEND m1, m3, m5
-
-        movu m2, [a2]
-        movu m3, [a7]
-        mova m4, m2
-        pavgb m2, m3
-        ABS_DIFF m4, m3, m6
-        pminub m0, m4
-        pcmpeqb m4, m0
-        BLEND m1, m2, m4
-
-        movu [dstq], m1
-        add srcq, mmsize
-        add dstq, mmsize
-        sub pixelsd, mmsize
-    jg .loop
-RET
-
-%if ARCH_X86_64
-cglobal rg_fl_mode_15_16, 4, 5, 16, 0, dst, src, stride, pixels
-    mov r4q, strideq
-    neg r4q
-    %define stride_p strideq
-    %define stride_n r4q
-
-    pxor m15, m15
-    .loop:
-        LOAD_SQUARE_16 m15
-
-        mova m9, m1
-        mova m10, m2
-        mova m11, m3
-        ABS_DIFF_W m9, m8, m12
-        ABS_DIFF_W m10, m7, m13
-        ABS_DIFF_W m11, m6, m14
-        pminsw m9, m10
-        pminsw m9, m11
-        pcmpeqw m10, m9
-        pcmpeqw m11, m9
-
-        mova m12, m2
-        mova m13, m1
-        mova m14, m6
-        paddw m12, m7
-        psllw m12, 1
-        paddw m13, m3
-        paddw m14, m8
-        paddw m12, [pw_4]
-        paddw m13, m14
-        paddw m12, m13
-        psrlw m12, 3
-
-        SORT_PAIR ub, m1, m8, m0
-        SORT_PAIR ub, m2, m7, m9
-        SORT_PAIR ub, m3, m6, m14
-        mova m4, m12
-        mova m5, m12
-        CLIPW m4, m1, m8
-        CLIPW m5, m2, m7
-        CLIPW m12, m3, m6
-
-        BLEND m4, m12, m11
-        BLEND m4,  m5, m10
-        packuswb m4, m4
-
-        movh [dstq], m4
-        add srcq, mmsize/2
-        add dstq, mmsize/2
-        sub pixelsd, mmsize/2
-    jg .loop
-RET
-
-cglobal rg_fl_mode_17, 4, 5, 9, 0, dst, src, stride, pixels
-    mov r4q, strideq
-    neg r4q
-    %define stride_p strideq
-    %define stride_n r4q
-
-    .loop:
-        LOAD_SQUARE
-        SORT_AXIS
-
-        pmaxub m1, m2
-        pmaxub m3, m4
-
-        pminub m8, m7
-        pminub m5, m6
-
-        pmaxub m1, m3
-        pminub m8, m5
-
-        mova m2, m1
-        pminub m1, m8
-        pmaxub m8, m2
-
-        CLIPUB m0, m1, m8
-
-        movu [dstq], m0
-        add srcq, mmsize
-        add dstq, mmsize
-        sub pixelsd, mmsize
-    jg .loop
-RET
-
-cglobal rg_fl_mode_18, 4, 5, 16, 0, dst, src, stride, pixels
-    mov r4q, strideq
-    neg r4q
-    %define stride_p strideq
-    %define stride_n r4q
-
-    .loop:
-        LOAD_SQUARE
-
-        mova m9, m1
-        mova m10, m8
-        ABS_DIFF m9, m0, m11
-        ABS_DIFF m10, m0, m12
-        pmaxub m9, m10 ; m9 = d1
-
-        mova m10, m2
-        mova m11, m7
-        ABS_DIFF m10, m0, m12
-        ABS_DIFF m11, m0, m13
-        pmaxub m10, m11 ; m10 = d2
-
-        mova m11, m3
-        mova m12, m6
-        ABS_DIFF m11, m0, m13
-        ABS_DIFF m12, m0, m14
-        pmaxub m11, m12 ; m11 = d3
-
-        mova m12, m4
-        mova m13, m5
-        ABS_DIFF m12, m0, m14
-        ABS_DIFF m13, m0, m15
-        pmaxub m12, m13 ; m12 = d4
-
-        mova m13, m9
-        pminub m13, m10
-        pminub m13, m11
-        pminub m13, m12 ; m13 = mindiff
-
-        pcmpeqb m10, m13
-        pcmpeqb m11, m13
-        pcmpeqb m12, m13
-
-        mova m14, m1
-        pminub m1, m8
-        pmaxub m8, m14
-
-        mova m13, m0
-        mova m14, m1
-        pminub m1, m8
-        pmaxub m8, m14
-        CLIPUB m13, m1, m8 ; m13 = ret...d1
-
-        mova m14, m0
-        mova m15, m3
-        pminub m3, m6
-        pmaxub m6, m15
-        CLIPUB m14, m3, m6
-        pand m14, m11
-        pandn m11, m13
-        por m14, m11 ; m14 = ret...d3
-
-        mova m15, m0
-        mova m1, m2
-        pminub m2, m7
-        pmaxub m7, m1
-        CLIPUB m15, m2, m7
-        pand m15, m10
-        pandn m10, m14
-        por m15, m10 ; m15 = ret...d2
-
-        mova m1, m0
-        mova m2, m4
-        pminub m4, m5
-        pmaxub m5, m2
-        CLIPUB m1, m4, m5
-        pand m1, m12
-        pandn m12, m15
-        por m1, m12 ; m15 = ret...d4
-
-        movu [dstq], m1
-        add srcq, mmsize
-        add dstq, mmsize
-        sub pixelsd, mmsize
-    jg .loop
-RET
-%endif
-
-cglobal rg_fl_mode_19, 4, 5, 7, 0, dst, src, stride, pixels
-    mov r4q, strideq
-    neg r4q
-    %define stride_p strideq
-    %define stride_n r4q
-
-    pxor m0, m0
-    .loop:
-        LOAD m1, [a1], m0
-        LOAD m2, [a2], m0
-        paddw m1, m2
-
-        LOAD m3, [a3], m0
-        LOAD m4, [a4], m0
-        paddw m3, m4
-
-        LOAD m5, [a5], m0
-        LOAD m6, [a6], m0
-        paddw m5, m6
-
-        LOAD m2, [a7], m0
-        LOAD m4, [a8], m0
-        paddw m2, m4
-
-        paddw m1, m3
-        paddw m2, m5
-        paddw m1, m2
-
-        paddw m1, [pw_4]
-        psraw m1, 3
-
-        packuswb m1, m1
-
-        movh [dstq], m1
-        add srcq, mmsize/2
-        add dstq, mmsize/2
-        sub pixelsd, mmsize/2
-    jg .loop
-RET
-
-cglobal rg_fl_mode_20, 4, 5, 7, 0, dst, src, stride, pixels
-    mov r4q, strideq
-    neg r4q
-    %define stride_p strideq
-    %define stride_n r4q
-
-    pxor m0, m0
-    .loop:
-        LOAD m1, [a1], m0
-        LOAD m2, [a2], m0
-        paddw m1, m2
-
-        LOAD m3, [a3], m0
-        LOAD m4, [a4], m0
-        paddw m3, m4
-
-        LOAD m5, [a5], m0
-        LOAD m6, [a6], m0
-        paddw m5, m6
-
-        LOAD m2, [a7], m0
-        LOAD m4, [a8], m0
-        paddw m2, m4
-
-        LOAD m6, [c], m0
-        paddw m1, m3
-        paddw m2, m5
-        paddw m6, [pw_4]
-
-        paddw m1, m2
-        paddw m1, m6
-
-        pmulhuw m1, [pw_div9]
-
-        packuswb m1, m1
-
-        movh [dstq], m1
-        add srcq, mmsize/2
-        add dstq, mmsize/2
-        sub pixelsd, mmsize/2
-    jg .loop
-RET
-
-cglobal rg_fl_mode_21, 4, 5, 8, 0, dst, src, stride, pixels
-    mov r4q, strideq
-    neg r4q
-    %define stride_p strideq
-    %define stride_n r4q
-
-    pxor m0, m0
-    .loop:
-        movu m1, [a1]
-        movu m2, [a8]
-        pavgb m7, m1, m2
-        punpckhbw m3, m1, m0
-        punpcklbw m1, m0
-        punpckhbw m4, m2, m0
-        punpcklbw m2, m0
-        paddw m3, m4
-        paddw m1, m2
-        psrlw m3, 1
-        psrlw m1, 1
-        packuswb m1, m3
-
-        movu m2, [a2]
-        movu m3, [a7]
-        pavgb m6, m2, m3
-        punpckhbw m4, m2, m0
-        punpcklbw m2, m0
-        punpckhbw m5, m3, m0
-        punpcklbw m3, m0
-        paddw m4, m5
-        paddw m2, m3
-        psrlw m4, 1
-        psrlw m2, 1
-        packuswb m2, m4
-
-        pminub m1, m2
-        pmaxub m7, m6
-
-        movu m2, [a3]
-        movu m3, [a6]
-        pavgb m6, m2, m3
-        punpckhbw m4, m2, m0
-        punpcklbw m2, m0
-        punpckhbw m5, m3, m0
-        punpcklbw m3, m0
-        paddw m4, m5
-        paddw m2, m3
-        psrlw m4, 1
-        psrlw m2, 1
-        packuswb m2, m4
-
-        pminub m1, m2
-        pmaxub m7, m6
-
-        movu m2, [a4]
-        movu m3, [a5]
-        pavgb m6, m2, m3
-        punpckhbw m4, m2, m0
-        punpcklbw m2, m0
-        punpckhbw m5, m3, m0
-        punpcklbw m3, m0
-        paddw m4, m5
-        paddw m2, m3
-        psrlw m4, 1
-        psrlw m2, 1
-        packuswb m2, m4
-
-        pminub m1, m2
-        pmaxub m7, m6
-
-        movu m3, [c]
-        CLIPUB m3, m1, m7
-
-        movu [dstq], m3
-        add srcq, mmsize
-        add dstq, mmsize
-        sub pixelsd, mmsize
-    jg .loop
-RET
-
-cglobal rg_fl_mode_22, 4, 5, 8, 0, dst, src, stride, pixels
-    mov r4q, strideq
-    neg r4q
-    %define stride_p strideq
-    %define stride_n r4q
-
-    .loop:
-        movu m0, [a1]
-        movu m1, [a8]
-        pavgb m0, m1
-        movu m2, [a2]
-        movu m3, [a7]
-        pavgb m2, m3
-        movu m4, [a3]
-        movu m5, [a6]
-        pavgb m4, m5
-        movu m6, [a4]
-        movu m7, [a5]
-        pavgb m6, m7
-
-        mova m1, m0
-        mova m3, m2
-        mova m5, m4
-        mova m7, m6
-        pminub m0, m2
-        pminub m4, m6
-        pmaxub m1, m3
-        pmaxub m5, m7
-        pminub m0, m4
-        pmaxub m1, m5
-
-        movu m2, [c]
-        CLIPUB m2, m0, m1
-
-        movu [dstq], m2
-        add srcq, mmsize
-        add dstq, mmsize
-        sub pixelsd, mmsize
-    jg .loop
-RET
-
-%if ARCH_X86_64
-cglobal rg_fl_mode_23, 4, 5, 16, 0, dst, src, stride, pixels
-    mov r4q, strideq
-    neg r4q
-    %define stride_p strideq
-    %define stride_n r4q
-
-    pxor m15, m15
-    .loop:
-        LOAD_SQUARE_16 m15
-        SORT_AXIS_16
-
-        mova m9, m8
-        mova m10, m7
-        mova m11, m6
-        mova m12, m5
-        psubw m9, m1  ; linediff1
-        psubw m10, m2 ; linediff2
-        psubw m11, m3 ; linediff3
-        psubw m12, m4 ; linediff4
-
-        psubw m1, m0
-        psubw m2, m0
-        psubw m3, m0
-        psubw m4, m0
-        pminsw m1, m9  ; d1
-        pminsw m2, m10 ; d2
-        pminsw m3, m11 ; d3
-        pminsw m4, m12 ; d4
-        pmaxsw m1, m2
-        pmaxsw m3, m4
-        pmaxsw m1, m3
-        pmaxsw m1, m15 ; d
-
-        mova m13, m0
-        mova m14, m0
-        mova m2, m0
-        mova m4, m0
-        psubw m13, m8
-        psubw m14, m7
-        psubw m2, m6
-        psubw m4, m5
-        pminsw m9, m13  ; u1
-        pminsw m10, m14 ; u2
-        pminsw m11, m2  ; u3
-        pminsw m12, m4  ; u4
-        pmaxsw m9, m10
-        pmaxsw m11, m12
-        pmaxsw m9, m11
-        pmaxsw m9, m15  ; u
-
-        paddw m0, m1
-        psubw m0, m9
-        packuswb m0, m0
-
-        movh [dstq], m0
-        add srcq, mmsize/2
-        add dstq, mmsize/2
-        sub pixelsd, mmsize/2
-    jg .loop
-RET
-
-cglobal rg_fl_mode_24, 4, 5, 16, mmsize, dst, src, stride, pixels
-    mov r4q, strideq
-    neg r4q
-    %define stride_p strideq
-    %define stride_n r4q
-
-    pxor m15, m15
-    .loop:
-        LOAD_SQUARE_16 m15
-        mova [rsp], m0
-        SORT_AXIS_16
-
-        mova m9, m8
-        mova m10, m7
-        mova m11, m6
-        mova m12, m5
-        psubw m9, m1  ; linediff1
-        psubw m10, m2 ; linediff2
-        psubw m11, m3 ; linediff3
-        psubw m12, m4 ; linediff4
-
-        psubw m1, [rsp] ; td1
-        psubw m2, [rsp] ; td2
-        psubw m3, [rsp] ; td3
-        psubw m4, [rsp] ; td4
-        mova m0, m9
-        mova m13, m10
-        mova m14, m11
-        mova m15, m12
-        psubw m0, m1
-        psubw m13, m2
-        psubw m14, m3
-        psubw m15, m4
-        pminsw m1, m0  ; d1
-        pminsw m2, m13 ; d2
-        pminsw m3, m14 ; d3
-        pminsw m4, m15 ; d4
-        pmaxsw m1, m2
-        pmaxsw m3, m4
-
-        mova m0, [rsp]
-        mova m13, [rsp]
-        mova m14, [rsp]
-        mova m15, [rsp]
-        psubw m0, m8  ; tu1
-        psubw m13, m7 ; tu2
-        psubw m14, m6 ; tu3
-        psubw m15, m5 ; tu4
-        psubw m9, m0
-        psubw m10, m13
-        psubw m11, m14
-        psubw m12, m15
-        pminsw m9, m0   ; u1
-        pminsw m10, m13 ; u2
-        pminsw m11, m14 ; u3
-        pminsw m12, m15 ; u4
-        pmaxsw m9, m10
-        pmaxsw m11, m12
-
-        pmaxsw m1, m3  ; d without max(d,0)
-        pmaxsw m9, m11  ; u without max(u,0)
-        pxor m15, m15
-        pmaxsw m1, m15
-        pmaxsw m9, m15
-
-        mova m0, [rsp]
-        paddw m0, m1
-        psubw m0, m9
-        packuswb m0, m0
-
-        movh [dstq], m0
-        add srcq, mmsize/2
-        add dstq, mmsize/2
-        sub pixelsd, mmsize/2
-    jg .loop
-RET
-%endif
diff -uparN ffmpeg-4.1/libavfilter/x86/vf_ssim.asm ffmpeg-y/libavfilter/x86/vf_ssim.asm
--- ffmpeg-4.1/libavfilter/x86/vf_ssim.asm	2018-01-01 06:35:48.000000000 +0800
+++ ffmpeg-y/libavfilter/x86/vf_ssim.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,247 +0,0 @@
-;*****************************************************************************
-;* x86-optimized functions for ssim filter
-;*
-;* Copyright (C) 2015 Ronald S. Bultje <rsbultje@gmail.com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-
-pw_1: times 8 dw 1
-ssim_c1: times 4 dd 416 ;(.01*.01*255*255*64 + .5)
-ssim_c2: times 4 dd 235963 ;(.03*.03*255*255*64*63 + .5)
-
-SECTION .text
-
-%macro SSIM_4X4_LINE 1
-%if ARCH_X86_64
-cglobal ssim_4x4_line, 6, 8, %1, buf, buf_stride, ref, ref_stride, sums, w, buf_stride3, ref_stride3
-%else
-cglobal ssim_4x4_line, 5, 7, %1, buf, buf_stride, ref, ref_stride, sums, buf_stride3, ref_stride3
-%define wd r5mp
-%endif
-    lea     ref_stride3q, [ref_strideq*3]
-    lea     buf_stride3q, [buf_strideq*3]
-%if notcpuflag(xop)
-    pxor              m7, m7
-    mova             m15, [pw_1]
-%endif
-
-.loop:
-%if cpuflag(xop)
-    pmovzxbw          m0, [bufq+buf_strideq*0]
-    pmovzxbw          m1, [refq+ref_strideq*0]
-    pmaddwd           m4, m0, m0
-    pmaddwd           m6, m0, m1
-    pmovzxbw          m2, [bufq+buf_strideq*1]
-    vpmadcswd         m4, m1, m1, m4
-    pmovzxbw          m3, [refq+ref_strideq*1]
-    paddw             m0, m2
-    vpmadcswd         m4, m2, m2, m4
-    vpmadcswd         m6, m2, m3, m6
-    paddw             m1, m3
-    vpmadcswd         m4, m3, m3, m4
-
-    pmovzxbw          m2, [bufq+buf_strideq*2]
-    pmovzxbw          m3, [refq+ref_strideq*2]
-    vpmadcswd         m4, m2, m2, m4
-    vpmadcswd         m6, m2, m3, m6
-    pmovzxbw          m5, [bufq+buf_stride3q]
-    pmovzxbw          m7, [refq+ref_stride3q]
-    vpmadcswd         m4, m3, m3, m4
-    vpmadcswd         m6, m5, m7, m6
-    paddw             m0, m2
-    paddw             m1, m3
-    vpmadcswd         m4, m5, m5, m4
-    paddw             m0, m5
-    paddw             m1, m7
-    vpmadcswd         m4, m7, m7, m4
-%else
-    movh              m0, [bufq+buf_strideq*0]  ; a1
-    movh              m1, [refq+ref_strideq*0]  ; b1
-    movh              m2, [bufq+buf_strideq*1]  ; a2
-    movh              m3, [refq+ref_strideq*1]  ; b2
-    punpcklbw         m0, m7                    ; s1 [word]
-    punpcklbw         m1, m7                    ; s2 [word]
-    punpcklbw         m2, m7                    ; s1 [word]
-    punpcklbw         m3, m7                    ; s2 [word]
-    pmaddwd           m4, m0, m0                ; a1 * a1
-    pmaddwd           m5, m1, m1                ; b1 * b1
-    pmaddwd           m8, m2, m2                ; a2 * a2
-    pmaddwd           m9, m3, m3                ; b2 * b2
-    paddd             m4, m5                    ; ss
-    paddd             m8, m9                    ; ss
-    pmaddwd           m6, m0, m1                ; a1 * b1 = ss12
-    pmaddwd           m5, m2, m3                ; a2 * b2 = ss12
-    paddw             m0, m2
-    paddw             m1, m3
-    paddd             m6, m5                    ; s12
-    paddd             m4, m8                    ; ss
-
-    movh              m2, [bufq+buf_strideq*2]  ; a3
-    movh              m3, [refq+ref_strideq*2]  ; b3
-    movh              m5, [bufq+buf_stride3q]   ; a4
-    movh              m8, [refq+ref_stride3q]   ; b4
-    punpcklbw         m2, m7                    ; s1 [word]
-    punpcklbw         m3, m7                    ; s2 [word]
-    punpcklbw         m5, m7                    ; s1 [word]
-    punpcklbw         m8, m7                    ; s2 [word]
-    pmaddwd           m9, m2, m2                ; a3 * a3
-    pmaddwd          m10, m3, m3                ; b3 * b3
-    pmaddwd          m12, m5, m5                ; a4 * a4
-    pmaddwd          m13, m8, m8                ; b4 * b4
-    pmaddwd          m11, m2, m3                ; a3 * b3 = ss12
-    pmaddwd          m14, m5, m8                ; a4 * b4 = ss12
-    paddd             m9, m10
-    paddd            m12, m13
-    paddw             m0, m2
-    paddw             m1, m3
-    paddw             m0, m5
-    paddw             m1, m8
-    paddd             m6, m11
-    paddd             m4, m9
-    paddd             m6, m14
-    paddd             m4, m12
-%endif
-
-    ; m0 = [word] s1 a,a,a,a,b,b,b,b
-    ; m1 = [word] s2 a,a,a,a,b,b,b,b
-    ; m4 = [dword] ss a,a,b,b
-    ; m6 = [dword] s12 a,a,b,b
-
-%if cpuflag(xop)
-    vphaddwq          m0, m0                    ; [dword] s1  a, 0, b, 0
-    vphaddwq          m1, m1                    ; [dword] s2  a, 0, b, 0
-    vphadddq          m4, m4                    ; [dword] ss  a, 0, b, 0
-    vphadddq          m6, m6                    ; [dword] s12 a, 0, b, 0
-    punpckhdq     m2, m0, m1                    ; [dword] s1  b, s2 b, 0, 0
-    punpckldq         m0, m1                    ; [dword] s1  a, s2 a, 0, 0
-    punpckhdq     m3, m4, m6                    ; [dword] ss  b, s12 b, 0, 0
-    punpckldq         m4, m6                    ; [dword] ss  a, s12 a, 0, 0
-    punpcklqdq    m1, m2, m3                    ; [dword] b s1, s2, ss, s12
-    punpcklqdq        m0, m4                    ; [dword] a s1, s2, ss, s12
-%else
-    pmaddwd           m0, m15                   ; [dword] s1 a,a,b,b
-    pmaddwd           m1, m15                   ; [dword] s2 a,a,b,b
-    phaddd            m0, m4                    ; [dword] s1 a, b, ss a, b
-    phaddd            m1, m6                    ; [dword] s2 a, b, s12 a, b
-    punpckhdq     m2, m0, m1                    ; [dword] ss a, s12 a, ss b, s12 b
-    punpckldq         m0, m1                    ; [dword] s1 a, s2 a, s1 b, s2 b
-    punpckhqdq    m1, m0, m2                    ; [dword] b s1, s2, ss, s12
-    punpcklqdq        m0, m2                    ; [dword] a s1, s2, ss, s12
-%endif
-
-    mova  [sumsq+     0], m0
-    mova  [sumsq+mmsize], m1
-
-    add             bufq, mmsize/2
-    add             refq, mmsize/2
-    add            sumsq, mmsize*2
-    sub               wd, mmsize/8
-    jg .loop
-    RET
-%endmacro
-
-%if ARCH_X86_64
-INIT_XMM ssse3
-SSIM_4X4_LINE 16
-%endif
-%if HAVE_XOP_EXTERNAL
-INIT_XMM xop
-SSIM_4X4_LINE 8
-%endif
-
-INIT_XMM sse4
-cglobal ssim_end_line, 3, 3, 6, sum0, sum1, w
-    pxor              m0, m0
-.loop:
-    mova              m1, [sum0q+mmsize*0]
-    mova              m2, [sum0q+mmsize*1]
-    mova              m3, [sum0q+mmsize*2]
-    mova              m4, [sum0q+mmsize*3]
-    paddd             m1, [sum1q+mmsize*0]
-    paddd             m2, [sum1q+mmsize*1]
-    paddd             m3, [sum1q+mmsize*2]
-    paddd             m4, [sum1q+mmsize*3]
-    paddd             m1, m2
-    paddd             m2, m3
-    paddd             m3, m4
-    paddd             m4, [sum0q+mmsize*4]
-    paddd             m4, [sum1q+mmsize*4]
-    TRANSPOSE4x4D      1, 2, 3, 4, 5
-
-    ; m1 = fs1, m2 = fs2, m3 = fss, m4 = fs12
-    pslld             m3, 6
-    pslld             m4, 6
-    pmulld            m5, m1, m2                ; fs1 * fs2
-    pmulld            m1, m1                    ; fs1 * fs1
-    pmulld            m2, m2                    ; fs2 * fs2
-    psubd             m3, m1
-    psubd             m4, m5                    ; covariance
-    psubd             m3, m2                    ; variance
-
-    ; m1 = fs1 * fs1, m2 = fs2 * fs2, m3 = variance, m4 = covariance, m5 = fs1 * fs2
-    paddd             m4, m4                    ; 2 * covariance
-    paddd             m5, m5                    ; 2 * fs1 * fs2
-    paddd             m1, m2                    ; fs1 * fs1 + fs2 * fs2
-    paddd             m3, [ssim_c2]             ; variance + ssim_c2
-    paddd             m4, [ssim_c2]             ; 2 * covariance + ssim_c2
-    paddd             m5, [ssim_c1]             ; 2 * fs1 * fs2 + ssim_c1
-    paddd             m1, [ssim_c1]             ; fs1 * fs1 + fs2 * fs2 + ssim_c1
-
-    ; convert to float
-    cvtdq2ps          m3, m3
-    cvtdq2ps          m4, m4
-    cvtdq2ps          m5, m5
-    cvtdq2ps          m1, m1
-    mulps             m4, m5
-    mulps             m3, m1
-    divps             m4, m3                    ; ssim_endl
-    addps             m0, m4                    ; ssim
-    add            sum0q, mmsize*4
-    add            sum1q, mmsize*4
-    sub               wd, 4
-    jg .loop
-
-    ; subps the ones we added too much
-    test              wd, wd
-    jz .end
-    add               wd, 4
-    test              wd, 2
-    jz .skip2
-    psrldq            m4, 8
-.skip2:
-    test              wd, 1
-    jz .skip1
-    psrldq            m4, 4
-.skip1:
-    subps             m0, m4
-
-.end:
-    movhlps           m4, m0
-    addps             m0, m4
-    movss             m4, m0
-    shufps            m0, m0, 1
-    addss             m0, m4
-%if ARCH_X86_32
-    movss            r0m, m0
-    fld             r0mp
-%endif
-    RET
diff -uparN ffmpeg-4.1/libavfilter/x86/vf_stereo3d.asm ffmpeg-y/libavfilter/x86/vf_stereo3d.asm
--- ffmpeg-4.1/libavfilter/x86/vf_stereo3d.asm	2018-07-17 17:27:42.000000000 +0800
+++ ffmpeg-y/libavfilter/x86/vf_stereo3d.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,216 +0,0 @@
-;*****************************************************************************
-;* x86-optimized functions for stereo3d filter
-;*
-;* Copyright (C) 2015 Paul B Mahol
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;*****************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-
-; rgbrgbrgbrgb
-; rrrrggggbbbb
-
-shuf: db 0, 4, 8, 1,5, 9, 2, 6,10,3, 7,11,-1,-1,-1,-1
-ex_r: db 0,-1,-1,-1,3,-1,-1,-1,6,-1,-1,-1, 9,-1,-1,-1
-ex_g: db 1,-1,-1,-1,4,-1,-1,-1,7,-1,-1,-1,10,-1,-1,-1
-ex_b: db 2,-1,-1,-1,5,-1,-1,-1,8,-1,-1,-1,11,-1,-1,-1
-
-SECTION .text
-
-INIT_XMM sse4
-%if ARCH_X86_64
-cglobal anaglyph, 6, 10, 14, 2*6*mmsize, dst, lsrc, rsrc, dst_linesize, l_linesize, r_linesize, width, height, o, cnt
-%define ana_matrix_rq r6q
-%define ana_matrix_gq r7q
-%define ana_matrix_bq r8q
-
-%else ; ARCH_X86_32
-%if HAVE_ALIGNED_STACK
-cglobal anaglyph, 3, 7, 8, 2*9*mmsize, dst, lsrc, rsrc, dst_linesize, l_linesize, o, cnt
-%else
-cglobal anaglyph, 3, 6, 8, 2*9*mmsize, dst, lsrc, rsrc, dst_linesize, o, cnt
-%define l_linesizeq r4mp
-%endif ; HAVE_ALIGNED_STACK
-%define ana_matrix_rq r3q
-%define ana_matrix_gq r4q
-%define ana_matrix_bq r5q
-%define r_linesizeq r5mp
-%define widthd  r6mp
-%define heightd r7mp
-%define  m8 [rsp+mmsize*12]
-%define  m9 [rsp+mmsize*13]
-%define m10 [rsp+mmsize*14]
-%define m11 [rsp+mmsize*15]
-%define m12 [rsp+mmsize*16]
-%define m13 [rsp+mmsize*17]
-%endif ; ARCH
-
-    mov        ana_matrix_rq, r8m
-    mov        ana_matrix_gq, r9m
-    mov        ana_matrix_bq, r10m
-    movu                  m3, [ana_matrix_rq+ 0]
-    movq                  m5, [ana_matrix_rq+16]
-    pshufd                m0, m3, q0000
-    pshufd                m1, m3, q1111
-    pshufd                m2, m3, q2222
-    pshufd                m3, m3, q3333
-    pshufd                m4, m5, q0000
-    pshufd                m5, m5, q1111
-    mova      [rsp+mmsize*0], m0
-    mova      [rsp+mmsize*1], m1
-    mova      [rsp+mmsize*2], m2
-    mova      [rsp+mmsize*3], m3
-    mova      [rsp+mmsize*4], m4
-    mova      [rsp+mmsize*5], m5
-
-    movu                  m3, [ana_matrix_gq+ 0]
-    movq                  m5, [ana_matrix_gq+16]
-    pshufd                m0, m3, q0000
-    pshufd                m1, m3, q1111
-    pshufd                m2, m3, q2222
-    pshufd                m3, m3, q3333
-    pshufd                m4, m5, q0000
-    pshufd                m5, m5, q1111
-    mova     [rsp+mmsize*6 ], m0
-    mova     [rsp+mmsize*7 ], m1
-    mova     [rsp+mmsize*8 ], m2
-    mova     [rsp+mmsize*9 ], m3
-    mova     [rsp+mmsize*10], m4
-    mova     [rsp+mmsize*11], m5
-
-%if ARCH_X86_64
-    movu                 m11, [ana_matrix_bq+ 0]
-    movq                 m13, [ana_matrix_bq+16]
-    pshufd                m8, m11, q0000
-    pshufd                m9, m11, q1111
-    pshufd               m10, m11, q2222
-    pshufd               m11, m11, q3333
-    pshufd               m12, m13, q0000
-    pshufd               m13, m13, q1111
-    mov               widthd, dword widthm
-    mov              heightd, dword heightm
-%else
-    movu                  m3, [ana_matrix_bq+ 0]
-    movq                  m5, [ana_matrix_bq+16]
-    pshufd                m0, m3, q0000
-    pshufd                m1, m3, q1111
-    pshufd                m2, m3, q2222
-    pshufd                m3, m3, q3333
-    pshufd                m4, m5, q0000
-    pshufd                m5, m5, q1111
-    mova     [rsp+mmsize*12], m0
-    mova     [rsp+mmsize*13], m1
-    mova     [rsp+mmsize*14], m2
-    mova     [rsp+mmsize*15], m3
-    mova     [rsp+mmsize*16], m4
-    mova     [rsp+mmsize*17], m5
-    mov        dst_linesizeq, r3m
-%if HAVE_ALIGNED_STACK
-    mov          l_linesizeq, r4m
-%endif
-%endif ; ARCH
-
-.nextrow:
-    mov                   od, widthd
-    xor                 cntd, cntd
-
-    .loop:
-        movu                 m3, [lsrcq+cntq]
-        pshufb               m1, m3, [ex_r]
-        pshufb               m2, m3, [ex_g]
-        pshufb               m3, [ex_b]
-        movu                 m0, [rsrcq+cntq]
-        pshufb               m4, m0, [ex_r]
-        pshufb               m5, m0, [ex_g]
-        pshufb               m0, [ex_b]
-        pmulld               m1, [rsp+mmsize*0]
-        pmulld               m2, [rsp+mmsize*1]
-        pmulld               m3, [rsp+mmsize*2]
-        pmulld               m4, [rsp+mmsize*3]
-        pmulld               m5, [rsp+mmsize*4]
-        pmulld               m0, [rsp+mmsize*5]
-        paddd                m1, m2
-        paddd                m3, m4
-        paddd                m5, m0
-        paddd                m1, m3
-        paddd                m1, m5
-
-        movu                 m3, [lsrcq+cntq]
-        pshufb               m7, m3, [ex_r]
-        pshufb               m2, m3, [ex_g]
-        pshufb               m3, [ex_b]
-        movu                 m0, [rsrcq+cntq]
-        pshufb               m4, m0, [ex_r]
-        pshufb               m5, m0, [ex_g]
-        pshufb               m0, [ex_b]
-        pmulld               m7, [rsp+mmsize*6]
-        pmulld               m2, [rsp+mmsize*7]
-        pmulld               m3, [rsp+mmsize*8]
-        pmulld               m4, [rsp+mmsize*9]
-        pmulld               m5, [rsp+mmsize*10]
-        pmulld               m0, [rsp+mmsize*11]
-        paddd                m7, m2
-        paddd                m3, m4
-        paddd                m5, m0
-        paddd                m7, m3
-        paddd                m7, m5
-
-        movu                 m4, [lsrcq+cntq]
-        pshufb               m2, m4, [ex_r]
-        pshufb               m3, m4, [ex_g]
-        pshufb               m4, [ex_b]
-        movu                 m0, [rsrcq+cntq]
-        pshufb               m5, m0, [ex_r]
-        pshufb               m6, m0, [ex_g]
-        pshufb               m0, [ex_b]
-        pmulld               m2, m8
-        pmulld               m3, m9
-        pmulld               m4, m10
-        pmulld               m5, m11
-        pmulld               m6, m12
-        pmulld               m0, m13
-        paddd                m2, m3
-        paddd                m4, m5
-        paddd                m6, m0
-        paddd                m2, m4
-        paddd                m2, m6
-
-        psrld                m1, 16
-        psrld                m7, 16
-        psrld                m2, 16
-
-        packusdw             m1, m7
-        packusdw             m2, m2
-        packuswb             m1, m2
-        pshufb               m1, [shuf]
-
-        movq      [dstq+cntq+0], m1
-        psrldq               m1, 8
-        movd      [dstq+cntq+8], m1
-        add                cntd, 12
-        sub                  od, 4
-    jg .loop
-
-    add          dstq, dst_linesizeq
-    add         lsrcq, l_linesizeq
-    add         rsrcq, r_linesizeq
-    sub       heightd, 1
-    jg .nextrow
-REP_RET
diff -uparN ffmpeg-4.1/libavfilter/x86/vf_threshold.asm ffmpeg-y/libavfilter/x86/vf_threshold.asm
--- ffmpeg-4.1/libavfilter/x86/vf_threshold.asm	2018-11-03 08:17:29.000000000 +0800
+++ ffmpeg-y/libavfilter/x86/vf_threshold.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,92 +0,0 @@
-;*****************************************************************************
-;* x86-optimized functions for threshold filter
-;*
-;* Copyright (C) 2017 Paul B Mahol
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;*****************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-
-pb_128: times 16 db 128
-pb_128_0 : times 8 db 0, 128
-
-SECTION .text
-
-;%1 depth (8 or 16) ; %2 b or w ; %3 constant
-%macro THRESHOLD 3
-%if ARCH_X86_64
-cglobal threshold%1, 10, 13, 5, in, threshold, min, max, out, ilinesize, tlinesize, flinesize, slinesize, olinesize, w, h, x
-    mov             wd, dword wm
-    mov             hd, dword hm
-%else
-cglobal threshold%1, 5, 7, 5, in, threshold, min, max, out, w, x
-    mov             wd, r10m
-%define     ilinesizeq  r5mp
-%define     tlinesizeq  r6mp
-%define     flinesizeq  r7mp
-%define     slinesizeq  r8mp
-%define     olinesizeq  r9mp
-%define             hd  r11mp
-%endif
-    VBROADCASTI128  m4, [%3]
-%if %1 == 16
-    add             wq, wq ; w *= 2 (16 bits instead of 8)
-%endif
-    add            inq, wq
-    add     thresholdq, wq
-    add           minq, wq
-    add           maxq, wq
-    add           outq, wq
-    neg             wq
-.nextrow:
-    mov         xq, wq
-
-    .loop:
-        movu            m1, [inq + xq]
-        movu            m0, [thresholdq + xq]
-        movu            m2, [minq + xq]
-        movu            m3, [maxq + xq]
-        pxor            m0, m4
-        pxor            m1, m4
-        pcmpgt%2        m0, m1
-        PBLENDVB        m3, m2, m0
-        movu   [outq + xq], m3
-        add             xq, mmsize
-    jl .loop
-
-    add          inq, ilinesizeq
-    add   thresholdq, tlinesizeq
-    add         minq, flinesizeq
-    add         maxq, slinesizeq
-    add         outq, olinesizeq
-    sub         hd, 1
-    jg .nextrow
-RET
-%endmacro
-
-INIT_XMM sse4
-THRESHOLD 8, b, pb_128
-THRESHOLD 16, w, pb_128_0
-
-%if HAVE_AVX2_EXTERNAL
-INIT_YMM avx2
-THRESHOLD 8, b, pb_128
-THRESHOLD 16, w, pb_128_0
-%endif
diff -uparN ffmpeg-4.1/libavfilter/x86/vf_w3fdif.asm ffmpeg-y/libavfilter/x86/vf_w3fdif.asm
--- ffmpeg-4.1/libavfilter/x86/vf_w3fdif.asm	2018-07-17 17:27:42.000000000 +0800
+++ ffmpeg-y/libavfilter/x86/vf_w3fdif.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,259 +0,0 @@
-;*****************************************************************************
-;* x86-optimized functions for w3fdif filter
-;*
-;* Copyright (c) 2015 Paul B Mahol
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION .text
-
-INIT_XMM sse2
-cglobal w3fdif_scale, 3, 3, 2, 0, out_pixel, work_pixel, linesize
-.loop:
-    mova                         m0, [work_pixelq]
-    mova                         m1, [work_pixelq+mmsize]
-    psrad                        m0, 15
-    psrad                        m1, 15
-    packssdw                     m0, m1
-    packuswb                     m0, m0
-    movh               [out_pixelq], m0
-    add                  out_pixelq, mmsize/2
-    add                 work_pixelq, mmsize*2
-    sub                   linesized, mmsize/2
-    jg .loop
-REP_RET
-
-cglobal w3fdif_simple_low, 4, 5, 6, 0, work_line, in_lines_cur0, coef, linesize, offset
-    movd                  m1, [coefq]
-    DEFINE_ARGS    work_line, in_lines_cur0, in_lines_cur1, linesize, offset
-    SPLATW                m0, m1, 0
-    SPLATW                m1, m1, 1
-    pxor                  m4, m4
-    mov              offsetq, 0
-    mov       in_lines_cur1q, [in_lines_cur0q + gprsize]
-    mov       in_lines_cur0q, [in_lines_cur0q]
-
-.loop:
-    movh                                   m2, [in_lines_cur0q+offsetq]
-    movh                                   m3, [in_lines_cur1q+offsetq]
-    punpcklbw                              m2, m4
-    punpcklbw                              m3, m4
-    SBUTTERFLY                             wd, 2, 3, 5
-    pmaddwd                                m2, m0
-    pmaddwd                                m3, m1
-    mova               [work_lineq+offsetq*4], m2
-    mova        [work_lineq+offsetq*4+mmsize], m3
-    add                               offsetq, mmsize/2
-    sub                             linesized, mmsize/2
-    jg .loop
-REP_RET
-
-cglobal w3fdif_complex_low, 4, 7, 8, 0, work_line, in_lines_cur0, coef, linesize
-    movq                  m0, [coefq]
-    DEFINE_ARGS    work_line, in_lines_cur0, in_lines_cur1, linesize, offset, in_lines_cur2, in_lines_cur3
-    pshufd                m2, m0, q1111
-    SPLATD                m0
-    pxor                  m1, m1
-    mov              offsetq, 0
-    mov       in_lines_cur3q, [in_lines_cur0q+gprsize*3]
-    mov       in_lines_cur2q, [in_lines_cur0q+gprsize*2]
-    mov       in_lines_cur1q, [in_lines_cur0q+gprsize]
-    mov       in_lines_cur0q, [in_lines_cur0q]
-
-.loop:
-    movh                                   m4, [in_lines_cur0q+offsetq]
-    movh                                   m5, [in_lines_cur1q+offsetq]
-    punpcklbw                              m4, m1
-    punpcklbw                              m5, m1
-    SBUTTERFLY                             wd, 4, 5, 7
-    pmaddwd                                m4, m0
-    pmaddwd                                m5, m0
-    movh                                   m6, [in_lines_cur2q+offsetq]
-    movh                                   m3, [in_lines_cur3q+offsetq]
-    punpcklbw                              m6, m1
-    punpcklbw                              m3, m1
-    SBUTTERFLY                             wd, 6, 3, 7
-    pmaddwd                                m6, m2
-    pmaddwd                                m3, m2
-    paddd                                  m4, m6
-    paddd                                  m5, m3
-    mova               [work_lineq+offsetq*4], m4
-    mova        [work_lineq+offsetq*4+mmsize], m5
-    add                               offsetq, mmsize/2
-    sub                             linesized, mmsize/2
-    jg .loop
-REP_RET
-
-%if ARCH_X86_64
-cglobal w3fdif_simple_high, 5, 9, 8, 0, work_line, in_lines_cur0, in_lines_adj0, coef, linesize
-%else
-cglobal w3fdif_simple_high, 4, 7, 8, 0, work_line, in_lines_cur0, in_lines_adj0, coef, linesize
-%endif
-    movq                  m2, [coefq]
-%if ARCH_X86_64
-    DEFINE_ARGS    work_line, in_lines_cur0, in_lines_adj0, in_lines_cur1, linesize, offset, in_lines_cur2, in_lines_adj1, in_lines_adj2
-    xor              offsetq, offsetq
-%else
-    DEFINE_ARGS    work_line, in_lines_cur0, in_lines_adj0, in_lines_cur1, in_lines_cur2, in_lines_adj1, in_lines_adj2
-    %define linesized r4mp
-%endif
-
-    pshufd                m0, m2, q0000
-    SPLATW                m2, m2, 2
-    pxor                  m7, m7
-    mov       in_lines_cur2q, [in_lines_cur0q+gprsize*2]
-    mov       in_lines_cur1q, [in_lines_cur0q+gprsize]
-    mov       in_lines_cur0q, [in_lines_cur0q]
-    mov       in_lines_adj2q, [in_lines_adj0q+gprsize*2]
-    mov       in_lines_adj1q, [in_lines_adj0q+gprsize]
-    mov       in_lines_adj0q, [in_lines_adj0q]
-
-%if ARCH_X86_32
-    sub in_lines_cur1q, in_lines_cur0q
-    sub in_lines_cur2q, in_lines_cur0q
-    sub in_lines_adj0q, in_lines_cur0q
-    sub in_lines_adj1q, in_lines_cur0q
-    sub in_lines_adj2q, in_lines_cur0q
-    %define offsetq in_lines_cur0q
-%endif
-
-.loop:
-%if ARCH_X86_64
-    movh                                   m3, [in_lines_cur0q+offsetq]
-%else
-    movh                                   m3, [in_lines_cur0q]
-%endif
-    movh                                   m4, [in_lines_cur1q+offsetq]
-    punpcklbw                              m3, m7
-    punpcklbw                              m4, m7
-    SBUTTERFLY                             wd, 3, 4, 1
-    pmaddwd                                m3, m0
-    pmaddwd                                m4, m0
-    movh                                   m5, [in_lines_adj0q+offsetq]
-    movh                                   m6, [in_lines_adj1q+offsetq]
-    punpcklbw                              m5, m7
-    punpcklbw                              m6, m7
-    SBUTTERFLY                             wd, 5, 6, 1
-    pmaddwd                                m5, m0
-    pmaddwd                                m6, m0
-    paddd                                  m3, m5
-    paddd                                  m4, m6
-    movh                                   m5, [in_lines_cur2q+offsetq]
-    movh                                   m6, [in_lines_adj2q+offsetq]
-    punpcklbw                              m5, m7
-    punpcklbw                              m6, m7
-    SBUTTERFLY                             wd, 5, 6, 1
-    pmaddwd                                m5, m2
-    pmaddwd                                m6, m2
-    paddd                                  m3, m5
-    paddd                                  m4, m6
-%if ARCH_X86_64
-    paddd                                  m3, [work_lineq+offsetq*4]
-    paddd                                  m4, [work_lineq+offsetq*4+mmsize]
-    mova               [work_lineq+offsetq*4], m3
-    mova        [work_lineq+offsetq*4+mmsize], m4
-%else
-    paddd                                  m3, [work_lineq]
-    paddd                                  m4, [work_lineq+mmsize]
-    mova                         [work_lineq], m3
-    mova                  [work_lineq+mmsize], m4
-    add                            work_lineq, mmsize*2
-%endif
-    add                               offsetq, mmsize/2
-    sub                             linesized, mmsize/2
-    jg .loop
-REP_RET
-
-%if ARCH_X86_64
-
-cglobal w3fdif_complex_high, 5, 13, 10, 0, work_line, in_lines_cur0, in_lines_adj0, coef, linesize
-    movq                  m0, [coefq+0]
-    movd                  m4, [coefq+8]
-    DEFINE_ARGS    work_line, in_lines_cur0, in_lines_adj0, in_lines_cur1, linesize, offset, in_lines_cur2, in_lines_cur3, in_lines_cur4, in_lines_adj1, in_lines_adj2, in_lines_adj3, in_lines_adj4
-    pshufd                m1, m0, q1111
-    SPLATD                m0
-    SPLATW                m4, m4
-    pxor                  m3, m3
-    mov              offsetq, 0
-    mov       in_lines_cur4q, [in_lines_cur0q+gprsize*4]
-    mov       in_lines_cur3q, [in_lines_cur0q+gprsize*3]
-    mov       in_lines_cur2q, [in_lines_cur0q+gprsize*2]
-    mov       in_lines_cur1q, [in_lines_cur0q+gprsize]
-    mov       in_lines_cur0q, [in_lines_cur0q]
-    mov       in_lines_adj4q, [in_lines_adj0q+gprsize*4]
-    mov       in_lines_adj3q, [in_lines_adj0q+gprsize*3]
-    mov       in_lines_adj2q, [in_lines_adj0q+gprsize*2]
-    mov       in_lines_adj1q, [in_lines_adj0q+gprsize]
-    mov       in_lines_adj0q, [in_lines_adj0q]
-
-.loop:
-    movh                                   m5, [in_lines_cur0q+offsetq]
-    movh                                   m6, [in_lines_cur1q+offsetq]
-    punpcklbw                              m5, m3
-    punpcklbw                              m6, m3
-    SBUTTERFLY                             wd, 5, 6, 2
-    pmaddwd                                m5, m0
-    pmaddwd                                m6, m0
-    movh                                   m8, [in_lines_cur2q+offsetq]
-    movh                                   m9, [in_lines_cur3q+offsetq]
-    punpcklbw                              m8, m3
-    punpcklbw                              m9, m3
-    SBUTTERFLY                             wd, 8, 9, 2
-    pmaddwd                                m8, m1
-    pmaddwd                                m9, m1
-    paddd                                  m5, m8
-    paddd                                  m6, m9
-    movh                                   m8, [in_lines_adj0q+offsetq]
-    movh                                   m9, [in_lines_adj1q+offsetq]
-    punpcklbw                              m8, m3
-    punpcklbw                              m9, m3
-    SBUTTERFLY                             wd, 8, 9, 2
-    pmaddwd                                m8, m0
-    pmaddwd                                m9, m0
-    paddd                                  m5, m8
-    paddd                                  m6, m9
-    movh                                   m8, [in_lines_adj2q+offsetq]
-    movh                                   m9, [in_lines_adj3q+offsetq]
-    punpcklbw                              m8, m3
-    punpcklbw                              m9, m3
-    SBUTTERFLY                             wd, 8, 9, 2
-    pmaddwd                                m8, m1
-    pmaddwd                                m9, m1
-    paddd                                  m5, m8
-    paddd                                  m6, m9
-    movh                                   m8, [in_lines_cur4q+offsetq]
-    movh                                   m9, [in_lines_adj4q+offsetq]
-    punpcklbw                              m8, m3
-    punpcklbw                              m9, m3
-    SBUTTERFLY                             wd, 8, 9, 2
-    pmaddwd                                m8, m4
-    pmaddwd                                m9, m4
-    paddd                                  m5, m8
-    paddd                                  m6, m9
-    paddd                                  m5, [work_lineq+offsetq*4]
-    paddd                                  m6, [work_lineq+offsetq*4+mmsize]
-    mova               [work_lineq+offsetq*4], m5
-    mova        [work_lineq+offsetq*4+mmsize], m6
-    add                               offsetq, mmsize/2
-    sub                             linesized, mmsize/2
-    jg .loop
-REP_RET
-
-%endif
diff -uparN ffmpeg-4.1/libavfilter/x86/vf_yadif.asm ffmpeg-y/libavfilter/x86/vf_yadif.asm
--- ffmpeg-4.1/libavfilter/x86/vf_yadif.asm	2016-03-29 10:25:27.000000000 +0800
+++ ffmpeg-y/libavfilter/x86/vf_yadif.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,243 +0,0 @@
-;*****************************************************************************
-;* x86-optimized functions for yadif filter
-;*
-;* Copyright (C) 2006 Michael Niedermayer <michaelni@gmx.at>
-;* Copyright (c) 2013 Daniel Kang <daniel.d.kang@gmail.com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-
-pb_1: times 16 db 1
-pw_1: times  8 dw 1
-
-SECTION .text
-
-%macro CHECK 2
-    movu      m2, [curq+t1+%1]
-    movu      m3, [curq+t0+%2]
-    mova      m4, m2
-    mova      m5, m2
-    pxor      m4, m3
-    pavgb     m5, m3
-    pand      m4, [pb_1]
-    psubusb   m5, m4
-    RSHIFT    m5, 1
-    punpcklbw m5, m7
-    mova      m4, m2
-    psubusb   m2, m3
-    psubusb   m3, m4
-    pmaxub    m2, m3
-    mova      m3, m2
-    mova      m4, m2
-    RSHIFT    m3, 1
-    RSHIFT    m4, 2
-    punpcklbw m2, m7
-    punpcklbw m3, m7
-    punpcklbw m4, m7
-    paddw     m2, m3
-    paddw     m2, m4
-%endmacro
-
-%macro CHECK1 0
-    mova    m3, m0
-    pcmpgtw m3, m2
-    pminsw  m0, m2
-    mova    m6, m3
-    pand    m5, m3
-    pandn   m3, m1
-    por     m3, m5
-    mova    m1, m3
-%endmacro
-
-%macro CHECK2 0
-    paddw   m6, [pw_1]
-    psllw   m6, 14
-    paddsw  m2, m6
-    mova    m3, m0
-    pcmpgtw m3, m2
-    pminsw  m0, m2
-    pand    m5, m3
-    pandn   m3, m1
-    por     m3, m5
-    mova    m1, m3
-%endmacro
-
-%macro LOAD 2
-    movh      %1, %2
-    punpcklbw %1, m7
-%endmacro
-
-%macro FILTER 3
-.loop%1:
-    pxor         m7, m7
-    LOAD         m0, [curq+t1]
-    LOAD         m1, [curq+t0]
-    LOAD         m2, [%2]
-    LOAD         m3, [%3]
-    mova         m4, m3
-    paddw        m3, m2
-    psraw        m3, 1
-    mova   [rsp+ 0], m0
-    mova   [rsp+16], m3
-    mova   [rsp+32], m1
-    psubw        m2, m4
-    ABS1         m2, m4
-    LOAD         m3, [prevq+t1]
-    LOAD         m4, [prevq+t0]
-    psubw        m3, m0
-    psubw        m4, m1
-    ABS1         m3, m5
-    ABS1         m4, m5
-    paddw        m3, m4
-    psrlw        m2, 1
-    psrlw        m3, 1
-    pmaxsw       m2, m3
-    LOAD         m3, [nextq+t1]
-    LOAD         m4, [nextq+t0]
-    psubw        m3, m0
-    psubw        m4, m1
-    ABS1         m3, m5
-    ABS1         m4, m5
-    paddw        m3, m4
-    psrlw        m3, 1
-    pmaxsw       m2, m3
-    mova   [rsp+48], m2
-
-    paddw        m1, m0
-    paddw        m0, m0
-    psubw        m0, m1
-    psrlw        m1, 1
-    ABS1         m0, m2
-
-    movu         m2, [curq+t1-1]
-    movu         m3, [curq+t0-1]
-    mova         m4, m2
-    psubusb      m2, m3
-    psubusb      m3, m4
-    pmaxub       m2, m3
-%if mmsize == 16
-    mova         m3, m2
-    psrldq       m3, 2
-%else
-    pshufw       m3, m2, q0021
-%endif
-    punpcklbw    m2, m7
-    punpcklbw    m3, m7
-    paddw        m0, m2
-    paddw        m0, m3
-    psubw        m0, [pw_1]
-
-    CHECK -2, 0
-    CHECK1
-    CHECK -3, 1
-    CHECK2
-    CHECK 0, -2
-    CHECK1
-    CHECK 1, -3
-    CHECK2
-
-    mova         m6, [rsp+48]
-    cmp   DWORD r8m, 2
-    jge .end%1
-    LOAD         m2, [%2+t1*2]
-    LOAD         m4, [%3+t1*2]
-    LOAD         m3, [%2+t0*2]
-    LOAD         m5, [%3+t0*2]
-    paddw        m2, m4
-    paddw        m3, m5
-    psrlw        m2, 1
-    psrlw        m3, 1
-    mova         m4, [rsp+ 0]
-    mova         m5, [rsp+16]
-    mova         m7, [rsp+32]
-    psubw        m2, m4
-    psubw        m3, m7
-    mova         m0, m5
-    psubw        m5, m4
-    psubw        m0, m7
-    mova         m4, m2
-    pminsw       m2, m3
-    pmaxsw       m3, m4
-    pmaxsw       m2, m5
-    pminsw       m3, m5
-    pmaxsw       m2, m0
-    pminsw       m3, m0
-    pxor         m4, m4
-    pmaxsw       m6, m3
-    psubw        m4, m2
-    pmaxsw       m6, m4
-
-.end%1:
-    mova         m2, [rsp+16]
-    mova         m3, m2
-    psubw        m2, m6
-    paddw        m3, m6
-    pmaxsw       m1, m2
-    pminsw       m1, m3
-    packuswb     m1, m1
-
-    movh     [dstq], m1
-    add        dstq, mmsize/2
-    add       prevq, mmsize/2
-    add        curq, mmsize/2
-    add       nextq, mmsize/2
-    sub   DWORD r4m, mmsize/2
-    jg .loop%1
-%endmacro
-
-%macro YADIF 0
-%if ARCH_X86_32
-cglobal yadif_filter_line, 4, 6, 8, 80, dst, prev, cur, next, w, prefs, \
-                                        mrefs, parity, mode
-%else
-cglobal yadif_filter_line, 4, 7, 8, 80, dst, prev, cur, next, w, prefs, \
-                                        mrefs, parity, mode
-%endif
-%if ARCH_X86_32
-    mov            r4, r5mp
-    mov            r5, r6mp
-    DECLARE_REG_TMP 4,5
-%else
-    movsxd         r5, DWORD r5m
-    movsxd         r6, DWORD r6m
-    DECLARE_REG_TMP 5,6
-%endif
-
-    cmp DWORD paritym, 0
-    je .parity0
-    FILTER 1, prevq, curq
-    jmp .ret
-
-.parity0:
-    FILTER 0, curq, nextq
-
-.ret:
-    RET
-%endmacro
-
-INIT_XMM ssse3
-YADIF
-INIT_XMM sse2
-YADIF
-%if ARCH_X86_32
-INIT_MMX mmxext
-YADIF
-%endif
diff -uparN ffmpeg-4.1/libavfilter/x86/yadif-10.asm ffmpeg-y/libavfilter/x86/yadif-10.asm
--- ffmpeg-4.1/libavfilter/x86/yadif-10.asm	2016-03-29 10:25:27.000000000 +0800
+++ ffmpeg-y/libavfilter/x86/yadif-10.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,255 +0,0 @@
-;*****************************************************************************
-;* x86-optimized functions for yadif filter
-;*
-;* Copyright (C) 2006 Michael Niedermayer <michaelni@gmx.at>
-;* Copyright (c) 2013 Daniel Kang <daniel.d.kang@gmail.com>
-;* Copyright (c) 2011-2013 James Darnley <james.darnley@gmail.com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-
-pw_1: times 8 dw 1
-
-SECTION .text
-
-%macro PMAXUW 2
-%if cpuflag(sse4)
-    pmaxuw %1, %2
-%else
-    psubusw %1, %2
-    paddusw %1, %2
-%endif
-%endmacro
-
-%macro CHECK 2
-    movu      m2, [curq+t1+%1*2]
-    movu      m3, [curq+t0+%2*2]
-    mova      m4, m2
-    mova      m5, m2
-    pxor      m4, m3
-    pavgw     m5, m3
-    pand      m4, [pw_1]
-    psubusw   m5, m4
-    RSHIFT    m5, 2
-    mova      m4, m2
-    psubusw   m2, m3
-    psubusw   m3, m4
-    PMAXUW    m2, m3
-    mova      m3, m2
-    mova      m4, m2
-    RSHIFT    m3, 2
-    RSHIFT    m4, 4
-    paddw     m2, m3
-    paddw     m2, m4
-%endmacro
-
-%macro CHECK1 0
-    mova    m3, m0
-    pcmpgtw m3, m2
-    pminsw  m0, m2
-    mova    m6, m3
-    pand    m5, m3
-    pandn   m3, m1
-    por     m3, m5
-    mova    m1, m3
-%endmacro
-
-; %macro CHECK2 0
-;     paddw   m6, [pw_1]
-;     psllw   m6, 14
-;     paddsw  m2, m6
-;     mova    m3, m0
-;     pcmpgtw m3, m2
-;     pminsw  m0, m2
-;     pand    m5, m3
-;     pandn   m3, m1
-;     por     m3, m5
-;     mova    m1, m3
-; %endmacro
-
-; This version of CHECK2 is required for 14-bit samples.  The left-shift trick
-; in the old code is not large enough to correctly select pixels or scores.
-
-%macro CHECK2 0
-    mova    m3, m0
-    pcmpgtw m0, m2
-    pand    m0, m6
-    mova    m6, m0
-    pand    m5, m6
-    pand    m2, m0
-    pandn   m6, m1
-    pandn   m0, m3
-    por     m6, m5
-    por     m0, m2
-    mova    m1, m6
-%endmacro
-
-%macro LOAD 2
-    movu      %1, %2
-%endmacro
-
-%macro FILTER 3
-.loop%1:
-    pxor         m7, m7
-    LOAD         m0, [curq+t1]
-    LOAD         m1, [curq+t0]
-    LOAD         m2, [%2]
-    LOAD         m3, [%3]
-    mova         m4, m3
-    paddw        m3, m2
-    psraw        m3, 1
-    mova   [rsp+ 0], m0
-    mova   [rsp+16], m3
-    mova   [rsp+32], m1
-    psubw        m2, m4
-    ABS1         m2, m4
-    LOAD         m3, [prevq+t1]
-    LOAD         m4, [prevq+t0]
-    psubw        m3, m0
-    psubw        m4, m1
-    ABS2         m3, m4, m5, m6
-    paddw        m3, m4
-    psrlw        m2, 1
-    psrlw        m3, 1
-    pmaxsw       m2, m3
-    LOAD         m3, [nextq+t1]
-    LOAD         m4, [nextq+t0]
-    psubw        m3, m0
-    psubw        m4, m1
-    ABS2         m3, m4, m5, m6
-    paddw        m3, m4
-    psrlw        m3, 1
-    pmaxsw       m2, m3
-    mova   [rsp+48], m2
-
-    paddw        m1, m0
-    paddw        m0, m0
-    psubw        m0, m1
-    psrlw        m1, 1
-    ABS1         m0, m2
-
-    movu         m2, [curq+t1-1*2]
-    movu         m3, [curq+t0-1*2]
-    mova         m4, m2
-    psubusw      m2, m3
-    psubusw      m3, m4
-    PMAXUW       m2, m3
-    mova         m3, m2
-    RSHIFT       m3, 4
-    paddw        m0, m2
-    paddw        m0, m3
-    psubw        m0, [pw_1]
-
-    CHECK -2, 0
-    CHECK1
-    CHECK -3, 1
-    CHECK2
-    CHECK 0, -2
-    CHECK1
-    CHECK 1, -3
-    CHECK2
-
-    mova         m6, [rsp+48]
-    cmp   DWORD r8m, 2
-    jge .end%1
-    LOAD         m2, [%2+t1*2]
-    LOAD         m4, [%3+t1*2]
-    LOAD         m3, [%2+t0*2]
-    LOAD         m5, [%3+t0*2]
-    paddw        m2, m4
-    paddw        m3, m5
-    psrlw        m2, 1
-    psrlw        m3, 1
-    mova         m4, [rsp+ 0]
-    mova         m5, [rsp+16]
-    mova         m7, [rsp+32]
-    psubw        m2, m4
-    psubw        m3, m7
-    mova         m0, m5
-    psubw        m5, m4
-    psubw        m0, m7
-    mova         m4, m2
-    pminsw       m2, m3
-    pmaxsw       m3, m4
-    pmaxsw       m2, m5
-    pminsw       m3, m5
-    pmaxsw       m2, m0
-    pminsw       m3, m0
-    pxor         m4, m4
-    pmaxsw       m6, m3
-    psubw        m4, m2
-    pmaxsw       m6, m4
-
-.end%1:
-    mova         m2, [rsp+16]
-    mova         m3, m2
-    psubw        m2, m6
-    paddw        m3, m6
-    pmaxsw       m1, m2
-    pminsw       m1, m3
-
-    movu     [dstq], m1
-    add        dstq, mmsize-4
-    add       prevq, mmsize-4
-    add        curq, mmsize-4
-    add       nextq, mmsize-4
-    sub   DWORD r4m, mmsize/2-2
-    jg .loop%1
-%endmacro
-
-%macro YADIF 0
-%if ARCH_X86_32
-cglobal yadif_filter_line_10bit, 4, 6, 8, 80, dst, prev, cur, next, w, \
-                                              prefs, mrefs, parity, mode
-%else
-cglobal yadif_filter_line_10bit, 4, 7, 8, 80, dst, prev, cur, next, w, \
-                                              prefs, mrefs, parity, mode
-%endif
-%if ARCH_X86_32
-    mov            r4, r5mp
-    mov            r5, r6mp
-    DECLARE_REG_TMP 4,5
-%else
-    movsxd         r5, DWORD r5m
-    movsxd         r6, DWORD r6m
-    DECLARE_REG_TMP 5,6
-%endif
-
-    cmp DWORD paritym, 0
-    je .parity0
-    FILTER 1, prevq, curq
-    jmp .ret
-
-.parity0:
-    FILTER 0, curq, nextq
-
-.ret:
-    RET
-%endmacro
-
-INIT_XMM ssse3
-YADIF
-INIT_XMM sse2
-YADIF
-%if ARCH_X86_32
-INIT_MMX mmxext
-YADIF
-%endif
diff -uparN ffmpeg-4.1/libavfilter/x86/yadif-16.asm ffmpeg-y/libavfilter/x86/yadif-16.asm
--- ffmpeg-4.1/libavfilter/x86/yadif-16.asm	2018-11-03 08:17:29.000000000 +0800
+++ ffmpeg-y/libavfilter/x86/yadif-16.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,293 +0,0 @@
-;*****************************************************************************
-;* x86-optimized functions for yadif filter
-;*
-;* Copyright (C) 2006 Michael Niedermayer <michaelni@gmx.at>
-;* Copyright (c) 2013 Daniel Kang <daniel.d.kang@gmail.com>
-;* Copyright (c) 2011-2013 James Darnley <james.darnley@gmail.com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-
-pw_1:    times 8 dw 1
-pw_8000: times 8 dw 0x8000
-pd_1:    times 4 dd 1
-pd_8000: times 4 dd 0x8000
-
-SECTION .text
-
-%macro PABS 2
-%if cpuflag(ssse3)
-    pabsd %1, %1
-%else
-    pxor    %2, %2
-    pcmpgtd %2, %1
-    pxor    %1, %2
-    psubd   %1, %2
-%endif
-%endmacro
-
-%macro PACK 1
-%if cpuflag(sse4)
-    packusdw %1, %1
-%else
-    psubd    %1, [pd_8000]
-    packssdw %1, %1
-    paddw    %1, [pw_8000]
-%endif
-%endmacro
-
-%macro PMAXUW 2
-%if cpuflag(sse4)
-    pmaxuw %1, %2
-%else
-    psubusw %1, %2
-    paddusw %1, %2
-%endif
-%endmacro
-
-%macro CHECK 2
-    movu      m2, [curq+t1+%1*2]
-    movu      m3, [curq+t0+%2*2]
-    mova      m4, m2
-    mova      m5, m2
-    pxor      m4, m3
-    pavgw     m5, m3
-    pand      m4, [pw_1]
-    psubusw   m5, m4
-    RSHIFT    m5, 2
-    punpcklwd m5, m7
-    mova      m4, m2
-    psubusw   m2, m3
-    psubusw   m3, m4
-    PMAXUW    m2, m3
-    mova      m3, m2
-    mova      m4, m2
-    RSHIFT    m3, 2
-    RSHIFT    m4, 4
-    punpcklwd m2, m7
-    punpcklwd m3, m7
-    punpcklwd m4, m7
-    paddd     m2, m3
-    paddd     m2, m4
-%endmacro
-
-%macro CHECK1 0
-    mova    m3, m0
-    pcmpgtd m3, m2
-    PMINSD  m0, m2, m6
-    mova    m6, m3
-    pand    m5, m3
-    pandn   m3, m1
-    por     m3, m5
-    mova    m1, m3
-%endmacro
-
-%macro CHECK2 0
-    paddd   m6, [pd_1]
-    pslld   m6, 30
-    paddd   m2, m6
-    mova    m3, m0
-    pcmpgtd m3, m2
-    PMINSD  m0, m2, m4
-    pand    m5, m3
-    pandn   m3, m1
-    por     m3, m5
-    mova    m1, m3
-%endmacro
-
-; This version of CHECK2 has 3 fewer instructions on sets older than SSE4 but I
-; am not sure whether it is any faster.  A rewrite or refactor of the filter
-; code should make it possible to eliminate the move instruction at the end.  It
-; exists to satisfy the expectation that the "score" values are in m1.
-
-; %macro CHECK2 0
-;     mova    m3, m0
-;     pcmpgtd m0, m2
-;     pand    m0, m6
-;     mova    m6, m0
-;     pand    m5, m6
-;     pand    m2, m0
-;     pandn   m6, m1
-;     pandn   m0, m3
-;     por     m6, m5
-;     por     m0, m2
-;     mova    m1, m6
-; %endmacro
-
-%macro LOAD 2
-    movh      %1, %2
-    punpcklwd %1, m7
-%endmacro
-
-%macro FILTER 3
-.loop%1:
-    pxor         m7, m7
-    LOAD         m0, [curq+t1]
-    LOAD         m1, [curq+t0]
-    LOAD         m2, [%2]
-    LOAD         m3, [%3]
-    mova         m4, m3
-    paddd        m3, m2
-    psrad        m3, 1
-    mova   [rsp+ 0], m0
-    mova   [rsp+16], m3
-    mova   [rsp+32], m1
-    psubd        m2, m4
-    PABS         m2, m4
-    LOAD         m3, [prevq+t1]
-    LOAD         m4, [prevq+t0]
-    psubd        m3, m0
-    psubd        m4, m1
-    PABS         m3, m5
-    PABS         m4, m5
-    paddd        m3, m4
-    psrld        m2, 1
-    psrld        m3, 1
-    PMAXSD       m2, m3, m6
-    LOAD         m3, [nextq+t1]
-    LOAD         m4, [nextq+t0]
-    psubd        m3, m0
-    psubd        m4, m1
-    PABS         m3, m5
-    PABS         m4, m5
-    paddd        m3, m4
-    psrld        m3, 1
-    PMAXSD       m2, m3, m6
-    mova   [rsp+48], m2
-
-    paddd        m1, m0
-    paddd        m0, m0
-    psubd        m0, m1
-    psrld        m1, 1
-    PABS         m0, m2
-
-    movu         m2, [curq+t1-1*2]
-    movu         m3, [curq+t0-1*2]
-    mova         m4, m2
-    psubusw      m2, m3
-    psubusw      m3, m4
-    PMAXUW       m2, m3
-    mova         m3, m2
-    RSHIFT       m3, 4
-    punpcklwd    m2, m7
-    punpcklwd    m3, m7
-    paddd        m0, m2
-    paddd        m0, m3
-    psubd        m0, [pd_1]
-
-    CHECK -2, 0
-    CHECK1
-    CHECK -3, 1
-    CHECK2
-    CHECK 0, -2
-    CHECK1
-    CHECK 1, -3
-    CHECK2
-
-    mova         m6, [rsp+48]
-    cmp   DWORD r8m, 2
-    jge .end%1
-    LOAD         m2, [%2+t1*2]
-    LOAD         m4, [%3+t1*2]
-    LOAD         m3, [%2+t0*2]
-    LOAD         m5, [%3+t0*2]
-    paddd        m2, m4
-    paddd        m3, m5
-    psrld        m2, 1
-    psrld        m3, 1
-    mova         m4, [rsp+ 0]
-    mova         m5, [rsp+16]
-    mova         m7, [rsp+32]
-    psubd        m2, m4
-    psubd        m3, m7
-    mova         m0, m5
-    psubd        m5, m4
-    psubd        m0, m7
-    mova         m4, m2
-    PMINSD       m2, m3, m7
-    PMAXSD       m3, m4, m7
-    PMAXSD       m2, m5, m7
-    PMINSD       m3, m5, m7
-    PMAXSD       m2, m0, m7
-    PMINSD       m3, m0, m7
-    pxor         m4, m4
-    PMAXSD       m6, m3, m7
-    psubd        m4, m2
-    PMAXSD       m6, m4, m7
-
-.end%1:
-    mova         m2, [rsp+16]
-    mova         m3, m2
-    psubd        m2, m6
-    paddd        m3, m6
-    PMAXSD       m1, m2, m7
-    PMINSD       m1, m3, m7
-    PACK         m1
-
-    movh     [dstq], m1
-    add        dstq, mmsize/2
-    add       prevq, mmsize/2
-    add        curq, mmsize/2
-    add       nextq, mmsize/2
-    sub   DWORD r4m, mmsize/4
-    jg .loop%1
-%endmacro
-
-%macro YADIF 0
-%if ARCH_X86_32
-cglobal yadif_filter_line_16bit, 4, 6, 8, 80, dst, prev, cur, next, w, \
-                                              prefs, mrefs, parity, mode
-%else
-cglobal yadif_filter_line_16bit, 4, 7, 8, 80, dst, prev, cur, next, w, \
-                                              prefs, mrefs, parity, mode
-%endif
-%if ARCH_X86_32
-    mov            r4, r5mp
-    mov            r5, r6mp
-    DECLARE_REG_TMP 4,5
-%else
-    movsxd         r5, DWORD r5m
-    movsxd         r6, DWORD r6m
-    DECLARE_REG_TMP 5,6
-%endif
-
-    cmp DWORD paritym, 0
-    je .parity0
-    FILTER 1, prevq, curq
-    jmp .ret
-
-.parity0:
-    FILTER 0, curq, nextq
-
-.ret:
-    RET
-%endmacro
-
-INIT_XMM sse4
-YADIF
-INIT_XMM ssse3
-YADIF
-INIT_XMM sse2
-YADIF
-%if ARCH_X86_32
-INIT_MMX mmxext
-YADIF
-%endif
diff -uparN ffmpeg-4.1/libavformat/demuxer_list.c ffmpeg-y/libavformat/demuxer_list.c
--- ffmpeg-4.1/libavformat/demuxer_list.c	1970-01-01 08:00:00.000000000 +0800
+++ ffmpeg-y/libavformat/demuxer_list.c	2019-06-29 11:49:36.873017668 +0800
@@ -0,0 +1,4 @@
+static const AVInputFormat * const demuxer_list[] = {
+    &ff_mov_demuxer,
+    &ff_wav_demuxer,
+    NULL };
diff -uparN ffmpeg-4.1/libavformat/libavformat.version ffmpeg-y/libavformat/libavformat.version
--- ffmpeg-4.1/libavformat/libavformat.version	1970-01-01 08:00:00.000000000 +0800
+++ ffmpeg-y/libavformat/libavformat.version	2019-06-29 11:50:15.245018730 +0800
@@ -0,0 +1,3 @@
+libavformat_VERSION=58.20.100
+libavformat_VERSION_MAJOR=58
+libavformat_VERSION_MINOR=20
diff -uparN ffmpeg-4.1/libavformat/Makefile ffmpeg-y/libavformat/Makefile
--- ffmpeg-4.1/libavformat/Makefile	2018-11-06 07:22:26.000000000 +0800
+++ ffmpeg-y/libavformat/Makefile	2019-06-29 11:49:36.873017668 +0800
@@ -9,7 +9,6 @@ OBJS = allformats.o         \
        avio.o               \
        aviobuf.o            \
        cutils.o             \
-       dump.o               \
        format.o             \
        id3v1.o              \
        id3v2.o              \
@@ -648,7 +647,6 @@ TESTPROGS-$(CONFIG_SRTP)
 
 TOOLS     = aviocat                                                     \
             ismindex                                                    \
-            pktdumper                                                   \
             probetest                                                   \
             seek_print                                                  \
             sidxindex                                                   \
diff -uparN ffmpeg-4.1/libavformat/muxer_list.c ffmpeg-y/libavformat/muxer_list.c
--- ffmpeg-4.1/libavformat/muxer_list.c	1970-01-01 08:00:00.000000000 +0800
+++ ffmpeg-y/libavformat/muxer_list.c	2019-06-29 11:49:36.881017668 +0800
@@ -0,0 +1,2 @@
+static const AVOutputFormat * const muxer_list[] = {
+    NULL };
diff -uparN ffmpeg-4.1/libavformat/protocol_list.c ffmpeg-y/libavformat/protocol_list.c
--- ffmpeg-4.1/libavformat/protocol_list.c	1970-01-01 08:00:00.000000000 +0800
+++ ffmpeg-y/libavformat/protocol_list.c	2019-06-29 11:49:36.865017668 +0800
@@ -0,0 +1,3 @@
+static const URLProtocol * const url_protocols[] = {
+    &ff_file_protocol,
+    NULL };
diff -uparN ffmpeg-4.1/libavresample/x86/audio_convert.asm ffmpeg-y/libavresample/x86/audio_convert.asm
--- ffmpeg-4.1/libavresample/x86/audio_convert.asm	2018-01-01 06:35:49.000000000 +0800
+++ ffmpeg-y/libavresample/x86/audio_convert.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,1261 +0,0 @@
-;******************************************************************************
-;* x86 optimized Format Conversion Utils
-;* Copyright (c) 2008 Loren Merritt
-;* Copyright (c) 2012 Justin Ruggles <justin.ruggles@gmail.com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-%include "util.asm"
-
-SECTION_RODATA 32
-
-pf_s32_inv_scale: times 8 dd 0x30000000
-pf_s32_scale:     times 8 dd 0x4f000000
-pf_s32_clip:      times 8 dd 0x4effffff
-pf_s16_inv_scale: times 4 dd 0x38000000
-pf_s16_scale:     times 4 dd 0x47000000
-pb_shuf_unpack_even:      db -1, -1,  0,  1, -1, -1,  2,  3, -1, -1,  8,  9, -1, -1, 10, 11
-pb_shuf_unpack_odd:       db -1, -1,  4,  5, -1, -1,  6,  7, -1, -1, 12, 13, -1, -1, 14, 15
-pb_interleave_words: SHUFFLE_MASK_W  0,  4,  1,  5,  2,  6,  3,  7
-pb_deinterleave_words: SHUFFLE_MASK_W  0,  2,  4,  6,  1,  3,  5,  7
-pw_zero_even:     times 4 dw 0x0000, 0xffff
-
-SECTION .text
-
-;------------------------------------------------------------------------------
-; void ff_conv_s16_to_s32(int32_t *dst, const int16_t *src, int len);
-;------------------------------------------------------------------------------
-
-INIT_XMM sse2
-cglobal conv_s16_to_s32, 3,3,3, dst, src, len
-    lea      lenq, [2*lend]
-    lea      dstq, [dstq+2*lenq]
-    add      srcq, lenq
-    neg      lenq
-.loop:
-    mova       m2, [srcq+lenq]
-    pxor       m0, m0
-    pxor       m1, m1
-    punpcklwd  m0, m2
-    punpckhwd  m1, m2
-    mova  [dstq+2*lenq       ], m0
-    mova  [dstq+2*lenq+mmsize], m1
-    add      lenq, mmsize
-    jl .loop
-    REP_RET
-
-;------------------------------------------------------------------------------
-; void ff_conv_s16_to_flt(float *dst, const int16_t *src, int len);
-;------------------------------------------------------------------------------
-
-%macro CONV_S16_TO_FLT 0
-cglobal conv_s16_to_flt, 3,3,3, dst, src, len
-    lea      lenq, [2*lend]
-    add      srcq, lenq
-    lea      dstq, [dstq + 2*lenq]
-    neg      lenq
-    mova       m2, [pf_s16_inv_scale]
-    ALIGN 16
-.loop:
-    mova       m0, [srcq+lenq]
-    S16_TO_S32_SX 0, 1
-    cvtdq2ps   m0, m0
-    cvtdq2ps   m1, m1
-    mulps      m0, m2
-    mulps      m1, m2
-    mova  [dstq+2*lenq       ], m0
-    mova  [dstq+2*lenq+mmsize], m1
-    add      lenq, mmsize
-    jl .loop
-    REP_RET
-%endmacro
-
-INIT_XMM sse2
-CONV_S16_TO_FLT
-INIT_XMM sse4
-CONV_S16_TO_FLT
-
-;------------------------------------------------------------------------------
-; void ff_conv_s32_to_s16(int16_t *dst, const int32_t *src, int len);
-;------------------------------------------------------------------------------
-
-%macro CONV_S32_TO_S16 0
-cglobal conv_s32_to_s16, 3,3,4, dst, src, len
-    lea     lenq, [2*lend]
-    lea     srcq, [srcq+2*lenq]
-    add     dstq, lenq
-    neg     lenq
-.loop:
-    mova      m0, [srcq+2*lenq         ]
-    mova      m1, [srcq+2*lenq+  mmsize]
-    mova      m2, [srcq+2*lenq+2*mmsize]
-    mova      m3, [srcq+2*lenq+3*mmsize]
-    psrad     m0, 16
-    psrad     m1, 16
-    psrad     m2, 16
-    psrad     m3, 16
-    packssdw  m0, m1
-    packssdw  m2, m3
-    mova  [dstq+lenq       ], m0
-    mova  [dstq+lenq+mmsize], m2
-    add     lenq, mmsize*2
-    jl .loop
-%if mmsize == 8
-    emms
-    RET
-%else
-    REP_RET
-%endif
-%endmacro
-
-INIT_MMX mmx
-CONV_S32_TO_S16
-INIT_XMM sse2
-CONV_S32_TO_S16
-
-;------------------------------------------------------------------------------
-; void ff_conv_s32_to_flt(float *dst, const int32_t *src, int len);
-;------------------------------------------------------------------------------
-
-%macro CONV_S32_TO_FLT 0
-cglobal conv_s32_to_flt, 3,3,3, dst, src, len
-    lea     lenq, [4*lend]
-    add     srcq, lenq
-    add     dstq, lenq
-    neg     lenq
-    mova      m0, [pf_s32_inv_scale]
-    ALIGN 16
-.loop:
-    cvtdq2ps  m1, [srcq+lenq       ]
-    cvtdq2ps  m2, [srcq+lenq+mmsize]
-    mulps     m1, m1, m0
-    mulps     m2, m2, m0
-    mova  [dstq+lenq       ], m1
-    mova  [dstq+lenq+mmsize], m2
-    add     lenq, mmsize*2
-    jl .loop
-    REP_RET
-%endmacro
-
-INIT_XMM sse2
-CONV_S32_TO_FLT
-%if HAVE_AVX_EXTERNAL
-INIT_YMM avx
-CONV_S32_TO_FLT
-%endif
-
-;------------------------------------------------------------------------------
-; void ff_conv_flt_to_s16(int16_t *dst, const float *src, int len);
-;------------------------------------------------------------------------------
-
-INIT_XMM sse2
-cglobal conv_flt_to_s16, 3,3,5, dst, src, len
-    lea     lenq, [2*lend]
-    lea     srcq, [srcq+2*lenq]
-    add     dstq, lenq
-    neg     lenq
-    mova      m4, [pf_s16_scale]
-.loop:
-    mova      m0, [srcq+2*lenq         ]
-    mova      m1, [srcq+2*lenq+1*mmsize]
-    mova      m2, [srcq+2*lenq+2*mmsize]
-    mova      m3, [srcq+2*lenq+3*mmsize]
-    mulps     m0, m4
-    mulps     m1, m4
-    mulps     m2, m4
-    mulps     m3, m4
-    cvtps2dq  m0, m0
-    cvtps2dq  m1, m1
-    cvtps2dq  m2, m2
-    cvtps2dq  m3, m3
-    packssdw  m0, m1
-    packssdw  m2, m3
-    mova  [dstq+lenq       ], m0
-    mova  [dstq+lenq+mmsize], m2
-    add     lenq, mmsize*2
-    jl .loop
-    REP_RET
-
-;------------------------------------------------------------------------------
-; void ff_conv_flt_to_s32(int32_t *dst, const float *src, int len);
-;------------------------------------------------------------------------------
-
-%macro CONV_FLT_TO_S32 0
-cglobal conv_flt_to_s32, 3,3,6, dst, src, len
-    lea     lenq, [lend*4]
-    add     srcq, lenq
-    add     dstq, lenq
-    neg     lenq
-    mova      m4, [pf_s32_scale]
-    mova      m5, [pf_s32_clip]
-.loop:
-    mulps     m0, m4, [srcq+lenq         ]
-    mulps     m1, m4, [srcq+lenq+1*mmsize]
-    mulps     m2, m4, [srcq+lenq+2*mmsize]
-    mulps     m3, m4, [srcq+lenq+3*mmsize]
-    minps     m0, m0, m5
-    minps     m1, m1, m5
-    minps     m2, m2, m5
-    minps     m3, m3, m5
-    cvtps2dq  m0, m0
-    cvtps2dq  m1, m1
-    cvtps2dq  m2, m2
-    cvtps2dq  m3, m3
-    mova  [dstq+lenq         ], m0
-    mova  [dstq+lenq+1*mmsize], m1
-    mova  [dstq+lenq+2*mmsize], m2
-    mova  [dstq+lenq+3*mmsize], m3
-    add     lenq, mmsize*4
-    jl .loop
-    REP_RET
-%endmacro
-
-INIT_XMM sse2
-CONV_FLT_TO_S32
-%if HAVE_AVX_EXTERNAL
-INIT_YMM avx
-CONV_FLT_TO_S32
-%endif
-
-;------------------------------------------------------------------------------
-; void ff_conv_s16p_to_s16_2ch(int16_t *dst, int16_t *const *src, int len,
-;                              int channels);
-;------------------------------------------------------------------------------
-
-%macro CONV_S16P_TO_S16_2CH 0
-cglobal conv_s16p_to_s16_2ch, 3,4,5, dst, src0, len, src1
-    mov       src1q, [src0q+gprsize]
-    mov       src0q, [src0q        ]
-    lea        lenq, [2*lend]
-    add       src0q, lenq
-    add       src1q, lenq
-    lea        dstq, [dstq+2*lenq]
-    neg        lenq
-.loop:
-    mova         m0, [src0q+lenq       ]
-    mova         m1, [src1q+lenq       ]
-    mova         m2, [src0q+lenq+mmsize]
-    mova         m3, [src1q+lenq+mmsize]
-    SBUTTERFLY2  wd, 0, 1, 4
-    SBUTTERFLY2  wd, 2, 3, 4
-    mova  [dstq+2*lenq+0*mmsize], m0
-    mova  [dstq+2*lenq+1*mmsize], m1
-    mova  [dstq+2*lenq+2*mmsize], m2
-    mova  [dstq+2*lenq+3*mmsize], m3
-    add        lenq, 2*mmsize
-    jl .loop
-    REP_RET
-%endmacro
-
-INIT_XMM sse2
-CONV_S16P_TO_S16_2CH
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-CONV_S16P_TO_S16_2CH
-%endif
-
-;------------------------------------------------------------------------------
-; void ff_conv_s16p_to_s16_6ch(int16_t *dst, int16_t *const *src, int len,
-;                              int channels);
-;------------------------------------------------------------------------------
-
-;------------------------------------------------------------------------------
-; NOTE: In the 6-channel functions, len could be used as an index on x86-64
-;       instead of just a counter, which would avoid incrementing the
-;       pointers, but the extra complexity and amount of code is not worth
-;       the small gain. On x86-32 there are not enough registers to use len
-;       as an index without keeping two of the pointers on the stack and
-;       loading them in each iteration.
-;------------------------------------------------------------------------------
-
-%macro CONV_S16P_TO_S16_6CH 0
-%if ARCH_X86_64
-cglobal conv_s16p_to_s16_6ch, 3,8,7, dst, src0, len, src1, src2, src3, src4, src5
-%else
-cglobal conv_s16p_to_s16_6ch, 2,7,7, dst, src0, src1, src2, src3, src4, src5
-%define lend dword r2m
-%endif
-    mov      src1q, [src0q+1*gprsize]
-    mov      src2q, [src0q+2*gprsize]
-    mov      src3q, [src0q+3*gprsize]
-    mov      src4q, [src0q+4*gprsize]
-    mov      src5q, [src0q+5*gprsize]
-    mov      src0q, [src0q]
-    sub      src1q, src0q
-    sub      src2q, src0q
-    sub      src3q, src0q
-    sub      src4q, src0q
-    sub      src5q, src0q
-.loop:
-%if cpuflag(sse2slow)
-    movq        m0, [src0q      ]   ; m0 =  0,  6, 12, 18,  x,  x,  x,  x
-    movq        m1, [src0q+src1q]   ; m1 =  1,  7, 13, 19,  x,  x,  x,  x
-    movq        m2, [src0q+src2q]   ; m2 =  2,  8, 14, 20,  x,  x,  x,  x
-    movq        m3, [src0q+src3q]   ; m3 =  3,  9, 15, 21,  x,  x,  x,  x
-    movq        m4, [src0q+src4q]   ; m4 =  4, 10, 16, 22,  x,  x,  x,  x
-    movq        m5, [src0q+src5q]   ; m5 =  5, 11, 17, 23,  x,  x,  x,  x
-                                    ; unpack words:
-    punpcklwd   m0, m1              ; m0 =  0,  1,  6,  7, 12, 13, 18, 19
-    punpcklwd   m2, m3              ; m2 =  4,  5, 10, 11, 16, 17, 22, 23
-    punpcklwd   m4, m5              ; m4 =  2,  3,  8,  9, 14, 15, 20, 21
-                                    ; blend dwords
-    shufps      m1, m0, m2, q2020   ; m1 =  0,  1, 12, 13,  2,  3, 14, 15
-    shufps      m0, m4, q2031       ; m0 =  6,  7, 18, 19,  4,  5, 16, 17
-    shufps      m2, m4, q3131       ; m2 =  8,  9, 20, 21, 10, 11, 22, 23
-                                    ; shuffle dwords
-    pshufd      m0, m0, q1302       ; m0 =  4,  5,  6,  7, 16, 17, 18, 19
-    pshufd      m1, m1, q3120       ; m1 =  0,  1,  2,  3, 12, 13, 14, 15
-    pshufd      m2, m2, q3120       ; m2 =  8,  9, 10, 11, 20, 21, 22, 23
-    movq   [dstq+0*mmsize/2], m1
-    movq   [dstq+1*mmsize/2], m0
-    movq   [dstq+2*mmsize/2], m2
-    movhps [dstq+3*mmsize/2], m1
-    movhps [dstq+4*mmsize/2], m0
-    movhps [dstq+5*mmsize/2], m2
-    add      src0q, mmsize/2
-    add       dstq, mmsize*3
-    sub       lend, mmsize/4
-%else
-    mova        m0, [src0q      ]   ; m0 =  0,  6, 12, 18, 24, 30, 36, 42
-    mova        m1, [src0q+src1q]   ; m1 =  1,  7, 13, 19, 25, 31, 37, 43
-    mova        m2, [src0q+src2q]   ; m2 =  2,  8, 14, 20, 26, 32, 38, 44
-    mova        m3, [src0q+src3q]   ; m3 =  3,  9, 15, 21, 27, 33, 39, 45
-    mova        m4, [src0q+src4q]   ; m4 =  4, 10, 16, 22, 28, 34, 40, 46
-    mova        m5, [src0q+src5q]   ; m5 =  5, 11, 17, 23, 29, 35, 41, 47
-                                    ; unpack words:
-    SBUTTERFLY2 wd, 0, 1, 6         ; m0 =  0,  1,  6,  7, 12, 13, 18, 19
-                                    ; m1 = 24, 25, 30, 31, 36, 37, 42, 43
-    SBUTTERFLY2 wd, 2, 3, 6         ; m2 =  2,  3,  8,  9, 14, 15, 20, 21
-                                    ; m3 = 26, 27, 32, 33, 38, 39, 44, 45
-    SBUTTERFLY2 wd, 4, 5, 6         ; m4 =  4,  5, 10, 11, 16, 17, 22, 23
-                                    ; m5 = 28, 29, 34, 35, 40, 41, 46, 47
-                                    ; blend dwords
-    shufps      m6, m0, m2, q2020   ; m6 =  0,  1, 12, 13,  2,  3, 14, 15
-    shufps      m0, m4, q2031       ; m0 =  6,  7, 18, 19,  4,  5, 16, 17
-    shufps      m2, m4, q3131       ; m2 =  8,  9, 20, 21, 10, 11, 22, 23
-    SWAP 4,6                        ; m4 =  0,  1, 12, 13,  2,  3, 14, 15
-    shufps      m6, m1, m3, q2020   ; m6 = 24, 25, 36, 37, 26, 27, 38, 39
-    shufps      m1, m5, q2031       ; m1 = 30, 31, 42, 43, 28, 29, 40, 41
-    shufps      m3, m5, q3131       ; m3 = 32, 33, 44, 45, 34, 35, 46, 47
-    SWAP 5,6                        ; m5 = 24, 25, 36, 37, 26, 27, 38, 39
-                                    ; shuffle dwords
-    pshufd      m0, m0, q1302       ; m0 =  4,  5,  6,  7, 16, 17, 18, 19
-    pshufd      m2, m2, q3120       ; m2 =  8,  9, 10, 11, 20, 21, 22, 23
-    pshufd      m4, m4, q3120       ; m4 =  0,  1,  2,  3, 12, 13, 14, 15
-    pshufd      m1, m1, q1302       ; m1 = 28, 29, 30, 31, 40, 41, 42, 43
-    pshufd      m3, m3, q3120       ; m3 = 32, 33, 34, 35, 44, 45, 46, 47
-    pshufd      m5, m5, q3120       ; m5 = 24, 25, 26, 27, 36, 37, 38, 39
-                                    ; shuffle qwords
-    punpcklqdq  m6, m4, m0          ; m6 =  0,  1,  2,  3,  4,  5,  6,  7
-    punpckhqdq  m0, m2              ; m0 = 16, 17, 18, 19, 20, 21, 22, 23
-    shufps      m2, m4, q3210       ; m2 =  8,  9, 10, 11, 12, 13, 14, 15
-    SWAP 4,6                        ; m4 =  0,  1,  2,  3,  4,  5,  6,  7
-    punpcklqdq  m6, m5, m1          ; m6 = 24, 25, 26, 27, 28, 29, 30, 31
-    punpckhqdq  m1, m3              ; m1 = 40, 41, 42, 43, 44, 45, 46, 47
-    shufps      m3, m5, q3210       ; m3 = 32, 33, 34, 35, 36, 37, 38, 39
-    SWAP 5,6                        ; m5 = 24, 25, 26, 27, 28, 29, 30, 31
-    mova   [dstq+0*mmsize], m4
-    mova   [dstq+1*mmsize], m2
-    mova   [dstq+2*mmsize], m0
-    mova   [dstq+3*mmsize], m5
-    mova   [dstq+4*mmsize], m3
-    mova   [dstq+5*mmsize], m1
-    add      src0q, mmsize
-    add       dstq, mmsize*6
-    sub       lend, mmsize/2
-%endif
-    jg .loop
-    REP_RET
-%endmacro
-
-INIT_XMM sse2
-CONV_S16P_TO_S16_6CH
-INIT_XMM sse2slow
-CONV_S16P_TO_S16_6CH
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-CONV_S16P_TO_S16_6CH
-%endif
-
-;------------------------------------------------------------------------------
-; void ff_conv_s16p_to_flt_2ch(float *dst, int16_t *const *src, int len,
-;                              int channels);
-;------------------------------------------------------------------------------
-
-%macro CONV_S16P_TO_FLT_2CH 0
-cglobal conv_s16p_to_flt_2ch, 3,4,6, dst, src0, len, src1
-    lea       lenq, [2*lend]
-    mov      src1q, [src0q+gprsize]
-    mov      src0q, [src0q        ]
-    lea       dstq, [dstq+4*lenq]
-    add      src0q, lenq
-    add      src1q, lenq
-    neg       lenq
-    mova        m5, [pf_s32_inv_scale]
-.loop:
-    mova        m2, [src0q+lenq]    ; m2 =  0,  2,  4,  6,  8, 10, 12, 14
-    mova        m4, [src1q+lenq]    ; m4 =  1,  3,  5,  7,  9, 11, 13, 15
-    SBUTTERFLY2 wd, 2, 4, 3         ; m2 =  0,  1,  2,  3,  4,  5,  6,  7
-                                    ; m4 =  8,  9, 10, 11, 12, 13, 14, 15
-    pxor        m3, m3
-    punpcklwd   m0, m3, m2          ; m0 =      0,      1,      2,      3
-    punpckhwd   m1, m3, m2          ; m1 =      4,      5,      6,      7
-    punpcklwd   m2, m3, m4          ; m2 =      8,      9,     10,     11
-    punpckhwd   m3, m4              ; m3 =     12,     13,     14,     15
-    cvtdq2ps    m0, m0
-    cvtdq2ps    m1, m1
-    cvtdq2ps    m2, m2
-    cvtdq2ps    m3, m3
-    mulps       m0, m5
-    mulps       m1, m5
-    mulps       m2, m5
-    mulps       m3, m5
-    mova  [dstq+4*lenq         ], m0
-    mova  [dstq+4*lenq+  mmsize], m1
-    mova  [dstq+4*lenq+2*mmsize], m2
-    mova  [dstq+4*lenq+3*mmsize], m3
-    add       lenq, mmsize
-    jl .loop
-    REP_RET
-%endmacro
-
-INIT_XMM sse2
-CONV_S16P_TO_FLT_2CH
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-CONV_S16P_TO_FLT_2CH
-%endif
-
-;------------------------------------------------------------------------------
-; void ff_conv_s16p_to_flt_6ch(float *dst, int16_t *const *src, int len,
-;                              int channels);
-;------------------------------------------------------------------------------
-
-%macro CONV_S16P_TO_FLT_6CH 0
-%if ARCH_X86_64
-cglobal conv_s16p_to_flt_6ch, 3,8,8, dst, src, len, src1, src2, src3, src4, src5
-%else
-cglobal conv_s16p_to_flt_6ch, 2,7,8, dst, src, src1, src2, src3, src4, src5
-%define lend dword r2m
-%endif
-    mov     src1q, [srcq+1*gprsize]
-    mov     src2q, [srcq+2*gprsize]
-    mov     src3q, [srcq+3*gprsize]
-    mov     src4q, [srcq+4*gprsize]
-    mov     src5q, [srcq+5*gprsize]
-    mov      srcq, [srcq]
-    sub     src1q, srcq
-    sub     src2q, srcq
-    sub     src3q, srcq
-    sub     src4q, srcq
-    sub     src5q, srcq
-    mova       m7, [pf_s32_inv_scale]
-%if cpuflag(ssse3)
-    %define unpack_even m6
-    mova       m6, [pb_shuf_unpack_even]
-%if ARCH_X86_64
-    %define unpack_odd m8
-    mova       m8, [pb_shuf_unpack_odd]
-%else
-    %define unpack_odd [pb_shuf_unpack_odd]
-%endif
-%endif
-.loop:
-    movq       m0, [srcq      ]  ; m0 =  0,  6, 12, 18,  x,  x,  x,  x
-    movq       m1, [srcq+src1q]  ; m1 =  1,  7, 13, 19,  x,  x,  x,  x
-    movq       m2, [srcq+src2q]  ; m2 =  2,  8, 14, 20,  x,  x,  x,  x
-    movq       m3, [srcq+src3q]  ; m3 =  3,  9, 15, 21,  x,  x,  x,  x
-    movq       m4, [srcq+src4q]  ; m4 =  4, 10, 16, 22,  x,  x,  x,  x
-    movq       m5, [srcq+src5q]  ; m5 =  5, 11, 17, 23,  x,  x,  x,  x
-                                 ; unpack words:
-    punpcklwd  m0, m1            ; m0 =  0,  1,  6,  7, 12, 13, 18, 19
-    punpcklwd  m2, m3            ; m2 =  2,  3,  8,  9, 14, 15, 20, 21
-    punpcklwd  m4, m5            ; m4 =  4,  5, 10, 11, 16, 17, 22, 23
-                                 ; blend dwords
-    shufps     m1, m4, m0, q3120 ; m1 =  4,  5, 16, 17,  6,  7, 18, 19
-    shufps         m0, m2, q2020 ; m0 =  0,  1, 12, 13,  2,  3, 14, 15
-    shufps         m2, m4, q3131 ; m2 =  8,  9, 20, 21, 10, 11, 22, 23
-%if cpuflag(ssse3)
-    pshufb     m3, m0, unpack_odd   ; m3 =  12,     13,     14,     15
-    pshufb         m0, unpack_even  ; m0 =   0,      1,      2,      3
-    pshufb     m4, m1, unpack_odd   ; m4 =  16,     17,     18,     19
-    pshufb         m1, unpack_even  ; m1 =   4,      5,      6,      7
-    pshufb     m5, m2, unpack_odd   ; m5 =  20,     21,     22,     23
-    pshufb         m2, unpack_even  ; m2 =   8,      9,     10,     11
-%else
-                                 ; shuffle dwords
-    pshufd     m0, m0, q3120     ; m0 =  0,  1,  2,  3, 12, 13, 14, 15
-    pshufd     m1, m1, q3120     ; m1 =  4,  5,  6,  7, 16, 17, 18, 19
-    pshufd     m2, m2, q3120     ; m2 =  8,  9, 10, 11, 20, 21, 22, 23
-    pxor       m6, m6            ; convert s16 in m0-m2 to s32 in m0-m5
-    punpcklwd  m3, m6, m0        ; m3 =      0,      1,      2,      3
-    punpckhwd  m4, m6, m0        ; m4 =     12,     13,     14,     15
-    punpcklwd  m0, m6, m1        ; m0 =      4,      5,      6,      7
-    punpckhwd  m5, m6, m1        ; m5 =     16,     17,     18,     19
-    punpcklwd  m1, m6, m2        ; m1 =      8,      9,     10,     11
-    punpckhwd      m6, m2        ; m6 =     20,     21,     22,     23
-    SWAP 6,2,1,0,3,4,5           ; swap registers 3,0,1,4,5,6 to 0,1,2,3,4,5
-%endif
-    cvtdq2ps   m0, m0            ; convert s32 to float
-    cvtdq2ps   m1, m1
-    cvtdq2ps   m2, m2
-    cvtdq2ps   m3, m3
-    cvtdq2ps   m4, m4
-    cvtdq2ps   m5, m5
-    mulps      m0, m7            ; scale float from s32 range to [-1.0,1.0]
-    mulps      m1, m7
-    mulps      m2, m7
-    mulps      m3, m7
-    mulps      m4, m7
-    mulps      m5, m7
-    mova  [dstq         ], m0
-    mova  [dstq+  mmsize], m1
-    mova  [dstq+2*mmsize], m2
-    mova  [dstq+3*mmsize], m3
-    mova  [dstq+4*mmsize], m4
-    mova  [dstq+5*mmsize], m5
-    add      srcq, mmsize/2
-    add      dstq, mmsize*6
-    sub      lend, mmsize/4
-    jg .loop
-    REP_RET
-%endmacro
-
-INIT_XMM sse2
-CONV_S16P_TO_FLT_6CH
-INIT_XMM ssse3
-CONV_S16P_TO_FLT_6CH
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-CONV_S16P_TO_FLT_6CH
-%endif
-
-;------------------------------------------------------------------------------
-; void ff_conv_fltp_to_s16_2ch(int16_t *dst, float *const *src, int len,
-;                              int channels);
-;------------------------------------------------------------------------------
-
-%macro CONV_FLTP_TO_S16_2CH 0
-cglobal conv_fltp_to_s16_2ch, 3,4,3, dst, src0, len, src1
-    lea      lenq, [4*lend]
-    mov     src1q, [src0q+gprsize]
-    mov     src0q, [src0q        ]
-    add      dstq, lenq
-    add     src0q, lenq
-    add     src1q, lenq
-    neg      lenq
-    mova       m2, [pf_s16_scale]
-%if cpuflag(ssse3)
-    mova       m3, [pb_interleave_words]
-%endif
-.loop:
-    mulps      m0, m2, [src0q+lenq] ; m0 =    0,    2,    4,    6
-    mulps      m1, m2, [src1q+lenq] ; m1 =    1,    3,    5,    7
-    cvtps2dq   m0, m0
-    cvtps2dq   m1, m1
-%if cpuflag(ssse3)
-    packssdw   m0, m1               ; m0 = 0, 2, 4, 6, 1, 3, 5, 7
-    pshufb     m0, m3               ; m0 = 0, 1, 2, 3, 4, 5, 6, 7
-%else
-    packssdw   m0, m0               ; m0 = 0, 2, 4, 6, x, x, x, x
-    packssdw   m1, m1               ; m1 = 1, 3, 5, 7, x, x, x, x
-    punpcklwd  m0, m1               ; m0 = 0, 1, 2, 3, 4, 5, 6, 7
-%endif
-    mova  [dstq+lenq], m0
-    add      lenq, mmsize
-    jl .loop
-    REP_RET
-%endmacro
-
-INIT_XMM sse2
-CONV_FLTP_TO_S16_2CH
-INIT_XMM ssse3
-CONV_FLTP_TO_S16_2CH
-
-;------------------------------------------------------------------------------
-; void ff_conv_fltp_to_s16_6ch(int16_t *dst, float *const *src, int len,
-;                              int channels);
-;------------------------------------------------------------------------------
-
-%macro CONV_FLTP_TO_S16_6CH 0
-%if ARCH_X86_64
-cglobal conv_fltp_to_s16_6ch, 3,8,7, dst, src, len, src1, src2, src3, src4, src5
-%else
-cglobal conv_fltp_to_s16_6ch, 2,7,7, dst, src, src1, src2, src3, src4, src5
-%define lend dword r2m
-%endif
-    mov        src1q, [srcq+1*gprsize]
-    mov        src2q, [srcq+2*gprsize]
-    mov        src3q, [srcq+3*gprsize]
-    mov        src4q, [srcq+4*gprsize]
-    mov        src5q, [srcq+5*gprsize]
-    mov         srcq, [srcq]
-    sub        src1q, srcq
-    sub        src2q, srcq
-    sub        src3q, srcq
-    sub        src4q, srcq
-    sub        src5q, srcq
-    movaps      xmm6, [pf_s16_scale]
-.loop:
-%if cpuflag(sse2)
-    mulps         m0, m6, [srcq      ]
-    mulps         m1, m6, [srcq+src1q]
-    mulps         m2, m6, [srcq+src2q]
-    mulps         m3, m6, [srcq+src3q]
-    mulps         m4, m6, [srcq+src4q]
-    mulps         m5, m6, [srcq+src5q]
-    cvtps2dq      m0, m0
-    cvtps2dq      m1, m1
-    cvtps2dq      m2, m2
-    cvtps2dq      m3, m3
-    cvtps2dq      m4, m4
-    cvtps2dq      m5, m5
-    packssdw      m0, m3            ; m0 =  0,  6, 12, 18,  3,  9, 15, 21
-    packssdw      m1, m4            ; m1 =  1,  7, 13, 19,  4, 10, 16, 22
-    packssdw      m2, m5            ; m2 =  2,  8, 14, 20,  5, 11, 17, 23
-                                    ; unpack words:
-    movhlps       m3, m0            ; m3 =  3,  9, 15, 21,  x,  x,  x,  x
-    punpcklwd     m0, m1            ; m0 =  0,  1,  6,  7, 12, 13, 18, 19
-    punpckhwd     m1, m2            ; m1 =  4,  5, 10, 11, 16, 17, 22, 23
-    punpcklwd     m2, m3            ; m2 =  2,  3,  8,  9, 14, 15, 20, 21
-                                    ; blend dwords:
-    shufps        m3, m0, m2, q2020 ; m3 =  0,  1, 12, 13,  2,  3, 14, 15
-    shufps        m0, m1, q2031     ; m0 =  6,  7, 18, 19,  4,  5, 16, 17
-    shufps        m2, m1, q3131     ; m2 =  8,  9, 20, 21, 10, 11, 22, 23
-                                    ; shuffle dwords:
-    shufps        m1, m2, m3, q3120 ; m1 =  8,  9, 10, 11, 12, 13, 14, 15
-    shufps        m3, m0,     q0220 ; m3 =  0,  1,  2,  3,  4,  5,  6,  7
-    shufps        m0, m2,     q3113 ; m0 = 16, 17, 18, 19, 20, 21, 22, 23
-    mova  [dstq+0*mmsize], m3
-    mova  [dstq+1*mmsize], m1
-    mova  [dstq+2*mmsize], m0
-%else ; sse
-    movlps      xmm0, [srcq      ]
-    movlps      xmm1, [srcq+src1q]
-    movlps      xmm2, [srcq+src2q]
-    movlps      xmm3, [srcq+src3q]
-    movlps      xmm4, [srcq+src4q]
-    movlps      xmm5, [srcq+src5q]
-    mulps       xmm0, xmm6
-    mulps       xmm1, xmm6
-    mulps       xmm2, xmm6
-    mulps       xmm3, xmm6
-    mulps       xmm4, xmm6
-    mulps       xmm5, xmm6
-    cvtps2pi     mm0, xmm0
-    cvtps2pi     mm1, xmm1
-    cvtps2pi     mm2, xmm2
-    cvtps2pi     mm3, xmm3
-    cvtps2pi     mm4, xmm4
-    cvtps2pi     mm5, xmm5
-    packssdw     mm0, mm3           ; m0 =  0,  6,  3,  9
-    packssdw     mm1, mm4           ; m1 =  1,  7,  4, 10
-    packssdw     mm2, mm5           ; m2 =  2,  8,  5, 11
-                                    ; unpack words
-    pshufw       mm3, mm0, q1032    ; m3 =  3,  9,  0,  6
-    punpcklwd    mm0, mm1           ; m0 =  0,  1,  6,  7
-    punpckhwd    mm1, mm2           ; m1 =  4,  5, 10, 11
-    punpcklwd    mm2, mm3           ; m2 =  2,  3,  8,  9
-                                    ; unpack dwords
-    pshufw       mm3, mm0, q1032    ; m3 =  6,  7,  0,  1
-    punpckldq    mm0, mm2           ; m0 =  0,  1,  2,  3 (final)
-    punpckhdq    mm2, mm1           ; m2 =  8,  9, 10, 11 (final)
-    punpckldq    mm1, mm3           ; m1 =  4,  5,  6,  7 (final)
-    mova  [dstq+0*mmsize], mm0
-    mova  [dstq+1*mmsize], mm1
-    mova  [dstq+2*mmsize], mm2
-%endif
-    add       srcq, mmsize
-    add       dstq, mmsize*3
-    sub       lend, mmsize/4
-    jg .loop
-%if mmsize == 8
-    emms
-    RET
-%else
-    REP_RET
-%endif
-%endmacro
-
-INIT_MMX sse
-CONV_FLTP_TO_S16_6CH
-INIT_XMM sse2
-CONV_FLTP_TO_S16_6CH
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-CONV_FLTP_TO_S16_6CH
-%endif
-
-;------------------------------------------------------------------------------
-; void ff_conv_fltp_to_flt_2ch(float *dst, float *const *src, int len,
-;                              int channels);
-;------------------------------------------------------------------------------
-
-%macro CONV_FLTP_TO_FLT_2CH 0
-cglobal conv_fltp_to_flt_2ch, 3,4,5, dst, src0, len, src1
-    mov  src1q, [src0q+gprsize]
-    mov  src0q, [src0q]
-    lea   lenq, [4*lend]
-    add  src0q, lenq
-    add  src1q, lenq
-    lea   dstq, [dstq+2*lenq]
-    neg   lenq
-.loop:
-    mova    m0, [src0q+lenq       ]
-    mova    m1, [src1q+lenq       ]
-    mova    m2, [src0q+lenq+mmsize]
-    mova    m3, [src1q+lenq+mmsize]
-    SBUTTERFLYPS 0, 1, 4
-    SBUTTERFLYPS 2, 3, 4
-    mova  [dstq+2*lenq+0*mmsize], m0
-    mova  [dstq+2*lenq+1*mmsize], m1
-    mova  [dstq+2*lenq+2*mmsize], m2
-    mova  [dstq+2*lenq+3*mmsize], m3
-    add   lenq, 2*mmsize
-    jl .loop
-    REP_RET
-%endmacro
-
-INIT_XMM sse
-CONV_FLTP_TO_FLT_2CH
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-CONV_FLTP_TO_FLT_2CH
-%endif
-
-;-----------------------------------------------------------------------------
-; void ff_conv_fltp_to_flt_6ch(float *dst, float *const *src, int len,
-;                              int channels);
-;-----------------------------------------------------------------------------
-
-%macro CONV_FLTP_TO_FLT_6CH 0
-cglobal conv_fltp_to_flt_6ch, 2,8,7, dst, src, src1, src2, src3, src4, src5, len
-%if ARCH_X86_64
-    mov     lend, r2d
-%else
-    %define lend dword r2m
-%endif
-    mov    src1q, [srcq+1*gprsize]
-    mov    src2q, [srcq+2*gprsize]
-    mov    src3q, [srcq+3*gprsize]
-    mov    src4q, [srcq+4*gprsize]
-    mov    src5q, [srcq+5*gprsize]
-    mov     srcq, [srcq]
-    sub    src1q, srcq
-    sub    src2q, srcq
-    sub    src3q, srcq
-    sub    src4q, srcq
-    sub    src5q, srcq
-.loop:
-    mova      m0, [srcq      ]
-    mova      m1, [srcq+src1q]
-    mova      m2, [srcq+src2q]
-    mova      m3, [srcq+src3q]
-    mova      m4, [srcq+src4q]
-    mova      m5, [srcq+src5q]
-%if cpuflag(sse4)
-    SBUTTERFLYPS 0, 1, 6
-    SBUTTERFLYPS 2, 3, 6
-    SBUTTERFLYPS 4, 5, 6
-
-    blendps   m6, m4, m0, 1100b
-    movlhps   m0, m2
-    movhlps   m4, m2
-    blendps   m2, m5, m1, 1100b
-    movlhps   m1, m3
-    movhlps   m5, m3
-
-    movaps [dstq   ], m0
-    movaps [dstq+16], m6
-    movaps [dstq+32], m4
-    movaps [dstq+48], m1
-    movaps [dstq+64], m2
-    movaps [dstq+80], m5
-%else ; mmx
-    SBUTTERFLY dq, 0, 1, 6
-    SBUTTERFLY dq, 2, 3, 6
-    SBUTTERFLY dq, 4, 5, 6
-
-    movq   [dstq   ], m0
-    movq   [dstq+ 8], m2
-    movq   [dstq+16], m4
-    movq   [dstq+24], m1
-    movq   [dstq+32], m3
-    movq   [dstq+40], m5
-%endif
-    add      srcq, mmsize
-    add      dstq, mmsize*6
-    sub      lend, mmsize/4
-    jg .loop
-%if mmsize == 8
-    emms
-    RET
-%else
-    REP_RET
-%endif
-%endmacro
-
-INIT_MMX mmx
-CONV_FLTP_TO_FLT_6CH
-INIT_XMM sse4
-CONV_FLTP_TO_FLT_6CH
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-CONV_FLTP_TO_FLT_6CH
-%endif
-
-;------------------------------------------------------------------------------
-; void ff_conv_s16_to_s16p_2ch(int16_t *const *dst, int16_t *src, int len,
-;                              int channels);
-;------------------------------------------------------------------------------
-
-%macro CONV_S16_TO_S16P_2CH 0
-cglobal conv_s16_to_s16p_2ch, 3,4,4, dst0, src, len, dst1
-    lea       lenq, [2*lend]
-    mov      dst1q, [dst0q+gprsize]
-    mov      dst0q, [dst0q        ]
-    lea       srcq, [srcq+2*lenq]
-    add      dst0q, lenq
-    add      dst1q, lenq
-    neg       lenq
-%if cpuflag(ssse3)
-    mova        m3, [pb_deinterleave_words]
-%endif
-.loop:
-    mova        m0, [srcq+2*lenq       ]  ; m0 =  0,  1,  2,  3,  4,  5,  6,  7
-    mova        m1, [srcq+2*lenq+mmsize]  ; m1 =  8,  9, 10, 11, 12, 13, 14, 15
-%if cpuflag(ssse3)
-    pshufb      m0, m3                    ; m0 =  0,  2,  4,  6,  1,  3,  5,  7
-    pshufb      m1, m3                    ; m1 =  8, 10, 12, 14,  9, 11, 13, 15
-    SBUTTERFLY2 qdq, 0, 1, 2              ; m0 =  0,  2,  4,  6,  8, 10, 12, 14
-                                          ; m1 =  1,  3,  5,  7,  9, 11, 13, 15
-%else ; sse2
-    pshuflw     m0, m0, q3120             ; m0 =  0,  2,  1,  3,  4,  5,  6,  7
-    pshufhw     m0, m0, q3120             ; m0 =  0,  2,  1,  3,  4,  6,  5,  7
-    pshuflw     m1, m1, q3120             ; m1 =  8, 10,  9, 11, 12, 13, 14, 15
-    pshufhw     m1, m1, q3120             ; m1 =  8, 10,  9, 11, 12, 14, 13, 15
-    DEINT2_PS    0, 1, 2                  ; m0 =  0,  2,  4,  6,  8, 10, 12, 14
-                                          ; m1 =  1,  3,  5,  7,  9, 11, 13, 15
-%endif
-    mova  [dst0q+lenq], m0
-    mova  [dst1q+lenq], m1
-    add       lenq, mmsize
-    jl .loop
-    REP_RET
-%endmacro
-
-INIT_XMM sse2
-CONV_S16_TO_S16P_2CH
-INIT_XMM ssse3
-CONV_S16_TO_S16P_2CH
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-CONV_S16_TO_S16P_2CH
-%endif
-
-;------------------------------------------------------------------------------
-; void ff_conv_s16_to_s16p_6ch(int16_t *const *dst, int16_t *src, int len,
-;                              int channels);
-;------------------------------------------------------------------------------
-
-%macro CONV_S16_TO_S16P_6CH 0
-%if ARCH_X86_64
-cglobal conv_s16_to_s16p_6ch, 3,8,5, dst, src, len, dst1, dst2, dst3, dst4, dst5
-%else
-cglobal conv_s16_to_s16p_6ch, 2,7,5, dst, src, dst1, dst2, dst3, dst4, dst5
-%define lend dword r2m
-%endif
-    mov     dst1q, [dstq+  gprsize]
-    mov     dst2q, [dstq+2*gprsize]
-    mov     dst3q, [dstq+3*gprsize]
-    mov     dst4q, [dstq+4*gprsize]
-    mov     dst5q, [dstq+5*gprsize]
-    mov      dstq, [dstq          ]
-    sub     dst1q, dstq
-    sub     dst2q, dstq
-    sub     dst3q, dstq
-    sub     dst4q, dstq
-    sub     dst5q, dstq
-.loop:
-    mova       m0, [srcq+0*mmsize]      ; m0 =  0,  1,  2,  3,  4,  5,  6,  7
-    mova       m3, [srcq+1*mmsize]      ; m3 =  8,  9, 10, 11, 12, 13, 14, 15
-    mova       m2, [srcq+2*mmsize]      ; m2 = 16, 17, 18, 19, 20, 21, 22, 23
-    PALIGNR    m1, m3, m0, 12, m4       ; m1 =  6,  7,  8,  9, 10, 11,  x,  x
-    shufps     m3, m2, q1032            ; m3 = 12, 13, 14, 15, 16, 17, 18, 19
-    psrldq     m2, 4                    ; m2 = 18, 19, 20, 21, 22, 23,  x,  x
-    SBUTTERFLY2 wd, 0, 1, 4             ; m0 =  0,  6,  1,  7,  2,  8,  3,  9
-                                        ; m1 =  4, 10,  5, 11,  x,  x,  x,  x
-    SBUTTERFLY2 wd, 3, 2, 4             ; m3 = 12, 18, 13, 19, 14, 20, 15, 21
-                                        ; m2 = 16, 22, 17, 23,  x,  x,  x,  x
-    SBUTTERFLY2 dq, 0, 3, 4             ; m0 =  0,  6, 12, 18,  1,  7, 13, 19
-                                        ; m3 =  2,  8, 14, 20,  3,  9, 15, 21
-    punpckldq  m1, m2                   ; m1 =  4, 10, 16, 22,  5, 11, 17, 23
-    movq    [dstq      ], m0
-    movhps  [dstq+dst1q], m0
-    movq    [dstq+dst2q], m3
-    movhps  [dstq+dst3q], m3
-    movq    [dstq+dst4q], m1
-    movhps  [dstq+dst5q], m1
-    add      srcq, mmsize*3
-    add      dstq, mmsize/2
-    sub      lend, mmsize/4
-    jg .loop
-    REP_RET
-%endmacro
-
-INIT_XMM sse2
-CONV_S16_TO_S16P_6CH
-INIT_XMM ssse3
-CONV_S16_TO_S16P_6CH
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-CONV_S16_TO_S16P_6CH
-%endif
-
-;------------------------------------------------------------------------------
-; void ff_conv_s16_to_fltp_2ch(float *const *dst, int16_t *src, int len,
-;                              int channels);
-;------------------------------------------------------------------------------
-
-%macro CONV_S16_TO_FLTP_2CH 0
-cglobal conv_s16_to_fltp_2ch, 3,4,5, dst0, src, len, dst1
-    lea       lenq, [4*lend]
-    mov      dst1q, [dst0q+gprsize]
-    mov      dst0q, [dst0q        ]
-    add       srcq, lenq
-    add      dst0q, lenq
-    add      dst1q, lenq
-    neg       lenq
-    mova        m3, [pf_s32_inv_scale]
-    mova        m4, [pw_zero_even]
-.loop:
-    mova        m1, [srcq+lenq]
-    pslld       m0, m1, 16
-    pand        m1, m4
-    cvtdq2ps    m0, m0
-    cvtdq2ps    m1, m1
-    mulps       m0, m0, m3
-    mulps       m1, m1, m3
-    mova  [dst0q+lenq], m0
-    mova  [dst1q+lenq], m1
-    add       lenq, mmsize
-    jl .loop
-    REP_RET
-%endmacro
-
-INIT_XMM sse2
-CONV_S16_TO_FLTP_2CH
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-CONV_S16_TO_FLTP_2CH
-%endif
-
-;------------------------------------------------------------------------------
-; void ff_conv_s16_to_fltp_6ch(float *const *dst, int16_t *src, int len,
-;                              int channels);
-;------------------------------------------------------------------------------
-
-%macro CONV_S16_TO_FLTP_6CH 0
-%if ARCH_X86_64
-cglobal conv_s16_to_fltp_6ch, 3,8,7, dst, src, len, dst1, dst2, dst3, dst4, dst5
-%else
-cglobal conv_s16_to_fltp_6ch, 2,7,7, dst, src, dst1, dst2, dst3, dst4, dst5
-%define lend dword r2m
-%endif
-    mov     dst1q, [dstq+  gprsize]
-    mov     dst2q, [dstq+2*gprsize]
-    mov     dst3q, [dstq+3*gprsize]
-    mov     dst4q, [dstq+4*gprsize]
-    mov     dst5q, [dstq+5*gprsize]
-    mov      dstq, [dstq          ]
-    sub     dst1q, dstq
-    sub     dst2q, dstq
-    sub     dst3q, dstq
-    sub     dst4q, dstq
-    sub     dst5q, dstq
-    mova       m6, [pf_s16_inv_scale]
-.loop:
-    mova       m0, [srcq+0*mmsize]  ; m0 =  0,  1,  2,  3,  4,  5,  6,  7
-    mova       m3, [srcq+1*mmsize]  ; m3 =  8,  9, 10, 11, 12, 13, 14, 15
-    mova       m2, [srcq+2*mmsize]  ; m2 = 16, 17, 18, 19, 20, 21, 22, 23
-    PALIGNR    m1, m3, m0, 12, m4   ; m1 =  6,  7,  8,  9, 10, 11,  x,  x
-    shufps     m3, m2, q1032        ; m3 = 12, 13, 14, 15, 16, 17, 18, 19
-    psrldq     m2, 4                ; m2 = 18, 19, 20, 21, 22, 23,  x,  x
-    SBUTTERFLY2 wd, 0, 1, 4         ; m0 =  0,  6,  1,  7,  2,  8,  3,  9
-                                    ; m1 =  4, 10,  5, 11,  x,  x,  x,  x
-    SBUTTERFLY2 wd, 3, 2, 4         ; m3 = 12, 18, 13, 19, 14, 20, 15, 21
-                                    ; m2 = 16, 22, 17, 23,  x,  x,  x,  x
-    SBUTTERFLY2 dq, 0, 3, 4         ; m0 =  0,  6, 12, 18,  1,  7, 13, 19
-                                    ; m3 =  2,  8, 14, 20,  3,  9, 15, 21
-    punpckldq  m1, m2               ; m1 =  4, 10, 16, 22,  5, 11, 17, 23
-    S16_TO_S32_SX 0, 2              ; m0 =      0,      6,     12,     18
-                                    ; m2 =      1,      7,     13,     19
-    S16_TO_S32_SX 3, 4              ; m3 =      2,      8,     14,     20
-                                    ; m4 =      3,      9,     15,     21
-    S16_TO_S32_SX 1, 5              ; m1 =      4,     10,     16,     22
-                                    ; m5 =      5,     11,     17,     23
-    SWAP 1,2,3,4
-    cvtdq2ps   m0, m0
-    cvtdq2ps   m1, m1
-    cvtdq2ps   m2, m2
-    cvtdq2ps   m3, m3
-    cvtdq2ps   m4, m4
-    cvtdq2ps   m5, m5
-    mulps      m0, m6
-    mulps      m1, m6
-    mulps      m2, m6
-    mulps      m3, m6
-    mulps      m4, m6
-    mulps      m5, m6
-    mova  [dstq      ], m0
-    mova  [dstq+dst1q], m1
-    mova  [dstq+dst2q], m2
-    mova  [dstq+dst3q], m3
-    mova  [dstq+dst4q], m4
-    mova  [dstq+dst5q], m5
-    add      srcq, mmsize*3
-    add      dstq, mmsize
-    sub      lend, mmsize/4
-    jg .loop
-    REP_RET
-%endmacro
-
-INIT_XMM sse2
-CONV_S16_TO_FLTP_6CH
-INIT_XMM ssse3
-CONV_S16_TO_FLTP_6CH
-INIT_XMM sse4
-CONV_S16_TO_FLTP_6CH
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-CONV_S16_TO_FLTP_6CH
-%endif
-
-;------------------------------------------------------------------------------
-; void ff_conv_flt_to_s16p_2ch(int16_t *const *dst, float *src, int len,
-;                              int channels);
-;------------------------------------------------------------------------------
-
-%macro CONV_FLT_TO_S16P_2CH 0
-cglobal conv_flt_to_s16p_2ch, 3,4,6, dst0, src, len, dst1
-    lea       lenq, [2*lend]
-    mov      dst1q, [dst0q+gprsize]
-    mov      dst0q, [dst0q        ]
-    lea       srcq, [srcq+4*lenq]
-    add      dst0q, lenq
-    add      dst1q, lenq
-    neg       lenq
-    mova        m5, [pf_s16_scale]
-.loop:
-    mova       m0, [srcq+4*lenq         ]
-    mova       m1, [srcq+4*lenq+  mmsize]
-    mova       m2, [srcq+4*lenq+2*mmsize]
-    mova       m3, [srcq+4*lenq+3*mmsize]
-    DEINT2_PS   0, 1, 4
-    DEINT2_PS   2, 3, 4
-    mulps      m0, m0, m5
-    mulps      m1, m1, m5
-    mulps      m2, m2, m5
-    mulps      m3, m3, m5
-    cvtps2dq   m0, m0
-    cvtps2dq   m1, m1
-    cvtps2dq   m2, m2
-    cvtps2dq   m3, m3
-    packssdw   m0, m2
-    packssdw   m1, m3
-    mova  [dst0q+lenq], m0
-    mova  [dst1q+lenq], m1
-    add      lenq, mmsize
-    jl .loop
-    REP_RET
-%endmacro
-
-INIT_XMM sse2
-CONV_FLT_TO_S16P_2CH
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-CONV_FLT_TO_S16P_2CH
-%endif
-
-;------------------------------------------------------------------------------
-; void ff_conv_flt_to_s16p_6ch(int16_t *const *dst, float *src, int len,
-;                              int channels);
-;------------------------------------------------------------------------------
-
-%macro CONV_FLT_TO_S16P_6CH 0
-%if ARCH_X86_64
-cglobal conv_flt_to_s16p_6ch, 3,8,7, dst, src, len, dst1, dst2, dst3, dst4, dst5
-%else
-cglobal conv_flt_to_s16p_6ch, 2,7,7, dst, src, dst1, dst2, dst3, dst4, dst5
-%define lend dword r2m
-%endif
-    mov     dst1q, [dstq+  gprsize]
-    mov     dst2q, [dstq+2*gprsize]
-    mov     dst3q, [dstq+3*gprsize]
-    mov     dst4q, [dstq+4*gprsize]
-    mov     dst5q, [dstq+5*gprsize]
-    mov      dstq, [dstq          ]
-    sub     dst1q, dstq
-    sub     dst2q, dstq
-    sub     dst3q, dstq
-    sub     dst4q, dstq
-    sub     dst5q, dstq
-    mova       m6, [pf_s16_scale]
-.loop:
-    mulps      m0, m6, [srcq+0*mmsize]
-    mulps      m3, m6, [srcq+1*mmsize]
-    mulps      m1, m6, [srcq+2*mmsize]
-    mulps      m4, m6, [srcq+3*mmsize]
-    mulps      m2, m6, [srcq+4*mmsize]
-    mulps      m5, m6, [srcq+5*mmsize]
-    cvtps2dq   m0, m0
-    cvtps2dq   m1, m1
-    cvtps2dq   m2, m2
-    cvtps2dq   m3, m3
-    cvtps2dq   m4, m4
-    cvtps2dq   m5, m5
-    packssdw   m0, m3               ; m0 =  0,  1,  2,  3,  4,  5,  6,  7
-    packssdw   m1, m4               ; m1 =  8,  9, 10, 11, 12, 13, 14, 15
-    packssdw   m2, m5               ; m2 = 16, 17, 18, 19, 20, 21, 22, 23
-    PALIGNR    m3, m1, m0, 12, m4   ; m3 =  6,  7,  8,  9, 10, 11,  x,  x
-    shufps     m1, m2, q1032        ; m1 = 12, 13, 14, 15, 16, 17, 18, 19
-    psrldq     m2, 4                ; m2 = 18, 19, 20, 21, 22, 23,  x,  x
-    SBUTTERFLY2 wd, 0, 3, 4         ; m0 =  0,  6,  1,  7,  2,  8,  3,  9
-                                    ; m3 =  4, 10,  5, 11,  x,  x,  x,  x
-    SBUTTERFLY2 wd, 1, 2, 4         ; m1 = 12, 18, 13, 19, 14, 20, 15, 21
-                                    ; m2 = 16, 22, 17, 23,  x,  x,  x,  x
-    SBUTTERFLY2 dq, 0, 1, 4         ; m0 =  0,  6, 12, 18,  1,  7, 13, 19
-                                    ; m1 =  2,  8, 14, 20,  3,  9, 15, 21
-    punpckldq  m3, m2               ; m3 =  4, 10, 16, 22,  5, 11, 17, 23
-    movq    [dstq      ], m0
-    movhps  [dstq+dst1q], m0
-    movq    [dstq+dst2q], m1
-    movhps  [dstq+dst3q], m1
-    movq    [dstq+dst4q], m3
-    movhps  [dstq+dst5q], m3
-    add      srcq, mmsize*6
-    add      dstq, mmsize/2
-    sub      lend, mmsize/4
-    jg .loop
-    REP_RET
-%endmacro
-
-INIT_XMM sse2
-CONV_FLT_TO_S16P_6CH
-INIT_XMM ssse3
-CONV_FLT_TO_S16P_6CH
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-CONV_FLT_TO_S16P_6CH
-%endif
-
-;------------------------------------------------------------------------------
-; void ff_conv_flt_to_fltp_2ch(float *const *dst, float *src, int len,
-;                              int channels);
-;------------------------------------------------------------------------------
-
-%macro CONV_FLT_TO_FLTP_2CH 0
-cglobal conv_flt_to_fltp_2ch, 3,4,3, dst0, src, len, dst1
-    lea    lenq, [4*lend]
-    mov   dst1q, [dst0q+gprsize]
-    mov   dst0q, [dst0q        ]
-    lea    srcq, [srcq+2*lenq]
-    add   dst0q, lenq
-    add   dst1q, lenq
-    neg    lenq
-.loop:
-    mova     m0, [srcq+2*lenq       ]
-    mova     m1, [srcq+2*lenq+mmsize]
-    DEINT2_PS 0, 1, 2
-    mova  [dst0q+lenq], m0
-    mova  [dst1q+lenq], m1
-    add    lenq, mmsize
-    jl .loop
-    REP_RET
-%endmacro
-
-INIT_XMM sse
-CONV_FLT_TO_FLTP_2CH
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-CONV_FLT_TO_FLTP_2CH
-%endif
-
-;------------------------------------------------------------------------------
-; void ff_conv_flt_to_fltp_6ch(float *const *dst, float *src, int len,
-;                              int channels);
-;------------------------------------------------------------------------------
-
-%macro CONV_FLT_TO_FLTP_6CH 0
-%if ARCH_X86_64
-cglobal conv_flt_to_fltp_6ch, 3,8,7, dst, src, len, dst1, dst2, dst3, dst4, dst5
-%else
-cglobal conv_flt_to_fltp_6ch, 2,7,7, dst, src, dst1, dst2, dst3, dst4, dst5
-%define lend dword r2m
-%endif
-    mov     dst1q, [dstq+  gprsize]
-    mov     dst2q, [dstq+2*gprsize]
-    mov     dst3q, [dstq+3*gprsize]
-    mov     dst4q, [dstq+4*gprsize]
-    mov     dst5q, [dstq+5*gprsize]
-    mov      dstq, [dstq          ]
-    sub     dst1q, dstq
-    sub     dst2q, dstq
-    sub     dst3q, dstq
-    sub     dst4q, dstq
-    sub     dst5q, dstq
-.loop:
-    mova       m0, [srcq+0*mmsize]  ; m0 =  0,  1,  2,  3
-    mova       m1, [srcq+1*mmsize]  ; m1 =  4,  5,  6,  7
-    mova       m2, [srcq+2*mmsize]  ; m2 =  8,  9, 10, 11
-    mova       m3, [srcq+3*mmsize]  ; m3 = 12, 13, 14, 15
-    mova       m4, [srcq+4*mmsize]  ; m4 = 16, 17, 18, 19
-    mova       m5, [srcq+5*mmsize]  ; m5 = 20, 21, 22, 23
-
-    SBUTTERFLY2 dq, 0, 3, 6         ; m0 =  0, 12,  1, 13
-                                    ; m3 =  2, 14,  3, 15
-    SBUTTERFLY2 dq, 1, 4, 6         ; m1 =  4, 16,  5, 17
-                                    ; m4 =  6, 18,  7, 19
-    SBUTTERFLY2 dq, 2, 5, 6         ; m2 =  8, 20,  9, 21
-                                    ; m5 = 10, 22, 11, 23
-    SBUTTERFLY2 dq, 0, 4, 6         ; m0 =  0,  6, 12, 18
-                                    ; m4 =  1,  7, 13, 19
-    SBUTTERFLY2 dq, 3, 2, 6         ; m3 =  2,  8, 14, 20
-                                    ; m2 =  3,  9, 15, 21
-    SBUTTERFLY2 dq, 1, 5, 6         ; m1 =  4, 10, 16, 22
-                                    ; m5 =  5, 11, 17, 23
-    mova [dstq      ], m0
-    mova [dstq+dst1q], m4
-    mova [dstq+dst2q], m3
-    mova [dstq+dst3q], m2
-    mova [dstq+dst4q], m1
-    mova [dstq+dst5q], m5
-    add      srcq, mmsize*6
-    add      dstq, mmsize
-    sub      lend, mmsize/4
-    jg .loop
-    REP_RET
-%endmacro
-
-INIT_XMM sse2
-CONV_FLT_TO_FLTP_6CH
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-CONV_FLT_TO_FLTP_6CH
-%endif
diff -uparN ffmpeg-4.1/libavresample/x86/audio_mix.asm ffmpeg-y/libavresample/x86/audio_mix.asm
--- ffmpeg-4.1/libavresample/x86/audio_mix.asm	2018-01-01 06:35:49.000000000 +0800
+++ ffmpeg-y/libavresample/x86/audio_mix.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,511 +0,0 @@
-;******************************************************************************
-;* x86 optimized channel mixing
-;* Copyright (c) 2012 Justin Ruggles <justin.ruggles@gmail.com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-%include "util.asm"
-
-SECTION .text
-
-;-----------------------------------------------------------------------------
-; void ff_mix_2_to_1_fltp_flt(float **src, float **matrix, int len,
-;                             int out_ch, int in_ch);
-;-----------------------------------------------------------------------------
-
-%macro MIX_2_TO_1_FLTP_FLT 0
-cglobal mix_2_to_1_fltp_flt, 3,4,6, src, matrix, len, src1
-    mov       src1q, [srcq+gprsize]
-    mov        srcq, [srcq        ]
-    sub       src1q, srcq
-    mov     matrixq, [matrixq  ]
-    VBROADCASTSS m4, [matrixq  ]
-    VBROADCASTSS m5, [matrixq+4]
-    ALIGN 16
-.loop:
-    mulps        m0, m4, [srcq             ]
-    mulps        m1, m5, [srcq+src1q       ]
-    mulps        m2, m4, [srcq+      mmsize]
-    mulps        m3, m5, [srcq+src1q+mmsize]
-    addps        m0, m0, m1
-    addps        m2, m2, m3
-    mova  [srcq       ], m0
-    mova  [srcq+mmsize], m2
-    add        srcq, mmsize*2
-    sub        lend, mmsize*2/4
-    jg .loop
-    REP_RET
-%endmacro
-
-INIT_XMM sse
-MIX_2_TO_1_FLTP_FLT
-%if HAVE_AVX_EXTERNAL
-INIT_YMM avx
-MIX_2_TO_1_FLTP_FLT
-%endif
-
-;-----------------------------------------------------------------------------
-; void ff_mix_2_to_1_s16p_flt(int16_t **src, float **matrix, int len,
-;                             int out_ch, int in_ch);
-;-----------------------------------------------------------------------------
-
-%macro MIX_2_TO_1_S16P_FLT 0
-cglobal mix_2_to_1_s16p_flt, 3,4,6, src, matrix, len, src1
-    mov       src1q, [srcq+gprsize]
-    mov        srcq, [srcq]
-    sub       src1q, srcq
-    mov     matrixq, [matrixq  ]
-    VBROADCASTSS m4, [matrixq  ]
-    VBROADCASTSS m5, [matrixq+4]
-    ALIGN 16
-.loop:
-    mova         m0, [srcq      ]
-    mova         m2, [srcq+src1q]
-    S16_TO_S32_SX 0, 1
-    S16_TO_S32_SX 2, 3
-    cvtdq2ps     m0, m0
-    cvtdq2ps     m1, m1
-    cvtdq2ps     m2, m2
-    cvtdq2ps     m3, m3
-    mulps        m0, m4
-    mulps        m1, m4
-    mulps        m2, m5
-    mulps        m3, m5
-    addps        m0, m2
-    addps        m1, m3
-    cvtps2dq     m0, m0
-    cvtps2dq     m1, m1
-    packssdw     m0, m1
-    mova     [srcq], m0
-    add        srcq, mmsize
-    sub        lend, mmsize/2
-    jg .loop
-    REP_RET
-%endmacro
-
-INIT_XMM sse2
-MIX_2_TO_1_S16P_FLT
-INIT_XMM sse4
-MIX_2_TO_1_S16P_FLT
-
-;-----------------------------------------------------------------------------
-; void ff_mix_2_to_1_s16p_q8(int16_t **src, int16_t **matrix, int len,
-;                            int out_ch, int in_ch);
-;-----------------------------------------------------------------------------
-
-INIT_XMM sse2
-cglobal mix_2_to_1_s16p_q8, 3,4,6, src, matrix, len, src1
-    mov       src1q, [srcq+gprsize]
-    mov        srcq, [srcq]
-    sub       src1q, srcq
-    mov     matrixq, [matrixq]
-    movd         m4, [matrixq]
-    movd         m5, [matrixq]
-    SPLATW       m4, m4, 0
-    SPLATW       m5, m5, 1
-    pxor         m0, m0
-    punpcklwd    m4, m0
-    punpcklwd    m5, m0
-    ALIGN 16
-.loop:
-    mova         m0, [srcq      ]
-    mova         m2, [srcq+src1q]
-    punpckhwd    m1, m0, m0
-    punpcklwd    m0, m0
-    punpckhwd    m3, m2, m2
-    punpcklwd    m2, m2
-    pmaddwd      m0, m4
-    pmaddwd      m1, m4
-    pmaddwd      m2, m5
-    pmaddwd      m3, m5
-    paddd        m0, m2
-    paddd        m1, m3
-    psrad        m0, 8
-    psrad        m1, 8
-    packssdw     m0, m1
-    mova     [srcq], m0
-    add        srcq, mmsize
-    sub        lend, mmsize/2
-    jg .loop
-    REP_RET
-
-;-----------------------------------------------------------------------------
-; void ff_mix_1_to_2_fltp_flt(float **src, float **matrix, int len,
-;                             int out_ch, int in_ch);
-;-----------------------------------------------------------------------------
-
-%macro MIX_1_TO_2_FLTP_FLT 0
-cglobal mix_1_to_2_fltp_flt, 3,5,4, src0, matrix0, len, src1, matrix1
-    mov       src1q, [src0q+gprsize]
-    mov       src0q, [src0q]
-    sub       src1q, src0q
-    mov    matrix1q, [matrix0q+gprsize]
-    mov    matrix0q, [matrix0q]
-    VBROADCASTSS m2, [matrix0q]
-    VBROADCASTSS m3, [matrix1q]
-    ALIGN 16
-.loop:
-    mova         m0, [src0q]
-    mulps        m1, m0, m3
-    mulps        m0, m0, m2
-    mova  [src0q      ], m0
-    mova  [src0q+src1q], m1
-    add       src0q, mmsize
-    sub        lend, mmsize/4
-    jg .loop
-    REP_RET
-%endmacro
-
-INIT_XMM sse
-MIX_1_TO_2_FLTP_FLT
-%if HAVE_AVX_EXTERNAL
-INIT_YMM avx
-MIX_1_TO_2_FLTP_FLT
-%endif
-
-;-----------------------------------------------------------------------------
-; void ff_mix_1_to_2_s16p_flt(int16_t **src, float **matrix, int len,
-;                             int out_ch, int in_ch);
-;-----------------------------------------------------------------------------
-
-%macro MIX_1_TO_2_S16P_FLT 0
-cglobal mix_1_to_2_s16p_flt, 3,5,6, src0, matrix0, len, src1, matrix1
-    mov       src1q, [src0q+gprsize]
-    mov       src0q, [src0q]
-    sub       src1q, src0q
-    mov    matrix1q, [matrix0q+gprsize]
-    mov    matrix0q, [matrix0q]
-    VBROADCASTSS m4, [matrix0q]
-    VBROADCASTSS m5, [matrix1q]
-    ALIGN 16
-.loop:
-    mova         m0, [src0q]
-    S16_TO_S32_SX 0, 2
-    cvtdq2ps     m0, m0
-    cvtdq2ps     m2, m2
-    mulps        m1, m0, m5
-    mulps        m0, m0, m4
-    mulps        m3, m2, m5
-    mulps        m2, m2, m4
-    cvtps2dq     m0, m0
-    cvtps2dq     m1, m1
-    cvtps2dq     m2, m2
-    cvtps2dq     m3, m3
-    packssdw     m0, m2
-    packssdw     m1, m3
-    mova  [src0q      ], m0
-    mova  [src0q+src1q], m1
-    add       src0q, mmsize
-    sub        lend, mmsize/2
-    jg .loop
-    REP_RET
-%endmacro
-
-INIT_XMM sse2
-MIX_1_TO_2_S16P_FLT
-INIT_XMM sse4
-MIX_1_TO_2_S16P_FLT
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-MIX_1_TO_2_S16P_FLT
-%endif
-
-;-----------------------------------------------------------------------------
-; void ff_mix_3_8_to_1_2_fltp/s16p_flt(float/int16_t **src, float **matrix,
-;                                      int len, int out_ch, int in_ch);
-;-----------------------------------------------------------------------------
-
-%macro MIX_3_8_TO_1_2_FLT 3 ; %1 = in channels, %2 = out channels, %3 = s16p or fltp
-; define some names to make the code clearer
-%assign  in_channels %1
-%assign out_channels %2
-%assign stereo out_channels - 1
-%ifidn %3, s16p
-    %assign is_s16 1
-%else
-    %assign is_s16 0
-%endif
-
-; determine how many matrix elements must go on the stack vs. mmregs
-%assign matrix_elements in_channels * out_channels
-%if is_s16
-    %if stereo
-        %assign needed_mmregs 7
-    %else
-        %assign needed_mmregs 5
-    %endif
-%else
-    %if stereo
-        %assign needed_mmregs 4
-    %else
-        %assign needed_mmregs 3
-    %endif
-%endif
-%assign matrix_elements_mm num_mmregs - needed_mmregs
-%if matrix_elements < matrix_elements_mm
-    %assign matrix_elements_mm matrix_elements
-%endif
-%if matrix_elements_mm < matrix_elements
-    %assign matrix_elements_stack matrix_elements - matrix_elements_mm
-%else
-    %assign matrix_elements_stack 0
-%endif
-%assign matrix_stack_size matrix_elements_stack * mmsize
-
-%assign needed_stack_size -1 * matrix_stack_size
-%if ARCH_X86_32 && in_channels >= 7
-%assign needed_stack_size needed_stack_size - 16
-%endif
-
-cglobal mix_%1_to_%2_%3_flt, 3,in_channels+2,needed_mmregs+matrix_elements_mm, needed_stack_size, src0, src1, len, src2, src3, src4, src5, src6, src7
-
-; define src pointers on stack if needed
-%if matrix_elements_stack > 0 && ARCH_X86_32 && in_channels >= 7
-    %define src5m [rsp+matrix_stack_size+0]
-    %define src6m [rsp+matrix_stack_size+4]
-    %define src7m [rsp+matrix_stack_size+8]
-%endif
-
-; load matrix pointers
-%define matrix0q r1q
-%define matrix1q r3q
-%if stereo
-    mov      matrix1q, [matrix0q+gprsize]
-%endif
-    mov      matrix0q, [matrix0q]
-
-; define matrix coeff names
-%assign %%i 0
-%assign %%j needed_mmregs
-%rep in_channels
-    %if %%i >= matrix_elements_mm
-        CAT_XDEFINE mx_stack_0_, %%i, 1
-        CAT_XDEFINE mx_0_, %%i, [rsp+(%%i-matrix_elements_mm)*mmsize]
-    %else
-        CAT_XDEFINE mx_stack_0_, %%i, 0
-        CAT_XDEFINE mx_0_, %%i, m %+ %%j
-        %assign %%j %%j+1
-    %endif
-    %assign %%i %%i+1
-%endrep
-%if stereo
-%assign %%i 0
-%rep in_channels
-    %if in_channels + %%i >= matrix_elements_mm
-        CAT_XDEFINE mx_stack_1_, %%i, 1
-        CAT_XDEFINE mx_1_, %%i, [rsp+(in_channels+%%i-matrix_elements_mm)*mmsize]
-    %else
-        CAT_XDEFINE mx_stack_1_, %%i, 0
-        CAT_XDEFINE mx_1_, %%i, m %+ %%j
-        %assign %%j %%j+1
-    %endif
-    %assign %%i %%i+1
-%endrep
-%endif
-
-; load/splat matrix coeffs
-%assign %%i 0
-%rep in_channels
-    %if mx_stack_0_ %+ %%i
-        VBROADCASTSS m0, [matrix0q+4*%%i]
-        mova  mx_0_ %+ %%i, m0
-    %else
-        VBROADCASTSS mx_0_ %+ %%i, [matrix0q+4*%%i]
-    %endif
-    %if stereo
-    %if mx_stack_1_ %+ %%i
-        VBROADCASTSS m0, [matrix1q+4*%%i]
-        mova  mx_1_ %+ %%i, m0
-    %else
-        VBROADCASTSS mx_1_ %+ %%i, [matrix1q+4*%%i]
-    %endif
-    %endif
-    %assign %%i %%i+1
-%endrep
-
-; load channel pointers to registers as offsets from the first channel pointer
-%if ARCH_X86_64
-    movsxd       lenq, r2d
-%endif
-    shl          lenq, 2-is_s16
-%assign %%i 1
-%rep (in_channels - 1)
-    %if ARCH_X86_32 && in_channels >= 7 && %%i >= 5
-    mov         src5q, [src0q+%%i*gprsize]
-    add         src5q, lenq
-    mov         src %+ %%i %+ m, src5q
-    %else
-    mov         src %+ %%i %+ q, [src0q+%%i*gprsize]
-    add         src %+ %%i %+ q, lenq
-    %endif
-    %assign %%i %%i+1
-%endrep
-    mov         src0q, [src0q]
-    add         src0q, lenq
-    neg          lenq
-.loop:
-; for x86-32 with 7-8 channels we do not have enough gp registers for all src
-; pointers, so we have to load some of them from the stack each time
-%define copy_src_from_stack ARCH_X86_32 && in_channels >= 7 && %%i >= 5
-%if is_s16
-    ; mix with s16p input
-    mova           m0, [src0q+lenq]
-    S16_TO_S32_SX   0, 1
-    cvtdq2ps       m0, m0
-    cvtdq2ps       m1, m1
-    %if stereo
-    mulps          m2, m0, mx_1_0
-    mulps          m3, m1, mx_1_0
-    %endif
-    mulps          m0, m0, mx_0_0
-    mulps          m1, m1, mx_0_0
-%assign %%i 1
-%rep (in_channels - 1)
-    %if copy_src_from_stack
-        %define src_ptr src5q
-    %else
-        %define src_ptr src %+ %%i %+ q
-    %endif
-    %if stereo
-    %if copy_src_from_stack
-    mov       src_ptr, src %+ %%i %+ m
-    %endif
-    mova           m4, [src_ptr+lenq]
-    S16_TO_S32_SX   4, 5
-    cvtdq2ps       m4, m4
-    cvtdq2ps       m5, m5
-    FMULADD_PS     m2, m4, mx_1_ %+ %%i, m2, m6
-    FMULADD_PS     m3, m5, mx_1_ %+ %%i, m3, m6
-    FMULADD_PS     m0, m4, mx_0_ %+ %%i, m0, m4
-    FMULADD_PS     m1, m5, mx_0_ %+ %%i, m1, m5
-    %else
-    %if copy_src_from_stack
-    mov       src_ptr, src %+ %%i %+ m
-    %endif
-    mova           m2, [src_ptr+lenq]
-    S16_TO_S32_SX   2, 3
-    cvtdq2ps       m2, m2
-    cvtdq2ps       m3, m3
-    FMULADD_PS     m0, m2, mx_0_ %+ %%i, m0, m4
-    FMULADD_PS     m1, m3, mx_0_ %+ %%i, m1, m4
-    %endif
-    %assign %%i %%i+1
-%endrep
-    %if stereo
-    cvtps2dq       m2, m2
-    cvtps2dq       m3, m3
-    packssdw       m2, m3
-    mova [src1q+lenq], m2
-    %endif
-    cvtps2dq       m0, m0
-    cvtps2dq       m1, m1
-    packssdw       m0, m1
-    mova [src0q+lenq], m0
-%else
-    ; mix with fltp input
-    %if stereo || mx_stack_0_0
-    mova           m0, [src0q+lenq]
-    %endif
-    %if stereo
-    mulps          m1, m0, mx_1_0
-    %endif
-    %if stereo || mx_stack_0_0
-    mulps          m0, m0, mx_0_0
-    %else
-    mulps          m0, mx_0_0, [src0q+lenq]
-    %endif
-%assign %%i 1
-%rep (in_channels - 1)
-    %if copy_src_from_stack
-        %define src_ptr src5q
-        mov   src_ptr, src %+ %%i %+ m
-    %else
-        %define src_ptr src %+ %%i %+ q
-    %endif
-    ; avoid extra load for mono if matrix is in a mm register
-    %if stereo || mx_stack_0_ %+ %%i
-    mova           m2, [src_ptr+lenq]
-    %endif
-    %if stereo
-    FMULADD_PS     m1, m2, mx_1_ %+ %%i, m1, m3
-    %endif
-    %if stereo || mx_stack_0_ %+ %%i
-    FMULADD_PS     m0, m2, mx_0_ %+ %%i, m0, m2
-    %else
-    FMULADD_PS     m0, mx_0_ %+ %%i, [src_ptr+lenq], m0, m1
-    %endif
-    %assign %%i %%i+1
-%endrep
-    mova [src0q+lenq], m0
-    %if stereo
-    mova [src1q+lenq], m1
-    %endif
-%endif
-
-    add          lenq, mmsize
-    jl .loop
-; zero ymm high halves
-%if mmsize == 32
-    vzeroupper
-%endif
-    RET
-%endmacro
-
-%macro MIX_3_8_TO_1_2_FLT_FUNCS 0
-%assign %%i 3
-%rep 6
-    INIT_XMM sse
-    MIX_3_8_TO_1_2_FLT %%i, 1, fltp
-    MIX_3_8_TO_1_2_FLT %%i, 2, fltp
-    INIT_XMM sse2
-    MIX_3_8_TO_1_2_FLT %%i, 1, s16p
-    MIX_3_8_TO_1_2_FLT %%i, 2, s16p
-    INIT_XMM sse4
-    MIX_3_8_TO_1_2_FLT %%i, 1, s16p
-    MIX_3_8_TO_1_2_FLT %%i, 2, s16p
-    ; do not use ymm AVX or FMA4 in x86-32 for 6 or more channels due to stack alignment issues
-    %if HAVE_AVX_EXTERNAL
-    %if ARCH_X86_64 || %%i < 6
-    INIT_YMM avx
-    %else
-    INIT_XMM avx
-    %endif
-    MIX_3_8_TO_1_2_FLT %%i, 1, fltp
-    MIX_3_8_TO_1_2_FLT %%i, 2, fltp
-    INIT_XMM avx
-    MIX_3_8_TO_1_2_FLT %%i, 1, s16p
-    MIX_3_8_TO_1_2_FLT %%i, 2, s16p
-    %endif
-    %if HAVE_FMA4_EXTERNAL
-    %if ARCH_X86_64 || %%i < 6
-    INIT_YMM fma4
-    %else
-    INIT_XMM fma4
-    %endif
-    MIX_3_8_TO_1_2_FLT %%i, 1, fltp
-    MIX_3_8_TO_1_2_FLT %%i, 2, fltp
-    INIT_XMM fma4
-    MIX_3_8_TO_1_2_FLT %%i, 1, s16p
-    MIX_3_8_TO_1_2_FLT %%i, 2, s16p
-    %endif
-    %assign %%i %%i+1
-%endrep
-%endmacro
-
-MIX_3_8_TO_1_2_FLT_FUNCS
diff -uparN ffmpeg-4.1/libavresample/x86/dither.asm ffmpeg-y/libavresample/x86/dither.asm
--- ffmpeg-4.1/libavresample/x86/dither.asm	2018-01-01 06:35:49.000000000 +0800
+++ ffmpeg-y/libavresample/x86/dither.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,117 +0,0 @@
-;******************************************************************************
-;* x86 optimized dithering format conversion
-;* Copyright (c) 2012 Justin Ruggles <justin.ruggles@gmail.com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA 32
-
-; 1.0f / (2.0f * INT32_MAX)
-pf_dither_scale: times 8 dd 2.32830643762e-10
-
-pf_s16_scale: times 4 dd 32753.0
-
-SECTION .text
-
-;------------------------------------------------------------------------------
-; void ff_quantize(int16_t *dst, float *src, float *dither, int len);
-;------------------------------------------------------------------------------
-
-INIT_XMM sse2
-cglobal quantize, 4,4,3, dst, src, dither, len
-    lea         lenq, [2*lend]
-    add         dstq, lenq
-    lea         srcq, [srcq+2*lenq]
-    lea      ditherq, [ditherq+2*lenq]
-    neg         lenq
-    mova          m2, [pf_s16_scale]
-.loop:
-    mulps         m0, m2, [srcq+2*lenq]
-    mulps         m1, m2, [srcq+2*lenq+mmsize]
-    addps         m0, [ditherq+2*lenq]
-    addps         m1, [ditherq+2*lenq+mmsize]
-    cvtps2dq      m0, m0
-    cvtps2dq      m1, m1
-    packssdw      m0, m1
-    mova     [dstq+lenq], m0
-    add         lenq, mmsize
-    jl .loop
-    REP_RET
-
-;------------------------------------------------------------------------------
-; void ff_dither_int_to_float_rectangular(float *dst, int *src, int len)
-;------------------------------------------------------------------------------
-
-%macro DITHER_INT_TO_FLOAT_RECTANGULAR 0
-cglobal dither_int_to_float_rectangular, 3,3,3, dst, src, len
-    lea         lenq, [4*lend]
-    add         srcq, lenq
-    add         dstq, lenq
-    neg         lenq
-    mova          m0, [pf_dither_scale]
-.loop:
-    cvtdq2ps      m1, [srcq+lenq]
-    cvtdq2ps      m2, [srcq+lenq+mmsize]
-    mulps         m1, m1, m0
-    mulps         m2, m2, m0
-    mova  [dstq+lenq], m1
-    mova  [dstq+lenq+mmsize], m2
-    add         lenq, 2*mmsize
-    jl .loop
-    REP_RET
-%endmacro
-
-INIT_XMM sse2
-DITHER_INT_TO_FLOAT_RECTANGULAR
-INIT_YMM avx
-DITHER_INT_TO_FLOAT_RECTANGULAR
-
-;------------------------------------------------------------------------------
-; void ff_dither_int_to_float_triangular(float *dst, int *src0, int len)
-;------------------------------------------------------------------------------
-
-%macro DITHER_INT_TO_FLOAT_TRIANGULAR 0
-cglobal dither_int_to_float_triangular, 3,4,5, dst, src0, len, src1
-    lea         lenq, [4*lend]
-    lea        src1q, [src0q+2*lenq]
-    add        src0q, lenq
-    add         dstq, lenq
-    neg         lenq
-    mova          m0, [pf_dither_scale]
-.loop:
-    cvtdq2ps      m1, [src0q+lenq]
-    cvtdq2ps      m2, [src0q+lenq+mmsize]
-    cvtdq2ps      m3, [src1q+lenq]
-    cvtdq2ps      m4, [src1q+lenq+mmsize]
-    addps         m1, m1, m3
-    addps         m2, m2, m4
-    mulps         m1, m1, m0
-    mulps         m2, m2, m0
-    mova  [dstq+lenq], m1
-    mova  [dstq+lenq+mmsize], m2
-    add         lenq, 2*mmsize
-    jl .loop
-    REP_RET
-%endmacro
-
-INIT_XMM sse2
-DITHER_INT_TO_FLOAT_TRIANGULAR
-INIT_YMM avx
-DITHER_INT_TO_FLOAT_TRIANGULAR
diff -uparN ffmpeg-4.1/libavresample/x86/util.asm ffmpeg-y/libavresample/x86/util.asm
--- ffmpeg-4.1/libavresample/x86/util.asm	2016-03-29 10:25:31.000000000 +0800
+++ ffmpeg-y/libavresample/x86/util.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,41 +0,0 @@
-;******************************************************************************
-;* x86 utility macros for libavresample
-;* Copyright (c) 2012 Justin Ruggles <justin.ruggles@gmail.com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%macro S16_TO_S32_SX 2 ; src/low dst, high dst
-%if cpuflag(sse4)
-    pmovsxwd     m%2, m%1
-    psrldq       m%1, 8
-    pmovsxwd     m%1, m%1
-    SWAP %1, %2
-%else
-    mova         m%2, m%1
-    punpckhwd    m%2, m%2
-    punpcklwd    m%1, m%1
-    psrad        m%2, 16
-    psrad        m%1, 16
-%endif
-%endmacro
-
-%macro DEINT2_PS 3 ; src0/even dst, src1/odd dst, temp
-    shufps  m%3, m%1, m%2, q3131
-    shufps       m%1, m%2, q2020
-    SWAP %2,%3
-%endmacro
diff -uparN ffmpeg-4.1/libavutil/avconfig.h ffmpeg-y/libavutil/avconfig.h
--- ffmpeg-4.1/libavutil/avconfig.h	1970-01-01 08:00:00.000000000 +0800
+++ ffmpeg-y/libavutil/avconfig.h	2019-06-29 11:49:36.845017667 +0800
@@ -0,0 +1,6 @@
+/* Generated by ffmpeg configure */
+#ifndef AVUTIL_AVCONFIG_H
+#define AVUTIL_AVCONFIG_H
+#define AV_HAVE_BIGENDIAN 0
+#define AV_HAVE_FAST_UNALIGNED 1
+#endif /* AVUTIL_AVCONFIG_H */
diff -uparN ffmpeg-4.1/libavutil/ffversion.h ffmpeg-y/libavutil/ffversion.h
--- ffmpeg-4.1/libavutil/ffversion.h	1970-01-01 08:00:00.000000000 +0800
+++ ffmpeg-y/libavutil/ffversion.h	2019-06-29 11:49:36.849017667 +0800
@@ -0,0 +1,5 @@
+/* Automatically generated by version.sh, do not manually edit! */
+#ifndef AVUTIL_FFVERSION_H
+#define AVUTIL_FFVERSION_H
+#define FFMPEG_VERSION "4.1"
+#endif /* AVUTIL_FFVERSION_H */
diff -uparN ffmpeg-4.1/libavutil/libavutil.version ffmpeg-y/libavutil/libavutil.version
--- ffmpeg-4.1/libavutil/libavutil.version	1970-01-01 08:00:00.000000000 +0800
+++ ffmpeg-y/libavutil/libavutil.version	2019-06-29 11:50:15.229018729 +0800
@@ -0,0 +1,3 @@
+libavutil_VERSION=56.22.100
+libavutil_VERSION_MAJOR=56
+libavutil_VERSION_MINOR=22
diff -uparN ffmpeg-4.1/libavutil/Makefile ffmpeg-y/libavutil/Makefile
--- ffmpeg-4.1/libavutil/Makefile	2018-11-06 07:22:26.000000000 +0800
+++ ffmpeg-y/libavutil/Makefile	2019-06-29 11:49:36.841017667 +0800
@@ -89,17 +89,13 @@ ARCH_HEADERS = bswap.h
 BUILT_HEADERS = avconfig.h                                              \
                 ffversion.h
 
-OBJS = adler32.o                                                        \
-       aes.o                                                            \
+OBJS = aes.o                                                            \
        aes_ctr.o                                                        \
        audio_fifo.o                                                     \
        avstring.o                                                       \
        base64.o                                                         \
-       blowfish.o                                                       \
        bprint.o                                                         \
        buffer.o                                                         \
-       cast5.o                                                          \
-       camellia.o                                                       \
        channel_layout.o                                                 \
        color_utils.o                                                    \
        cpu.o                                                            \
@@ -117,8 +113,6 @@ OBJS = adler32.o
        float_dsp.o                                                      \
        fixed_dsp.o                                                      \
        frame.o                                                          \
-       hash.o                                                           \
-       hmac.o                                                           \
        hwcontext.o                                                      \
        imgutils.o                                                       \
        integer.o                                                        \
@@ -140,10 +134,8 @@ OBJS = adler32.o
        rational.o                                                       \
        reverse.o                                                        \
        rc4.o                                                            \
-       ripemd.o                                                         \
        samplefmt.o                                                      \
        sha.o                                                            \
-       sha512.o                                                         \
        slicethread.o                                                    \
        spherical.o                                                      \
        stereo3d.o                                                       \
@@ -151,11 +143,7 @@ OBJS = adler32.o
        time.o                                                           \
        timecode.o                                                       \
        tree.o                                                           \
-       twofish.o                                                        \
        utils.o                                                          \
-       xga_font_data.o                                                  \
-       xtea.o                                                           \
-       tea.o                                                            \
 
 OBJS-$(CONFIG_CUDA)                     += hwcontext_cuda.o
 OBJS-$(CONFIG_D3D11VA)                  += hwcontext_d3d11va.o
diff -uparN ffmpeg-4.1/libavutil/x86/cpuid.asm ffmpeg-y/libavutil/x86/cpuid.asm
--- ffmpeg-4.1/libavutil/x86/cpuid.asm	2016-03-29 10:25:32.000000000 +0800
+++ ffmpeg-y/libavutil/x86/cpuid.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,91 +0,0 @@
-;*****************************************************************************
-;* Copyright (C) 2005-2010 x264 project
-;*
-;* Authors: Loren Merritt <lorenm@u.washington.edu>
-;*          Fiona Glaser <fiona@x264.com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "x86util.asm"
-
-SECTION .text
-
-;-----------------------------------------------------------------------------
-; void ff_cpu_cpuid(int index, int *eax, int *ebx, int *ecx, int *edx)
-;-----------------------------------------------------------------------------
-cglobal cpu_cpuid, 5,7
-    push rbx
-    push  r4
-    push  r3
-    push  r2
-    push  r1
-    mov  eax, r0d
-    xor  ecx, ecx
-    cpuid
-    pop   r4
-    mov [r4], eax
-    pop   r4
-    mov [r4], ebx
-    pop   r4
-    mov [r4], ecx
-    pop   r4
-    mov [r4], edx
-    pop  rbx
-    RET
-
-;-----------------------------------------------------------------------------
-; void ff_cpu_xgetbv(int op, int *eax, int *edx)
-;-----------------------------------------------------------------------------
-cglobal cpu_xgetbv, 3,7
-    push  r2
-    push  r1
-    mov  ecx, r0d
-    xgetbv
-    pop   r4
-    mov [r4], eax
-    pop   r4
-    mov [r4], edx
-    RET
-
-%if ARCH_X86_64 == 0
-;-----------------------------------------------------------------------------
-; int ff_cpu_cpuid_test(void)
-; return 0 if unsupported
-;-----------------------------------------------------------------------------
-cglobal cpu_cpuid_test
-    pushfd
-    push    ebx
-    push    ebp
-    push    esi
-    push    edi
-    pushfd
-    pop     eax
-    mov     ebx, eax
-    xor     eax, 0x200000
-    push    eax
-    popfd
-    pushfd
-    pop     eax
-    xor     eax, ebx
-    pop     edi
-    pop     esi
-    pop     ebp
-    pop     ebx
-    popfd
-    ret
-%endif
diff -uparN ffmpeg-4.1/libavutil/x86/emms.asm ffmpeg-y/libavutil/x86/emms.asm
--- ffmpeg-4.1/libavutil/x86/emms.asm	2018-11-02 02:34:28.000000000 +0800
+++ ffmpeg-y/libavutil/x86/emms.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,30 +0,0 @@
-;*****************************************************************************
-;* Copyright (C) 2013 Martin Storsjo
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "x86util.asm"
-
-SECTION .text
-
-;-----------------------------------------------------------------------------
-; void avpriv_emms_asm(void)
-;-----------------------------------------------------------------------------
-cvisible emms_asm, 0, 0
-    emms
-    RET
diff -uparN ffmpeg-4.1/libavutil/x86/fixed_dsp.asm ffmpeg-y/libavutil/x86/fixed_dsp.asm
--- ffmpeg-4.1/libavutil/x86/fixed_dsp.asm	2018-07-17 17:27:43.000000000 +0800
+++ ffmpeg-y/libavutil/x86/fixed_dsp.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,48 +0,0 @@
-;*****************************************************************************
-;* x86-optimized Float DSP functions
-;*
-;* Copyright 2016 James Almer
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "x86util.asm"
-
-SECTION .text
-
-;-----------------------------------------------------------------------------
-; void ff_butterflies_fixed(float *src0, float *src1, int len);
-;-----------------------------------------------------------------------------
-INIT_XMM sse2
-cglobal butterflies_fixed, 3,3,3, src0, src1, len
-    shl       lend, 2
-    add      src0q, lenq
-    add      src1q, lenq
-    neg       lenq
-
-align 16
-.loop:
-    mova        m0, [src0q + lenq]
-    mova        m1, [src1q + lenq]
-    mova        m2, m0
-    paddd       m0, m1
-    psubd       m2, m1
-    mova        [src0q + lenq], m0
-    mova        [src1q + lenq], m2
-    add       lenq, mmsize
-    jl .loop
-    RET
diff -uparN ffmpeg-4.1/libavutil/x86/float_dsp.asm ffmpeg-y/libavutil/x86/float_dsp.asm
--- ffmpeg-4.1/libavutil/x86/float_dsp.asm	2018-11-06 07:22:26.000000000 +0800
+++ ffmpeg-y/libavutil/x86/float_dsp.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,484 +0,0 @@
-;*****************************************************************************
-;* x86-optimized Float DSP functions
-;*
-;* Copyright 2006 Loren Merritt
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "x86util.asm"
-
-SECTION_RODATA 32
-pd_reverse: dd 7, 6, 5, 4, 3, 2, 1, 0
-
-SECTION .text
-
-;-----------------------------------------------------------------------------
-; void vector_fmul(float *dst, const float *src0, const float *src1, int len)
-;-----------------------------------------------------------------------------
-%macro VECTOR_FMUL 0
-cglobal vector_fmul, 4,4,2, dst, src0, src1, len
-    lea       lenq, [lend*4 - 64]
-ALIGN 16
-.loop:
-%assign a 0
-%rep 32/mmsize
-    mova      m0,   [src0q + lenq + (a+0)*mmsize]
-    mova      m1,   [src0q + lenq + (a+1)*mmsize]
-    mulps     m0, m0, [src1q + lenq + (a+0)*mmsize]
-    mulps     m1, m1, [src1q + lenq + (a+1)*mmsize]
-    mova      [dstq + lenq + (a+0)*mmsize], m0
-    mova      [dstq + lenq + (a+1)*mmsize], m1
-%assign a a+2
-%endrep
-
-    sub       lenq, 64
-    jge       .loop
-    REP_RET
-%endmacro
-
-INIT_XMM sse
-VECTOR_FMUL
-%if HAVE_AVX_EXTERNAL
-INIT_YMM avx
-VECTOR_FMUL
-%endif
-
-;-----------------------------------------------------------------------------
-; void vector_dmul(double *dst, const double *src0, const double *src1, int len)
-;-----------------------------------------------------------------------------
-%macro VECTOR_DMUL 0
-cglobal vector_dmul, 4,4,4, dst, src0, src1, len
-    lea       lend, [lenq*8 - mmsize*4]
-ALIGN 16
-.loop:
-    movaps    m0,     [src0q + lenq + 0*mmsize]
-    movaps    m1,     [src0q + lenq + 1*mmsize]
-    movaps    m2,     [src0q + lenq + 2*mmsize]
-    movaps    m3,     [src0q + lenq + 3*mmsize]
-    mulpd     m0, m0, [src1q + lenq + 0*mmsize]
-    mulpd     m1, m1, [src1q + lenq + 1*mmsize]
-    mulpd     m2, m2, [src1q + lenq + 2*mmsize]
-    mulpd     m3, m3, [src1q + lenq + 3*mmsize]
-    movaps    [dstq + lenq + 0*mmsize], m0
-    movaps    [dstq + lenq + 1*mmsize], m1
-    movaps    [dstq + lenq + 2*mmsize], m2
-    movaps    [dstq + lenq + 3*mmsize], m3
-
-    sub       lenq, mmsize*4
-    jge       .loop
-    RET
-%endmacro
-
-INIT_XMM sse2
-VECTOR_DMUL
-%if HAVE_AVX_EXTERNAL
-INIT_YMM avx
-VECTOR_DMUL
-%endif
-
-;------------------------------------------------------------------------------
-; void ff_vector_fmac_scalar(float *dst, const float *src, float mul, int len)
-;------------------------------------------------------------------------------
-
-%macro VECTOR_FMAC_SCALAR 0
-%if UNIX64
-cglobal vector_fmac_scalar, 3,3,5, dst, src, len
-%else
-cglobal vector_fmac_scalar, 4,4,5, dst, src, mul, len
-%endif
-%if ARCH_X86_32
-    VBROADCASTSS m0, mulm
-%else
-%if WIN64
-    SWAP 0, 2
-%endif
-    shufps      xm0, xm0, 0
-%if cpuflag(avx)
-    vinsertf128  m0, m0, xm0, 1
-%endif
-%endif
-    lea    lenq, [lend*4-64]
-.loop:
-%if cpuflag(fma3)
-    mova     m1,     [dstq+lenq]
-    mova     m2,     [dstq+lenq+1*mmsize]
-    fmaddps  m1, m0, [srcq+lenq], m1
-    fmaddps  m2, m0, [srcq+lenq+1*mmsize], m2
-%else ; cpuflag
-    mulps    m1, m0, [srcq+lenq]
-    mulps    m2, m0, [srcq+lenq+1*mmsize]
-%if mmsize < 32
-    mulps    m3, m0, [srcq+lenq+2*mmsize]
-    mulps    m4, m0, [srcq+lenq+3*mmsize]
-%endif ; mmsize
-    addps    m1, m1, [dstq+lenq]
-    addps    m2, m2, [dstq+lenq+1*mmsize]
-%if mmsize < 32
-    addps    m3, m3, [dstq+lenq+2*mmsize]
-    addps    m4, m4, [dstq+lenq+3*mmsize]
-%endif ; mmsize
-%endif ; cpuflag
-    mova  [dstq+lenq], m1
-    mova  [dstq+lenq+1*mmsize], m2
-%if mmsize < 32
-    mova  [dstq+lenq+2*mmsize], m3
-    mova  [dstq+lenq+3*mmsize], m4
-%endif ; mmsize
-    sub    lenq, 64
-    jge .loop
-    REP_RET
-%endmacro
-
-INIT_XMM sse
-VECTOR_FMAC_SCALAR
-%if HAVE_AVX_EXTERNAL
-INIT_YMM avx
-VECTOR_FMAC_SCALAR
-%endif
-%if HAVE_FMA3_EXTERNAL
-INIT_YMM fma3
-VECTOR_FMAC_SCALAR
-%endif
-
-;------------------------------------------------------------------------------
-; void ff_vector_fmul_scalar(float *dst, const float *src, float mul, int len)
-;------------------------------------------------------------------------------
-
-%macro VECTOR_FMUL_SCALAR 0
-%if UNIX64
-cglobal vector_fmul_scalar, 3,3,2, dst, src, len
-%else
-cglobal vector_fmul_scalar, 4,4,3, dst, src, mul, len
-%endif
-%if ARCH_X86_32
-    movss    m0, mulm
-%elif WIN64
-    SWAP 0, 2
-%endif
-    shufps   m0, m0, 0
-    lea    lenq, [lend*4-mmsize]
-.loop:
-    mova     m1, [srcq+lenq]
-    mulps    m1, m0
-    mova  [dstq+lenq], m1
-    sub    lenq, mmsize
-    jge .loop
-    REP_RET
-%endmacro
-
-INIT_XMM sse
-VECTOR_FMUL_SCALAR
-
-;------------------------------------------------------------------------------
-; void ff_vector_dmac_scalar(double *dst, const double *src, double mul,
-;                            int len)
-;------------------------------------------------------------------------------
-
-%macro VECTOR_DMAC_SCALAR 0
-%if ARCH_X86_32
-cglobal vector_dmac_scalar, 2,4,5, dst, src, mul, len, lenaddr
-    mov          lenq, lenaddrm
-    VBROADCASTSD m0, mulm
-%else
-%if UNIX64
-cglobal vector_dmac_scalar, 3,3,5, dst, src, len
-%else
-cglobal vector_dmac_scalar, 4,4,5, dst, src, mul, len
-    SWAP 0, 2
-%endif
-    movlhps     xm0, xm0
-%if cpuflag(avx)
-    vinsertf128  m0, m0, xm0, 1
-%endif
-%endif
-    lea    lenq, [lend*8-mmsize*4]
-.loop:
-%if cpuflag(fma3)
-    movaps   m1,     [dstq+lenq]
-    movaps   m2,     [dstq+lenq+1*mmsize]
-    movaps   m3,     [dstq+lenq+2*mmsize]
-    movaps   m4,     [dstq+lenq+3*mmsize]
-    fmaddpd  m1, m0, [srcq+lenq], m1
-    fmaddpd  m2, m0, [srcq+lenq+1*mmsize], m2
-    fmaddpd  m3, m0, [srcq+lenq+2*mmsize], m3
-    fmaddpd  m4, m0, [srcq+lenq+3*mmsize], m4
-%else ; cpuflag
-    mulpd    m1, m0, [srcq+lenq]
-    mulpd    m2, m0, [srcq+lenq+1*mmsize]
-    mulpd    m3, m0, [srcq+lenq+2*mmsize]
-    mulpd    m4, m0, [srcq+lenq+3*mmsize]
-    addpd    m1, m1, [dstq+lenq]
-    addpd    m2, m2, [dstq+lenq+1*mmsize]
-    addpd    m3, m3, [dstq+lenq+2*mmsize]
-    addpd    m4, m4, [dstq+lenq+3*mmsize]
-%endif ; cpuflag
-    movaps [dstq+lenq], m1
-    movaps [dstq+lenq+1*mmsize], m2
-    movaps [dstq+lenq+2*mmsize], m3
-    movaps [dstq+lenq+3*mmsize], m4
-    sub    lenq, mmsize*4
-    jge .loop
-    REP_RET
-%endmacro
-
-INIT_XMM sse2
-VECTOR_DMAC_SCALAR
-%if HAVE_AVX_EXTERNAL
-INIT_YMM avx
-VECTOR_DMAC_SCALAR
-%endif
-%if HAVE_FMA3_EXTERNAL
-INIT_YMM fma3
-VECTOR_DMAC_SCALAR
-%endif
-
-;------------------------------------------------------------------------------
-; void ff_vector_dmul_scalar(double *dst, const double *src, double mul,
-;                            int len)
-;------------------------------------------------------------------------------
-
-%macro VECTOR_DMUL_SCALAR 0
-%if ARCH_X86_32
-cglobal vector_dmul_scalar, 3,4,3, dst, src, mul, len, lenaddr
-    mov          lenq, lenaddrm
-%elif UNIX64
-cglobal vector_dmul_scalar, 3,3,3, dst, src, len
-%else
-cglobal vector_dmul_scalar, 4,4,3, dst, src, mul, len
-%endif
-%if ARCH_X86_32
-    VBROADCASTSD   m0, mulm
-%else
-%if WIN64
-    SWAP 0, 2
-%endif
-    movlhps       xm0, xm0
-%if cpuflag(avx)
-    vinsertf128   ym0, ym0, xm0, 1
-%endif
-%endif
-    lea          lenq, [lend*8-2*mmsize]
-.loop:
-    mulpd          m1, m0, [srcq+lenq       ]
-    mulpd          m2, m0, [srcq+lenq+mmsize]
-    movaps [dstq+lenq       ], m1
-    movaps [dstq+lenq+mmsize], m2
-    sub          lenq, 2*mmsize
-    jge .loop
-    REP_RET
-%endmacro
-
-INIT_XMM sse2
-VECTOR_DMUL_SCALAR
-%if HAVE_AVX_EXTERNAL
-INIT_YMM avx
-VECTOR_DMUL_SCALAR
-%endif
-
-;-----------------------------------------------------------------------------
-; vector_fmul_window(float *dst, const float *src0,
-;                    const float *src1, const float *win, int len);
-;-----------------------------------------------------------------------------
-%macro VECTOR_FMUL_WINDOW 0
-cglobal vector_fmul_window, 5, 6, 6, dst, src0, src1, win, len, len1
-    shl     lend, 2
-    lea    len1q, [lenq - mmsize]
-    add    src0q, lenq
-    add     dstq, lenq
-    add     winq, lenq
-    neg     lenq
-.loop:
-    mova      m0, [winq  + lenq]
-    mova      m4, [src0q + lenq]
-%if cpuflag(sse)
-    mova      m1, [winq  + len1q]
-    mova      m5, [src1q + len1q]
-    shufps    m1, m1, 0x1b
-    shufps    m5, m5, 0x1b
-    mova      m2, m0
-    mova      m3, m1
-    mulps     m2, m4
-    mulps     m3, m5
-    mulps     m1, m4
-    mulps     m0, m5
-    addps     m2, m3
-    subps     m1, m0
-    shufps    m2, m2, 0x1b
-%else
-    pswapd    m1, [winq  + len1q]
-    pswapd    m5, [src1q + len1q]
-    mova      m2, m0
-    mova      m3, m1
-    pfmul     m2, m4
-    pfmul     m3, m5
-    pfmul     m1, m4
-    pfmul     m0, m5
-    pfadd     m2, m3
-    pfsub     m1, m0
-    pswapd    m2, m2
-%endif
-    mova      [dstq + lenq], m1
-    mova      [dstq + len1q], m2
-    sub       len1q, mmsize
-    add       lenq,  mmsize
-    jl .loop
-%if mmsize == 8
-    femms
-%endif
-    REP_RET
-%endmacro
-
-INIT_MMX 3dnowext
-VECTOR_FMUL_WINDOW
-INIT_XMM sse
-VECTOR_FMUL_WINDOW
-
-;-----------------------------------------------------------------------------
-; vector_fmul_add(float *dst, const float *src0, const float *src1,
-;                 const float *src2, int len)
-;-----------------------------------------------------------------------------
-%macro VECTOR_FMUL_ADD 0
-cglobal vector_fmul_add, 5,5,4, dst, src0, src1, src2, len
-    lea       lenq, [lend*4 - 2*mmsize]
-ALIGN 16
-.loop:
-    mova    m0,   [src0q + lenq]
-    mova    m1,   [src0q + lenq + mmsize]
-%if cpuflag(fma3)
-    mova    m2,     [src2q + lenq]
-    mova    m3,     [src2q + lenq + mmsize]
-    fmaddps m0, m0, [src1q + lenq], m2
-    fmaddps m1, m1, [src1q + lenq + mmsize], m3
-%else
-    mulps   m0, m0, [src1q + lenq]
-    mulps   m1, m1, [src1q + lenq + mmsize]
-    addps   m0, m0, [src2q + lenq]
-    addps   m1, m1, [src2q + lenq + mmsize]
-%endif
-    mova    [dstq + lenq], m0
-    mova    [dstq + lenq + mmsize], m1
-
-    sub     lenq,   2*mmsize
-    jge     .loop
-    REP_RET
-%endmacro
-
-INIT_XMM sse
-VECTOR_FMUL_ADD
-%if HAVE_AVX_EXTERNAL
-INIT_YMM avx
-VECTOR_FMUL_ADD
-%endif
-%if HAVE_FMA3_EXTERNAL
-INIT_YMM fma3
-VECTOR_FMUL_ADD
-%endif
-
-;-----------------------------------------------------------------------------
-; void vector_fmul_reverse(float *dst, const float *src0, const float *src1,
-;                          int len)
-;-----------------------------------------------------------------------------
-%macro VECTOR_FMUL_REVERSE 0
-cglobal vector_fmul_reverse, 4,4,2, dst, src0, src1, len
-%if cpuflag(avx2)
-    movaps  m2, [pd_reverse]
-%endif
-    lea       lenq, [lend*4 - 2*mmsize]
-ALIGN 16
-.loop:
-%if cpuflag(avx2)
-    vpermps m0, m2, [src1q]
-    vpermps m1, m2, [src1q+mmsize]
-%elif cpuflag(avx)
-    vmovaps     xmm0, [src1q + 16]
-    vinsertf128 m0, m0, [src1q], 1
-    vshufps     m0, m0, m0, q0123
-    vmovaps     xmm1, [src1q + mmsize + 16]
-    vinsertf128 m1, m1, [src1q + mmsize], 1
-    vshufps     m1, m1, m1, q0123
-%else
-    mova    m0, [src1q]
-    mova    m1, [src1q + mmsize]
-    shufps  m0, m0, q0123
-    shufps  m1, m1, q0123
-%endif
-    mulps   m0, m0, [src0q + lenq + mmsize]
-    mulps   m1, m1, [src0q + lenq]
-    movaps  [dstq + lenq + mmsize], m0
-    movaps  [dstq + lenq], m1
-    add     src1q, 2*mmsize
-    sub     lenq,  2*mmsize
-    jge     .loop
-    REP_RET
-%endmacro
-
-INIT_XMM sse
-VECTOR_FMUL_REVERSE
-%if HAVE_AVX_EXTERNAL
-INIT_YMM avx
-VECTOR_FMUL_REVERSE
-%endif
-%if HAVE_AVX2_EXTERNAL
-INIT_YMM avx2
-VECTOR_FMUL_REVERSE
-%endif
-
-; float scalarproduct_float_sse(const float *v1, const float *v2, int len)
-INIT_XMM sse
-cglobal scalarproduct_float, 3,3,2, v1, v2, offset
-    shl   offsetd, 2
-    add       v1q, offsetq
-    add       v2q, offsetq
-    neg   offsetq
-    xorps    xmm0, xmm0
-.loop:
-    movaps   xmm1, [v1q+offsetq]
-    mulps    xmm1, [v2q+offsetq]
-    addps    xmm0, xmm1
-    add   offsetq, 16
-    js .loop
-    movhlps  xmm1, xmm0
-    addps    xmm0, xmm1
-    movss    xmm1, xmm0
-    shufps   xmm0, xmm0, 1
-    addss    xmm0, xmm1
-%if ARCH_X86_64 == 0
-    movss     r0m,  xmm0
-    fld dword r0m
-%endif
-    RET
-
-;-----------------------------------------------------------------------------
-; void ff_butterflies_float(float *src0, float *src1, int len);
-;-----------------------------------------------------------------------------
-INIT_XMM sse
-cglobal butterflies_float, 3,3,3, src0, src1, len
-    shl       lend, 2
-    add      src0q, lenq
-    add      src1q, lenq
-    neg       lenq
-.loop:
-    mova        m0, [src0q + lenq]
-    mova        m1, [src1q + lenq]
-    subps       m2, m0, m1
-    addps       m0, m0, m1
-    mova        [src1q + lenq], m2
-    mova        [src0q + lenq], m0
-    add       lenq, mmsize
-    jl .loop
-    REP_RET
diff -uparN ffmpeg-4.1/libavutil/x86/imgutils.asm ffmpeg-y/libavutil/x86/imgutils.asm
--- ffmpeg-4.1/libavutil/x86/imgutils.asm	2018-11-02 02:34:28.000000000 +0800
+++ ffmpeg-y/libavutil/x86/imgutils.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,53 +0,0 @@
-;*****************************************************************************
-;* Copyright 2016 Anton Khirnov
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION .text
-
-INIT_XMM sse4
-cglobal image_copy_plane_uc_from, 6, 7, 4, dst, dst_linesize, src, src_linesize, bw, height, rowpos
-    add dstq, bwq
-    add srcq, bwq
-    neg bwq
-
-.row_start:
-    mov rowposq, bwq
-
-.loop:
-    movntdqa m0, [srcq + rowposq + 0 * mmsize]
-    movntdqa m1, [srcq + rowposq + 1 * mmsize]
-    movntdqa m2, [srcq + rowposq + 2 * mmsize]
-    movntdqa m3, [srcq + rowposq + 3 * mmsize]
-
-    mova [dstq + rowposq + 0 * mmsize], m0
-    mova [dstq + rowposq + 1 * mmsize], m1
-    mova [dstq + rowposq + 2 * mmsize], m2
-    mova [dstq + rowposq + 3 * mmsize], m3
-
-    add rowposq, 4 * mmsize
-    jnz .loop
-
-    add srcq, src_linesizeq
-    add dstq, dst_linesizeq
-    dec heightd
-    jnz .row_start
-
-    RET
diff -uparN ffmpeg-4.1/libavutil/x86/lls.asm ffmpeg-y/libavutil/x86/lls.asm
--- ffmpeg-4.1/libavutil/x86/lls.asm	2018-07-17 17:27:43.000000000 +0800
+++ ffmpeg-y/libavutil/x86/lls.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,290 +0,0 @@
-;******************************************************************************
-;* linear least squares model
-;*
-;* Copyright (c) 2013 Loren Merritt
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "x86util.asm"
-
-SECTION .text
-
-%define MAX_VARS 32
-%define MAX_VARS_ALIGN (MAX_VARS+4)
-%define COVAR_STRIDE MAX_VARS_ALIGN*8
-%define COVAR(x,y) [covarq + (x)*8 + (y)*COVAR_STRIDE]
-
-struc LLSModel
-    .covariance:  resq MAX_VARS_ALIGN*MAX_VARS_ALIGN
-    .coeff:       resq MAX_VARS*MAX_VARS
-    .variance:    resq MAX_VARS
-    .indep_count: resd 1
-endstruc
-
-%macro ADDPD_MEM 2
-%if cpuflag(avx)
-    vaddpd %2, %2, %1
-%else
-    addpd  %2, %1
-%endif
-    mova   %1, %2
-%endmacro
-
-INIT_XMM sse2
-%define movdqa movaps
-cglobal update_lls, 2,5,8, ctx, var, i, j, covar2
-    %define covarq ctxq
-    mov     id, [ctxq + LLSModel.indep_count]
-    lea   varq, [varq + iq*8]
-    neg     iq
-    mov covar2q, covarq
-.loopi:
-    ; Compute all 3 pairwise products of a 2x2 block that lies on the diagonal
-    mova    m1, [varq + iq*8]
-    mova    m3, [varq + iq*8 + 16]
-    pshufd  m4, m1, q1010
-    pshufd  m5, m1, q3232
-    pshufd  m6, m3, q1010
-    pshufd  m7, m3, q3232
-    mulpd   m0, m1, m4
-    mulpd   m1, m1, m5
-    lea covarq, [covar2q + 16]
-    ADDPD_MEM COVAR(-2,0), m0
-    ADDPD_MEM COVAR(-2,1), m1
-    lea     jq, [iq + 2]
-    cmp     jd, -2
-    jg .skip4x4
-.loop4x4:
-    ; Compute all 16 pairwise products of a 4x4 block
-    mulpd   m0, m4, m3
-    mulpd   m1, m5, m3
-    mulpd   m2, m6, m3
-    mulpd   m3, m3, m7
-    ADDPD_MEM COVAR(0,0), m0
-    ADDPD_MEM COVAR(0,1), m1
-    ADDPD_MEM COVAR(0,2), m2
-    ADDPD_MEM COVAR(0,3), m3
-    mova    m3, [varq + jq*8 + 16]
-    mulpd   m0, m4, m3
-    mulpd   m1, m5, m3
-    mulpd   m2, m6, m3
-    mulpd   m3, m3, m7
-    ADDPD_MEM COVAR(2,0), m0
-    ADDPD_MEM COVAR(2,1), m1
-    ADDPD_MEM COVAR(2,2), m2
-    ADDPD_MEM COVAR(2,3), m3
-    mova    m3, [varq + jq*8 + 32]
-    add covarq, 32
-    add     jq, 4
-    cmp     jd, -2
-    jle .loop4x4
-.skip4x4:
-    test    jd, jd
-    jg .skip2x4
-    mulpd   m4, m3
-    mulpd   m5, m3
-    mulpd   m6, m3
-    mulpd   m7, m3
-    ADDPD_MEM COVAR(0,0), m4
-    ADDPD_MEM COVAR(0,1), m5
-    ADDPD_MEM COVAR(0,2), m6
-    ADDPD_MEM COVAR(0,3), m7
-.skip2x4:
-    add     iq, 4
-    add covar2q, 4*COVAR_STRIDE+32
-    cmp     id, -2
-    jle .loopi
-    test    id, id
-    jg .ret
-    mov     jq, iq
-    %define covarq covar2q
-.loop2x1:
-    movsd   m0, [varq + iq*8]
-    movlhps m0, m0
-    mulpd   m0, [varq + jq*8]
-    ADDPD_MEM COVAR(0,0), m0
-    inc     iq
-    add covarq, COVAR_STRIDE
-    test    id, id
-    jle .loop2x1
-.ret:
-    REP_RET
-
-%macro UPDATE_LLS 0
-cglobal update_lls, 3,6,8, ctx, var, count, i, j, count2
-    %define covarq ctxq
-    mov  countd, [ctxq + LLSModel.indep_count]
-    lea count2d, [countq-2]
-    xor     id, id
-.loopi:
-    ; Compute all 10 pairwise products of a 4x4 block that lies on the diagonal
-    mova    ymm1, [varq + iq*8]
-    vbroadcastsd ymm4, [varq + iq*8]
-    vbroadcastsd ymm5, [varq + iq*8 + 8]
-    vbroadcastsd ymm6, [varq + iq*8 + 16]
-    vbroadcastsd ymm7, [varq + iq*8 + 24]
-    vextractf128 xmm3, ymm1, 1
-%if cpuflag(fma3)
-    mova ymm0, COVAR(iq  ,0)
-    mova xmm2, COVAR(iq+2,2)
-    fmaddpd ymm0, ymm1, ymm4, ymm0
-    fmaddpd xmm2, xmm3, xmm6, xmm2
-    fmaddpd ymm1, ymm5, ymm1, COVAR(iq  ,1)
-    fmaddpd xmm3, xmm7, xmm3, COVAR(iq+2,3)
-    mova COVAR(iq  ,0), ymm0
-    mova COVAR(iq  ,1), ymm1
-    mova COVAR(iq+2,2), xmm2
-    mova COVAR(iq+2,3), xmm3
-%else
-    vmulpd  ymm0, ymm1, ymm4
-    vmulpd  ymm1, ymm1, ymm5
-    vmulpd  xmm2, xmm3, xmm6
-    vmulpd  xmm3, xmm3, xmm7
-    ADDPD_MEM COVAR(iq  ,0), ymm0
-    ADDPD_MEM COVAR(iq  ,1), ymm1
-    ADDPD_MEM COVAR(iq+2,2), xmm2
-    ADDPD_MEM COVAR(iq+2,3), xmm3
-%endif ; cpuflag(fma3)
-    lea     jd, [iq + 4]
-    cmp     jd, count2d
-    jg .skip4x4
-.loop4x4:
-    ; Compute all 16 pairwise products of a 4x4 block
-    mova    ymm3, [varq + jq*8]
-%if cpuflag(fma3)
-    mova ymm0, COVAR(jq, 0)
-    mova ymm1, COVAR(jq, 1)
-    mova ymm2, COVAR(jq, 2)
-    fmaddpd ymm0, ymm3, ymm4, ymm0
-    fmaddpd ymm1, ymm3, ymm5, ymm1
-    fmaddpd ymm2, ymm3, ymm6, ymm2
-    fmaddpd ymm3, ymm7, ymm3, COVAR(jq,3)
-    mova COVAR(jq, 0), ymm0
-    mova COVAR(jq, 1), ymm1
-    mova COVAR(jq, 2), ymm2
-    mova COVAR(jq, 3), ymm3
-%else
-    vmulpd  ymm0, ymm3, ymm4
-    vmulpd  ymm1, ymm3, ymm5
-    vmulpd  ymm2, ymm3, ymm6
-    vmulpd  ymm3, ymm3, ymm7
-    ADDPD_MEM COVAR(jq,0), ymm0
-    ADDPD_MEM COVAR(jq,1), ymm1
-    ADDPD_MEM COVAR(jq,2), ymm2
-    ADDPD_MEM COVAR(jq,3), ymm3
-%endif ; cpuflag(fma3)
-    add     jd, 4
-    cmp     jd, count2d
-    jle .loop4x4
-.skip4x4:
-    cmp     jd, countd
-    jg .skip2x4
-    mova    xmm3, [varq + jq*8]
-%if cpuflag(fma3)
-    mova xmm0, COVAR(jq, 0)
-    mova xmm1, COVAR(jq, 1)
-    mova xmm2, COVAR(jq, 2)
-    fmaddpd xmm0, xmm3, xmm4, xmm0
-    fmaddpd xmm1, xmm3, xmm5, xmm1
-    fmaddpd xmm2, xmm3, xmm6, xmm2
-    fmaddpd xmm3, xmm7, xmm3, COVAR(jq,3)
-    mova COVAR(jq, 0), xmm0
-    mova COVAR(jq, 1), xmm1
-    mova COVAR(jq, 2), xmm2
-    mova COVAR(jq, 3), xmm3
-%else
-    vmulpd  xmm0, xmm3, xmm4
-    vmulpd  xmm1, xmm3, xmm5
-    vmulpd  xmm2, xmm3, xmm6
-    vmulpd  xmm3, xmm3, xmm7
-    ADDPD_MEM COVAR(jq,0), xmm0
-    ADDPD_MEM COVAR(jq,1), xmm1
-    ADDPD_MEM COVAR(jq,2), xmm2
-    ADDPD_MEM COVAR(jq,3), xmm3
-%endif ; cpuflag(fma3)
-.skip2x4:
-    add     id, 4
-    add covarq, 4*COVAR_STRIDE
-    cmp     id, count2d
-    jle .loopi
-    cmp     id, countd
-    jg .ret
-    mov     jd, id
-.loop2x1:
-    vmovddup xmm0, [varq + iq*8]
-%if cpuflag(fma3)
-    mova xmm1, [varq + jq*8]
-    fmaddpd xmm0, xmm1, xmm0, COVAR(jq,0)
-    mova COVAR(jq,0), xmm0
-%else
-    vmulpd   xmm0, [varq + jq*8]
-    ADDPD_MEM COVAR(jq,0), xmm0
-%endif ; cpuflag(fma3)
-    inc     id
-    add covarq, COVAR_STRIDE
-    cmp     id, countd
-    jle .loop2x1
-.ret:
-    REP_RET
-%endmacro ; UPDATE_LLS
-
-%if HAVE_AVX_EXTERNAL
-INIT_YMM avx
-UPDATE_LLS
-%endif
-%if HAVE_FMA3_EXTERNAL
-INIT_YMM fma3
-UPDATE_LLS
-%endif
-
-INIT_XMM sse2
-cglobal evaluate_lls, 3,4,2, ctx, var, order, i
-    ; This function is often called on the same buffer as update_lls, but with
-    ; an offset. They can't both be aligned.
-    ; Load halves rather than movu to avoid store-forwarding stalls, since the
-    ; input was initialized immediately prior to this function using scalar math.
-    %define coefsq ctxq
-    mov     id, orderd
-    imul    orderd, MAX_VARS
-    lea     coefsq, [ctxq + LLSModel.coeff + orderq*8]
-    movsd   m0, [varq]
-    movhpd  m0, [varq + 8]
-    mulpd   m0, [coefsq]
-    lea coefsq, [coefsq + iq*8]
-    lea   varq, [varq + iq*8]
-    neg     iq
-    add     iq, 2
-.loop:
-    movsd   m1, [varq + iq*8]
-    movhpd  m1, [varq + iq*8 + 8]
-    mulpd   m1, [coefsq + iq*8]
-    addpd   m0, m1
-    add     iq, 2
-    jl .loop
-    jg .skip1
-    movsd   m1, [varq + iq*8]
-    mulsd   m1, [coefsq + iq*8]
-    addpd   m0, m1
-.skip1:
-    movhlps m1, m0
-    addsd   m0, m1
-%if ARCH_X86_32
-    movsd  r0m, m0
-    fld   qword r0m
-%endif
-    RET
diff -uparN ffmpeg-4.1/libavutil/x86/pixelutils.asm ffmpeg-y/libavutil/x86/pixelutils.asm
--- ffmpeg-4.1/libavutil/x86/pixelutils.asm	2018-11-06 07:22:26.000000000 +0800
+++ ffmpeg-y/libavutil/x86/pixelutils.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,386 +0,0 @@
-;******************************************************************************
-;* Pixel utilities SIMD
-;*
-;* Copyright (C) 2002-2004 Michael Niedermayer <michaelni@gmx.at>
-;* Copyright (C) 2014 Clément Bœsch <u pkh me>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "x86util.asm"
-
-SECTION .text
-
-;-------------------------------------------------------------------------------
-; int ff_pixelutils_sad_8x8_mmx(const uint8_t *src1, ptrdiff_t stride1,
-;                               const uint8_t *src2, ptrdiff_t stride2);
-;-------------------------------------------------------------------------------
-INIT_MMX mmx
-cglobal pixelutils_sad_8x8, 4,4,0, src1, stride1, src2, stride2
-    pxor        m7, m7
-    pxor        m6, m6
-%rep 4
-    mova        m0, [src1q]
-    mova        m2, [src1q + stride1q]
-    mova        m1, [src2q]
-    mova        m3, [src2q + stride2q]
-    psubusb     m4, m0, m1
-    psubusb     m5, m2, m3
-    psubusb     m1, m0
-    psubusb     m3, m2
-    por         m1, m4
-    por         m3, m5
-    punpcklbw   m0, m1, m7
-    punpcklbw   m2, m3, m7
-    punpckhbw   m1, m7
-    punpckhbw   m3, m7
-    paddw       m0, m1
-    paddw       m2, m3
-    paddw       m0, m2
-    paddw       m6, m0
-    lea         src1q, [src1q + 2*stride1q]
-    lea         src2q, [src2q + 2*stride2q]
-%endrep
-    psrlq       m0, m6, 32
-    paddw       m6, m0
-    psrlq       m0, m6, 16
-    paddw       m6, m0
-    movd        eax, m6
-    movzx       eax, ax
-    RET
-
-;-------------------------------------------------------------------------------
-; int ff_pixelutils_sad_8x8_mmxext(const uint8_t *src1, ptrdiff_t stride1,
-;                                  const uint8_t *src2, ptrdiff_t stride2);
-;-------------------------------------------------------------------------------
-INIT_MMX mmxext
-cglobal pixelutils_sad_8x8, 4,4,0, src1, stride1, src2, stride2
-    pxor        m2, m2
-%rep 4
-    mova        m0, [src1q]
-    mova        m1, [src1q + stride1q]
-    psadbw      m0, [src2q]
-    psadbw      m1, [src2q + stride2q]
-    paddw       m2, m0
-    paddw       m2, m1
-    lea         src1q, [src1q + 2*stride1q]
-    lea         src2q, [src2q + 2*stride2q]
-%endrep
-    movd        eax, m2
-    RET
-
-;-------------------------------------------------------------------------------
-; int ff_pixelutils_sad_16x16_mmxext(const uint8_t *src1, ptrdiff_t stride1,
-;                                    const uint8_t *src2, ptrdiff_t stride2);
-;-------------------------------------------------------------------------------
-INIT_MMX mmxext
-cglobal pixelutils_sad_16x16, 4,4,0, src1, stride1, src2, stride2
-    pxor        m2, m2
-%rep 16
-    mova        m0, [src1q]
-    mova        m1, [src1q + 8]
-    psadbw      m0, [src2q]
-    psadbw      m1, [src2q + 8]
-    paddw       m2, m0
-    paddw       m2, m1
-    add         src1q, stride1q
-    add         src2q, stride2q
-%endrep
-    movd        eax, m2
-    RET
-
-;-------------------------------------------------------------------------------
-; int ff_pixelutils_sad_16x16_sse2(const uint8_t *src1, ptrdiff_t stride1,
-;                                  const uint8_t *src2, ptrdiff_t stride2);
-;-------------------------------------------------------------------------------
-INIT_XMM sse2
-cglobal pixelutils_sad_16x16, 4,4,5, src1, stride1, src2, stride2
-    movu        m4, [src1q]
-    movu        m2, [src2q]
-    movu        m1, [src1q + stride1q]
-    movu        m3, [src2q + stride2q]
-    psadbw      m4, m2
-    psadbw      m1, m3
-    paddw       m4, m1
-%rep 7
-    lea         src1q, [src1q + 2*stride1q]
-    lea         src2q, [src2q + 2*stride2q]
-    movu        m0, [src1q]
-    movu        m2, [src2q]
-    movu        m1, [src1q + stride1q]
-    movu        m3, [src2q + stride2q]
-    psadbw      m0, m2
-    psadbw      m1, m3
-    paddw       m4, m0
-    paddw       m4, m1
-%endrep
-    movhlps     m0, m4
-    paddw       m4, m0
-    movd        eax, m4
-    RET
-
-;-------------------------------------------------------------------------------
-; int ff_pixelutils_sad_[au]_16x16_sse2(const uint8_t *src1, ptrdiff_t stride1,
-;                                       const uint8_t *src2, ptrdiff_t stride2);
-;-------------------------------------------------------------------------------
-%macro SAD_XMM_16x16 1
-INIT_XMM sse2
-cglobal pixelutils_sad_%1_16x16, 4,4,3, src1, stride1, src2, stride2
-    mov%1       m2, [src2q]
-    psadbw      m2, [src1q]
-    mov%1       m1, [src2q + stride2q]
-    psadbw      m1, [src1q + stride1q]
-    paddw       m2, m1
-%rep 7
-    lea         src1q, [src1q + 2*stride1q]
-    lea         src2q, [src2q + 2*stride2q]
-    mov%1       m0, [src2q]
-    psadbw      m0, [src1q]
-    mov%1       m1, [src2q + stride2q]
-    psadbw      m1, [src1q + stride1q]
-    paddw       m2, m0
-    paddw       m2, m1
-%endrep
-    movhlps     m0, m2
-    paddw       m2, m0
-    movd        eax, m2
-    RET
-%endmacro
-
-SAD_XMM_16x16 a
-SAD_XMM_16x16 u
-
-
-%macro PROCESS_SAD_32x4_U 0
-    movu    m1,  [r2]
-    movu    m2,  [r2 + 16]
-    movu    m3,  [r0]
-    movu    m4,  [r0 + 16]
-    psadbw  m1,  m3
-    psadbw  m2,  m4
-    paddd   m1,  m2
-    paddd   m0,  m1
-    lea     r2,  [r2 + r3]
-    lea     r0,  [r0 + r1]
-
-    movu    m1,  [r2]
-    movu    m2,  [r2 + 16]
-    movu    m3,  [r0]
-    movu    m4,  [r0 + 16]
-    psadbw  m1,  m3
-    psadbw  m2,  m4
-    paddd   m1,  m2
-    paddd   m0,  m1
-    lea     r2,  [r2 + r3]
-    lea     r0,  [r0 + r1]
-
-    movu    m1,  [r2]
-    movu    m2,  [r2 + 16]
-    movu    m3,  [r0]
-    movu    m4,  [r0 + 16]
-    psadbw  m1,  m3
-    psadbw  m2,  m4
-    paddd   m1,  m2
-    paddd   m0,  m1
-    lea     r2,  [r2 + r3]
-    lea     r0,  [r0 + r1]
-
-    movu    m1,  [r2]
-    movu    m2,  [r2 + 16]
-    movu    m3,  [r0]
-    movu    m4,  [r0 + 16]
-    psadbw  m1,  m3
-    psadbw  m2,  m4
-    paddd   m1,  m2
-    paddd   m0,  m1
-    lea     r2,  [r2 + r3]
-    lea     r0,  [r0 + r1]
-%endmacro
-
-%macro PROCESS_SAD_32x4 1
-    mov%1   m1,  [r2]
-    mov%1   m2,  [r2 + 16]
-    psadbw  m1,  [r0]
-    psadbw  m2,  [r0 + 16]
-    paddd   m1,  m2
-    paddd   m0,  m1
-    lea     r2,  [r2 + r3]
-    lea     r0,  [r0 + r1]
-
-    mov%1   m1,  [r2]
-    mov%1   m2,  [r2 + 16]
-    psadbw  m1,  [r0]
-    psadbw  m2,  [r0 + 16]
-    paddd   m1,  m2
-    paddd   m0,  m1
-    lea     r2,  [r2 + r3]
-    lea     r0,  [r0 + r1]
-
-    mov%1   m1,  [r2]
-    mov%1   m2,  [r2 + 16]
-    psadbw  m1,  [r0]
-    psadbw  m2,  [r0 + 16]
-    paddd   m1,  m2
-    paddd   m0,  m1
-    lea     r2,  [r2 + r3]
-    lea     r0,  [r0 + r1]
-
-    mov%1   m1,  [r2]
-    mov%1   m2,  [r2 + 16]
-    psadbw  m1,  [r0]
-    psadbw  m2,  [r0 + 16]
-    paddd   m1,  m2
-    paddd   m0,  m1
-    lea     r2,  [r2 + r3]
-    lea     r0,  [r0 + r1]
-%endmacro
-
-;-----------------------------------------------------------------------------
-; int ff_pixelutils_sad_32x32_sse2(const uint8_t *src1, ptrdiff_t stride1,
-;                                  const uint8_t *src2, ptrdiff_t stride2);
-;-----------------------------------------------------------------------------
-INIT_XMM sse2
-cglobal pixelutils_sad_32x32, 4,5,5, src1, stride1, src2, stride2
-    pxor  m0,  m0
-    mov   r4d, 4
-.loop:
-    PROCESS_SAD_32x4_U
-    PROCESS_SAD_32x4_U
-    dec r4d
-    jnz .loop
-
-    movhlps m1,  m0
-    paddd   m0,  m1
-    movd    eax, m0
-    RET
-
-;-------------------------------------------------------------------------------
-; int ff_pixelutils_sad_[au]_32x32_sse2(const uint8_t *src1, ptrdiff_t stride1,
-;                                       const uint8_t *src2, ptrdiff_t stride2);
-;-------------------------------------------------------------------------------
-%macro SAD_XMM_32x32 1
-INIT_XMM sse2
-cglobal pixelutils_sad_%1_32x32, 4,5,3, src1, stride1, src2, stride2
-    pxor  m0,  m0
-    mov   r4d, 4
-.loop:
-    PROCESS_SAD_32x4 %1
-    PROCESS_SAD_32x4 %1
-    dec r4d
-    jnz .loop
-
-    movhlps m1,  m0
-    paddd   m0,  m1
-    movd    eax, m0
-    RET
-%endmacro
-
-SAD_XMM_32x32 a
-SAD_XMM_32x32 u
-
-%if HAVE_AVX2_EXTERNAL
-;-------------------------------------------------------------------------------
-; int ff_pixelutils_sad_32x32_avx2(const uint8_t *src1, ptrdiff_t stride1,
-;                                  const uint8_t *src2, ptrdiff_t stride2);
-;-------------------------------------------------------------------------------
-INIT_YMM avx2
-cglobal pixelutils_sad_32x32, 4,7,5, src1, stride1, src2, stride2
-    pxor            m0, m0
-    mov             r4d, 32/4
-    lea             r5, [stride1q * 3]
-    lea             r6, [stride2q * 3]
-
-.loop:
-    movu           m1, [src1q]               ; row 0 of pix0
-    movu           m2, [src2q]               ; row 0 of pix1
-    movu           m3, [src1q + stride1q]    ; row 1 of pix0
-    movu           m4, [src2q + stride2q]    ; row 1 of pix1
-
-    psadbw         m1, m2
-    psadbw         m3, m4
-    paddd          m0, m1
-    paddd          m0, m3
-
-    movu           m1, [src1q + 2 * stride1q] ; row 2 of pix0
-    movu           m2, [src2q + 2 * stride2q] ; row 2 of pix1
-    movu           m3, [src1q + r5]           ; row 3 of pix0
-    movu           m4, [src2q + r6]           ; row 3 of pix1
-
-    psadbw         m1, m2
-    psadbw         m3, m4
-    paddd          m0, m1
-    paddd          m0, m3
-
-    lea            src2q,     [src2q + 4 * stride2q]
-    lea            src1q,     [src1q + 4 * stride1q]
-
-    dec            r4d
-    jnz           .loop
-
-    vextracti128   xm1, m0, 1
-    paddd          xm0, xm1
-    pshufd         xm1, xm0, 2
-    paddd          xm0, xm1
-    movd           eax, xm0
-    RET
-
-;-------------------------------------------------------------------------------
-; int ff_pixelutils_sad_[au]_32x32_avx2(const uint8_t *src1, ptrdiff_t stride1,
-;                                       const uint8_t *src2, ptrdiff_t stride2);
-;-------------------------------------------------------------------------------
-%macro SAD_AVX2_32x32 1
-INIT_YMM avx2
-cglobal pixelutils_sad_%1_32x32, 4,7,3, src1, stride1, src2, stride2
-    pxor           m0, m0
-    mov            r4d, 32/4
-    lea            r5, [stride1q * 3]
-    lea            r6, [stride2q * 3]
-
-.loop:
-    mov%1          m1, [src2q]                ; row 0 of pix1
-    psadbw         m1, [src1q]
-    mov%1          m2, [src2q + stride2q]     ; row 1 of pix1
-    psadbw         m2, [src1q + stride1q]
-
-    paddd          m0, m1
-    paddd          m0, m2
-
-    mov%1          m1, [src2q + 2 * stride2q] ; row 2 of pix1
-    psadbw         m1, [src1q + 2 * stride1q]
-    mov%1          m2, [src2q + r6]           ; row 3 of pix1
-    psadbw         m2, [src1q + r5]
-
-    paddd          m0, m1
-    paddd          m0, m2
-
-    lea            src2q,     [src2q + 4 * stride2q]
-    lea            src1q,     [src1q + 4 * stride1q]
-
-    dec            r4d
-    jnz           .loop
-
-    vextracti128   xm1, m0, 1
-    paddd          xm0, xm1
-    pshufd         xm1, xm0, 2
-    paddd          xm0, xm1
-    movd           eax, xm0
-    RET
-%endmacro
-
-SAD_AVX2_32x32 a
-SAD_AVX2_32x32 u
-%endif
diff -uparN ffmpeg-4.1/libavutil/x86/x86inc.asm ffmpeg-y/libavutil/x86/x86inc.asm
--- ffmpeg-4.1/libavutil/x86/x86inc.asm	2018-11-03 08:17:30.000000000 +0800
+++ ffmpeg-y/libavutil/x86/x86inc.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,1701 +0,0 @@
-;*****************************************************************************
-;* x86inc.asm: x264asm abstraction layer
-;*****************************************************************************
-;* Copyright (C) 2005-2018 x264 project
-;*
-;* Authors: Loren Merritt <lorenm@u.washington.edu>
-;*          Henrik Gramner <henrik@gramner.com>
-;*          Anton Mitrofanov <BugMaster@narod.ru>
-;*          Fiona Glaser <fiona@x264.com>
-;*
-;* Permission to use, copy, modify, and/or distribute this software for any
-;* purpose with or without fee is hereby granted, provided that the above
-;* copyright notice and this permission notice appear in all copies.
-;*
-;* THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
-;* WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
-;* MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
-;* ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
-;* WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
-;* ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
-;* OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
-;*****************************************************************************
-
-; This is a header file for the x264ASM assembly language, which uses
-; NASM/YASM syntax combined with a large number of macros to provide easy
-; abstraction between different calling conventions (x86_32, win64, linux64).
-; It also has various other useful features to simplify writing the kind of
-; DSP functions that are most often used in x264.
-
-; Unlike the rest of x264, this file is available under an ISC license, as it
-; has significant usefulness outside of x264 and we want it to be available
-; to the largest audience possible.  Of course, if you modify it for your own
-; purposes to add a new feature, we strongly encourage contributing a patch
-; as this feature might be useful for others as well.  Send patches or ideas
-; to x264-devel@videolan.org .
-
-%ifndef private_prefix
-    %define private_prefix x264
-%endif
-
-%ifndef public_prefix
-    %define public_prefix private_prefix
-%endif
-
-%if HAVE_ALIGNED_STACK
-    %define STACK_ALIGNMENT 16
-%endif
-%ifndef STACK_ALIGNMENT
-    %if ARCH_X86_64
-        %define STACK_ALIGNMENT 16
-    %else
-        %define STACK_ALIGNMENT 4
-    %endif
-%endif
-
-%define WIN64  0
-%define UNIX64 0
-%if ARCH_X86_64
-    %ifidn __OUTPUT_FORMAT__,win32
-        %define WIN64  1
-    %elifidn __OUTPUT_FORMAT__,win64
-        %define WIN64  1
-    %elifidn __OUTPUT_FORMAT__,x64
-        %define WIN64  1
-    %else
-        %define UNIX64 1
-    %endif
-%endif
-
-%define FORMAT_ELF 0
-%ifidn __OUTPUT_FORMAT__,elf
-    %define FORMAT_ELF 1
-%elifidn __OUTPUT_FORMAT__,elf32
-    %define FORMAT_ELF 1
-%elifidn __OUTPUT_FORMAT__,elf64
-    %define FORMAT_ELF 1
-%endif
-
-%ifdef PREFIX
-    %define mangle(x) _ %+ x
-%else
-    %define mangle(x) x
-%endif
-
-; aout does not support align=
-; NOTE: This section is out of sync with x264, in order to
-; keep supporting OS/2.
-%macro SECTION_RODATA 0-1 16
-    %ifidn __OUTPUT_FORMAT__,aout
-        SECTION .text
-    %elifidn __OUTPUT_FORMAT__,coff
-        SECTION .text
-    %elifidn __OUTPUT_FORMAT__,win32
-        SECTION .rdata align=%1
-    %elif WIN64
-        SECTION .rdata align=%1
-    %else
-        SECTION .rodata align=%1
-    %endif
-%endmacro
-
-%if WIN64
-    %define PIC
-%elif ARCH_X86_64 == 0
-; x86_32 doesn't require PIC.
-; Some distros prefer shared objects to be PIC, but nothing breaks if
-; the code contains a few textrels, so we'll skip that complexity.
-    %undef PIC
-%endif
-%ifdef PIC
-    default rel
-%endif
-
-%macro CPUNOP 1
-    %if HAVE_CPUNOP
-        CPU %1
-    %endif
-%endmacro
-
-; Macros to eliminate most code duplication between x86_32 and x86_64:
-; Currently this works only for leaf functions which load all their arguments
-; into registers at the start, and make no other use of the stack. Luckily that
-; covers most of x264's asm.
-
-; PROLOGUE:
-; %1 = number of arguments. loads them from stack if needed.
-; %2 = number of registers used. pushes callee-saved regs if needed.
-; %3 = number of xmm registers used. pushes callee-saved xmm regs if needed.
-; %4 = (optional) stack size to be allocated. The stack will be aligned before
-;      allocating the specified stack size. If the required stack alignment is
-;      larger than the known stack alignment the stack will be manually aligned
-;      and an extra register will be allocated to hold the original stack
-;      pointer (to not invalidate r0m etc.). To prevent the use of an extra
-;      register as stack pointer, request a negative stack size.
-; %4+/%5+ = list of names to define to registers
-; PROLOGUE can also be invoked by adding the same options to cglobal
-
-; e.g.
-; cglobal foo, 2,3,7,0x40, dst, src, tmp
-; declares a function (foo) that automatically loads two arguments (dst and
-; src) into registers, uses one additional register (tmp) plus 7 vector
-; registers (m0-m6) and allocates 0x40 bytes of stack space.
-
-; TODO Some functions can use some args directly from the stack. If they're the
-; last args then you can just not declare them, but if they're in the middle
-; we need more flexible macro.
-
-; RET:
-; Pops anything that was pushed by PROLOGUE, and returns.
-
-; REP_RET:
-; Use this instead of RET if it's a branch target.
-
-; registers:
-; rN and rNq are the native-size register holding function argument N
-; rNd, rNw, rNb are dword, word, and byte size
-; rNh is the high 8 bits of the word size
-; rNm is the original location of arg N (a register or on the stack), dword
-; rNmp is native size
-
-%macro DECLARE_REG 2-3
-    %define r%1q %2
-    %define r%1d %2d
-    %define r%1w %2w
-    %define r%1b %2b
-    %define r%1h %2h
-    %define %2q %2
-    %if %0 == 2
-        %define r%1m  %2d
-        %define r%1mp %2
-    %elif ARCH_X86_64 ; memory
-        %define r%1m [rstk + stack_offset + %3]
-        %define r%1mp qword r %+ %1 %+ m
-    %else
-        %define r%1m [rstk + stack_offset + %3]
-        %define r%1mp dword r %+ %1 %+ m
-    %endif
-    %define r%1  %2
-%endmacro
-
-%macro DECLARE_REG_SIZE 3
-    %define r%1q r%1
-    %define e%1q r%1
-    %define r%1d e%1
-    %define e%1d e%1
-    %define r%1w %1
-    %define e%1w %1
-    %define r%1h %3
-    %define e%1h %3
-    %define r%1b %2
-    %define e%1b %2
-    %if ARCH_X86_64 == 0
-        %define r%1 e%1
-    %endif
-%endmacro
-
-DECLARE_REG_SIZE ax, al, ah
-DECLARE_REG_SIZE bx, bl, bh
-DECLARE_REG_SIZE cx, cl, ch
-DECLARE_REG_SIZE dx, dl, dh
-DECLARE_REG_SIZE si, sil, null
-DECLARE_REG_SIZE di, dil, null
-DECLARE_REG_SIZE bp, bpl, null
-
-; t# defines for when per-arch register allocation is more complex than just function arguments
-
-%macro DECLARE_REG_TMP 1-*
-    %assign %%i 0
-    %rep %0
-        CAT_XDEFINE t, %%i, r%1
-        %assign %%i %%i+1
-        %rotate 1
-    %endrep
-%endmacro
-
-%macro DECLARE_REG_TMP_SIZE 0-*
-    %rep %0
-        %define t%1q t%1 %+ q
-        %define t%1d t%1 %+ d
-        %define t%1w t%1 %+ w
-        %define t%1h t%1 %+ h
-        %define t%1b t%1 %+ b
-        %rotate 1
-    %endrep
-%endmacro
-
-DECLARE_REG_TMP_SIZE 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14
-
-%if ARCH_X86_64
-    %define gprsize 8
-%else
-    %define gprsize 4
-%endif
-
-%macro PUSH 1
-    push %1
-    %ifidn rstk, rsp
-        %assign stack_offset stack_offset+gprsize
-    %endif
-%endmacro
-
-%macro POP 1
-    pop %1
-    %ifidn rstk, rsp
-        %assign stack_offset stack_offset-gprsize
-    %endif
-%endmacro
-
-%macro PUSH_IF_USED 1-*
-    %rep %0
-        %if %1 < regs_used
-            PUSH r%1
-        %endif
-        %rotate 1
-    %endrep
-%endmacro
-
-%macro POP_IF_USED 1-*
-    %rep %0
-        %if %1 < regs_used
-            pop r%1
-        %endif
-        %rotate 1
-    %endrep
-%endmacro
-
-%macro LOAD_IF_USED 1-*
-    %rep %0
-        %if %1 < num_args
-            mov r%1, r %+ %1 %+ mp
-        %endif
-        %rotate 1
-    %endrep
-%endmacro
-
-%macro SUB 2
-    sub %1, %2
-    %ifidn %1, rstk
-        %assign stack_offset stack_offset+(%2)
-    %endif
-%endmacro
-
-%macro ADD 2
-    add %1, %2
-    %ifidn %1, rstk
-        %assign stack_offset stack_offset-(%2)
-    %endif
-%endmacro
-
-%macro movifnidn 2
-    %ifnidn %1, %2
-        mov %1, %2
-    %endif
-%endmacro
-
-%macro movsxdifnidn 2
-    %ifnidn %1, %2
-        movsxd %1, %2
-    %endif
-%endmacro
-
-%macro ASSERT 1
-    %if (%1) == 0
-        %error assertion ``%1'' failed
-    %endif
-%endmacro
-
-%macro DEFINE_ARGS 0-*
-    %ifdef n_arg_names
-        %assign %%i 0
-        %rep n_arg_names
-            CAT_UNDEF arg_name %+ %%i, q
-            CAT_UNDEF arg_name %+ %%i, d
-            CAT_UNDEF arg_name %+ %%i, w
-            CAT_UNDEF arg_name %+ %%i, h
-            CAT_UNDEF arg_name %+ %%i, b
-            CAT_UNDEF arg_name %+ %%i, m
-            CAT_UNDEF arg_name %+ %%i, mp
-            CAT_UNDEF arg_name, %%i
-            %assign %%i %%i+1
-        %endrep
-    %endif
-
-    %xdefine %%stack_offset stack_offset
-    %undef stack_offset ; so that the current value of stack_offset doesn't get baked in by xdefine
-    %assign %%i 0
-    %rep %0
-        %xdefine %1q r %+ %%i %+ q
-        %xdefine %1d r %+ %%i %+ d
-        %xdefine %1w r %+ %%i %+ w
-        %xdefine %1h r %+ %%i %+ h
-        %xdefine %1b r %+ %%i %+ b
-        %xdefine %1m r %+ %%i %+ m
-        %xdefine %1mp r %+ %%i %+ mp
-        CAT_XDEFINE arg_name, %%i, %1
-        %assign %%i %%i+1
-        %rotate 1
-    %endrep
-    %xdefine stack_offset %%stack_offset
-    %assign n_arg_names %0
-%endmacro
-
-%define required_stack_alignment ((mmsize + 15) & ~15)
-%define vzeroupper_required (mmsize > 16 && (ARCH_X86_64 == 0 || xmm_regs_used > 16 || notcpuflag(avx512)))
-%define high_mm_regs (16*cpuflag(avx512))
-
-%macro ALLOC_STACK 1-2 0 ; stack_size, n_xmm_regs (for win64 only)
-    %ifnum %1
-        %if %1 != 0
-            %assign %%pad 0
-            %assign stack_size %1
-            %if stack_size < 0
-                %assign stack_size -stack_size
-            %endif
-            %if WIN64
-                %assign %%pad %%pad + 32 ; shadow space
-                %if mmsize != 8
-                    %assign xmm_regs_used %2
-                    %if xmm_regs_used > 8
-                        %assign %%pad %%pad + (xmm_regs_used-8)*16 ; callee-saved xmm registers
-                    %endif
-                %endif
-            %endif
-            %if required_stack_alignment <= STACK_ALIGNMENT
-                ; maintain the current stack alignment
-                %assign stack_size_padded stack_size + %%pad + ((-%%pad-stack_offset-gprsize) & (STACK_ALIGNMENT-1))
-                SUB rsp, stack_size_padded
-            %else
-                %assign %%reg_num (regs_used - 1)
-                %xdefine rstk r %+ %%reg_num
-                ; align stack, and save original stack location directly above
-                ; it, i.e. in [rsp+stack_size_padded], so we can restore the
-                ; stack in a single instruction (i.e. mov rsp, rstk or mov
-                ; rsp, [rsp+stack_size_padded])
-                %if %1 < 0 ; need to store rsp on stack
-                    %xdefine rstkm [rsp + stack_size + %%pad]
-                    %assign %%pad %%pad + gprsize
-                %else ; can keep rsp in rstk during whole function
-                    %xdefine rstkm rstk
-                %endif
-                %assign stack_size_padded stack_size + ((%%pad + required_stack_alignment-1) & ~(required_stack_alignment-1))
-                mov rstk, rsp
-                and rsp, ~(required_stack_alignment-1)
-                sub rsp, stack_size_padded
-                movifnidn rstkm, rstk
-            %endif
-            WIN64_PUSH_XMM
-        %endif
-    %endif
-%endmacro
-
-%macro SETUP_STACK_POINTER 1
-    %ifnum %1
-        %if %1 != 0 && required_stack_alignment > STACK_ALIGNMENT
-            %if %1 > 0
-                ; Reserve an additional register for storing the original stack pointer, but avoid using
-                ; eax/rax for this purpose since it can potentially get overwritten as a return value.
-                %assign regs_used (regs_used + 1)
-                %if ARCH_X86_64 && regs_used == 7
-                    %assign regs_used 8
-                %elif ARCH_X86_64 == 0 && regs_used == 1
-                    %assign regs_used 2
-                %endif
-            %endif
-            %if ARCH_X86_64 && regs_used < 5 + UNIX64 * 3
-                ; Ensure that we don't clobber any registers containing arguments. For UNIX64 we also preserve r6 (rax)
-                ; since it's used as a hidden argument in vararg functions to specify the number of vector registers used.
-                %assign regs_used 5 + UNIX64 * 3
-            %endif
-        %endif
-    %endif
-%endmacro
-
-%macro DEFINE_ARGS_INTERNAL 3+
-    %ifnum %2
-        DEFINE_ARGS %3
-    %elif %1 == 4
-        DEFINE_ARGS %2
-    %elif %1 > 4
-        DEFINE_ARGS %2, %3
-    %endif
-%endmacro
-
-%if WIN64 ; Windows x64 ;=================================================
-
-DECLARE_REG 0,  rcx
-DECLARE_REG 1,  rdx
-DECLARE_REG 2,  R8
-DECLARE_REG 3,  R9
-DECLARE_REG 4,  R10, 40
-DECLARE_REG 5,  R11, 48
-DECLARE_REG 6,  rax, 56
-DECLARE_REG 7,  rdi, 64
-DECLARE_REG 8,  rsi, 72
-DECLARE_REG 9,  rbx, 80
-DECLARE_REG 10, rbp, 88
-DECLARE_REG 11, R14, 96
-DECLARE_REG 12, R15, 104
-DECLARE_REG 13, R12, 112
-DECLARE_REG 14, R13, 120
-
-%macro PROLOGUE 2-5+ 0 ; #args, #regs, #xmm_regs, [stack_size,] arg_names...
-    %assign num_args %1
-    %assign regs_used %2
-    ASSERT regs_used >= num_args
-    SETUP_STACK_POINTER %4
-    ASSERT regs_used <= 15
-    PUSH_IF_USED 7, 8, 9, 10, 11, 12, 13, 14
-    ALLOC_STACK %4, %3
-    %if mmsize != 8 && stack_size == 0
-        WIN64_SPILL_XMM %3
-    %endif
-    LOAD_IF_USED 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14
-    DEFINE_ARGS_INTERNAL %0, %4, %5
-%endmacro
-
-%macro WIN64_PUSH_XMM 0
-    ; Use the shadow space to store XMM6 and XMM7, the rest needs stack space allocated.
-    %if xmm_regs_used > 6 + high_mm_regs
-        movaps [rstk + stack_offset +  8], xmm6
-    %endif
-    %if xmm_regs_used > 7 + high_mm_regs
-        movaps [rstk + stack_offset + 24], xmm7
-    %endif
-    %assign %%xmm_regs_on_stack xmm_regs_used - high_mm_regs - 8
-    %if %%xmm_regs_on_stack > 0
-        %assign %%i 8
-        %rep %%xmm_regs_on_stack
-            movaps [rsp + (%%i-8)*16 + stack_size + 32], xmm %+ %%i
-            %assign %%i %%i+1
-        %endrep
-    %endif
-%endmacro
-
-%macro WIN64_SPILL_XMM 1
-    %assign xmm_regs_used %1
-    ASSERT xmm_regs_used <= 16 + high_mm_regs
-    %assign %%xmm_regs_on_stack xmm_regs_used - high_mm_regs - 8
-    %if %%xmm_regs_on_stack > 0
-        ; Allocate stack space for callee-saved xmm registers plus shadow space and align the stack.
-        %assign %%pad %%xmm_regs_on_stack*16 + 32
-        %assign stack_size_padded %%pad + ((-%%pad-stack_offset-gprsize) & (STACK_ALIGNMENT-1))
-        SUB rsp, stack_size_padded
-    %endif
-    WIN64_PUSH_XMM
-%endmacro
-
-%macro WIN64_RESTORE_XMM_INTERNAL 0
-    %assign %%pad_size 0
-    %assign %%xmm_regs_on_stack xmm_regs_used - high_mm_regs - 8
-    %if %%xmm_regs_on_stack > 0
-        %assign %%i xmm_regs_used - high_mm_regs
-        %rep %%xmm_regs_on_stack
-            %assign %%i %%i-1
-            movaps xmm %+ %%i, [rsp + (%%i-8)*16 + stack_size + 32]
-        %endrep
-    %endif
-    %if stack_size_padded > 0
-        %if stack_size > 0 && required_stack_alignment > STACK_ALIGNMENT
-            mov rsp, rstkm
-        %else
-            add rsp, stack_size_padded
-            %assign %%pad_size stack_size_padded
-        %endif
-    %endif
-    %if xmm_regs_used > 7 + high_mm_regs
-        movaps xmm7, [rsp + stack_offset - %%pad_size + 24]
-    %endif
-    %if xmm_regs_used > 6 + high_mm_regs
-        movaps xmm6, [rsp + stack_offset - %%pad_size +  8]
-    %endif
-%endmacro
-
-%macro WIN64_RESTORE_XMM 0
-    WIN64_RESTORE_XMM_INTERNAL
-    %assign stack_offset (stack_offset-stack_size_padded)
-    %assign stack_size_padded 0
-    %assign xmm_regs_used 0
-%endmacro
-
-%define has_epilogue regs_used > 7 || stack_size > 0 || vzeroupper_required || xmm_regs_used > 6+high_mm_regs
-
-%macro RET 0
-    WIN64_RESTORE_XMM_INTERNAL
-    POP_IF_USED 14, 13, 12, 11, 10, 9, 8, 7
-    %if vzeroupper_required
-        vzeroupper
-    %endif
-    AUTO_REP_RET
-%endmacro
-
-%elif ARCH_X86_64 ; *nix x64 ;=============================================
-
-DECLARE_REG 0,  rdi
-DECLARE_REG 1,  rsi
-DECLARE_REG 2,  rdx
-DECLARE_REG 3,  rcx
-DECLARE_REG 4,  R8
-DECLARE_REG 5,  R9
-DECLARE_REG 6,  rax, 8
-DECLARE_REG 7,  R10, 16
-DECLARE_REG 8,  R11, 24
-DECLARE_REG 9,  rbx, 32
-DECLARE_REG 10, rbp, 40
-DECLARE_REG 11, R14, 48
-DECLARE_REG 12, R15, 56
-DECLARE_REG 13, R12, 64
-DECLARE_REG 14, R13, 72
-
-%macro PROLOGUE 2-5+ 0 ; #args, #regs, #xmm_regs, [stack_size,] arg_names...
-    %assign num_args %1
-    %assign regs_used %2
-    %assign xmm_regs_used %3
-    ASSERT regs_used >= num_args
-    SETUP_STACK_POINTER %4
-    ASSERT regs_used <= 15
-    PUSH_IF_USED 9, 10, 11, 12, 13, 14
-    ALLOC_STACK %4
-    LOAD_IF_USED 6, 7, 8, 9, 10, 11, 12, 13, 14
-    DEFINE_ARGS_INTERNAL %0, %4, %5
-%endmacro
-
-%define has_epilogue regs_used > 9 || stack_size > 0 || vzeroupper_required
-
-%macro RET 0
-    %if stack_size_padded > 0
-        %if required_stack_alignment > STACK_ALIGNMENT
-            mov rsp, rstkm
-        %else
-            add rsp, stack_size_padded
-        %endif
-    %endif
-    POP_IF_USED 14, 13, 12, 11, 10, 9
-    %if vzeroupper_required
-        vzeroupper
-    %endif
-    AUTO_REP_RET
-%endmacro
-
-%else ; X86_32 ;==============================================================
-
-DECLARE_REG 0, eax, 4
-DECLARE_REG 1, ecx, 8
-DECLARE_REG 2, edx, 12
-DECLARE_REG 3, ebx, 16
-DECLARE_REG 4, esi, 20
-DECLARE_REG 5, edi, 24
-DECLARE_REG 6, ebp, 28
-%define rsp esp
-
-%macro DECLARE_ARG 1-*
-    %rep %0
-        %define r%1m [rstk + stack_offset + 4*%1 + 4]
-        %define r%1mp dword r%1m
-        %rotate 1
-    %endrep
-%endmacro
-
-DECLARE_ARG 7, 8, 9, 10, 11, 12, 13, 14
-
-%macro PROLOGUE 2-5+ ; #args, #regs, #xmm_regs, [stack_size,] arg_names...
-    %assign num_args %1
-    %assign regs_used %2
-    ASSERT regs_used >= num_args
-    %if num_args > 7
-        %assign num_args 7
-    %endif
-    %if regs_used > 7
-        %assign regs_used 7
-    %endif
-    SETUP_STACK_POINTER %4
-    ASSERT regs_used <= 7
-    PUSH_IF_USED 3, 4, 5, 6
-    ALLOC_STACK %4
-    LOAD_IF_USED 0, 1, 2, 3, 4, 5, 6
-    DEFINE_ARGS_INTERNAL %0, %4, %5
-%endmacro
-
-%define has_epilogue regs_used > 3 || stack_size > 0 || vzeroupper_required
-
-%macro RET 0
-    %if stack_size_padded > 0
-        %if required_stack_alignment > STACK_ALIGNMENT
-            mov rsp, rstkm
-        %else
-            add rsp, stack_size_padded
-        %endif
-    %endif
-    POP_IF_USED 6, 5, 4, 3
-    %if vzeroupper_required
-        vzeroupper
-    %endif
-    AUTO_REP_RET
-%endmacro
-
-%endif ;======================================================================
-
-%if WIN64 == 0
-    %macro WIN64_SPILL_XMM 1
-    %endmacro
-    %macro WIN64_RESTORE_XMM 0
-    %endmacro
-    %macro WIN64_PUSH_XMM 0
-    %endmacro
-%endif
-
-; On AMD cpus <=K10, an ordinary ret is slow if it immediately follows either
-; a branch or a branch target. So switch to a 2-byte form of ret in that case.
-; We can automatically detect "follows a branch", but not a branch target.
-; (SSSE3 is a sufficient condition to know that your cpu doesn't have this problem.)
-%macro REP_RET 0
-    %if has_epilogue || cpuflag(ssse3)
-        RET
-    %else
-        rep ret
-    %endif
-    annotate_function_size
-%endmacro
-
-%define last_branch_adr $$
-%macro AUTO_REP_RET 0
-    %if notcpuflag(ssse3)
-        times ((last_branch_adr-$)>>31)+1 rep ; times 1 iff $ == last_branch_adr.
-    %endif
-    ret
-    annotate_function_size
-%endmacro
-
-%macro BRANCH_INSTR 0-*
-    %rep %0
-        %macro %1 1-2 %1
-            %2 %1
-            %if notcpuflag(ssse3)
-                %%branch_instr equ $
-                %xdefine last_branch_adr %%branch_instr
-            %endif
-        %endmacro
-        %rotate 1
-    %endrep
-%endmacro
-
-BRANCH_INSTR jz, je, jnz, jne, jl, jle, jnl, jnle, jg, jge, jng, jnge, ja, jae, jna, jnae, jb, jbe, jnb, jnbe, jc, jnc, js, jns, jo, jno, jp, jnp
-
-%macro TAIL_CALL 2 ; callee, is_nonadjacent
-    %if has_epilogue
-        call %1
-        RET
-    %elif %2
-        jmp %1
-    %endif
-    annotate_function_size
-%endmacro
-
-;=============================================================================
-; arch-independent part
-;=============================================================================
-
-%assign function_align 16
-
-; Begin a function.
-; Applies any symbol mangling needed for C linkage, and sets up a define such that
-; subsequent uses of the function name automatically refer to the mangled version.
-; Appends cpuflags to the function name if cpuflags has been specified.
-; The "" empty default parameter is a workaround for nasm, which fails if SUFFIX
-; is empty and we call cglobal_internal with just %1 %+ SUFFIX (without %2).
-%macro cglobal 1-2+ "" ; name, [PROLOGUE args]
-    cglobal_internal 1, %1 %+ SUFFIX, %2
-%endmacro
-%macro cvisible 1-2+ "" ; name, [PROLOGUE args]
-    cglobal_internal 0, %1 %+ SUFFIX, %2
-%endmacro
-%macro cglobal_internal 2-3+
-    annotate_function_size
-    %if %1
-        %xdefine %%FUNCTION_PREFIX private_prefix
-        %xdefine %%VISIBILITY hidden
-    %else
-        %xdefine %%FUNCTION_PREFIX public_prefix
-        %xdefine %%VISIBILITY
-    %endif
-    %ifndef cglobaled_%2
-        %xdefine %2 mangle(%%FUNCTION_PREFIX %+ _ %+ %2)
-        %xdefine %2.skip_prologue %2 %+ .skip_prologue
-        CAT_XDEFINE cglobaled_, %2, 1
-    %endif
-    %xdefine current_function %2
-    %xdefine current_function_section __SECT__
-    %if FORMAT_ELF
-        global %2:function %%VISIBILITY
-    %else
-        global %2
-    %endif
-    align function_align
-    %2:
-    RESET_MM_PERMUTATION        ; needed for x86-64, also makes disassembly somewhat nicer
-    %xdefine rstk rsp           ; copy of the original stack pointer, used when greater alignment than the known stack alignment is required
-    %assign stack_offset 0      ; stack pointer offset relative to the return address
-    %assign stack_size 0        ; amount of stack space that can be freely used inside a function
-    %assign stack_size_padded 0 ; total amount of allocated stack space, including space for callee-saved xmm registers on WIN64 and alignment padding
-    %assign xmm_regs_used 0     ; number of XMM registers requested, used for dealing with callee-saved registers on WIN64 and vzeroupper
-    %ifnidn %3, ""
-        PROLOGUE %3
-    %endif
-%endmacro
-
-; Create a global symbol from a local label with the correct name mangling and type
-%macro cglobal_label 1
-    %if FORMAT_ELF
-        global current_function %+ %1:function hidden
-    %else
-        global current_function %+ %1
-    %endif
-    %1:
-%endmacro
-
-%macro cextern 1
-    %xdefine %1 mangle(private_prefix %+ _ %+ %1)
-    CAT_XDEFINE cglobaled_, %1, 1
-    extern %1
-%endmacro
-
-; like cextern, but without the prefix
-%macro cextern_naked 1
-    %ifdef PREFIX
-        %xdefine %1 mangle(%1)
-    %endif
-    CAT_XDEFINE cglobaled_, %1, 1
-    extern %1
-%endmacro
-
-%macro const 1-2+
-    %xdefine %1 mangle(private_prefix %+ _ %+ %1)
-    %if FORMAT_ELF
-        global %1:data hidden
-    %else
-        global %1
-    %endif
-    %1: %2
-%endmacro
-
-; This is needed for ELF, otherwise the GNU linker assumes the stack is executable by default.
-%if FORMAT_ELF
-    [SECTION .note.GNU-stack noalloc noexec nowrite progbits]
-%endif
-
-; Tell debuggers how large the function was.
-; This may be invoked multiple times per function; we rely on later instances overriding earlier ones.
-; This is invoked by RET and similar macros, and also cglobal does it for the previous function,
-; but if the last function in a source file doesn't use any of the standard macros for its epilogue,
-; then its size might be unspecified.
-%macro annotate_function_size 0
-    %ifdef __YASM_VER__
-        %ifdef current_function
-            %if FORMAT_ELF
-                current_function_section
-                %%ecf equ $
-                size current_function %%ecf - current_function
-                __SECT__
-            %endif
-        %endif
-    %endif
-%endmacro
-
-; cpuflags
-
-%assign cpuflags_mmx      (1<<0)
-%assign cpuflags_mmx2     (1<<1) | cpuflags_mmx
-%assign cpuflags_3dnow    (1<<2) | cpuflags_mmx
-%assign cpuflags_3dnowext (1<<3) | cpuflags_3dnow
-%assign cpuflags_sse      (1<<4) | cpuflags_mmx2
-%assign cpuflags_sse2     (1<<5) | cpuflags_sse
-%assign cpuflags_sse2slow (1<<6) | cpuflags_sse2
-%assign cpuflags_lzcnt    (1<<7) | cpuflags_sse2
-%assign cpuflags_sse3     (1<<8) | cpuflags_sse2
-%assign cpuflags_ssse3    (1<<9) | cpuflags_sse3
-%assign cpuflags_sse4     (1<<10)| cpuflags_ssse3
-%assign cpuflags_sse42    (1<<11)| cpuflags_sse4
-%assign cpuflags_aesni    (1<<12)| cpuflags_sse42
-%assign cpuflags_avx      (1<<13)| cpuflags_sse42
-%assign cpuflags_xop      (1<<14)| cpuflags_avx
-%assign cpuflags_fma4     (1<<15)| cpuflags_avx
-%assign cpuflags_fma3     (1<<16)| cpuflags_avx
-%assign cpuflags_bmi1     (1<<17)| cpuflags_avx|cpuflags_lzcnt
-%assign cpuflags_bmi2     (1<<18)| cpuflags_bmi1
-%assign cpuflags_avx2     (1<<19)| cpuflags_fma3|cpuflags_bmi2
-%assign cpuflags_avx512   (1<<20)| cpuflags_avx2 ; F, CD, BW, DQ, VL
-
-%assign cpuflags_cache32  (1<<21)
-%assign cpuflags_cache64  (1<<22)
-%assign cpuflags_aligned  (1<<23) ; not a cpu feature, but a function variant
-%assign cpuflags_atom     (1<<24)
-
-; Returns a boolean value expressing whether or not the specified cpuflag is enabled.
-%define    cpuflag(x) (((((cpuflags & (cpuflags_ %+ x)) ^ (cpuflags_ %+ x)) - 1) >> 31) & 1)
-%define notcpuflag(x) (cpuflag(x) ^ 1)
-
-; Takes an arbitrary number of cpuflags from the above list.
-; All subsequent functions (up to the next INIT_CPUFLAGS) is built for the specified cpu.
-; You shouldn't need to invoke this macro directly, it's a subroutine for INIT_MMX &co.
-%macro INIT_CPUFLAGS 0-*
-    %xdefine SUFFIX
-    %undef cpuname
-    %assign cpuflags 0
-
-    %if %0 >= 1
-        %rep %0
-            %ifdef cpuname
-                %xdefine cpuname cpuname %+ _%1
-            %else
-                %xdefine cpuname %1
-            %endif
-            %assign cpuflags cpuflags | cpuflags_%1
-            %rotate 1
-        %endrep
-        %xdefine SUFFIX _ %+ cpuname
-
-        %if cpuflag(avx)
-            %assign avx_enabled 1
-        %endif
-        %if (mmsize == 16 && notcpuflag(sse2)) || (mmsize == 32 && notcpuflag(avx2))
-            %define mova movaps
-            %define movu movups
-            %define movnta movntps
-        %endif
-        %if cpuflag(aligned)
-            %define movu mova
-        %elif cpuflag(sse3) && notcpuflag(ssse3)
-            %define movu lddqu
-        %endif
-    %endif
-
-    %if ARCH_X86_64 || cpuflag(sse2)
-        CPUNOP amdnop
-    %else
-        CPUNOP basicnop
-    %endif
-%endmacro
-
-; Merge mmx, sse*, and avx*
-; m# is a simd register of the currently selected size
-; xm# is the corresponding xmm register if mmsize >= 16, otherwise the same as m#
-; ym# is the corresponding ymm register if mmsize >= 32, otherwise the same as m#
-; zm# is the corresponding zmm register if mmsize >= 64, otherwise the same as m#
-; (All 4 remain in sync through SWAP.)
-
-%macro CAT_XDEFINE 3
-    %xdefine %1%2 %3
-%endmacro
-
-%macro CAT_UNDEF 2
-    %undef %1%2
-%endmacro
-
-%macro DEFINE_MMREGS 1 ; mmtype
-    %assign %%prev_mmregs 0
-    %ifdef num_mmregs
-        %assign %%prev_mmregs num_mmregs
-    %endif
-
-    %assign num_mmregs 8
-    %if ARCH_X86_64 && mmsize >= 16
-        %assign num_mmregs 16
-        %if cpuflag(avx512) || mmsize == 64
-            %assign num_mmregs 32
-        %endif
-    %endif
-
-    %assign %%i 0
-    %rep num_mmregs
-        CAT_XDEFINE m, %%i, %1 %+ %%i
-        CAT_XDEFINE nn%1, %%i, %%i
-        %assign %%i %%i+1
-    %endrep
-    %if %%prev_mmregs > num_mmregs
-        %rep %%prev_mmregs - num_mmregs
-            CAT_UNDEF m, %%i
-            CAT_UNDEF nn %+ mmtype, %%i
-            %assign %%i %%i+1
-        %endrep
-    %endif
-    %xdefine mmtype %1
-%endmacro
-
-; Prefer registers 16-31 over 0-15 to avoid having to use vzeroupper
-%macro AVX512_MM_PERMUTATION 0-1 0 ; start_reg
-    %if ARCH_X86_64 && cpuflag(avx512)
-        %assign %%i %1
-        %rep 16-%1
-            %assign %%i_high %%i+16
-            SWAP %%i, %%i_high
-            %assign %%i %%i+1
-        %endrep
-    %endif
-%endmacro
-
-%macro INIT_MMX 0-1+
-    %assign avx_enabled 0
-    %define RESET_MM_PERMUTATION INIT_MMX %1
-    %define mmsize 8
-    %define mova movq
-    %define movu movq
-    %define movh movd
-    %define movnta movntq
-    INIT_CPUFLAGS %1
-    DEFINE_MMREGS mm
-%endmacro
-
-%macro INIT_XMM 0-1+
-    %assign avx_enabled 0
-    %define RESET_MM_PERMUTATION INIT_XMM %1
-    %define mmsize 16
-    %define mova movdqa
-    %define movu movdqu
-    %define movh movq
-    %define movnta movntdq
-    INIT_CPUFLAGS %1
-    DEFINE_MMREGS xmm
-    %if WIN64
-        AVX512_MM_PERMUTATION 6 ; Swap callee-saved registers with volatile registers
-    %endif
-%endmacro
-
-%macro INIT_YMM 0-1+
-    %assign avx_enabled 1
-    %define RESET_MM_PERMUTATION INIT_YMM %1
-    %define mmsize 32
-    %define mova movdqa
-    %define movu movdqu
-    %undef movh
-    %define movnta movntdq
-    INIT_CPUFLAGS %1
-    DEFINE_MMREGS ymm
-    AVX512_MM_PERMUTATION
-%endmacro
-
-%macro INIT_ZMM 0-1+
-    %assign avx_enabled 1
-    %define RESET_MM_PERMUTATION INIT_ZMM %1
-    %define mmsize 64
-    %define mova movdqa
-    %define movu movdqu
-    %undef movh
-    %define movnta movntdq
-    INIT_CPUFLAGS %1
-    DEFINE_MMREGS zmm
-    AVX512_MM_PERMUTATION
-%endmacro
-
-INIT_XMM
-
-%macro DECLARE_MMCAST 1
-    %define  mmmm%1   mm%1
-    %define  mmxmm%1  mm%1
-    %define  mmymm%1  mm%1
-    %define  mmzmm%1  mm%1
-    %define xmmmm%1   mm%1
-    %define xmmxmm%1 xmm%1
-    %define xmmymm%1 xmm%1
-    %define xmmzmm%1 xmm%1
-    %define ymmmm%1   mm%1
-    %define ymmxmm%1 xmm%1
-    %define ymmymm%1 ymm%1
-    %define ymmzmm%1 ymm%1
-    %define zmmmm%1   mm%1
-    %define zmmxmm%1 xmm%1
-    %define zmmymm%1 ymm%1
-    %define zmmzmm%1 zmm%1
-    %define xm%1 xmm %+ m%1
-    %define ym%1 ymm %+ m%1
-    %define zm%1 zmm %+ m%1
-%endmacro
-
-%assign i 0
-%rep 32
-    DECLARE_MMCAST i
-    %assign i i+1
-%endrep
-
-; I often want to use macros that permute their arguments. e.g. there's no
-; efficient way to implement butterfly or transpose or dct without swapping some
-; arguments.
-;
-; I would like to not have to manually keep track of the permutations:
-; If I insert a permutation in the middle of a function, it should automatically
-; change everything that follows. For more complex macros I may also have multiple
-; implementations, e.g. the SSE2 and SSSE3 versions may have different permutations.
-;
-; Hence these macros. Insert a PERMUTE or some SWAPs at the end of a macro that
-; permutes its arguments. It's equivalent to exchanging the contents of the
-; registers, except that this way you exchange the register names instead, so it
-; doesn't cost any cycles.
-
-%macro PERMUTE 2-* ; takes a list of pairs to swap
-    %rep %0/2
-        %xdefine %%tmp%2 m%2
-        %rotate 2
-    %endrep
-    %rep %0/2
-        %xdefine m%1 %%tmp%2
-        CAT_XDEFINE nn, m%1, %1
-        %rotate 2
-    %endrep
-%endmacro
-
-%macro SWAP 2+ ; swaps a single chain (sometimes more concise than pairs)
-    %ifnum %1 ; SWAP 0, 1, ...
-        SWAP_INTERNAL_NUM %1, %2
-    %else ; SWAP m0, m1, ...
-        SWAP_INTERNAL_NAME %1, %2
-    %endif
-%endmacro
-
-%macro SWAP_INTERNAL_NUM 2-*
-    %rep %0-1
-        %xdefine %%tmp m%1
-        %xdefine m%1 m%2
-        %xdefine m%2 %%tmp
-        CAT_XDEFINE nn, m%1, %1
-        CAT_XDEFINE nn, m%2, %2
-        %rotate 1
-    %endrep
-%endmacro
-
-%macro SWAP_INTERNAL_NAME 2-*
-    %xdefine %%args nn %+ %1
-    %rep %0-1
-        %xdefine %%args %%args, nn %+ %2
-        %rotate 1
-    %endrep
-    SWAP_INTERNAL_NUM %%args
-%endmacro
-
-; If SAVE_MM_PERMUTATION is placed at the end of a function, then any later
-; calls to that function will automatically load the permutation, so values can
-; be returned in mmregs.
-%macro SAVE_MM_PERMUTATION 0-1
-    %if %0
-        %xdefine %%f %1_m
-    %else
-        %xdefine %%f current_function %+ _m
-    %endif
-    %assign %%i 0
-    %rep num_mmregs
-        CAT_XDEFINE %%f, %%i, m %+ %%i
-        %assign %%i %%i+1
-    %endrep
-%endmacro
-
-%macro LOAD_MM_PERMUTATION 1 ; name to load from
-    %ifdef %1_m0
-        %assign %%i 0
-        %rep num_mmregs
-            CAT_XDEFINE m, %%i, %1_m %+ %%i
-            CAT_XDEFINE nn, m %+ %%i, %%i
-            %assign %%i %%i+1
-        %endrep
-    %endif
-%endmacro
-
-; Append cpuflags to the callee's name iff the appended name is known and the plain name isn't
-%macro call 1
-    %ifid %1
-        call_internal %1 %+ SUFFIX, %1
-    %else
-        call %1
-    %endif
-%endmacro
-%macro call_internal 2
-    %xdefine %%i %2
-    %ifndef cglobaled_%2
-        %ifdef cglobaled_%1
-            %xdefine %%i %1
-        %endif
-    %endif
-    call %%i
-    LOAD_MM_PERMUTATION %%i
-%endmacro
-
-; Substitutions that reduce instruction size but are functionally equivalent
-%macro add 2
-    %ifnum %2
-        %if %2==128
-            sub %1, -128
-        %else
-            add %1, %2
-        %endif
-    %else
-        add %1, %2
-    %endif
-%endmacro
-
-%macro sub 2
-    %ifnum %2
-        %if %2==128
-            add %1, -128
-        %else
-            sub %1, %2
-        %endif
-    %else
-        sub %1, %2
-    %endif
-%endmacro
-
-;=============================================================================
-; AVX abstraction layer
-;=============================================================================
-
-%assign i 0
-%rep 32
-    %if i < 8
-        CAT_XDEFINE sizeofmm, i, 8
-        CAT_XDEFINE regnumofmm, i, i
-    %endif
-    CAT_XDEFINE sizeofxmm, i, 16
-    CAT_XDEFINE sizeofymm, i, 32
-    CAT_XDEFINE sizeofzmm, i, 64
-    CAT_XDEFINE regnumofxmm, i, i
-    CAT_XDEFINE regnumofymm, i, i
-    CAT_XDEFINE regnumofzmm, i, i
-    %assign i i+1
-%endrep
-%undef i
-
-%macro CHECK_AVX_INSTR_EMU 3-*
-    %xdefine %%opcode %1
-    %xdefine %%dst %2
-    %rep %0-2
-        %ifidn %%dst, %3
-            %error non-avx emulation of ``%%opcode'' is not supported
-        %endif
-        %rotate 1
-    %endrep
-%endmacro
-
-;%1 == instruction
-;%2 == minimal instruction set
-;%3 == 1 if float, 0 if int
-;%4 == 1 if 4-operand emulation, 0 if 3-operand emulation, 255 otherwise (no emulation)
-;%5 == 1 if commutative (i.e. doesn't matter which src arg is which), 0 if not
-;%6+: operands
-%macro RUN_AVX_INSTR 6-9+
-    %ifnum sizeof%7
-        %assign __sizeofreg sizeof%7
-    %elifnum sizeof%6
-        %assign __sizeofreg sizeof%6
-    %else
-        %assign __sizeofreg mmsize
-    %endif
-    %assign __emulate_avx 0
-    %if avx_enabled && __sizeofreg >= 16
-        %xdefine __instr v%1
-    %else
-        %xdefine __instr %1
-        %if %0 >= 8+%4
-            %assign __emulate_avx 1
-        %endif
-    %endif
-    %ifnidn %2, fnord
-        %ifdef cpuname
-            %if notcpuflag(%2)
-                %error use of ``%1'' %2 instruction in cpuname function: current_function
-            %elif cpuflags_%2 < cpuflags_sse && notcpuflag(sse2) && __sizeofreg > 8
-                %error use of ``%1'' sse2 instruction in cpuname function: current_function
-            %endif
-        %endif
-    %endif
-
-    %if __emulate_avx
-        %xdefine __src1 %7
-        %xdefine __src2 %8
-        %if %5 && %4 == 0
-            %ifnidn %6, %7
-                %ifidn %6, %8
-                    %xdefine __src1 %8
-                    %xdefine __src2 %7
-                %elifnnum sizeof%8
-                    ; 3-operand AVX instructions with a memory arg can only have it in src2,
-                    ; whereas SSE emulation prefers to have it in src1 (i.e. the mov).
-                    ; So, if the instruction is commutative with a memory arg, swap them.
-                    %xdefine __src1 %8
-                    %xdefine __src2 %7
-                %endif
-            %endif
-        %endif
-        %ifnidn %6, __src1
-            %if %0 >= 9
-                CHECK_AVX_INSTR_EMU {%1 %6, %7, %8, %9}, %6, __src2, %9
-            %else
-                CHECK_AVX_INSTR_EMU {%1 %6, %7, %8}, %6, __src2
-            %endif
-            %if __sizeofreg == 8
-                MOVQ %6, __src1
-            %elif %3
-                MOVAPS %6, __src1
-            %else
-                MOVDQA %6, __src1
-            %endif
-        %endif
-        %if %0 >= 9
-            %1 %6, __src2, %9
-        %else
-            %1 %6, __src2
-        %endif
-    %elif %0 >= 9
-        __instr %6, %7, %8, %9
-    %elif %0 == 8
-        __instr %6, %7, %8
-    %elif %0 == 7
-        __instr %6, %7
-    %else
-        __instr %6
-    %endif
-%endmacro
-
-;%1 == instruction
-;%2 == minimal instruction set
-;%3 == 1 if float, 0 if int
-;%4 == 1 if 4-operand emulation, 0 if 3-operand emulation, 255 otherwise (no emulation)
-;%5 == 1 if commutative (i.e. doesn't matter which src arg is which), 0 if not
-%macro AVX_INSTR 1-5 fnord, 0, 255, 0
-    %macro %1 1-10 fnord, fnord, fnord, fnord, %1, %2, %3, %4, %5
-        %ifidn %2, fnord
-            RUN_AVX_INSTR %6, %7, %8, %9, %10, %1
-        %elifidn %3, fnord
-            RUN_AVX_INSTR %6, %7, %8, %9, %10, %1, %2
-        %elifidn %4, fnord
-            RUN_AVX_INSTR %6, %7, %8, %9, %10, %1, %2, %3
-        %elifidn %5, fnord
-            RUN_AVX_INSTR %6, %7, %8, %9, %10, %1, %2, %3, %4
-        %else
-            RUN_AVX_INSTR %6, %7, %8, %9, %10, %1, %2, %3, %4, %5
-        %endif
-    %endmacro
-%endmacro
-
-; Instructions with both VEX/EVEX and legacy encodings
-; Non-destructive instructions are written without parameters
-AVX_INSTR addpd, sse2, 1, 0, 1
-AVX_INSTR addps, sse, 1, 0, 1
-AVX_INSTR addsd, sse2, 1, 0, 0
-AVX_INSTR addss, sse, 1, 0, 0
-AVX_INSTR addsubpd, sse3, 1, 0, 0
-AVX_INSTR addsubps, sse3, 1, 0, 0
-AVX_INSTR aesdec, aesni, 0, 0, 0
-AVX_INSTR aesdeclast, aesni, 0, 0, 0
-AVX_INSTR aesenc, aesni, 0, 0, 0
-AVX_INSTR aesenclast, aesni, 0, 0, 0
-AVX_INSTR aesimc, aesni
-AVX_INSTR aeskeygenassist, aesni
-AVX_INSTR andnpd, sse2, 1, 0, 0
-AVX_INSTR andnps, sse, 1, 0, 0
-AVX_INSTR andpd, sse2, 1, 0, 1
-AVX_INSTR andps, sse, 1, 0, 1
-AVX_INSTR blendpd, sse4, 1, 1, 0
-AVX_INSTR blendps, sse4, 1, 1, 0
-AVX_INSTR blendvpd, sse4 ; can't be emulated
-AVX_INSTR blendvps, sse4 ; can't be emulated
-AVX_INSTR cmpeqpd, sse2, 1, 0, 1
-AVX_INSTR cmpeqps, sse, 1, 0, 1
-AVX_INSTR cmpeqsd, sse2, 1, 0, 0
-AVX_INSTR cmpeqss, sse, 1, 0, 0
-AVX_INSTR cmplepd, sse2, 1, 0, 0
-AVX_INSTR cmpleps, sse, 1, 0, 0
-AVX_INSTR cmplesd, sse2, 1, 0, 0
-AVX_INSTR cmpless, sse, 1, 0, 0
-AVX_INSTR cmpltpd, sse2, 1, 0, 0
-AVX_INSTR cmpltps, sse, 1, 0, 0
-AVX_INSTR cmpltsd, sse2, 1, 0, 0
-AVX_INSTR cmpltss, sse, 1, 0, 0
-AVX_INSTR cmpneqpd, sse2, 1, 0, 1
-AVX_INSTR cmpneqps, sse, 1, 0, 1
-AVX_INSTR cmpneqsd, sse2, 1, 0, 0
-AVX_INSTR cmpneqss, sse, 1, 0, 0
-AVX_INSTR cmpnlepd, sse2, 1, 0, 0
-AVX_INSTR cmpnleps, sse, 1, 0, 0
-AVX_INSTR cmpnlesd, sse2, 1, 0, 0
-AVX_INSTR cmpnless, sse, 1, 0, 0
-AVX_INSTR cmpnltpd, sse2, 1, 0, 0
-AVX_INSTR cmpnltps, sse, 1, 0, 0
-AVX_INSTR cmpnltsd, sse2, 1, 0, 0
-AVX_INSTR cmpnltss, sse, 1, 0, 0
-AVX_INSTR cmpordpd, sse2 1, 0, 1
-AVX_INSTR cmpordps, sse 1, 0, 1
-AVX_INSTR cmpordsd, sse2 1, 0, 0
-AVX_INSTR cmpordss, sse 1, 0, 0
-AVX_INSTR cmppd, sse2, 1, 1, 0
-AVX_INSTR cmpps, sse, 1, 1, 0
-AVX_INSTR cmpsd, sse2, 1, 1, 0
-AVX_INSTR cmpss, sse, 1, 1, 0
-AVX_INSTR cmpunordpd, sse2, 1, 0, 1
-AVX_INSTR cmpunordps, sse, 1, 0, 1
-AVX_INSTR cmpunordsd, sse2, 1, 0, 0
-AVX_INSTR cmpunordss, sse, 1, 0, 0
-AVX_INSTR comisd, sse2
-AVX_INSTR comiss, sse
-AVX_INSTR cvtdq2pd, sse2
-AVX_INSTR cvtdq2ps, sse2
-AVX_INSTR cvtpd2dq, sse2
-AVX_INSTR cvtpd2ps, sse2
-AVX_INSTR cvtps2dq, sse2
-AVX_INSTR cvtps2pd, sse2
-AVX_INSTR cvtsd2si, sse2
-AVX_INSTR cvtsd2ss, sse2, 1, 0, 0
-AVX_INSTR cvtsi2sd, sse2, 1, 0, 0
-AVX_INSTR cvtsi2ss, sse, 1, 0, 0
-AVX_INSTR cvtss2sd, sse2, 1, 0, 0
-AVX_INSTR cvtss2si, sse
-AVX_INSTR cvttpd2dq, sse2
-AVX_INSTR cvttps2dq, sse2
-AVX_INSTR cvttsd2si, sse2
-AVX_INSTR cvttss2si, sse
-AVX_INSTR divpd, sse2, 1, 0, 0
-AVX_INSTR divps, sse, 1, 0, 0
-AVX_INSTR divsd, sse2, 1, 0, 0
-AVX_INSTR divss, sse, 1, 0, 0
-AVX_INSTR dppd, sse4, 1, 1, 0
-AVX_INSTR dpps, sse4, 1, 1, 0
-AVX_INSTR extractps, sse4
-AVX_INSTR haddpd, sse3, 1, 0, 0
-AVX_INSTR haddps, sse3, 1, 0, 0
-AVX_INSTR hsubpd, sse3, 1, 0, 0
-AVX_INSTR hsubps, sse3, 1, 0, 0
-AVX_INSTR insertps, sse4, 1, 1, 0
-AVX_INSTR lddqu, sse3
-AVX_INSTR ldmxcsr, sse
-AVX_INSTR maskmovdqu, sse2
-AVX_INSTR maxpd, sse2, 1, 0, 1
-AVX_INSTR maxps, sse, 1, 0, 1
-AVX_INSTR maxsd, sse2, 1, 0, 0
-AVX_INSTR maxss, sse, 1, 0, 0
-AVX_INSTR minpd, sse2, 1, 0, 1
-AVX_INSTR minps, sse, 1, 0, 1
-AVX_INSTR minsd, sse2, 1, 0, 0
-AVX_INSTR minss, sse, 1, 0, 0
-AVX_INSTR movapd, sse2
-AVX_INSTR movaps, sse
-AVX_INSTR movd, mmx
-AVX_INSTR movddup, sse3
-AVX_INSTR movdqa, sse2
-AVX_INSTR movdqu, sse2
-AVX_INSTR movhlps, sse, 1, 0, 0
-AVX_INSTR movhpd, sse2, 1, 0, 0
-AVX_INSTR movhps, sse, 1, 0, 0
-AVX_INSTR movlhps, sse, 1, 0, 0
-AVX_INSTR movlpd, sse2, 1, 0, 0
-AVX_INSTR movlps, sse, 1, 0, 0
-AVX_INSTR movmskpd, sse2
-AVX_INSTR movmskps, sse
-AVX_INSTR movntdq, sse2
-AVX_INSTR movntdqa, sse4
-AVX_INSTR movntpd, sse2
-AVX_INSTR movntps, sse
-AVX_INSTR movq, mmx
-AVX_INSTR movsd, sse2, 1, 0, 0
-AVX_INSTR movshdup, sse3
-AVX_INSTR movsldup, sse3
-AVX_INSTR movss, sse, 1, 0, 0
-AVX_INSTR movupd, sse2
-AVX_INSTR movups, sse
-AVX_INSTR mpsadbw, sse4, 0, 1, 0
-AVX_INSTR mulpd, sse2, 1, 0, 1
-AVX_INSTR mulps, sse, 1, 0, 1
-AVX_INSTR mulsd, sse2, 1, 0, 0
-AVX_INSTR mulss, sse, 1, 0, 0
-AVX_INSTR orpd, sse2, 1, 0, 1
-AVX_INSTR orps, sse, 1, 0, 1
-AVX_INSTR pabsb, ssse3
-AVX_INSTR pabsd, ssse3
-AVX_INSTR pabsw, ssse3
-AVX_INSTR packsswb, mmx, 0, 0, 0
-AVX_INSTR packssdw, mmx, 0, 0, 0
-AVX_INSTR packuswb, mmx, 0, 0, 0
-AVX_INSTR packusdw, sse4, 0, 0, 0
-AVX_INSTR paddb, mmx, 0, 0, 1
-AVX_INSTR paddw, mmx, 0, 0, 1
-AVX_INSTR paddd, mmx, 0, 0, 1
-AVX_INSTR paddq, sse2, 0, 0, 1
-AVX_INSTR paddsb, mmx, 0, 0, 1
-AVX_INSTR paddsw, mmx, 0, 0, 1
-AVX_INSTR paddusb, mmx, 0, 0, 1
-AVX_INSTR paddusw, mmx, 0, 0, 1
-AVX_INSTR palignr, ssse3, 0, 1, 0
-AVX_INSTR pand, mmx, 0, 0, 1
-AVX_INSTR pandn, mmx, 0, 0, 0
-AVX_INSTR pavgb, mmx2, 0, 0, 1
-AVX_INSTR pavgw, mmx2, 0, 0, 1
-AVX_INSTR pblendvb, sse4 ; can't be emulated
-AVX_INSTR pblendw, sse4, 0, 1, 0
-AVX_INSTR pclmulqdq, fnord, 0, 1, 0
-AVX_INSTR pclmulhqhqdq, fnord, 0, 0, 0
-AVX_INSTR pclmulhqlqdq, fnord, 0, 0, 0
-AVX_INSTR pclmullqhqdq, fnord, 0, 0, 0
-AVX_INSTR pclmullqlqdq, fnord, 0, 0, 0
-AVX_INSTR pcmpestri, sse42
-AVX_INSTR pcmpestrm, sse42
-AVX_INSTR pcmpistri, sse42
-AVX_INSTR pcmpistrm, sse42
-AVX_INSTR pcmpeqb, mmx, 0, 0, 1
-AVX_INSTR pcmpeqw, mmx, 0, 0, 1
-AVX_INSTR pcmpeqd, mmx, 0, 0, 1
-AVX_INSTR pcmpeqq, sse4, 0, 0, 1
-AVX_INSTR pcmpgtb, mmx, 0, 0, 0
-AVX_INSTR pcmpgtw, mmx, 0, 0, 0
-AVX_INSTR pcmpgtd, mmx, 0, 0, 0
-AVX_INSTR pcmpgtq, sse42, 0, 0, 0
-AVX_INSTR pextrb, sse4
-AVX_INSTR pextrd, sse4
-AVX_INSTR pextrq, sse4
-AVX_INSTR pextrw, mmx2
-AVX_INSTR phaddw, ssse3, 0, 0, 0
-AVX_INSTR phaddd, ssse3, 0, 0, 0
-AVX_INSTR phaddsw, ssse3, 0, 0, 0
-AVX_INSTR phminposuw, sse4
-AVX_INSTR phsubw, ssse3, 0, 0, 0
-AVX_INSTR phsubd, ssse3, 0, 0, 0
-AVX_INSTR phsubsw, ssse3, 0, 0, 0
-AVX_INSTR pinsrb, sse4, 0, 1, 0
-AVX_INSTR pinsrd, sse4, 0, 1, 0
-AVX_INSTR pinsrq, sse4, 0, 1, 0
-AVX_INSTR pinsrw, mmx2, 0, 1, 0
-AVX_INSTR pmaddwd, mmx, 0, 0, 1
-AVX_INSTR pmaddubsw, ssse3, 0, 0, 0
-AVX_INSTR pmaxsb, sse4, 0, 0, 1
-AVX_INSTR pmaxsw, mmx2, 0, 0, 1
-AVX_INSTR pmaxsd, sse4, 0, 0, 1
-AVX_INSTR pmaxub, mmx2, 0, 0, 1
-AVX_INSTR pmaxuw, sse4, 0, 0, 1
-AVX_INSTR pmaxud, sse4, 0, 0, 1
-AVX_INSTR pminsb, sse4, 0, 0, 1
-AVX_INSTR pminsw, mmx2, 0, 0, 1
-AVX_INSTR pminsd, sse4, 0, 0, 1
-AVX_INSTR pminub, mmx2, 0, 0, 1
-AVX_INSTR pminuw, sse4, 0, 0, 1
-AVX_INSTR pminud, sse4, 0, 0, 1
-AVX_INSTR pmovmskb, mmx2
-AVX_INSTR pmovsxbw, sse4
-AVX_INSTR pmovsxbd, sse4
-AVX_INSTR pmovsxbq, sse4
-AVX_INSTR pmovsxwd, sse4
-AVX_INSTR pmovsxwq, sse4
-AVX_INSTR pmovsxdq, sse4
-AVX_INSTR pmovzxbw, sse4
-AVX_INSTR pmovzxbd, sse4
-AVX_INSTR pmovzxbq, sse4
-AVX_INSTR pmovzxwd, sse4
-AVX_INSTR pmovzxwq, sse4
-AVX_INSTR pmovzxdq, sse4
-AVX_INSTR pmuldq, sse4, 0, 0, 1
-AVX_INSTR pmulhrsw, ssse3, 0, 0, 1
-AVX_INSTR pmulhuw, mmx2, 0, 0, 1
-AVX_INSTR pmulhw, mmx, 0, 0, 1
-AVX_INSTR pmullw, mmx, 0, 0, 1
-AVX_INSTR pmulld, sse4, 0, 0, 1
-AVX_INSTR pmuludq, sse2, 0, 0, 1
-AVX_INSTR por, mmx, 0, 0, 1
-AVX_INSTR psadbw, mmx2, 0, 0, 1
-AVX_INSTR pshufb, ssse3, 0, 0, 0
-AVX_INSTR pshufd, sse2
-AVX_INSTR pshufhw, sse2
-AVX_INSTR pshuflw, sse2
-AVX_INSTR psignb, ssse3, 0, 0, 0
-AVX_INSTR psignw, ssse3, 0, 0, 0
-AVX_INSTR psignd, ssse3, 0, 0, 0
-AVX_INSTR psllw, mmx, 0, 0, 0
-AVX_INSTR pslld, mmx, 0, 0, 0
-AVX_INSTR psllq, mmx, 0, 0, 0
-AVX_INSTR pslldq, sse2, 0, 0, 0
-AVX_INSTR psraw, mmx, 0, 0, 0
-AVX_INSTR psrad, mmx, 0, 0, 0
-AVX_INSTR psrlw, mmx, 0, 0, 0
-AVX_INSTR psrld, mmx, 0, 0, 0
-AVX_INSTR psrlq, mmx, 0, 0, 0
-AVX_INSTR psrldq, sse2, 0, 0, 0
-AVX_INSTR psubb, mmx, 0, 0, 0
-AVX_INSTR psubw, mmx, 0, 0, 0
-AVX_INSTR psubd, mmx, 0, 0, 0
-AVX_INSTR psubq, sse2, 0, 0, 0
-AVX_INSTR psubsb, mmx, 0, 0, 0
-AVX_INSTR psubsw, mmx, 0, 0, 0
-AVX_INSTR psubusb, mmx, 0, 0, 0
-AVX_INSTR psubusw, mmx, 0, 0, 0
-AVX_INSTR ptest, sse4
-AVX_INSTR punpckhbw, mmx, 0, 0, 0
-AVX_INSTR punpckhwd, mmx, 0, 0, 0
-AVX_INSTR punpckhdq, mmx, 0, 0, 0
-AVX_INSTR punpckhqdq, sse2, 0, 0, 0
-AVX_INSTR punpcklbw, mmx, 0, 0, 0
-AVX_INSTR punpcklwd, mmx, 0, 0, 0
-AVX_INSTR punpckldq, mmx, 0, 0, 0
-AVX_INSTR punpcklqdq, sse2, 0, 0, 0
-AVX_INSTR pxor, mmx, 0, 0, 1
-AVX_INSTR rcpps, sse
-AVX_INSTR rcpss, sse, 1, 0, 0
-AVX_INSTR roundpd, sse4
-AVX_INSTR roundps, sse4
-AVX_INSTR roundsd, sse4, 1, 1, 0
-AVX_INSTR roundss, sse4, 1, 1, 0
-AVX_INSTR rsqrtps, sse
-AVX_INSTR rsqrtss, sse, 1, 0, 0
-AVX_INSTR shufpd, sse2, 1, 1, 0
-AVX_INSTR shufps, sse, 1, 1, 0
-AVX_INSTR sqrtpd, sse2
-AVX_INSTR sqrtps, sse
-AVX_INSTR sqrtsd, sse2, 1, 0, 0
-AVX_INSTR sqrtss, sse, 1, 0, 0
-AVX_INSTR stmxcsr, sse
-AVX_INSTR subpd, sse2, 1, 0, 0
-AVX_INSTR subps, sse, 1, 0, 0
-AVX_INSTR subsd, sse2, 1, 0, 0
-AVX_INSTR subss, sse, 1, 0, 0
-AVX_INSTR ucomisd, sse2
-AVX_INSTR ucomiss, sse
-AVX_INSTR unpckhpd, sse2, 1, 0, 0
-AVX_INSTR unpckhps, sse, 1, 0, 0
-AVX_INSTR unpcklpd, sse2, 1, 0, 0
-AVX_INSTR unpcklps, sse, 1, 0, 0
-AVX_INSTR xorpd, sse2, 1, 0, 1
-AVX_INSTR xorps, sse, 1, 0, 1
-
-; 3DNow instructions, for sharing code between AVX, SSE and 3DN
-AVX_INSTR pfadd, 3dnow, 1, 0, 1
-AVX_INSTR pfsub, 3dnow, 1, 0, 0
-AVX_INSTR pfmul, 3dnow, 1, 0, 1
-
-; base-4 constants for shuffles
-%assign i 0
-%rep 256
-    %assign j ((i>>6)&3)*1000 + ((i>>4)&3)*100 + ((i>>2)&3)*10 + (i&3)
-    %if j < 10
-        CAT_XDEFINE q000, j, i
-    %elif j < 100
-        CAT_XDEFINE q00, j, i
-    %elif j < 1000
-        CAT_XDEFINE q0, j, i
-    %else
-        CAT_XDEFINE q, j, i
-    %endif
-    %assign i i+1
-%endrep
-%undef i
-%undef j
-
-%macro FMA_INSTR 3
-    %macro %1 4-7 %1, %2, %3
-        %if cpuflag(xop)
-            v%5 %1, %2, %3, %4
-        %elifnidn %1, %4
-            %6 %1, %2, %3
-            %7 %1, %4
-        %else
-            %error non-xop emulation of ``%5 %1, %2, %3, %4'' is not supported
-        %endif
-    %endmacro
-%endmacro
-
-FMA_INSTR  pmacsww,  pmullw, paddw
-FMA_INSTR  pmacsdd,  pmulld, paddd ; sse4 emulation
-FMA_INSTR pmacsdql,  pmuldq, paddq ; sse4 emulation
-FMA_INSTR pmadcswd, pmaddwd, paddd
-
-; tzcnt is equivalent to "rep bsf" and is backwards-compatible with bsf.
-; This lets us use tzcnt without bumping the yasm version requirement yet.
-%define tzcnt rep bsf
-
-; Macros for consolidating FMA3 and FMA4 using 4-operand (dst, src1, src2, src3) syntax.
-; FMA3 is only possible if dst is the same as one of the src registers.
-; Either src2 or src3 can be a memory operand.
-%macro FMA4_INSTR 2-*
-    %push fma4_instr
-    %xdefine %$prefix %1
-    %rep %0 - 1
-        %macro %$prefix%2 4-6 %$prefix, %2
-            %if notcpuflag(fma3) && notcpuflag(fma4)
-                %error use of ``%5%6'' fma instruction in cpuname function: current_function
-            %elif cpuflag(fma4)
-                v%5%6 %1, %2, %3, %4
-            %elifidn %1, %2
-                ; If %3 or %4 is a memory operand it needs to be encoded as the last operand.
-                %ifnum sizeof%3
-                    v%{5}213%6 %2, %3, %4
-                %else
-                    v%{5}132%6 %2, %4, %3
-                %endif
-            %elifidn %1, %3
-                v%{5}213%6 %3, %2, %4
-            %elifidn %1, %4
-                v%{5}231%6 %4, %2, %3
-            %else
-                %error fma3 emulation of ``%5%6 %1, %2, %3, %4'' is not supported
-            %endif
-        %endmacro
-        %rotate 1
-    %endrep
-    %pop
-%endmacro
-
-FMA4_INSTR fmadd,    pd, ps, sd, ss
-FMA4_INSTR fmaddsub, pd, ps
-FMA4_INSTR fmsub,    pd, ps, sd, ss
-FMA4_INSTR fmsubadd, pd, ps
-FMA4_INSTR fnmadd,   pd, ps, sd, ss
-FMA4_INSTR fnmsub,   pd, ps, sd, ss
-
-; Macros for converting VEX instructions to equivalent EVEX ones.
-%macro EVEX_INSTR 2-3 0 ; vex, evex, prefer_evex
-    %macro %1 2-7 fnord, fnord, %1, %2, %3
-        %ifidn %3, fnord
-            %define %%args %1, %2
-        %elifidn %4, fnord
-            %define %%args %1, %2, %3
-        %else
-            %define %%args %1, %2, %3, %4
-        %endif
-        %assign %%evex_required cpuflag(avx512) & %7
-        %ifnum regnumof%1
-            %if regnumof%1 >= 16 || sizeof%1 > 32
-                %assign %%evex_required 1
-            %endif
-        %endif
-        %ifnum regnumof%2
-            %if regnumof%2 >= 16 || sizeof%2 > 32
-                %assign %%evex_required 1
-            %endif
-        %endif
-        %if %%evex_required
-            %6 %%args
-        %else
-            %5 %%args ; Prefer VEX over EVEX due to shorter instruction length
-        %endif
-    %endmacro
-%endmacro
-
-EVEX_INSTR vbroadcastf128, vbroadcastf32x4
-EVEX_INSTR vbroadcasti128, vbroadcasti32x4
-EVEX_INSTR vextractf128,   vextractf32x4
-EVEX_INSTR vextracti128,   vextracti32x4
-EVEX_INSTR vinsertf128,    vinsertf32x4
-EVEX_INSTR vinserti128,    vinserti32x4
-EVEX_INSTR vmovdqa,        vmovdqa32
-EVEX_INSTR vmovdqu,        vmovdqu32
-EVEX_INSTR vpand,          vpandd
-EVEX_INSTR vpandn,         vpandnd
-EVEX_INSTR vpor,           vpord
-EVEX_INSTR vpxor,          vpxord
-EVEX_INSTR vrcpps,         vrcp14ps,   1 ; EVEX versions have higher precision
-EVEX_INSTR vrcpss,         vrcp14ss,   1
-EVEX_INSTR vrsqrtps,       vrsqrt14ps, 1
-EVEX_INSTR vrsqrtss,       vrsqrt14ss, 1
-
-; workaround: vpbroadcastq is broken in x86_32 due to a yasm bug (fixed in 1.3.0)
-%ifdef __YASM_VER__
-    %if __YASM_VERSION_ID__ < 0x01030000 && ARCH_X86_64 == 0
-        %macro vpbroadcastq 2
-            %if sizeof%1 == 16
-                movddup %1, %2
-            %else
-                vbroadcastsd %1, %2
-            %endif
-        %endmacro
-    %endif
-%endif
diff -uparN ffmpeg-4.1/libavutil/x86/x86util.asm ffmpeg-y/libavutil/x86/x86util.asm
--- ffmpeg-4.1/libavutil/x86/x86util.asm	2018-11-03 08:17:30.000000000 +0800
+++ ffmpeg-y/libavutil/x86/x86util.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,1028 +0,0 @@
-;*****************************************************************************
-;* x86util.asm
-;*****************************************************************************
-;* Copyright (C) 2008-2010 x264 project
-;*
-;* Authors: Loren Merritt <lorenm@u.washington.edu>
-;*          Holger Lubitz <holger@lubitz.org>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%define private_prefix ff
-%define public_prefix  avpriv
-%define cpuflags_mmxext cpuflags_mmx2
-
-%include "libavutil/x86/x86inc.asm"
-
-; expands to [base],...,[base+7*stride]
-%define PASS8ROWS(base, base3, stride, stride3) \
-    [base],           [base  + stride],   [base  + 2*stride], [base3], \
-    [base3 + stride], [base3 + 2*stride], [base3 + stride3],  [base3 + stride*4]
-
-; Interleave low src0 with low src1 and store in src0,
-; interleave high src0 with high src1 and store in src1.
-; %1 - types
-; %2 - index of the register with src0
-; %3 - index of the register with src1
-; %4 - index of the register for intermediate results
-; example for %1 - wd: input: src0: x0 x1 x2 x3 z0 z1 z2 z3
-;                             src1: y0 y1 y2 y3 q0 q1 q2 q3
-;                     output: src0: x0 y0 x1 y1 x2 y2 x3 y3
-;                             src1: z0 q0 z1 q1 z2 q2 z3 q3
-%macro SBUTTERFLY 4
-%ifidn %1, dqqq
-    vperm2i128  m%4, m%2, m%3, q0301
-    vinserti128 m%2, m%2, xm%3, 1
-%elif avx_enabled == 0
-    mova      m%4, m%2
-    punpckl%1 m%2, m%3
-    punpckh%1 m%4, m%3
-%else
-    punpckh%1 m%4, m%2, m%3
-    punpckl%1 m%2, m%3
-%endif
-    SWAP %3, %4
-%endmacro
-
-%macro SBUTTERFLY2 4
-    punpckl%1 m%4, m%2, m%3
-    punpckh%1 m%2, m%2, m%3
-    SWAP %2, %4, %3
-%endmacro
-
-%macro SBUTTERFLYPS 3
-    unpcklps m%3, m%1, m%2
-    unpckhps m%1, m%1, m%2
-    SWAP %1, %3, %2
-%endmacro
-
-%macro SBUTTERFLYPD 3
-    movlhps m%3, m%1, m%2
-    movhlps m%2, m%2, m%1
-    SWAP %1, %3
-%endmacro
-
-%macro TRANSPOSE4x4B 5
-    SBUTTERFLY bw, %1, %2, %5
-    SBUTTERFLY bw, %3, %4, %5
-    SBUTTERFLY wd, %1, %3, %5
-    SBUTTERFLY wd, %2, %4, %5
-    SWAP %2, %3
-%endmacro
-
-%macro TRANSPOSE4x4W 5
-    SBUTTERFLY wd, %1, %2, %5
-    SBUTTERFLY wd, %3, %4, %5
-    SBUTTERFLY dq, %1, %3, %5
-    SBUTTERFLY dq, %2, %4, %5
-    SWAP %2, %3
-%endmacro
-
-%macro TRANSPOSE2x4x4B 5
-    SBUTTERFLY bw,  %1, %2, %5
-    SBUTTERFLY bw,  %3, %4, %5
-    SBUTTERFLY wd,  %1, %3, %5
-    SBUTTERFLY wd,  %2, %4, %5
-    SBUTTERFLY dq,  %1, %2, %5
-    SBUTTERFLY dq,  %3, %4, %5
-%endmacro
-
-%macro TRANSPOSE2x4x4W 5
-    SBUTTERFLY wd,  %1, %2, %5
-    SBUTTERFLY wd,  %3, %4, %5
-    SBUTTERFLY dq,  %1, %3, %5
-    SBUTTERFLY dq,  %2, %4, %5
-    SBUTTERFLY qdq, %1, %2, %5
-    SBUTTERFLY qdq, %3, %4, %5
-%endmacro
-
-%macro TRANSPOSE4x4D 5
-    SBUTTERFLY dq,  %1, %2, %5
-    SBUTTERFLY dq,  %3, %4, %5
-    SBUTTERFLY qdq, %1, %3, %5
-    SBUTTERFLY qdq, %2, %4, %5
-    SWAP %2, %3
-%endmacro
-
-; identical behavior to TRANSPOSE4x4D, but using SSE1 float ops
-%macro TRANSPOSE4x4PS 5
-    SBUTTERFLYPS %1, %2, %5
-    SBUTTERFLYPS %3, %4, %5
-    SBUTTERFLYPD %1, %3, %5
-    SBUTTERFLYPD %2, %4, %5
-    SWAP %2, %3
-%endmacro
-
-%macro TRANSPOSE8x4D 9-11
-%if ARCH_X86_64
-    SBUTTERFLY dq,  %1, %2, %9
-    SBUTTERFLY dq,  %3, %4, %9
-    SBUTTERFLY dq,  %5, %6, %9
-    SBUTTERFLY dq,  %7, %8, %9
-    SBUTTERFLY qdq, %1, %3, %9
-    SBUTTERFLY qdq, %2, %4, %9
-    SBUTTERFLY qdq, %5, %7, %9
-    SBUTTERFLY qdq, %6, %8, %9
-    SWAP %2, %5
-    SWAP %4, %7
-%else
-; in:  m0..m7
-; out: m0..m7, unless %11 in which case m2 is in %9
-; spills into %9 and %10
-    movdqa %9, m%7
-    SBUTTERFLY dq,  %1, %2, %7
-    movdqa %10, m%2
-    movdqa m%7, %9
-    SBUTTERFLY dq,  %3, %4, %2
-    SBUTTERFLY dq,  %5, %6, %2
-    SBUTTERFLY dq,  %7, %8, %2
-    SBUTTERFLY qdq, %1, %3, %2
-    movdqa %9, m%3
-    movdqa m%2, %10
-    SBUTTERFLY qdq, %2, %4, %3
-    SBUTTERFLY qdq, %5, %7, %3
-    SBUTTERFLY qdq, %6, %8, %3
-    SWAP %2, %5
-    SWAP %4, %7
-%if %0<11
-    movdqa m%3, %9
-%endif
-%endif
-%endmacro
-
-%macro TRANSPOSE8x8W 9-11
-%if ARCH_X86_64
-    SBUTTERFLY wd,  %1, %2, %9
-    SBUTTERFLY wd,  %3, %4, %9
-    SBUTTERFLY wd,  %5, %6, %9
-    SBUTTERFLY wd,  %7, %8, %9
-    SBUTTERFLY dq,  %1, %3, %9
-    SBUTTERFLY dq,  %2, %4, %9
-    SBUTTERFLY dq,  %5, %7, %9
-    SBUTTERFLY dq,  %6, %8, %9
-    SBUTTERFLY qdq, %1, %5, %9
-    SBUTTERFLY qdq, %2, %6, %9
-    SBUTTERFLY qdq, %3, %7, %9
-    SBUTTERFLY qdq, %4, %8, %9
-    SWAP %2, %5
-    SWAP %4, %7
-%else
-; in:  m0..m7, unless %11 in which case m6 is in %9
-; out: m0..m7, unless %11 in which case m4 is in %10
-; spills into %9 and %10
-%if %0<11
-    movdqa %9, m%7
-%endif
-    SBUTTERFLY wd,  %1, %2, %7
-    movdqa %10, m%2
-    movdqa m%7, %9
-    SBUTTERFLY wd,  %3, %4, %2
-    SBUTTERFLY wd,  %5, %6, %2
-    SBUTTERFLY wd,  %7, %8, %2
-    SBUTTERFLY dq,  %1, %3, %2
-    movdqa %9, m%3
-    movdqa m%2, %10
-    SBUTTERFLY dq,  %2, %4, %3
-    SBUTTERFLY dq,  %5, %7, %3
-    SBUTTERFLY dq,  %6, %8, %3
-    SBUTTERFLY qdq, %1, %5, %3
-    SBUTTERFLY qdq, %2, %6, %3
-    movdqa %10, m%2
-    movdqa m%3, %9
-    SBUTTERFLY qdq, %3, %7, %2
-    SBUTTERFLY qdq, %4, %8, %2
-    SWAP %2, %5
-    SWAP %4, %7
-%if %0<11
-    movdqa m%5, %10
-%endif
-%endif
-%endmacro
-
-%macro TRANSPOSE16x16W 18-19
-; in:  m0..m15, unless %19 in which case m6 is in %17
-; out: m0..m15, unless %19 in which case m4 is in %18
-; spills into %17 and %18
-%if %0 < 19
-    mova       %17, m%7
-%endif
-
-    SBUTTERFLY dqqq, %1,  %9, %7
-    SBUTTERFLY dqqq, %2, %10, %7
-    SBUTTERFLY dqqq, %3, %11, %7
-    SBUTTERFLY dqqq, %4, %12, %7
-    SBUTTERFLY dqqq, %5, %13, %7
-    SBUTTERFLY dqqq, %6, %14, %7
-    mova       %18, m%14
-    mova       m%7, %17
-    SBUTTERFLY dqqq, %7, %15, %14
-    SBUTTERFLY dqqq, %8, %16, %14
-
-    SBUTTERFLY  wd,  %1,  %2, %14
-    SBUTTERFLY  wd,  %3,  %4, %14
-    SBUTTERFLY  wd,  %5,  %6, %14
-    SBUTTERFLY  wd,  %7,  %8, %14
-    SBUTTERFLY  wd,  %9, %10, %14
-    SBUTTERFLY  wd, %11, %12, %14
-    mova       %17, m%12
-    mova      m%14, %18
-    SBUTTERFLY  wd, %13, %14, %12
-    SBUTTERFLY  wd, %15, %16, %12
-
-    SBUTTERFLY  dq,  %1,  %3, %12
-    SBUTTERFLY  dq,  %2,  %4, %12
-    SBUTTERFLY  dq,  %5,  %7, %12
-    SBUTTERFLY  dq,  %6,  %8, %12
-    SBUTTERFLY  dq,  %9, %11, %12
-    mova       %18, m%11
-    mova      m%12, %17
-    SBUTTERFLY  dq, %10, %12, %11
-    SBUTTERFLY  dq, %13, %15, %11
-    SBUTTERFLY  dq, %14, %16, %11
-
-    SBUTTERFLY qdq,  %1,  %5, %11
-    SBUTTERFLY qdq,  %2,  %6, %11
-    SBUTTERFLY qdq,  %3,  %7, %11
-    SBUTTERFLY qdq,  %4,  %8, %11
-
-    SWAP        %2, %5
-    SWAP        %4, %7
-
-    SBUTTERFLY qdq,  %9, %13, %11
-    SBUTTERFLY qdq, %10, %14, %11
-    mova      m%11, %18
-    mova       %18, m%5
-    SBUTTERFLY qdq, %11, %15, %5
-    SBUTTERFLY qdq, %12, %16, %5
-
-%if %0 < 19
-    mova       m%5, %18
-%endif
-
-    SWAP       %10, %13
-    SWAP       %12, %15
-%endmacro
-
-%macro TRANSPOSE_8X8B 8
-    %if mmsize == 8
-        %error "This macro does not support mmsize == 8"
-    %endif
-    punpcklbw m%1, m%2
-    punpcklbw m%3, m%4
-    punpcklbw m%5, m%6
-    punpcklbw m%7, m%8
-    TRANSPOSE4x4W %1, %3, %5, %7, %2
-    MOVHL m%2, m%1
-    MOVHL m%4, m%3
-    MOVHL m%6, m%5
-    MOVHL m%8, m%7
-%endmacro
-
-; PABSW macro assumes %1 != %2, while ABS1/2 macros work in-place
-%macro PABSW 2
-%if cpuflag(ssse3)
-    pabsw      %1, %2
-%elif cpuflag(mmxext)
-    pxor    %1, %1
-    psubw   %1, %2
-    pmaxsw  %1, %2
-%else
-    pxor       %1, %1
-    pcmpgtw    %1, %2
-    pxor       %2, %1
-    psubw      %2, %1
-    SWAP       %1, %2
-%endif
-%endmacro
-
-%macro PSIGNW 2
-%if cpuflag(ssse3)
-    psignw     %1, %2
-%else
-    pxor       %1, %2
-    psubw      %1, %2
-%endif
-%endmacro
-
-%macro ABS1 2
-%if cpuflag(ssse3)
-    pabsw   %1, %1
-%elif cpuflag(mmxext) ; a, tmp
-    pxor    %2, %2
-    psubw   %2, %1
-    pmaxsw  %1, %2
-%else ; a, tmp
-    pxor       %2, %2
-    pcmpgtw    %2, %1
-    pxor       %1, %2
-    psubw      %1, %2
-%endif
-%endmacro
-
-%macro ABS2 4
-%if cpuflag(ssse3)
-    pabsw   %1, %1
-    pabsw   %2, %2
-%elif cpuflag(mmxext) ; a, b, tmp0, tmp1
-    pxor    %3, %3
-    pxor    %4, %4
-    psubw   %3, %1
-    psubw   %4, %2
-    pmaxsw  %1, %3
-    pmaxsw  %2, %4
-%else ; a, b, tmp0, tmp1
-    pxor       %3, %3
-    pxor       %4, %4
-    pcmpgtw    %3, %1
-    pcmpgtw    %4, %2
-    pxor       %1, %3
-    pxor       %2, %4
-    psubw      %1, %3
-    psubw      %2, %4
-%endif
-%endmacro
-
-%macro ABSB 2 ; source mmreg, temp mmreg (unused for SSSE3)
-%if cpuflag(ssse3)
-    pabsb   %1, %1
-%else
-    pxor    %2, %2
-    psubb   %2, %1
-    pminub  %1, %2
-%endif
-%endmacro
-
-%macro ABSB2 4 ; src1, src2, tmp1, tmp2 (tmp1/2 unused for SSSE3)
-%if cpuflag(ssse3)
-    pabsb   %1, %1
-    pabsb   %2, %2
-%else
-    pxor    %3, %3
-    pxor    %4, %4
-    psubb   %3, %1
-    psubb   %4, %2
-    pminub  %1, %3
-    pminub  %2, %4
-%endif
-%endmacro
-
-%macro ABSD2 4
-    pxor    %3, %3
-    pxor    %4, %4
-    pcmpgtd %3, %1
-    pcmpgtd %4, %2
-    pxor    %1, %3
-    pxor    %2, %4
-    psubd   %1, %3
-    psubd   %2, %4
-%endmacro
-
-%macro ABS4 6
-    ABS2 %1, %2, %5, %6
-    ABS2 %3, %4, %5, %6
-%endmacro
-
-%macro SPLATB_LOAD 3
-%if cpuflag(ssse3)
-    movd      %1, [%2-3]
-    pshufb    %1, %3
-%else
-    movd      %1, [%2-3] ;to avoid crossing a cacheline
-    punpcklbw %1, %1
-    SPLATW    %1, %1, 3
-%endif
-%endmacro
-
-%macro SPLATB_REG 3
-%if cpuflag(ssse3)
-    movd      %1, %2d
-    pshufb    %1, %3
-%else
-    movd      %1, %2d
-    punpcklbw %1, %1
-    SPLATW    %1, %1, 0
-%endif
-%endmacro
-
-%macro HADDD 2 ; sum junk
-%if sizeof%1 == 32
-%define %2 xmm%2
-    vextracti128 %2, %1, 1
-%define %1 xmm%1
-    paddd   %1, %2
-%endif
-%if mmsize >= 16
-%if cpuflag(xop) && sizeof%1 == 16
-    vphadddq %1, %1
-%endif
-    movhlps %2, %1
-    paddd   %1, %2
-%endif
-%if notcpuflag(xop) || sizeof%1 != 16
-%if cpuflag(mmxext)
-    PSHUFLW %2, %1, q0032
-%else ; mmx
-    mova    %2, %1
-    psrlq   %2, 32
-%endif
-    paddd   %1, %2
-%endif
-%undef %1
-%undef %2
-%endmacro
-
-%macro HADDW 2 ; reg, tmp
-%if cpuflag(xop) && sizeof%1 == 16
-    vphaddwq  %1, %1
-    movhlps   %2, %1
-    paddd     %1, %2
-%else
-    pmaddwd %1, [pw_1]
-    HADDD   %1, %2
-%endif
-%endmacro
-
-%macro HADDPS 3 ; dst, src, tmp
-%if cpuflag(sse3)
-    haddps  %1, %1, %2
-%else
-    movaps  %3, %1
-    shufps  %1, %2, q2020
-    shufps  %3, %2, q3131
-    addps   %1, %3
-%endif
-%endmacro
-
-%macro PALIGNR 4-5
-%if cpuflag(ssse3)
-%if %0==5
-    palignr %1, %2, %3, %4
-%else
-    palignr %1, %2, %3
-%endif
-%else ; [dst,] src1, src2, imm, tmp
-    %define %%dst %1
-%if %0==5
-%ifnidn %1, %2
-    mova    %%dst, %2
-%endif
-    %rotate 1
-%endif
-%ifnidn %4, %2
-    mova    %4, %2
-%endif
-%if mmsize==8
-    psllq   %%dst, (8-%3)*8
-    psrlq   %4, %3*8
-%else
-    pslldq  %%dst, 16-%3
-    psrldq  %4, %3
-%endif
-    por     %%dst, %4
-%endif
-%endmacro
-
-%macro PAVGB 2-4
-%if cpuflag(mmxext)
-    pavgb   %1, %2
-%elif cpuflag(3dnow)
-    pavgusb %1, %2
-%elif cpuflag(mmx)
-    movu   %3, %2
-    por    %3, %1
-    pxor   %1, %2
-    pand   %1, %4
-    psrlq  %1, 1
-    psubb  %3, %1
-    SWAP   %1, %3
-%endif
-%endmacro
-
-%macro PSHUFLW 1+
-    %if mmsize == 8
-        pshufw %1
-    %else
-        pshuflw %1
-    %endif
-%endmacro
-
-%macro PSWAPD 2
-%if cpuflag(mmxext)
-    pshufw    %1, %2, q1032
-%elif cpuflag(3dnowext)
-    pswapd    %1, %2
-%elif cpuflag(3dnow)
-    movq      %1, %2
-    psrlq     %1, 32
-    punpckldq %1, %2
-%endif
-%endmacro
-
-%macro DEINTB 5 ; mask, reg1, mask, reg2, optional src to fill masks from
-%ifnum %5
-    pand   m%3, m%5, m%4 ; src .. y6 .. y4
-    pand   m%1, m%5, m%2 ; dst .. y6 .. y4
-%else
-    mova   m%1, %5
-    pand   m%3, m%1, m%4 ; src .. y6 .. y4
-    pand   m%1, m%1, m%2 ; dst .. y6 .. y4
-%endif
-    psrlw  m%2, 8        ; dst .. y7 .. y5
-    psrlw  m%4, 8        ; src .. y7 .. y5
-%endmacro
-
-%macro SUMSUB_BA 3-4
-%if %0==3
-    padd%1  m%2, m%3
-    padd%1  m%3, m%3
-    psub%1  m%3, m%2
-%else
-%if avx_enabled == 0
-    mova    m%4, m%2
-    padd%1  m%2, m%3
-    psub%1  m%3, m%4
-%else
-    padd%1  m%4, m%2, m%3
-    psub%1  m%3, m%2
-    SWAP    %2, %4
-%endif
-%endif
-%endmacro
-
-%macro SUMSUB_BADC 5-6
-%if %0==6
-    SUMSUB_BA %1, %2, %3, %6
-    SUMSUB_BA %1, %4, %5, %6
-%else
-    padd%1  m%2, m%3
-    padd%1  m%4, m%5
-    padd%1  m%3, m%3
-    padd%1  m%5, m%5
-    psub%1  m%3, m%2
-    psub%1  m%5, m%4
-%endif
-%endmacro
-
-%macro SUMSUB2_AB 4
-%ifnum %3
-    psub%1  m%4, m%2, m%3
-    psub%1  m%4, m%3
-    padd%1  m%2, m%2
-    padd%1  m%2, m%3
-%else
-    mova    m%4, m%2
-    padd%1  m%2, m%2
-    padd%1  m%2, %3
-    psub%1  m%4, %3
-    psub%1  m%4, %3
-%endif
-%endmacro
-
-%macro SUMSUB2_BA 4
-%if avx_enabled == 0
-    mova    m%4, m%2
-    padd%1  m%2, m%3
-    padd%1  m%2, m%3
-    psub%1  m%3, m%4
-    psub%1  m%3, m%4
-%else
-    padd%1  m%4, m%2, m%3
-    padd%1  m%4, m%3
-    psub%1  m%3, m%2
-    psub%1  m%3, m%2
-    SWAP     %2,  %4
-%endif
-%endmacro
-
-%macro SUMSUBD2_AB 5
-%ifnum %4
-    psra%1  m%5, m%2, 1  ; %3: %3>>1
-    psra%1  m%4, m%3, 1  ; %2: %2>>1
-    padd%1  m%4, m%2     ; %3: %3>>1+%2
-    psub%1  m%5, m%3     ; %2: %2>>1-%3
-    SWAP     %2, %5
-    SWAP     %3, %4
-%else
-    mova    %5, m%2
-    mova    %4, m%3
-    psra%1  m%3, 1  ; %3: %3>>1
-    psra%1  m%2, 1  ; %2: %2>>1
-    padd%1  m%3, %5 ; %3: %3>>1+%2
-    psub%1  m%2, %4 ; %2: %2>>1-%3
-%endif
-%endmacro
-
-%macro DCT4_1D 5
-%ifnum %5
-    SUMSUB_BADC w, %4, %1, %3, %2, %5
-    SUMSUB_BA   w, %3, %4, %5
-    SUMSUB2_AB  w, %1, %2, %5
-    SWAP %1, %3, %4, %5, %2
-%else
-    SUMSUB_BADC w, %4, %1, %3, %2
-    SUMSUB_BA   w, %3, %4
-    mova     [%5], m%2
-    SUMSUB2_AB  w, %1, [%5], %2
-    SWAP %1, %3, %4, %2
-%endif
-%endmacro
-
-%macro IDCT4_1D 6-7
-%ifnum %6
-    SUMSUBD2_AB %1, %3, %5, %7, %6
-    ; %3: %3>>1-%5 %5: %3+%5>>1
-    SUMSUB_BA   %1, %4, %2, %7
-    ; %4: %2+%4 %2: %2-%4
-    SUMSUB_BADC %1, %5, %4, %3, %2, %7
-    ; %5: %2+%4 + (%3+%5>>1)
-    ; %4: %2+%4 - (%3+%5>>1)
-    ; %3: %2-%4 + (%3>>1-%5)
-    ; %2: %2-%4 - (%3>>1-%5)
-%else
-%ifidn %1, w
-    SUMSUBD2_AB %1, %3, %5, [%6], [%6+16]
-%else
-    SUMSUBD2_AB %1, %3, %5, [%6], [%6+32]
-%endif
-    SUMSUB_BA   %1, %4, %2
-    SUMSUB_BADC %1, %5, %4, %3, %2
-%endif
-    SWAP %2, %5, %4
-    ; %2: %2+%4 + (%3+%5>>1) row0
-    ; %3: %2-%4 + (%3>>1-%5) row1
-    ; %4: %2-%4 - (%3>>1-%5) row2
-    ; %5: %2+%4 - (%3+%5>>1) row3
-%endmacro
-
-
-%macro LOAD_DIFF 5
-%ifidn %3, none
-    movh       %1, %4
-    movh       %2, %5
-    punpcklbw  %1, %2
-    punpcklbw  %2, %2
-    psubw      %1, %2
-%else
-    movh       %1, %4
-    punpcklbw  %1, %3
-    movh       %2, %5
-    punpcklbw  %2, %3
-    psubw      %1, %2
-%endif
-%endmacro
-
-%macro STORE_DCT 6
-    movq   [%5+%6+ 0], m%1
-    movq   [%5+%6+ 8], m%2
-    movq   [%5+%6+16], m%3
-    movq   [%5+%6+24], m%4
-    movhps [%5+%6+32], m%1
-    movhps [%5+%6+40], m%2
-    movhps [%5+%6+48], m%3
-    movhps [%5+%6+56], m%4
-%endmacro
-
-%macro LOAD_DIFF_8x4P 7-10 r0,r2,0 ; 4x dest, 2x temp, 2x pointer, increment?
-    LOAD_DIFF m%1, m%5, m%7, [%8],      [%9]
-    LOAD_DIFF m%2, m%6, m%7, [%8+r1],   [%9+r3]
-    LOAD_DIFF m%3, m%5, m%7, [%8+2*r1], [%9+2*r3]
-    LOAD_DIFF m%4, m%6, m%7, [%8+r4],   [%9+r5]
-%if %10
-    lea %8, [%8+4*r1]
-    lea %9, [%9+4*r3]
-%endif
-%endmacro
-
-%macro DIFFx2 6-7
-    movh       %3, %5
-    punpcklbw  %3, %4
-    psraw      %1, 6
-    paddsw     %1, %3
-    movh       %3, %6
-    punpcklbw  %3, %4
-    psraw      %2, 6
-    paddsw     %2, %3
-    packuswb   %2, %1
-%endmacro
-
-%macro STORE_DIFF 4
-    movh       %2, %4
-    punpcklbw  %2, %3
-    psraw      %1, 6
-    paddsw     %1, %2
-    packuswb   %1, %1
-    movh       %4, %1
-%endmacro
-
-%macro STORE_DIFFx2 8 ; add1, add2, reg1, reg2, zero, shift, source, stride
-    movh       %3, [%7]
-    movh       %4, [%7+%8]
-    psraw      %1, %6
-    psraw      %2, %6
-    punpcklbw  %3, %5
-    punpcklbw  %4, %5
-    paddw      %3, %1
-    paddw      %4, %2
-    packuswb   %3, %5
-    packuswb   %4, %5
-    movh     [%7], %3
-    movh  [%7+%8], %4
-%endmacro
-
-%macro PMINUB 3 ; dst, src, ignored
-%if cpuflag(mmxext)
-    pminub   %1, %2
-%else ; dst, src, tmp
-    mova     %3, %1
-    psubusb  %3, %2
-    psubb    %1, %3
-%endif
-%endmacro
-
-%macro SPLATW 2-3 0
-%if cpuflag(avx2) && %3 == 0
-    vpbroadcastw %1, %2
-%elif mmsize == 16
-    pshuflw    %1, %2, (%3)*0x55
-    punpcklqdq %1, %1
-%elif cpuflag(mmxext)
-    pshufw     %1, %2, (%3)*0x55
-%else
-    %ifnidn %1, %2
-        mova       %1, %2
-    %endif
-    %if %3 & 2
-        punpckhwd  %1, %1
-    %else
-        punpcklwd  %1, %1
-    %endif
-    %if %3 & 1
-        punpckhwd  %1, %1
-    %else
-        punpcklwd  %1, %1
-    %endif
-%endif
-%endmacro
-
-%macro SPLATD 1
-%if mmsize == 8
-    punpckldq  %1, %1
-%elif cpuflag(sse2)
-    pshufd  %1, %1, 0
-%elif cpuflag(sse)
-    shufps  %1, %1, 0
-%endif
-%endmacro
-
-%macro CLIPUB 3 ;(dst, min, max)
-    pmaxub %1, %2
-    pminub %1, %3
-%endmacro
-
-%macro CLIPW 3 ;(dst, min, max)
-    pmaxsw %1, %2
-    pminsw %1, %3
-%endmacro
-
-%macro PMINSD 3 ; dst, src, tmp/unused
-%if cpuflag(sse4)
-    pminsd    %1, %2
-%elif cpuflag(sse2)
-    cvtdq2ps  %1, %1
-    minps     %1, %2
-    cvtps2dq  %1, %1
-%else
-    mova      %3, %2
-    pcmpgtd   %3, %1
-    pxor      %1, %2
-    pand      %1, %3
-    pxor      %1, %2
-%endif
-%endmacro
-
-%macro PMAXSD 3 ; dst, src, tmp/unused
-%if cpuflag(sse4)
-    pmaxsd    %1, %2
-%else
-    mova      %3, %1
-    pcmpgtd   %3, %2
-    pand      %1, %3
-    pandn     %3, %2
-    por       %1, %3
-%endif
-%endmacro
-
-%macro CLIPD 3-4
-%if cpuflag(sse4);  src/dst, min, max, unused
-    pminsd  %1, %3
-    pmaxsd  %1, %2
-%elif cpuflag(sse2) ; src/dst, min (float), max (float), unused
-    cvtdq2ps  %1, %1
-    minps     %1, %3
-    maxps     %1, %2
-    cvtps2dq  %1, %1
-%else               ; src/dst, min, max, tmp
-    PMINSD    %1, %3, %4
-    PMAXSD    %1, %2, %4
-%endif
-%endmacro
-
-%macro VBROADCASTSS 2 ; dst xmm/ymm, src m32/xmm
-%if cpuflag(avx2)
-    vbroadcastss  %1, %2
-%elif cpuflag(avx)
-    %ifnum sizeof%2         ; avx1 register
-        shufps  xmm%1, xmm%2, xmm%2, q0000
-        %if sizeof%1 >= 32  ; mmsize>=32
-            vinsertf128  %1, %1, xmm%1, 1
-        %endif
-    %else                   ; avx1 memory
-        vbroadcastss  %1, %2
-    %endif
-%else
-    %ifnum sizeof%2         ; sse register
-        shufps  %1, %2, %2, q0000
-    %else                   ; sse memory
-        movss   %1, %2
-        shufps  %1, %1, 0
-    %endif
-%endif
-%endmacro
-
-%macro VBROADCASTSD 2 ; dst xmm/ymm, src m64
-%if cpuflag(avx) && mmsize == 32
-    vbroadcastsd %1, %2
-%elif cpuflag(sse3)
-    movddup      %1, %2
-%else ; sse2
-    movsd        %1, %2
-    movlhps      %1, %1
-%endif
-%endmacro
-
-%macro VPBROADCASTD 2 ; dst xmm/ymm, src m32/xmm
-%if cpuflag(avx2)
-    vpbroadcastd  %1, %2
-%elif cpuflag(avx) && sizeof%1 >= 32
-    %error vpbroadcastd not possible with ymm on avx1. try vbroadcastss
-%else
-    %ifnum sizeof%2         ; sse2 register
-        pshufd  %1, %2, q0000
-    %else                   ; sse memory
-        movd    %1, %2
-        pshufd  %1, %1, 0
-    %endif
-%endif
-%endmacro
-
-%macro VBROADCASTI128 2 ; dst xmm/ymm, src : 128bits val
-%if mmsize > 16
-    vbroadcasti128 %1, %2
-%else
-    mova           %1, %2
-%endif
-%endmacro
-
-%macro SHUFFLE_MASK_W 8
-    %rep 8
-        %if %1>=0x80
-            db %1, %1
-        %else
-            db %1*2
-            db %1*2+1
-        %endif
-        %rotate 1
-    %endrep
-%endmacro
-
-%macro PMOVSXWD 2; dst, src
-%if cpuflag(sse4)
-    pmovsxwd     %1, %2
-%else
-    %ifnidn %1, %2
-    mova         %1, %2
-    %endif
-    punpcklwd    %1, %1
-    psrad        %1, 16
-%endif
-%endmacro
-
-; Wrapper for non-FMA version of fmaddps
-%macro FMULADD_PS 5
-    %if cpuflag(fma3) || cpuflag(fma4)
-        fmaddps %1, %2, %3, %4
-    %elifidn %1, %4
-        mulps   %5, %2, %3
-        addps   %1, %4, %5
-    %else
-        mulps   %1, %2, %3
-        addps   %1, %4
-    %endif
-%endmacro
-
-%macro LSHIFT 2
-%if mmsize > 8
-    pslldq  %1, %2
-%else
-    psllq   %1, 8*(%2)
-%endif
-%endmacro
-
-%macro RSHIFT 2
-%if mmsize > 8
-    psrldq  %1, %2
-%else
-    psrlq   %1, 8*(%2)
-%endif
-%endmacro
-
-%macro MOVHL 2 ; dst, src
-%ifidn %1, %2
-    punpckhqdq %1, %2
-%elif cpuflag(avx)
-    punpckhqdq %1, %2, %2
-%elif cpuflag(sse4)
-    pshufd     %1, %2, q3232 ; pshufd is slow on some older CPUs, so only use it on more modern ones
-%else
-    movhlps    %1, %2        ; may cause an int/float domain transition and has a dependency on dst
-%endif
-%endmacro
-
-; Horizontal Sum of Packed Single precision floats
-; The resulting sum is in all elements.
-%macro HSUMPS 2 ; dst/src, tmp
-%if cpuflag(avx)
-    %if sizeof%1>=32  ; avx
-        vperm2f128  %2, %1, %1, (0)*16+(1)
-        addps       %1, %2
-    %endif
-    shufps      %2, %1, %1, q1032
-    addps       %1, %2
-    shufps      %2, %1, %1, q0321
-    addps       %1, %2
-%else  ; this form is a bit faster than the short avx-like emulation.
-    movaps      %2, %1
-    shufps      %1, %1, q1032
-    addps       %1, %2
-    movaps      %2, %1
-    shufps      %1, %1, q0321
-    addps       %1, %2
-    ; all %1 members should be equal for as long as float a+b==b+a
-%endif
-%endmacro
-
-; Emulate blendvps if not available
-;
-; src_b is destroyed when using emulation with logical operands
-; SSE41 blendv instruction is hard coded to use xmm0 as mask
-%macro BLENDVPS 3 ; dst/src_a, src_b, mask
-%if cpuflag(avx)
-    blendvps  %1, %1, %2, %3
-%elif cpuflag(sse4)
-    %ifnidn %3,xmm0
-        %error sse41 blendvps uses xmm0 as default 3d operand, you used %3
-    %endif
-    blendvps  %1, %2, %3
-%else
-    xorps  %2, %1
-    andps  %2, %3
-    xorps  %1, %2
-%endif
-%endmacro
-
-; Emulate pblendvb if not available
-;
-; src_b is destroyed when using emulation with logical operands
-; SSE41 blendv instruction is hard coded to use xmm0 as mask
-%macro PBLENDVB 3 ; dst/src_a, src_b, mask
-%if cpuflag(avx)
-    %if cpuflag(avx) && notcpuflag(avx2) && sizeof%1 >= 32
-        %error pblendb not possible with ymm on avx1, try blendvps.
-    %endif
-    pblendvb  %1, %1, %2, %3
-%elif cpuflag(sse4)
-    %ifnidn %3,xmm0
-        %error sse41 pblendvd uses xmm0 as default 3d operand, you used %3
-    %endif
-    pblendvb  %1, %2, %3
-%else
-    pxor  %2, %1
-    pand  %2, %3
-    pxor  %1, %2
-%endif
-%endmacro
diff -uparN ffmpeg-4.1/libswresample/x86/audio_convert.asm ffmpeg-y/libswresample/x86/audio_convert.asm
--- ffmpeg-4.1/libswresample/x86/audio_convert.asm	2018-07-17 17:27:43.000000000 +0800
+++ ffmpeg-y/libswresample/x86/audio_convert.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,739 +0,0 @@
-;******************************************************************************
-;* Copyright (c) 2012 Michael Niedermayer
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA 32
-flt2pm31: times 8 dd 4.6566129e-10
-flt2p31 : times 8 dd 2147483648.0
-flt2p15 : times 8 dd 32768.0
-
-word_unpack_shuf : db  0, 1, 4, 5, 8, 9,12,13, 2, 3, 6, 7,10,11,14,15
-
-SECTION .text
-
-
-;to, from, a/u, log2_outsize, log_intsize, const
-%macro PACK_2CH 5-7
-cglobal pack_2ch_%2_to_%1_%3, 3, 4, 6, dst, src, len, src2
-    mov src2q   , [srcq+gprsize]
-    mov srcq    , [srcq]
-    mov dstq    , [dstq]
-%ifidn %3, a
-    test dstq, mmsize-1
-        jne pack_2ch_%2_to_%1_u_int %+ SUFFIX
-    test srcq, mmsize-1
-        jne pack_2ch_%2_to_%1_u_int %+ SUFFIX
-    test src2q, mmsize-1
-        jne pack_2ch_%2_to_%1_u_int %+ SUFFIX
-%else
-pack_2ch_%2_to_%1_u_int %+ SUFFIX:
-%endif
-    lea     srcq , [srcq  + (1<<%5)*lenq]
-    lea     src2q, [src2q + (1<<%5)*lenq]
-    lea     dstq , [dstq  + (2<<%4)*lenq]
-    neg     lenq
-    %7 m0,m1,m2,m3,m4,m5
-.next:
-%if %4 >= %5
-    mov%3     m0, [         srcq +(1<<%5)*lenq]
-    mova      m1, m0
-    mov%3     m2, [         src2q+(1<<%5)*lenq]
-%if %5 == 1
-    punpcklwd m0, m2
-    punpckhwd m1, m2
-%else
-    punpckldq m0, m2
-    punpckhdq m1, m2
-%endif
-    %6 m0,m1,m2,m3,m4,m5
-%else
-    mov%3     m0, [         srcq +(1<<%5)*lenq]
-    mov%3     m1, [mmsize + srcq +(1<<%5)*lenq]
-    mov%3     m2, [         src2q+(1<<%5)*lenq]
-    mov%3     m3, [mmsize + src2q+(1<<%5)*lenq]
-    %6 m0,m1,m2,m3,m4,m5
-    mova      m2, m0
-    punpcklwd m0, m1
-    punpckhwd m2, m1
-    SWAP 1,2
-%endif
-    mov%3 [           dstq+(2<<%4)*lenq], m0
-    mov%3 [  mmsize + dstq+(2<<%4)*lenq], m1
-%if %4 > %5
-    mov%3 [2*mmsize + dstq+(2<<%4)*lenq], m2
-    mov%3 [3*mmsize + dstq+(2<<%4)*lenq], m3
-    add lenq, 4*mmsize/(2<<%4)
-%else
-    add lenq, 2*mmsize/(2<<%4)
-%endif
-        jl .next
-    REP_RET
-%endmacro
-
-%macro UNPACK_2CH 5-7
-cglobal unpack_2ch_%2_to_%1_%3, 3, 4, 7, dst, src, len, dst2
-    mov dst2q   , [dstq+gprsize]
-    mov srcq    , [srcq]
-    mov dstq    , [dstq]
-%ifidn %3, a
-    test dstq, mmsize-1
-        jne unpack_2ch_%2_to_%1_u_int %+ SUFFIX
-    test srcq, mmsize-1
-        jne unpack_2ch_%2_to_%1_u_int %+ SUFFIX
-    test dst2q, mmsize-1
-        jne unpack_2ch_%2_to_%1_u_int %+ SUFFIX
-%else
-unpack_2ch_%2_to_%1_u_int %+ SUFFIX:
-%endif
-    lea     srcq , [srcq  + (2<<%5)*lenq]
-    lea     dstq , [dstq  + (1<<%4)*lenq]
-    lea     dst2q, [dst2q + (1<<%4)*lenq]
-    neg     lenq
-    %7 m0,m1,m2,m3,m4,m5
-    mova      m6, [word_unpack_shuf]
-.next:
-    mov%3     m0, [           srcq +(2<<%5)*lenq]
-    mov%3     m2, [  mmsize + srcq +(2<<%5)*lenq]
-%if %5 == 1
-%ifidn SUFFIX, _ssse3
-    pshufb    m0, m6
-    mova      m1, m0
-    pshufb    m2, m6
-    punpcklqdq m0,m2
-    punpckhqdq m1,m2
-%else
-    mova      m1, m0
-    punpcklwd m0,m2
-    punpckhwd m1,m2
-
-    mova      m2, m0
-    punpcklwd m0,m1
-    punpckhwd m2,m1
-
-    mova      m1, m0
-    punpcklwd m0,m2
-    punpckhwd m1,m2
-%endif
-%else
-    mova      m1, m0
-    shufps    m0, m2, 10001000b
-    shufps    m1, m2, 11011101b
-%endif
-%if %4 < %5
-    mov%3     m2, [2*mmsize + srcq +(2<<%5)*lenq]
-    mova      m3, m2
-    mov%3     m4, [3*mmsize + srcq +(2<<%5)*lenq]
-    shufps    m2, m4, 10001000b
-    shufps    m3, m4, 11011101b
-    SWAP 1,2
-%endif
-    %6 m0,m1,m2,m3,m4,m5
-    mov%3 [           dstq+(1<<%4)*lenq], m0
-%if %4 > %5
-    mov%3 [          dst2q+(1<<%4)*lenq], m2
-    mov%3 [ mmsize +  dstq+(1<<%4)*lenq], m1
-    mov%3 [ mmsize + dst2q+(1<<%4)*lenq], m3
-    add lenq, 2*mmsize/(1<<%4)
-%else
-    mov%3 [          dst2q+(1<<%4)*lenq], m1
-    add lenq, mmsize/(1<<%4)
-%endif
-        jl .next
-    REP_RET
-%endmacro
-
-%macro CONV 5-7
-cglobal %2_to_%1_%3, 3, 3, 6, dst, src, len
-    mov srcq    , [srcq]
-    mov dstq    , [dstq]
-%ifidn %3, a
-    test dstq, mmsize-1
-        jne %2_to_%1_u_int %+ SUFFIX
-    test srcq, mmsize-1
-        jne %2_to_%1_u_int %+ SUFFIX
-%else
-%2_to_%1_u_int %+ SUFFIX:
-%endif
-    lea     srcq , [srcq  + (1<<%5)*lenq]
-    lea     dstq , [dstq  + (1<<%4)*lenq]
-    neg     lenq
-    %7 m0,m1,m2,m3,m4,m5
-.next:
-    mov%3     m0, [           srcq +(1<<%5)*lenq]
-    mov%3     m1, [  mmsize + srcq +(1<<%5)*lenq]
-%if %4 < %5
-    mov%3     m2, [2*mmsize + srcq +(1<<%5)*lenq]
-    mov%3     m3, [3*mmsize + srcq +(1<<%5)*lenq]
-%endif
-    %6 m0,m1,m2,m3,m4,m5
-    mov%3 [           dstq+(1<<%4)*lenq], m0
-    mov%3 [  mmsize + dstq+(1<<%4)*lenq], m1
-%if %4 > %5
-    mov%3 [2*mmsize + dstq+(1<<%4)*lenq], m2
-    mov%3 [3*mmsize + dstq+(1<<%4)*lenq], m3
-    add lenq, 4*mmsize/(1<<%4)
-%else
-    add lenq, 2*mmsize/(1<<%4)
-%endif
-        jl .next
-%if mmsize == 8
-    emms
-    RET
-%else
-    REP_RET
-%endif
-%endmacro
-
-%macro PACK_6CH 8
-cglobal pack_6ch_%2_to_%1_%3, 2, 8, %6, dst, src, src1, src2, src3, src4, src5, len
-%if ARCH_X86_64
-    mov     lend, r2d
-%else
-    %define lend dword r2m
-%endif
-    mov    src1q, [srcq+1*gprsize]
-    mov    src2q, [srcq+2*gprsize]
-    mov    src3q, [srcq+3*gprsize]
-    mov    src4q, [srcq+4*gprsize]
-    mov    src5q, [srcq+5*gprsize]
-    mov     srcq, [srcq]
-    mov     dstq, [dstq]
-%ifidn %3, a
-    test dstq, mmsize-1
-        jne pack_6ch_%2_to_%1_u_int %+ SUFFIX
-    test srcq, mmsize-1
-        jne pack_6ch_%2_to_%1_u_int %+ SUFFIX
-    test src1q, mmsize-1
-        jne pack_6ch_%2_to_%1_u_int %+ SUFFIX
-    test src2q, mmsize-1
-        jne pack_6ch_%2_to_%1_u_int %+ SUFFIX
-    test src3q, mmsize-1
-        jne pack_6ch_%2_to_%1_u_int %+ SUFFIX
-    test src4q, mmsize-1
-        jne pack_6ch_%2_to_%1_u_int %+ SUFFIX
-    test src5q, mmsize-1
-        jne pack_6ch_%2_to_%1_u_int %+ SUFFIX
-%else
-pack_6ch_%2_to_%1_u_int %+ SUFFIX:
-%endif
-    sub    src1q, srcq
-    sub    src2q, srcq
-    sub    src3q, srcq
-    sub    src4q, srcq
-    sub    src5q, srcq
-    %8 x,x,x,x,m7,x
-.loop:
-    mov%3     m0, [srcq      ]
-    mov%3     m1, [srcq+src1q]
-    mov%3     m2, [srcq+src2q]
-    mov%3     m3, [srcq+src3q]
-    mov%3     m4, [srcq+src4q]
-    mov%3     m5, [srcq+src5q]
-%if cpuflag(sse)
-    SBUTTERFLYPS 0, 1, 6
-    SBUTTERFLYPS 2, 3, 6
-    SBUTTERFLYPS 4, 5, 6
-
-%if cpuflag(avx)
-    blendps   m6, m4, m0, 1100b
-%else
-    movaps    m6, m4
-    shufps    m4, m0, q3210
-    SWAP 4,6
-%endif
-    movlhps   m0, m2
-    movhlps   m4, m2
-%if cpuflag(avx)
-    blendps   m2, m5, m1, 1100b
-%else
-    movaps    m2, m5
-    shufps    m5, m1, q3210
-    SWAP 2,5
-%endif
-    movlhps   m1, m3
-    movhlps   m5, m3
-
-    %7 m0,m6,x,x,m7,m3
-    %7 m4,m1,x,x,m7,m3
-    %7 m2,m5,x,x,m7,m3
-
-    mov %+ %3 %+ ps [dstq   ], m0
-    mov %+ %3 %+ ps [dstq+16], m6
-    mov %+ %3 %+ ps [dstq+32], m4
-    mov %+ %3 %+ ps [dstq+48], m1
-    mov %+ %3 %+ ps [dstq+64], m2
-    mov %+ %3 %+ ps [dstq+80], m5
-%else ; mmx
-    SBUTTERFLY dq, 0, 1, 6
-    SBUTTERFLY dq, 2, 3, 6
-    SBUTTERFLY dq, 4, 5, 6
-
-    movq   [dstq   ], m0
-    movq   [dstq+ 8], m2
-    movq   [dstq+16], m4
-    movq   [dstq+24], m1
-    movq   [dstq+32], m3
-    movq   [dstq+40], m5
-%endif
-    add      srcq, mmsize
-    add      dstq, mmsize*6
-    sub      lend, mmsize/4
-    jg .loop
-%if mmsize == 8
-    emms
-    RET
-%else
-    REP_RET
-%endif
-%endmacro
-
-%macro UNPACK_6CH 8
-cglobal unpack_6ch_%2_to_%1_%3, 2, 8, %6, dst, src, dst1, dst2, dst3, dst4, dst5, len
-%if ARCH_X86_64
-    mov     lend, r2d
-%else
-    %define lend dword r2m
-%endif
-    mov    dst1q, [dstq+1*gprsize]
-    mov    dst2q, [dstq+2*gprsize]
-    mov    dst3q, [dstq+3*gprsize]
-    mov    dst4q, [dstq+4*gprsize]
-    mov    dst5q, [dstq+5*gprsize]
-    mov     dstq, [dstq]
-    mov     srcq, [srcq]
-%ifidn %3, a
-    test dstq, mmsize-1
-        jne unpack_6ch_%2_to_%1_u_int %+ SUFFIX
-    test srcq, mmsize-1
-        jne unpack_6ch_%2_to_%1_u_int %+ SUFFIX
-    test dst1q, mmsize-1
-        jne unpack_6ch_%2_to_%1_u_int %+ SUFFIX
-    test dst2q, mmsize-1
-        jne unpack_6ch_%2_to_%1_u_int %+ SUFFIX
-    test dst3q, mmsize-1
-        jne unpack_6ch_%2_to_%1_u_int %+ SUFFIX
-    test dst4q, mmsize-1
-        jne unpack_6ch_%2_to_%1_u_int %+ SUFFIX
-    test dst5q, mmsize-1
-        jne unpack_6ch_%2_to_%1_u_int %+ SUFFIX
-%else
-unpack_6ch_%2_to_%1_u_int %+ SUFFIX:
-%endif
-    sub    dst1q, dstq
-    sub    dst2q, dstq
-    sub    dst3q, dstq
-    sub    dst4q, dstq
-    sub    dst5q, dstq
-    %8 x,x,x,x,m7,x
-.loop:
-    mov%3     m0, [srcq   ]
-    mov%3     m1, [srcq+16]
-    mov%3     m2, [srcq+32]
-    mov%3     m3, [srcq+48]
-    mov%3     m4, [srcq+64]
-    mov%3     m5, [srcq+80]
-
-    SBUTTERFLYPS 0, 3, 6
-    SBUTTERFLYPS 1, 4, 6
-    SBUTTERFLYPS 2, 5, 6
-    SBUTTERFLYPS 0, 4, 6
-    SBUTTERFLYPS 3, 2, 6
-    SBUTTERFLYPS 1, 5, 6
-    SWAP 1, 4
-    SWAP 2, 3
-
-    %7 m0,m1,x,x,m7,m6
-    %7 m2,m3,x,x,m7,m6
-    %7 m4,m5,x,x,m7,m6
-
-    mov %+ %3 %+ ps [dstq      ], m0
-    mov %+ %3 %+ ps [dstq+dst1q], m1
-    mov %+ %3 %+ ps [dstq+dst2q], m2
-    mov %+ %3 %+ ps [dstq+dst3q], m3
-    mov %+ %3 %+ ps [dstq+dst4q], m4
-    mov %+ %3 %+ ps [dstq+dst5q], m5
-
-    add      srcq, mmsize*6
-    add      dstq, mmsize
-    sub      lend, mmsize/4
-    jg .loop
-    REP_RET
-%endmacro
-
-%define PACK_8CH_GPRS (10 * ARCH_X86_64) + ((6 + HAVE_ALIGNED_STACK) * ARCH_X86_32)
-
-%macro PACK_8CH 8
-cglobal pack_8ch_%2_to_%1_%3, 2, PACK_8CH_GPRS, %6, ARCH_X86_32*48, dst, src, len, src1, src2, src3, src4, src5, src6, src7
-    mov     dstq, [dstq]
-%if ARCH_X86_32
-    DEFINE_ARGS dst, src, src2, src3, src4, src5, src6
-    %define lend dword r2m
-    %define src1q r0q
-    %define src1m dword [rsp+32]
-%if HAVE_ALIGNED_STACK == 0
-    DEFINE_ARGS dst, src, src2, src3, src5, src6
-    %define src4q r0q
-    %define src4m dword [rsp+36]
-%endif
-    %define src7q r0q
-    %define src7m dword [rsp+40]
-    mov     dstm, dstq
-%endif
-    mov    src7q, [srcq+7*gprsize]
-    mov    src6q, [srcq+6*gprsize]
-%if ARCH_X86_32
-    mov    src7m, src7q
-%endif
-    mov    src5q, [srcq+5*gprsize]
-    mov    src4q, [srcq+4*gprsize]
-    mov    src3q, [srcq+3*gprsize]
-%if ARCH_X86_32 && HAVE_ALIGNED_STACK == 0
-    mov    src4m, src4q
-%endif
-    mov    src2q, [srcq+2*gprsize]
-    mov    src1q, [srcq+1*gprsize]
-    mov     srcq, [srcq]
-%ifidn %3, a
-%if ARCH_X86_32
-    test dstmp, mmsize-1
-%else
-    test dstq, mmsize-1
-%endif
-        jne pack_8ch_%2_to_%1_u_int %+ SUFFIX
-    test srcq, mmsize-1
-        jne pack_8ch_%2_to_%1_u_int %+ SUFFIX
-    test src1q, mmsize-1
-        jne pack_8ch_%2_to_%1_u_int %+ SUFFIX
-    test src2q, mmsize-1
-        jne pack_8ch_%2_to_%1_u_int %+ SUFFIX
-    test src3q, mmsize-1
-        jne pack_8ch_%2_to_%1_u_int %+ SUFFIX
-%if ARCH_X86_32 && HAVE_ALIGNED_STACK == 0
-    test src4m, mmsize-1
-%else
-    test src4q, mmsize-1
-%endif
-        jne pack_8ch_%2_to_%1_u_int %+ SUFFIX
-    test src5q, mmsize-1
-        jne pack_8ch_%2_to_%1_u_int %+ SUFFIX
-    test src6q, mmsize-1
-        jne pack_8ch_%2_to_%1_u_int %+ SUFFIX
-%if ARCH_X86_32
-    test src7m, mmsize-1
-%else
-    test src7q, mmsize-1
-%endif
-        jne pack_8ch_%2_to_%1_u_int %+ SUFFIX
-%else
-pack_8ch_%2_to_%1_u_int %+ SUFFIX:
-%endif
-    sub    src1q, srcq
-    sub    src2q, srcq
-    sub    src3q, srcq
-%if ARCH_X86_64 || HAVE_ALIGNED_STACK
-    sub    src4q, srcq
-%else
-    sub    src4m, srcq
-%endif
-    sub    src5q, srcq
-    sub    src6q, srcq
-%if ARCH_X86_64
-    sub    src7q, srcq
-%else
-    mov src1m, src1q
-    sub src7m, srcq
-%endif
-
-%if ARCH_X86_64
-    %8 x,x,x,x,m9,x
-%elifidn %1, int32
-    %define m9 [flt2p31]
-%else
-    %define m9 [flt2pm31]
-%endif
-
-.loop:
-    mov%3     m0, [srcq      ]
-    mov%3     m1, [srcq+src1q]
-    mov%3     m2, [srcq+src2q]
-%if ARCH_X86_32 && HAVE_ALIGNED_STACK == 0
-    mov    src4q, src4m
-%endif
-    mov%3     m3, [srcq+src3q]
-    mov%3     m4, [srcq+src4q]
-    mov%3     m5, [srcq+src5q]
-%if ARCH_X86_32
-    mov    src7q, src7m
-%endif
-    mov%3     m6, [srcq+src6q]
-    mov%3     m7, [srcq+src7q]
-
-%if ARCH_X86_64
-    TRANSPOSE8x4D 0, 1, 2, 3, 4, 5, 6, 7, 8
-
-    %7 m0,m1,x,x,m9,m8
-    %7 m2,m3,x,x,m9,m8
-    %7 m4,m5,x,x,m9,m8
-    %7 m6,m7,x,x,m9,m8
-
-    mov%3 [dstq], m0
-%else
-    mov     dstq, dstm
-
-    TRANSPOSE8x4D 0, 1, 2, 3, 4, 5, 6, 7, [rsp], [rsp+16], 1
-
-    %7 m0,m1,x,x,m9,m2
-    mova     m2, [rsp]
-    mov%3   [dstq], m0
-    %7 m2,m3,x,x,m9,m0
-    %7 m4,m5,x,x,m9,m0
-    %7 m6,m7,x,x,m9,m0
-
-%endif
-
-    mov%3 [dstq+16],  m1
-    mov%3 [dstq+32],  m2
-    mov%3 [dstq+48],  m3
-    mov%3 [dstq+64],  m4
-    mov%3 [dstq+80],  m5
-    mov%3 [dstq+96],  m6
-    mov%3 [dstq+112], m7
-
-    add      srcq, mmsize
-    add      dstq, mmsize*8
-%if ARCH_X86_32
-    mov      dstm, dstq
-    mov      src1q, src1m
-%endif
-    sub      lend, mmsize/4
-    jg .loop
-    REP_RET
-%endmacro
-
-%macro INT16_TO_INT32_N 6
-    pxor      m2, m2
-    pxor      m3, m3
-    punpcklwd m2, m1
-    punpckhwd m3, m1
-    SWAP 4,0
-    pxor      m0, m0
-    pxor      m1, m1
-    punpcklwd m0, m4
-    punpckhwd m1, m4
-%endmacro
-
-%macro INT32_TO_INT16_N 6
-    psrad     m0, 16
-    psrad     m1, 16
-    psrad     m2, 16
-    psrad     m3, 16
-    packssdw  m0, m1
-    packssdw  m2, m3
-    SWAP 1,2
-%endmacro
-
-%macro INT32_TO_FLOAT_INIT 6
-    mova      %5, [flt2pm31]
-%endmacro
-%macro INT32_TO_FLOAT_N 6
-    cvtdq2ps  %1, %1
-    cvtdq2ps  %2, %2
-    mulps %1, %1, %5
-    mulps %2, %2, %5
-%endmacro
-
-%macro FLOAT_TO_INT32_INIT 6
-    mova      %5, [flt2p31]
-%endmacro
-%macro FLOAT_TO_INT32_N 6
-    mulps %1, %5
-    mulps %2, %5
-    cvtps2dq  %6, %1
-    cmpps %1, %1, %5, 5
-    paddd %1, %6
-    cvtps2dq  %6, %2
-    cmpps %2, %2, %5, 5
-    paddd %2, %6
-%endmacro
-
-%macro INT16_TO_FLOAT_INIT 6
-    mova      m5, [flt2pm31]
-%endmacro
-%macro INT16_TO_FLOAT_N 6
-    INT16_TO_INT32_N %1,%2,%3,%4,%5,%6
-    cvtdq2ps  m0, m0
-    cvtdq2ps  m1, m1
-    cvtdq2ps  m2, m2
-    cvtdq2ps  m3, m3
-    mulps m0, m0, m5
-    mulps m1, m1, m5
-    mulps m2, m2, m5
-    mulps m3, m3, m5
-%endmacro
-
-%macro FLOAT_TO_INT16_INIT 6
-    mova      m5, [flt2p15]
-%endmacro
-%macro FLOAT_TO_INT16_N 6
-    mulps m0, m5
-    mulps m1, m5
-    mulps m2, m5
-    mulps m3, m5
-    cvtps2dq  m0, m0
-    cvtps2dq  m1, m1
-    packssdw  m0, m1
-    cvtps2dq  m1, m2
-    cvtps2dq  m3, m3
-    packssdw  m1, m3
-%endmacro
-
-%macro NOP_N 0-6
-%endmacro
-
-INIT_MMX mmx
-CONV int32, int16, u, 2, 1, INT16_TO_INT32_N, NOP_N
-CONV int32, int16, a, 2, 1, INT16_TO_INT32_N, NOP_N
-CONV int16, int32, u, 1, 2, INT32_TO_INT16_N, NOP_N
-CONV int16, int32, a, 1, 2, INT32_TO_INT16_N, NOP_N
-
-PACK_6CH float, float, u, 2, 2, 0, NOP_N, NOP_N
-PACK_6CH float, float, a, 2, 2, 0, NOP_N, NOP_N
-
-INIT_XMM sse
-PACK_6CH float, float, u, 2, 2, 7, NOP_N, NOP_N
-PACK_6CH float, float, a, 2, 2, 7, NOP_N, NOP_N
-
-UNPACK_6CH float, float, u, 2, 2, 7, NOP_N, NOP_N
-UNPACK_6CH float, float, a, 2, 2, 7, NOP_N, NOP_N
-
-INIT_XMM sse2
-CONV int32, int16, u, 2, 1, INT16_TO_INT32_N, NOP_N
-CONV int32, int16, a, 2, 1, INT16_TO_INT32_N, NOP_N
-CONV int16, int32, u, 1, 2, INT32_TO_INT16_N, NOP_N
-CONV int16, int32, a, 1, 2, INT32_TO_INT16_N, NOP_N
-
-PACK_2CH int16, int16, u, 1, 1, NOP_N, NOP_N
-PACK_2CH int16, int16, a, 1, 1, NOP_N, NOP_N
-PACK_2CH int32, int32, u, 2, 2, NOP_N, NOP_N
-PACK_2CH int32, int32, a, 2, 2, NOP_N, NOP_N
-PACK_2CH int32, int16, u, 2, 1, INT16_TO_INT32_N, NOP_N
-PACK_2CH int32, int16, a, 2, 1, INT16_TO_INT32_N, NOP_N
-PACK_2CH int16, int32, u, 1, 2, INT32_TO_INT16_N, NOP_N
-PACK_2CH int16, int32, a, 1, 2, INT32_TO_INT16_N, NOP_N
-
-UNPACK_2CH int16, int16, u, 1, 1, NOP_N, NOP_N
-UNPACK_2CH int16, int16, a, 1, 1, NOP_N, NOP_N
-UNPACK_2CH int32, int32, u, 2, 2, NOP_N, NOP_N
-UNPACK_2CH int32, int32, a, 2, 2, NOP_N, NOP_N
-UNPACK_2CH int32, int16, u, 2, 1, INT16_TO_INT32_N, NOP_N
-UNPACK_2CH int32, int16, a, 2, 1, INT16_TO_INT32_N, NOP_N
-UNPACK_2CH int16, int32, u, 1, 2, INT32_TO_INT16_N, NOP_N
-UNPACK_2CH int16, int32, a, 1, 2, INT32_TO_INT16_N, NOP_N
-
-CONV float, int32, u, 2, 2, INT32_TO_FLOAT_N, INT32_TO_FLOAT_INIT
-CONV float, int32, a, 2, 2, INT32_TO_FLOAT_N, INT32_TO_FLOAT_INIT
-CONV int32, float, u, 2, 2, FLOAT_TO_INT32_N, FLOAT_TO_INT32_INIT
-CONV int32, float, a, 2, 2, FLOAT_TO_INT32_N, FLOAT_TO_INT32_INIT
-CONV float, int16, u, 2, 1, INT16_TO_FLOAT_N, INT16_TO_FLOAT_INIT
-CONV float, int16, a, 2, 1, INT16_TO_FLOAT_N, INT16_TO_FLOAT_INIT
-CONV int16, float, u, 1, 2, FLOAT_TO_INT16_N, FLOAT_TO_INT16_INIT
-CONV int16, float, a, 1, 2, FLOAT_TO_INT16_N, FLOAT_TO_INT16_INIT
-
-PACK_2CH float, int32, u, 2, 2, INT32_TO_FLOAT_N, INT32_TO_FLOAT_INIT
-PACK_2CH float, int32, a, 2, 2, INT32_TO_FLOAT_N, INT32_TO_FLOAT_INIT
-PACK_2CH int32, float, u, 2, 2, FLOAT_TO_INT32_N, FLOAT_TO_INT32_INIT
-PACK_2CH int32, float, a, 2, 2, FLOAT_TO_INT32_N, FLOAT_TO_INT32_INIT
-PACK_2CH float, int16, u, 2, 1, INT16_TO_FLOAT_N, INT16_TO_FLOAT_INIT
-PACK_2CH float, int16, a, 2, 1, INT16_TO_FLOAT_N, INT16_TO_FLOAT_INIT
-PACK_2CH int16, float, u, 1, 2, FLOAT_TO_INT16_N, FLOAT_TO_INT16_INIT
-PACK_2CH int16, float, a, 1, 2, FLOAT_TO_INT16_N, FLOAT_TO_INT16_INIT
-
-UNPACK_2CH float, int32, u, 2, 2, INT32_TO_FLOAT_N, INT32_TO_FLOAT_INIT
-UNPACK_2CH float, int32, a, 2, 2, INT32_TO_FLOAT_N, INT32_TO_FLOAT_INIT
-UNPACK_2CH int32, float, u, 2, 2, FLOAT_TO_INT32_N, FLOAT_TO_INT32_INIT
-UNPACK_2CH int32, float, a, 2, 2, FLOAT_TO_INT32_N, FLOAT_TO_INT32_INIT
-UNPACK_2CH float, int16, u, 2, 1, INT16_TO_FLOAT_N, INT16_TO_FLOAT_INIT
-UNPACK_2CH float, int16, a, 2, 1, INT16_TO_FLOAT_N, INT16_TO_FLOAT_INIT
-UNPACK_2CH int16, float, u, 1, 2, FLOAT_TO_INT16_N, FLOAT_TO_INT16_INIT
-UNPACK_2CH int16, float, a, 1, 2, FLOAT_TO_INT16_N, FLOAT_TO_INT16_INIT
-
-PACK_6CH float, int32, u, 2, 2, 8, INT32_TO_FLOAT_N, INT32_TO_FLOAT_INIT
-PACK_6CH float, int32, a, 2, 2, 8, INT32_TO_FLOAT_N, INT32_TO_FLOAT_INIT
-PACK_6CH int32, float, u, 2, 2, 8, FLOAT_TO_INT32_N, FLOAT_TO_INT32_INIT
-PACK_6CH int32, float, a, 2, 2, 8, FLOAT_TO_INT32_N, FLOAT_TO_INT32_INIT
-
-UNPACK_6CH float, int32, u, 2, 2, 8, INT32_TO_FLOAT_N, INT32_TO_FLOAT_INIT
-UNPACK_6CH float, int32, a, 2, 2, 8, INT32_TO_FLOAT_N, INT32_TO_FLOAT_INIT
-UNPACK_6CH int32, float, u, 2, 2, 8, FLOAT_TO_INT32_N, FLOAT_TO_INT32_INIT
-UNPACK_6CH int32, float, a, 2, 2, 8, FLOAT_TO_INT32_N, FLOAT_TO_INT32_INIT
-
-PACK_8CH float, float, u, 2, 2, 9, NOP_N, NOP_N
-PACK_8CH float, float, a, 2, 2, 9, NOP_N, NOP_N
-
-PACK_8CH float, int32, u, 2, 2, 10, INT32_TO_FLOAT_N, INT32_TO_FLOAT_INIT
-PACK_8CH float, int32, a, 2, 2, 10, INT32_TO_FLOAT_N, INT32_TO_FLOAT_INIT
-PACK_8CH int32, float, u, 2, 2, 10, FLOAT_TO_INT32_N, FLOAT_TO_INT32_INIT
-PACK_8CH int32, float, a, 2, 2, 10, FLOAT_TO_INT32_N, FLOAT_TO_INT32_INIT
-
-INIT_XMM ssse3
-UNPACK_2CH int16, int16, u, 1, 1, NOP_N, NOP_N
-UNPACK_2CH int16, int16, a, 1, 1, NOP_N, NOP_N
-UNPACK_2CH int32, int16, u, 2, 1, INT16_TO_INT32_N, NOP_N
-UNPACK_2CH int32, int16, a, 2, 1, INT16_TO_INT32_N, NOP_N
-UNPACK_2CH float, int16, u, 2, 1, INT16_TO_FLOAT_N, INT16_TO_FLOAT_INIT
-UNPACK_2CH float, int16, a, 2, 1, INT16_TO_FLOAT_N, INT16_TO_FLOAT_INIT
-
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-PACK_6CH float, float, u, 2, 2, 8, NOP_N, NOP_N
-PACK_6CH float, float, a, 2, 2, 8, NOP_N, NOP_N
-
-UNPACK_6CH float, float, u, 2, 2, 8, NOP_N, NOP_N
-UNPACK_6CH float, float, a, 2, 2, 8, NOP_N, NOP_N
-
-PACK_6CH float, int32, u, 2, 2, 8, INT32_TO_FLOAT_N, INT32_TO_FLOAT_INIT
-PACK_6CH float, int32, a, 2, 2, 8, INT32_TO_FLOAT_N, INT32_TO_FLOAT_INIT
-PACK_6CH int32, float, u, 2, 2, 8, FLOAT_TO_INT32_N, FLOAT_TO_INT32_INIT
-PACK_6CH int32, float, a, 2, 2, 8, FLOAT_TO_INT32_N, FLOAT_TO_INT32_INIT
-
-UNPACK_6CH float, int32, u, 2, 2, 8, INT32_TO_FLOAT_N, INT32_TO_FLOAT_INIT
-UNPACK_6CH float, int32, a, 2, 2, 8, INT32_TO_FLOAT_N, INT32_TO_FLOAT_INIT
-UNPACK_6CH int32, float, u, 2, 2, 8, FLOAT_TO_INT32_N, FLOAT_TO_INT32_INIT
-UNPACK_6CH int32, float, a, 2, 2, 8, FLOAT_TO_INT32_N, FLOAT_TO_INT32_INIT
-
-PACK_8CH float, float, u, 2, 2, 9, NOP_N, NOP_N
-PACK_8CH float, float, a, 2, 2, 9, NOP_N, NOP_N
-
-PACK_8CH float, int32, u, 2, 2, 10, INT32_TO_FLOAT_N, INT32_TO_FLOAT_INIT
-PACK_8CH float, int32, a, 2, 2, 10, INT32_TO_FLOAT_N, INT32_TO_FLOAT_INIT
-PACK_8CH int32, float, u, 2, 2, 10, FLOAT_TO_INT32_N, FLOAT_TO_INT32_INIT
-PACK_8CH int32, float, a, 2, 2, 10, FLOAT_TO_INT32_N, FLOAT_TO_INT32_INIT
-
-INIT_YMM avx
-CONV float, int32, u, 2, 2, INT32_TO_FLOAT_N, INT32_TO_FLOAT_INIT
-CONV float, int32, a, 2, 2, INT32_TO_FLOAT_N, INT32_TO_FLOAT_INIT
-%endif
-
-%if HAVE_AVX2_EXTERNAL
-INIT_YMM avx2
-CONV int32, float, u, 2, 2, FLOAT_TO_INT32_N, FLOAT_TO_INT32_INIT
-CONV int32, float, a, 2, 2, FLOAT_TO_INT32_N, FLOAT_TO_INT32_INIT
-%endif
diff -uparN ffmpeg-4.1/libswresample/x86/rematrix.asm ffmpeg-y/libswresample/x86/rematrix.asm
--- ffmpeg-4.1/libswresample/x86/rematrix.asm	2018-01-01 06:35:49.000000000 +0800
+++ ffmpeg-y/libswresample/x86/rematrix.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,250 +0,0 @@
-;******************************************************************************
-;* Copyright (c) 2012 Michael Niedermayer
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-
-SECTION_RODATA 32
-dw1: times 8  dd 1
-w1 : times 16 dw 1
-
-SECTION .text
-
-%macro MIX2_FLT 1
-cglobal mix_2_1_%1_float, 7, 7, 6, out, in1, in2, coeffp, index1, index2, len
-%ifidn %1, a
-    test in1q, mmsize-1
-        jne mix_2_1_float_u_int %+ SUFFIX
-    test in2q, mmsize-1
-        jne mix_2_1_float_u_int %+ SUFFIX
-    test outq, mmsize-1
-        jne mix_2_1_float_u_int %+ SUFFIX
-%else
-mix_2_1_float_u_int %+ SUFFIX:
-%endif
-    VBROADCASTSS m4, [coeffpq + 4*index1q]
-    VBROADCASTSS m5, [coeffpq + 4*index2q]
-    shl lend    , 2
-    add in1q    , lenq
-    add in2q    , lenq
-    add outq    , lenq
-    neg lenq
-.next:
-%ifidn %1, a
-    mulps        m0, m4, [in1q + lenq         ]
-    mulps        m1, m5, [in2q + lenq         ]
-    mulps        m2, m4, [in1q + lenq + mmsize]
-    mulps        m3, m5, [in2q + lenq + mmsize]
-%else
-    movu         m0, [in1q + lenq         ]
-    movu         m1, [in2q + lenq         ]
-    movu         m2, [in1q + lenq + mmsize]
-    movu         m3, [in2q + lenq + mmsize]
-    mulps        m0, m0, m4
-    mulps        m1, m1, m5
-    mulps        m2, m2, m4
-    mulps        m3, m3, m5
-%endif
-    addps        m0, m0, m1
-    addps        m2, m2, m3
-    mov%1  [outq + lenq         ], m0
-    mov%1  [outq + lenq + mmsize], m2
-    add        lenq, mmsize*2
-        jl .next
-    REP_RET
-%endmacro
-
-%macro MIX1_FLT 1
-cglobal mix_1_1_%1_float, 5, 5, 3, out, in, coeffp, index, len
-%ifidn %1, a
-    test inq, mmsize-1
-        jne mix_1_1_float_u_int %+ SUFFIX
-    test outq, mmsize-1
-        jne mix_1_1_float_u_int %+ SUFFIX
-%else
-mix_1_1_float_u_int %+ SUFFIX:
-%endif
-    VBROADCASTSS m2, [coeffpq + 4*indexq]
-    shl lenq    , 2
-    add inq     , lenq
-    add outq    , lenq
-    neg lenq
-.next:
-%ifidn %1, a
-    mulps        m0, m2, [inq + lenq         ]
-    mulps        m1, m2, [inq + lenq + mmsize]
-%else
-    movu         m0, [inq + lenq         ]
-    movu         m1, [inq + lenq + mmsize]
-    mulps        m0, m0, m2
-    mulps        m1, m1, m2
-%endif
-    mov%1  [outq + lenq         ], m0
-    mov%1  [outq + lenq + mmsize], m1
-    add        lenq, mmsize*2
-        jl .next
-    REP_RET
-%endmacro
-
-%macro MIX1_INT16 1
-cglobal mix_1_1_%1_int16, 5, 5, 6, out, in, coeffp, index, len
-%ifidn %1, a
-    test inq, mmsize-1
-        jne mix_1_1_int16_u_int %+ SUFFIX
-    test outq, mmsize-1
-        jne mix_1_1_int16_u_int %+ SUFFIX
-%else
-mix_1_1_int16_u_int %+ SUFFIX:
-%endif
-    movd   m4, [coeffpq + 4*indexq]
-    SPLATW m5, m4
-    psllq  m4, 32
-    psrlq  m4, 48
-    mova   m0, [w1]
-    psllw  m0, m4
-    psrlw  m0, 1
-    punpcklwd m5, m0
-    add lenq    , lenq
-    add inq     , lenq
-    add outq    , lenq
-    neg lenq
-.next:
-    mov%1        m0, [inq + lenq         ]
-    mov%1        m2, [inq + lenq + mmsize]
-    mova         m1, m0
-    mova         m3, m2
-    punpcklwd    m0, [w1]
-    punpckhwd    m1, [w1]
-    punpcklwd    m2, [w1]
-    punpckhwd    m3, [w1]
-    pmaddwd      m0, m5
-    pmaddwd      m1, m5
-    pmaddwd      m2, m5
-    pmaddwd      m3, m5
-    psrad        m0, m4
-    psrad        m1, m4
-    psrad        m2, m4
-    psrad        m3, m4
-    packssdw     m0, m1
-    packssdw     m2, m3
-    mov%1  [outq + lenq         ], m0
-    mov%1  [outq + lenq + mmsize], m2
-    add        lenq, mmsize*2
-        jl .next
-%if mmsize == 8
-    emms
-    RET
-%else
-    REP_RET
-%endif
-%endmacro
-
-%macro MIX2_INT16 1
-cglobal mix_2_1_%1_int16, 7, 7, 8, out, in1, in2, coeffp, index1, index2, len
-%ifidn %1, a
-    test in1q, mmsize-1
-        jne mix_2_1_int16_u_int %+ SUFFIX
-    test in2q, mmsize-1
-        jne mix_2_1_int16_u_int %+ SUFFIX
-    test outq, mmsize-1
-        jne mix_2_1_int16_u_int %+ SUFFIX
-%else
-mix_2_1_int16_u_int %+ SUFFIX:
-%endif
-    movd   m4, [coeffpq + 4*index1q]
-    movd   m6, [coeffpq + 4*index2q]
-    SPLATW m5, m4
-    SPLATW m6, m6
-    psllq  m4, 32
-    psrlq  m4, 48
-    mova   m7, [dw1]
-    pslld  m7, m4
-    psrld  m7, 1
-    punpcklwd m5, m6
-    add lend    , lend
-    add in1q    , lenq
-    add in2q    , lenq
-    add outq    , lenq
-    neg lenq
-.next:
-    mov%1        m0, [in1q + lenq         ]
-    mov%1        m2, [in2q + lenq         ]
-    mova         m1, m0
-    punpcklwd    m0, m2
-    punpckhwd    m1, m2
-
-    mov%1        m2, [in1q + lenq + mmsize]
-    mov%1        m6, [in2q + lenq + mmsize]
-    mova         m3, m2
-    punpcklwd    m2, m6
-    punpckhwd    m3, m6
-
-    pmaddwd      m0, m5
-    pmaddwd      m1, m5
-    pmaddwd      m2, m5
-    pmaddwd      m3, m5
-    paddd        m0, m7
-    paddd        m1, m7
-    paddd        m2, m7
-    paddd        m3, m7
-    psrad        m0, m4
-    psrad        m1, m4
-    psrad        m2, m4
-    psrad        m3, m4
-    packssdw     m0, m1
-    packssdw     m2, m3
-    mov%1  [outq + lenq         ], m0
-    mov%1  [outq + lenq + mmsize], m2
-    add        lenq, mmsize*2
-        jl .next
-%if mmsize == 8
-    emms
-    RET
-%else
-    REP_RET
-%endif
-%endmacro
-
-
-INIT_MMX mmx
-MIX1_INT16 u
-MIX1_INT16 a
-MIX2_INT16 u
-MIX2_INT16 a
-
-INIT_XMM sse
-MIX2_FLT u
-MIX2_FLT a
-MIX1_FLT u
-MIX1_FLT a
-
-INIT_XMM sse2
-MIX1_INT16 u
-MIX1_INT16 a
-MIX2_INT16 u
-MIX2_INT16 a
-
-%if HAVE_AVX_EXTERNAL
-INIT_YMM avx
-MIX2_FLT u
-MIX2_FLT a
-MIX1_FLT u
-MIX1_FLT a
-%endif
diff -uparN ffmpeg-4.1/libswresample/x86/resample.asm ffmpeg-y/libswresample/x86/resample.asm
--- ffmpeg-4.1/libswresample/x86/resample.asm	2018-11-02 02:34:28.000000000 +0800
+++ ffmpeg-y/libswresample/x86/resample.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,619 +0,0 @@
-;******************************************************************************
-;* Copyright (c) 2012 Michael Niedermayer
-;* Copyright (c) 2014 James Almer <jamrial <at> gmail.com>
-;* Copyright (c) 2014 Ronald S. Bultje <rsbultje@gmail.com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-%if ARCH_X86_64
-%define pointer resq
-%else
-%define pointer resd
-%endif
-
-struc ResampleContext
-    .av_class:              pointer 1
-    .filter_bank:           pointer 1
-    .filter_length:         resd 1
-    .filter_alloc:          resd 1
-    .ideal_dst_incr:        resd 1
-    .dst_incr:              resd 1
-    .dst_incr_div:          resd 1
-    .dst_incr_mod:          resd 1
-    .index:                 resd 1
-    .frac:                  resd 1
-    .src_incr:              resd 1
-    .compensation_distance: resd 1
-    .phase_count:           resd 1
-
-    ; there's a few more here but we only care about the first few
-endstruc
-
-SECTION_RODATA
-
-pf_1:      dd 1.0
-pdbl_1:    dq 1.0
-pd_0x4000: dd 0x4000
-
-SECTION .text
-
-; FIXME remove unneeded variables (index_incr, phase_mask)
-%macro RESAMPLE_FNS 3-5 ; format [float or int16], bps, log2_bps, float op suffix [s or d], 1.0 constant
-; int resample_common_$format(ResampleContext *ctx, $format *dst,
-;                             const $format *src, int size, int update_ctx)
-%if ARCH_X86_64 ; unix64 and win64
-cglobal resample_common_%1, 0, 15, 2, ctx, dst, src, phase_count, index, frac, \
-                                      dst_incr_mod, size, min_filter_count_x4, \
-                                      min_filter_len_x4, dst_incr_div, src_incr, \
-                                      phase_mask, dst_end, filter_bank
-
-    ; use red-zone for variable storage
-%define ctx_stackq            [rsp-0x8]
-%define src_stackq            [rsp-0x10]
-%if WIN64
-%define update_context_stackd r4m
-%else ; unix64
-%define update_context_stackd [rsp-0x14]
-%endif
-
-    ; load as many variables in registers as possible; for the rest, store
-    ; on stack so that we have 'ctx' available as one extra register
-    mov                        sized, r3d
-%if UNIX64
-    mov        update_context_stackd, r4d
-%endif
-    mov                       indexd, [ctxq+ResampleContext.index]
-    mov                        fracd, [ctxq+ResampleContext.frac]
-    mov                dst_incr_modd, [ctxq+ResampleContext.dst_incr_mod]
-    mov                 filter_bankq, [ctxq+ResampleContext.filter_bank]
-    mov                    src_incrd, [ctxq+ResampleContext.src_incr]
-    mov                   ctx_stackq, ctxq
-    mov           min_filter_len_x4d, [ctxq+ResampleContext.filter_length]
-    mov                dst_incr_divd, [ctxq+ResampleContext.dst_incr_div]
-    shl           min_filter_len_x4d, %3
-    lea                     dst_endq, [dstq+sizeq*%2]
-
-%if UNIX64
-    mov                          ecx, [ctxq+ResampleContext.phase_count]
-    mov                          edi, [ctxq+ResampleContext.filter_alloc]
-
-    DEFINE_ARGS filter_alloc, dst, src, phase_count, index, frac, dst_incr_mod, \
-                filter, min_filter_count_x4, min_filter_len_x4, dst_incr_div, \
-                src_incr, phase_mask, dst_end, filter_bank
-%elif WIN64
-    mov                          R9d, [ctxq+ResampleContext.filter_alloc]
-    mov                          ecx, [ctxq+ResampleContext.phase_count]
-
-    DEFINE_ARGS phase_count, dst, src, filter_alloc, index, frac, dst_incr_mod, \
-                filter, min_filter_count_x4, min_filter_len_x4, dst_incr_div, \
-                src_incr, phase_mask, dst_end, filter_bank
-%endif
-
-    neg           min_filter_len_x4q
-    sub                 filter_bankq, min_filter_len_x4q
-    sub                         srcq, min_filter_len_x4q
-    mov                   src_stackq, srcq
-%else ; x86-32
-cglobal resample_common_%1, 1, 7, 2, ctx, phase_count, dst, frac, \
-                                     index, min_filter_length_x4, filter_bank
-
-    ; push temp variables to stack
-%define ctx_stackq            r0mp
-%define src_stackq            r2mp
-%define update_context_stackd r4m
-
-    mov                         dstq, r1mp
-    mov                           r3, r3mp
-    lea                           r3, [dstq+r3*%2]
-    PUSH                              dword [ctxq+ResampleContext.dst_incr_div]
-    PUSH                              dword [ctxq+ResampleContext.dst_incr_mod]
-    PUSH                              dword [ctxq+ResampleContext.filter_alloc]
-    PUSH                              r3
-    PUSH                              dword [ctxq+ResampleContext.phase_count]  ; unneeded replacement for phase_mask
-    PUSH                              dword [ctxq+ResampleContext.src_incr]
-    mov        min_filter_length_x4d, [ctxq+ResampleContext.filter_length]
-    mov                       indexd, [ctxq+ResampleContext.index]
-    shl        min_filter_length_x4d, %3
-    mov                        fracd, [ctxq+ResampleContext.frac]
-    neg        min_filter_length_x4q
-    mov                 filter_bankq, [ctxq+ResampleContext.filter_bank]
-    sub                         r2mp, min_filter_length_x4q
-    sub                 filter_bankq, min_filter_length_x4q
-    PUSH                              min_filter_length_x4q
-    PUSH                              filter_bankq
-    mov                 phase_countd, [ctxq+ResampleContext.phase_count]
-
-    DEFINE_ARGS src, phase_count, dst, frac, index, min_filter_count_x4, filter
-
-%define filter_bankq          dword [rsp+0x0]
-%define min_filter_length_x4q dword [rsp+0x4]
-%define src_incrd             dword [rsp+0x8]
-%define phase_maskd           dword [rsp+0xc]
-%define dst_endq              dword [rsp+0x10]
-%define filter_allocd         dword [rsp+0x14]
-%define dst_incr_modd         dword [rsp+0x18]
-%define dst_incr_divd         dword [rsp+0x1c]
-
-    mov                         srcq, r2mp
-%endif
-
-.loop:
-    mov                      filterd, filter_allocd
-    imul                     filterd, indexd
-%if ARCH_X86_64
-    mov         min_filter_count_x4q, min_filter_len_x4q
-    lea                      filterq, [filter_bankq+filterq*%2]
-%else ; x86-32
-    mov         min_filter_count_x4q, filter_bankq
-    lea                      filterq, [min_filter_count_x4q+filterq*%2]
-    mov         min_filter_count_x4q, min_filter_length_x4q
-%endif
-%ifidn %1, int16
-    movd                          m0, [pd_0x4000]
-%else ; float/double
-    xorps                         m0, m0, m0
-%endif
-
-    align 16
-.inner_loop:
-    movu                          m1, [srcq+min_filter_count_x4q*1]
-%ifidn %1, int16
-%if cpuflag(xop)
-    vpmadcswd                     m0, m1, [filterq+min_filter_count_x4q*1], m0
-%else
-    pmaddwd                       m1, [filterq+min_filter_count_x4q*1]
-    paddd                         m0, m1
-%endif
-%else ; float/double
-%if cpuflag(fma4) || cpuflag(fma3)
-    fmaddp%4                      m0, m1, [filterq+min_filter_count_x4q*1], m0
-%else
-    mulp%4                        m1, m1, [filterq+min_filter_count_x4q*1]
-    addp%4                        m0, m0, m1
-%endif ; cpuflag
-%endif
-    add         min_filter_count_x4q, mmsize
-    js .inner_loop
-
-%ifidn %1, int16
-    HADDD                         m0, m1
-    psrad                         m0, 15
-    add                        fracd, dst_incr_modd
-    packssdw                      m0, m0
-    add                       indexd, dst_incr_divd
-    movd                      [dstq], m0
-%else ; float/double
-    ; horizontal sum & store
-%if mmsize == 32
-    vextractf128                 xm1, m0, 0x1
-    addp%4                       xm0, xm1
-%endif
-    movhlps                      xm1, xm0
-%ifidn %1, float
-    addps                        xm0, xm1
-    shufps                       xm1, xm0, xm0, q0001
-%endif
-    add                        fracd, dst_incr_modd
-    addp%4                       xm0, xm1
-    add                       indexd, dst_incr_divd
-    movs%4                    [dstq], xm0
-%endif
-    cmp                        fracd, src_incrd
-    jl .skip
-    sub                        fracd, src_incrd
-    inc                       indexd
-
-%if UNIX64
-    DEFINE_ARGS filter_alloc, dst, src, phase_count, index, frac, dst_incr_mod, \
-                index_incr, min_filter_count_x4, min_filter_len_x4, dst_incr_div, \
-                src_incr, phase_mask, dst_end, filter_bank
-%elif WIN64
-    DEFINE_ARGS phase_count, dst, src, filter_alloc, index, frac, dst_incr_mod, \
-                index_incr, min_filter_count_x4, min_filter_len_x4, dst_incr_div, \
-                src_incr, phase_mask, dst_end, filter_bank
-%else ; x86-32
-    DEFINE_ARGS src, phase_count, dst, frac, index, index_incr
-%endif
-
-.skip:
-    add                         dstq, %2
-    cmp                       indexd, phase_countd
-    jb .index_skip
-.index_while:
-    sub                       indexd, phase_countd
-    lea                         srcq, [srcq+%2]
-    cmp                       indexd, phase_countd
-    jnb .index_while
-.index_skip:
-    cmp                         dstq, dst_endq
-    jne .loop
-
-%if ARCH_X86_64
-    DEFINE_ARGS ctx, dst, src, phase_count, index, frac
-%else ; x86-32
-    DEFINE_ARGS src, ctx, update_context, frac, index
-%endif
-
-    cmp  dword update_context_stackd, 0
-    jz .skip_store
-    ; strictly speaking, the function should always return the consumed
-    ; number of bytes; however, we only use the value if update_context
-    ; is true, so let's just leave it uninitialized otherwise
-    mov                         ctxq, ctx_stackq
-    movifnidn                    rax, srcq
-    mov [ctxq+ResampleContext.frac ], fracd
-    sub                          rax, src_stackq
-    mov [ctxq+ResampleContext.index], indexd
-    shr                          rax, %3
-
-.skip_store:
-%if ARCH_X86_32
-    ADD                          rsp, 0x20
-%endif
-    RET
-
-; int resample_linear_$format(ResampleContext *ctx, float *dst,
-;                             const float *src, int size, int update_ctx)
-%if ARCH_X86_64 ; unix64 and win64
-%if UNIX64
-cglobal resample_linear_%1, 0, 15, 5, ctx, dst, phase_mask, phase_count, index, frac, \
-                                      size, dst_incr_mod, min_filter_count_x4, \
-                                      min_filter_len_x4, dst_incr_div, src_incr, \
-                                      src, dst_end, filter_bank
-
-    mov                         srcq, r2mp
-%else ; win64
-cglobal resample_linear_%1, 0, 15, 5, ctx, phase_mask, src, phase_count, index, frac, \
-                                      size, dst_incr_mod, min_filter_count_x4, \
-                                      min_filter_len_x4, dst_incr_div, src_incr, \
-                                      dst, dst_end, filter_bank
-
-    mov                         dstq, r1mp
-%endif
-
-    ; use red-zone for variable storage
-%define ctx_stackq            [rsp-0x8]
-%define src_stackq            [rsp-0x10]
-%define phase_mask_stackd     [rsp-0x14]
-%if WIN64
-%define update_context_stackd r4m
-%else ; unix64
-%define update_context_stackd [rsp-0x18]
-%endif
-
-    ; load as many variables in registers as possible; for the rest, store
-    ; on stack so that we have 'ctx' available as one extra register
-    mov                        sized, r3d
-%if UNIX64
-    mov        update_context_stackd, r4d
-%endif
-    mov                       indexd, [ctxq+ResampleContext.index]
-    mov                        fracd, [ctxq+ResampleContext.frac]
-    mov                dst_incr_modd, [ctxq+ResampleContext.dst_incr_mod]
-    mov                 filter_bankq, [ctxq+ResampleContext.filter_bank]
-    mov                    src_incrd, [ctxq+ResampleContext.src_incr]
-    mov                   ctx_stackq, ctxq
-    mov           min_filter_len_x4d, [ctxq+ResampleContext.filter_length]
-%ifidn %1, int16
-    movd                          m4, [pd_0x4000]
-%else ; float/double
-    cvtsi2s%4                    xm0, src_incrd
-    movs%4                       xm4, [%5]
-    divs%4                       xm4, xm0
-%endif
-    mov                dst_incr_divd, [ctxq+ResampleContext.dst_incr_div]
-    shl           min_filter_len_x4d, %3
-    lea                     dst_endq, [dstq+sizeq*%2]
-
-%if UNIX64
-    mov                          ecx, [ctxq+ResampleContext.phase_count]
-    mov                          edi, [ctxq+ResampleContext.filter_alloc]
-
-    DEFINE_ARGS filter_alloc, dst, filter2, phase_count, index, frac, filter1, \
-                dst_incr_mod, min_filter_count_x4, min_filter_len_x4, \
-                dst_incr_div, src_incr, src, dst_end, filter_bank
-%elif WIN64
-    mov                          R9d, [ctxq+ResampleContext.filter_alloc]
-    mov                          ecx, [ctxq+ResampleContext.phase_count]
-
-    DEFINE_ARGS phase_count, filter2, src, filter_alloc, index, frac, filter1, \
-                dst_incr_mod, min_filter_count_x4, min_filter_len_x4, \
-                dst_incr_div, src_incr, dst, dst_end, filter_bank
-%endif
-
-    neg           min_filter_len_x4q
-    sub                 filter_bankq, min_filter_len_x4q
-    sub                         srcq, min_filter_len_x4q
-    mov                   src_stackq, srcq
-%else ; x86-32
-cglobal resample_linear_%1, 1, 7, 5, ctx, min_filter_length_x4, filter2, \
-                                     frac, index, dst, filter_bank
-
-    ; push temp variables to stack
-%define ctx_stackq            r0mp
-%define src_stackq            r2mp
-%define update_context_stackd r4m
-
-    mov                         dstq, r1mp
-    mov                           r3, r3mp
-    lea                           r3, [dstq+r3*%2]
-    PUSH                              dword [ctxq+ResampleContext.dst_incr_div]
-    PUSH                              r3
-    mov                           r3, dword [ctxq+ResampleContext.filter_alloc]
-    PUSH                              dword [ctxq+ResampleContext.dst_incr_mod]
-    PUSH                              r3
-    shl                           r3, %3
-    PUSH                              r3
-    mov                           r3, dword [ctxq+ResampleContext.src_incr]
-    PUSH                              dword [ctxq+ResampleContext.phase_count]  ; unneeded replacement of phase_mask
-    PUSH                              r3d
-%ifidn %1, int16
-    movd                          m4, [pd_0x4000]
-%else ; float/double
-    cvtsi2s%4                    xm0, r3d
-    movs%4                       xm4, [%5]
-    divs%4                       xm4, xm0
-%endif
-    mov        min_filter_length_x4d, [ctxq+ResampleContext.filter_length]
-    mov                       indexd, [ctxq+ResampleContext.index]
-    shl        min_filter_length_x4d, %3
-    mov                        fracd, [ctxq+ResampleContext.frac]
-    neg        min_filter_length_x4q
-    mov                 filter_bankq, [ctxq+ResampleContext.filter_bank]
-    sub                         r2mp, min_filter_length_x4q
-    sub                 filter_bankq, min_filter_length_x4q
-    PUSH                              min_filter_length_x4q
-    PUSH                              filter_bankq
-    PUSH                              dword [ctxq+ResampleContext.phase_count]
-
-    DEFINE_ARGS filter1, min_filter_count_x4, filter2, frac, index, dst, src
-
-%define phase_count_stackd    dword [rsp+0x0]
-%define filter_bankq          dword [rsp+0x4]
-%define min_filter_length_x4q dword [rsp+0x8]
-%define src_incrd             dword [rsp+0xc]
-%define phase_mask_stackd     dword [rsp+0x10]
-%define filter_alloc_x4q      dword [rsp+0x14]
-%define filter_allocd         dword [rsp+0x18]
-%define dst_incr_modd         dword [rsp+0x1c]
-%define dst_endq              dword [rsp+0x20]
-%define dst_incr_divd         dword [rsp+0x24]
-
-    mov                         srcq, r2mp
-%endif
-
-.loop:
-    mov                     filter1d, filter_allocd
-    imul                    filter1d, indexd
-%if ARCH_X86_64
-    mov         min_filter_count_x4q, min_filter_len_x4q
-    lea                     filter1q, [filter_bankq+filter1q*%2]
-    lea                     filter2q, [filter1q+filter_allocq*%2]
-%else ; x86-32
-    mov         min_filter_count_x4q, filter_bankq
-    lea                     filter1q, [min_filter_count_x4q+filter1q*%2]
-    mov         min_filter_count_x4q, min_filter_length_x4q
-    mov                     filter2q, filter1q
-    add                     filter2q, filter_alloc_x4q
-%endif
-%ifidn %1, int16
-    mova                          m0, m4
-    mova                          m2, m4
-%else ; float/double
-    xorps                         m0, m0, m0
-    xorps                         m2, m2, m2
-%endif
-
-    align 16
-.inner_loop:
-    movu                          m1, [srcq+min_filter_count_x4q*1]
-%ifidn %1, int16
-%if cpuflag(xop)
-    vpmadcswd                     m2, m1, [filter2q+min_filter_count_x4q*1], m2
-    vpmadcswd                     m0, m1, [filter1q+min_filter_count_x4q*1], m0
-%else
-    pmaddwd                       m3, m1, [filter2q+min_filter_count_x4q*1]
-    pmaddwd                       m1, [filter1q+min_filter_count_x4q*1]
-    paddd                         m2, m3
-    paddd                         m0, m1
-%endif ; cpuflag
-%else ; float/double
-%if cpuflag(fma4) || cpuflag(fma3)
-    fmaddp%4                      m2, m1, [filter2q+min_filter_count_x4q*1], m2
-    fmaddp%4                      m0, m1, [filter1q+min_filter_count_x4q*1], m0
-%else
-    mulp%4                        m3, m1, [filter2q+min_filter_count_x4q*1]
-    mulp%4                        m1, m1, [filter1q+min_filter_count_x4q*1]
-    addp%4                        m2, m2, m3
-    addp%4                        m0, m0, m1
-%endif ; cpuflag
-%endif
-    add         min_filter_count_x4q, mmsize
-    js .inner_loop
-
-%ifidn %1, int16
-%if mmsize == 16
-%if cpuflag(xop)
-    vphadddq                      m2, m2
-    vphadddq                      m0, m0
-%endif
-    pshufd                        m3, m2, q0032
-    pshufd                        m1, m0, q0032
-    paddd                         m2, m3
-    paddd                         m0, m1
-%endif
-%if notcpuflag(xop)
-    PSHUFLW                       m3, m2, q0032
-    PSHUFLW                       m1, m0, q0032
-    paddd                         m2, m3
-    paddd                         m0, m1
-%endif
-    psubd                         m2, m0
-    ; This is probably a really bad idea on atom and other machines with a
-    ; long transfer latency between GPRs and XMMs (atom). However, it does
-    ; make the clip a lot simpler...
-    movd                         eax, m2
-    add                       indexd, dst_incr_divd
-    imul                              fracd
-    idiv                              src_incrd
-    movd                          m1, eax
-    add                        fracd, dst_incr_modd
-    paddd                         m0, m1
-    psrad                         m0, 15
-    packssdw                      m0, m0
-    movd                      [dstq], m0
-
-    ; note that for imul/idiv, I need to move filter to edx/eax for each:
-    ; - 32bit: eax=r0[filter1], edx=r2[filter2]
-    ; - win64: eax=r6[filter1], edx=r1[todo]
-    ; - unix64: eax=r6[filter1], edx=r2[todo]
-%else ; float/double
-    ; val += (v2 - val) * (FELEML) frac / c->src_incr;
-%if mmsize == 32
-    vextractf128                 xm1, m0, 0x1
-    vextractf128                 xm3, m2, 0x1
-    addp%4                       xm0, xm1
-    addp%4                       xm2, xm3
-%endif
-    cvtsi2s%4                    xm1, fracd
-    subp%4                       xm2, xm0
-    mulp%4                       xm1, xm4
-    shufp%4                      xm1, xm1, q0000
-%if cpuflag(fma4) || cpuflag(fma3)
-    fmaddp%4                     xm0, xm2, xm1, xm0
-%else
-    mulp%4                       xm2, xm1
-    addp%4                       xm0, xm2
-%endif ; cpuflag
-
-    ; horizontal sum & store
-    movhlps                      xm1, xm0
-%ifidn %1, float
-    addps                        xm0, xm1
-    shufps                       xm1, xm0, xm0, q0001
-%endif
-    add                        fracd, dst_incr_modd
-    addp%4                       xm0, xm1
-    add                       indexd, dst_incr_divd
-    movs%4                    [dstq], xm0
-%endif
-    cmp                        fracd, src_incrd
-    jl .skip
-    sub                        fracd, src_incrd
-    inc                       indexd
-
-%if UNIX64
-    DEFINE_ARGS filter_alloc, dst, filter2, phase_count, index, frac, index_incr, \
-                dst_incr_mod, min_filter_count_x4, min_filter_len_x4, \
-                dst_incr_div, src_incr, src, dst_end, filter_bank
-%elif WIN64
-    DEFINE_ARGS phase_count, filter2, src, filter_alloc, index, frac, index_incr, \
-                dst_incr_mod, min_filter_count_x4, min_filter_len_x4, \
-                dst_incr_div, src_incr, dst, dst_end, filter_bank
-%else ; x86-32
-    DEFINE_ARGS filter1, phase_count, index_incr, frac, index, dst, src
-%endif
-
-.skip:
-%if ARCH_X86_32
-    mov                 phase_countd, phase_count_stackd
-%endif
-    add                         dstq, %2
-    cmp                       indexd, phase_countd
-    jb .index_skip
-.index_while:
-    sub                       indexd, phase_countd
-    lea                         srcq, [srcq+%2]
-    cmp                       indexd, phase_countd
-    jnb .index_while
-.index_skip:
-    cmp                         dstq, dst_endq
-    jne .loop
-
-%if UNIX64
-    DEFINE_ARGS ctx, dst, filter2, phase_count, index, frac, index_incr, \
-                dst_incr_mod, min_filter_count_x4, min_filter_len_x4, \
-                dst_incr_div, src_incr, src, dst_end, filter_bank
-%elif WIN64
-    DEFINE_ARGS ctx, filter2, src, phase_count, index, frac, index_incr, \
-                dst_incr_mod, min_filter_count_x4, min_filter_len_x4, \
-                dst_incr_div, src_incr, dst, dst_end, filter_bank
-%else ; x86-32
-    DEFINE_ARGS filter1, ctx, update_context, frac, index, dst, src
-%endif
-
-    cmp  dword update_context_stackd, 0
-    jz .skip_store
-    ; strictly speaking, the function should always return the consumed
-    ; number of bytes; however, we only use the value if update_context
-    ; is true, so let's just leave it uninitialized otherwise
-    mov                         ctxq, ctx_stackq
-    movifnidn                    rax, srcq
-    mov [ctxq+ResampleContext.frac ], fracd
-    sub                          rax, src_stackq
-    mov [ctxq+ResampleContext.index], indexd
-    shr                          rax, %3
-
-.skip_store:
-%if ARCH_X86_32
-    ADD                          rsp, 0x28
-%endif
-    RET
-%endmacro
-
-INIT_XMM sse
-RESAMPLE_FNS float, 4, 2, s, pf_1
-
-%if HAVE_AVX_EXTERNAL
-INIT_YMM avx
-RESAMPLE_FNS float, 4, 2, s, pf_1
-%endif
-%if HAVE_FMA3_EXTERNAL
-INIT_YMM fma3
-RESAMPLE_FNS float, 4, 2, s, pf_1
-%endif
-%if HAVE_FMA4_EXTERNAL
-INIT_XMM fma4
-RESAMPLE_FNS float, 4, 2, s, pf_1
-%endif
-
-%if ARCH_X86_32
-INIT_MMX mmxext
-RESAMPLE_FNS int16, 2, 1
-%endif
-
-INIT_XMM sse2
-RESAMPLE_FNS int16, 2, 1
-%if HAVE_XOP_EXTERNAL
-INIT_XMM xop
-RESAMPLE_FNS int16, 2, 1
-%endif
-
-INIT_XMM sse2
-RESAMPLE_FNS double, 8, 3, d, pdbl_1
-
-%if HAVE_AVX_EXTERNAL
-INIT_YMM avx
-RESAMPLE_FNS double, 8, 3, d, pdbl_1
-%endif
-%if HAVE_FMA3_EXTERNAL
-INIT_YMM fma3
-RESAMPLE_FNS double, 8, 3, d, pdbl_1
-%endif
diff -uparN ffmpeg-4.1/libswscale/x86/input.asm ffmpeg-y/libswscale/x86/input.asm
--- ffmpeg-4.1/libswscale/x86/input.asm	2016-03-29 10:25:33.000000000 +0800
+++ ffmpeg-y/libswscale/x86/input.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,740 +0,0 @@
-;******************************************************************************
-;* x86-optimized input routines; does shuffling of packed
-;* YUV formats into individual planes, and converts RGB
-;* into YUV planes also.
-;* Copyright (c) 2012 Ronald S. Bultje <rsbultje@gmail.com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-
-%define RY 0x20DE
-%define GY 0x4087
-%define BY 0x0C88
-%define RU 0xECFF
-%define GU 0xDAC8
-%define BU 0x3838
-%define RV 0x3838
-%define GV 0xD0E3
-%define BV 0xF6E4
-
-rgb_Yrnd:        times 4 dd 0x80100        ;  16.5 << 15
-rgb_UVrnd:       times 4 dd 0x400100       ; 128.5 << 15
-%define bgr_Ycoeff_12x4 16*4 + 16* 0 + tableq
-%define bgr_Ycoeff_3x56 16*4 + 16* 1 + tableq
-%define rgb_Ycoeff_12x4 16*4 + 16* 2 + tableq
-%define rgb_Ycoeff_3x56 16*4 + 16* 3 + tableq
-%define bgr_Ucoeff_12x4 16*4 + 16* 4 + tableq
-%define bgr_Ucoeff_3x56 16*4 + 16* 5 + tableq
-%define rgb_Ucoeff_12x4 16*4 + 16* 6 + tableq
-%define rgb_Ucoeff_3x56 16*4 + 16* 7 + tableq
-%define bgr_Vcoeff_12x4 16*4 + 16* 8 + tableq
-%define bgr_Vcoeff_3x56 16*4 + 16* 9 + tableq
-%define rgb_Vcoeff_12x4 16*4 + 16*10 + tableq
-%define rgb_Vcoeff_3x56 16*4 + 16*11 + tableq
-
-%define rgba_Ycoeff_rb 16*4 + 16*12 + tableq
-%define rgba_Ycoeff_br 16*4 + 16*13 + tableq
-%define rgba_Ycoeff_ga 16*4 + 16*14 + tableq
-%define rgba_Ycoeff_ag 16*4 + 16*15 + tableq
-%define rgba_Ucoeff_rb 16*4 + 16*16 + tableq
-%define rgba_Ucoeff_br 16*4 + 16*17 + tableq
-%define rgba_Ucoeff_ga 16*4 + 16*18 + tableq
-%define rgba_Ucoeff_ag 16*4 + 16*19 + tableq
-%define rgba_Vcoeff_rb 16*4 + 16*20 + tableq
-%define rgba_Vcoeff_br 16*4 + 16*21 + tableq
-%define rgba_Vcoeff_ga 16*4 + 16*22 + tableq
-%define rgba_Vcoeff_ag 16*4 + 16*23 + tableq
-
-; bgr_Ycoeff_12x4: times 2 dw BY, GY, 0, BY
-; bgr_Ycoeff_3x56: times 2 dw RY, 0, GY, RY
-; rgb_Ycoeff_12x4: times 2 dw RY, GY, 0, RY
-; rgb_Ycoeff_3x56: times 2 dw BY, 0, GY, BY
-; bgr_Ucoeff_12x4: times 2 dw BU, GU, 0, BU
-; bgr_Ucoeff_3x56: times 2 dw RU, 0, GU, RU
-; rgb_Ucoeff_12x4: times 2 dw RU, GU, 0, RU
-; rgb_Ucoeff_3x56: times 2 dw BU, 0, GU, BU
-; bgr_Vcoeff_12x4: times 2 dw BV, GV, 0, BV
-; bgr_Vcoeff_3x56: times 2 dw RV, 0, GV, RV
-; rgb_Vcoeff_12x4: times 2 dw RV, GV, 0, RV
-; rgb_Vcoeff_3x56: times 2 dw BV, 0, GV, BV
-
-; rgba_Ycoeff_rb:  times 4 dw RY, BY
-; rgba_Ycoeff_br:  times 4 dw BY, RY
-; rgba_Ycoeff_ga:  times 4 dw GY, 0
-; rgba_Ycoeff_ag:  times 4 dw 0,  GY
-; rgba_Ucoeff_rb:  times 4 dw RU, BU
-; rgba_Ucoeff_br:  times 4 dw BU, RU
-; rgba_Ucoeff_ga:  times 4 dw GU, 0
-; rgba_Ucoeff_ag:  times 4 dw 0,  GU
-; rgba_Vcoeff_rb:  times 4 dw RV, BV
-; rgba_Vcoeff_br:  times 4 dw BV, RV
-; rgba_Vcoeff_ga:  times 4 dw GV, 0
-; rgba_Vcoeff_ag:  times 4 dw 0,  GV
-
-shuf_rgb_12x4:   db 0, 0x80, 1, 0x80,  2, 0x80,  3, 0x80, \
-                    6, 0x80, 7, 0x80,  8, 0x80,  9, 0x80
-shuf_rgb_3x56:   db 2, 0x80, 3, 0x80,  4, 0x80,  5, 0x80, \
-                    8, 0x80, 9, 0x80, 10, 0x80, 11, 0x80
-
-SECTION .text
-
-;-----------------------------------------------------------------------------
-; RGB to Y/UV.
-;
-; void <fmt>ToY_<opt>(uint8_t *dst, const uint8_t *src, int w);
-; and
-; void <fmt>toUV_<opt>(uint8_t *dstU, uint8_t *dstV, const uint8_t *src,
-;                      const uint8_t *unused, int w);
-;-----------------------------------------------------------------------------
-
-; %1 = nr. of XMM registers
-; %2 = rgb or bgr
-%macro RGB24_TO_Y_FN 2-3
-cglobal %2 %+ 24ToY, 6, 6, %1, dst, src, u1, u2, w, table
-%if mmsize == 8
-    mova           m5, [%2_Ycoeff_12x4]
-    mova           m6, [%2_Ycoeff_3x56]
-%define coeff1 m5
-%define coeff2 m6
-%elif ARCH_X86_64
-    mova           m8, [%2_Ycoeff_12x4]
-    mova           m9, [%2_Ycoeff_3x56]
-%define coeff1 m8
-%define coeff2 m9
-%else ; x86-32 && mmsize == 16
-%define coeff1 [%2_Ycoeff_12x4]
-%define coeff2 [%2_Ycoeff_3x56]
-%endif ; x86-32/64 && mmsize == 8/16
-%if (ARCH_X86_64 || mmsize == 8) && %0 == 3
-    jmp mangle(private_prefix %+ _ %+ %3 %+ 24ToY %+ SUFFIX).body
-%else ; (ARCH_X86_64 && %0 == 3) || mmsize == 8
-.body:
-%if cpuflag(ssse3)
-    mova           m7, [shuf_rgb_12x4]
-%define shuf_rgb1 m7
-%if ARCH_X86_64
-    mova          m10, [shuf_rgb_3x56]
-%define shuf_rgb2 m10
-%else ; x86-32
-%define shuf_rgb2 [shuf_rgb_3x56]
-%endif ; x86-32/64
-%endif ; cpuflag(ssse3)
-%if ARCH_X86_64
-    movsxd         wq, wd
-%endif
-    add            wq, wq
-    add          dstq, wq
-    neg            wq
-%if notcpuflag(ssse3)
-    pxor           m7, m7
-%endif ; !cpuflag(ssse3)
-    mova           m4, [rgb_Yrnd]
-.loop:
-%if cpuflag(ssse3)
-    movu           m0, [srcq+0]           ; (byte) { Bx, Gx, Rx }[0-3]
-    movu           m2, [srcq+12]          ; (byte) { Bx, Gx, Rx }[4-7]
-    pshufb         m1, m0, shuf_rgb2      ; (word) { R0, B1, G1, R1, R2, B3, G3, R3 }
-    pshufb         m0, shuf_rgb1          ; (word) { B0, G0, R0, B1, B2, G2, R2, B3 }
-    pshufb         m3, m2, shuf_rgb2      ; (word) { R4, B5, G5, R5, R6, B7, G7, R7 }
-    pshufb         m2, shuf_rgb1          ; (word) { B4, G4, R4, B5, B6, G6, R6, B7 }
-%else ; !cpuflag(ssse3)
-    movd           m0, [srcq+0]           ; (byte) { B0, G0, R0, B1 }
-    movd           m1, [srcq+2]           ; (byte) { R0, B1, G1, R1 }
-    movd           m2, [srcq+6]           ; (byte) { B2, G2, R2, B3 }
-    movd           m3, [srcq+8]           ; (byte) { R2, B3, G3, R3 }
-%if mmsize == 16 ; i.e. sse2
-    punpckldq      m0, m2                 ; (byte) { B0, G0, R0, B1, B2, G2, R2, B3 }
-    punpckldq      m1, m3                 ; (byte) { R0, B1, G1, R1, R2, B3, G3, R3 }
-    movd           m2, [srcq+12]          ; (byte) { B4, G4, R4, B5 }
-    movd           m3, [srcq+14]          ; (byte) { R4, B5, G5, R5 }
-    movd           m5, [srcq+18]          ; (byte) { B6, G6, R6, B7 }
-    movd           m6, [srcq+20]          ; (byte) { R6, B7, G7, R7 }
-    punpckldq      m2, m5                 ; (byte) { B4, G4, R4, B5, B6, G6, R6, B7 }
-    punpckldq      m3, m6                 ; (byte) { R4, B5, G5, R5, R6, B7, G7, R7 }
-%endif ; mmsize == 16
-    punpcklbw      m0, m7                 ; (word) { B0, G0, R0, B1, B2, G2, R2, B3 }
-    punpcklbw      m1, m7                 ; (word) { R0, B1, G1, R1, R2, B3, G3, R3 }
-    punpcklbw      m2, m7                 ; (word) { B4, G4, R4, B5, B6, G6, R6, B7 }
-    punpcklbw      m3, m7                 ; (word) { R4, B5, G5, R5, R6, B7, G7, R7 }
-%endif ; cpuflag(ssse3)
-    add          srcq, 3 * mmsize / 2
-    pmaddwd        m0, coeff1             ; (dword) { B0*BY + G0*GY, B1*BY, B2*BY + G2*GY, B3*BY }
-    pmaddwd        m1, coeff2             ; (dword) { R0*RY, G1+GY + R1*RY, R2*RY, G3+GY + R3*RY }
-    pmaddwd        m2, coeff1             ; (dword) { B4*BY + G4*GY, B5*BY, B6*BY + G6*GY, B7*BY }
-    pmaddwd        m3, coeff2             ; (dword) { R4*RY, G5+GY + R5*RY, R6*RY, G7+GY + R7*RY }
-    paddd          m0, m1                 ; (dword) { Bx*BY + Gx*GY + Rx*RY }[0-3]
-    paddd          m2, m3                 ; (dword) { Bx*BY + Gx*GY + Rx*RY }[4-7]
-    paddd          m0, m4                 ; += rgb_Yrnd, i.e. (dword) { Y[0-3] }
-    paddd          m2, m4                 ; += rgb_Yrnd, i.e. (dword) { Y[4-7] }
-    psrad          m0, 9
-    psrad          m2, 9
-    packssdw       m0, m2                 ; (word) { Y[0-7] }
-    mova    [dstq+wq], m0
-    add            wq, mmsize
-    jl .loop
-    REP_RET
-%endif ; (ARCH_X86_64 && %0 == 3) || mmsize == 8
-%endmacro
-
-; %1 = nr. of XMM registers
-; %2 = rgb or bgr
-%macro RGB24_TO_UV_FN 2-3
-cglobal %2 %+ 24ToUV, 7, 7, %1, dstU, dstV, u1, src, u2, w, table
-%if ARCH_X86_64
-    mova           m8, [%2_Ucoeff_12x4]
-    mova           m9, [%2_Ucoeff_3x56]
-    mova          m10, [%2_Vcoeff_12x4]
-    mova          m11, [%2_Vcoeff_3x56]
-%define coeffU1 m8
-%define coeffU2 m9
-%define coeffV1 m10
-%define coeffV2 m11
-%else ; x86-32
-%define coeffU1 [%2_Ucoeff_12x4]
-%define coeffU2 [%2_Ucoeff_3x56]
-%define coeffV1 [%2_Vcoeff_12x4]
-%define coeffV2 [%2_Vcoeff_3x56]
-%endif ; x86-32/64
-%if ARCH_X86_64 && %0 == 3
-    jmp mangle(private_prefix %+ _ %+ %3 %+ 24ToUV %+ SUFFIX).body
-%else ; ARCH_X86_64 && %0 == 3
-.body:
-%if cpuflag(ssse3)
-    mova           m7, [shuf_rgb_12x4]
-%define shuf_rgb1 m7
-%if ARCH_X86_64
-    mova          m12, [shuf_rgb_3x56]
-%define shuf_rgb2 m12
-%else ; x86-32
-%define shuf_rgb2 [shuf_rgb_3x56]
-%endif ; x86-32/64
-%endif ; cpuflag(ssse3)
-%if ARCH_X86_64
-    movsxd         wq, dword r5m
-%else ; x86-32
-    mov            wq, r5m
-%endif
-    add            wq, wq
-    add         dstUq, wq
-    add         dstVq, wq
-    neg            wq
-    mova           m6, [rgb_UVrnd]
-%if notcpuflag(ssse3)
-    pxor           m7, m7
-%endif
-.loop:
-%if cpuflag(ssse3)
-    movu           m0, [srcq+0]           ; (byte) { Bx, Gx, Rx }[0-3]
-    movu           m4, [srcq+12]          ; (byte) { Bx, Gx, Rx }[4-7]
-    pshufb         m1, m0, shuf_rgb2      ; (word) { R0, B1, G1, R1, R2, B3, G3, R3 }
-    pshufb         m0, shuf_rgb1          ; (word) { B0, G0, R0, B1, B2, G2, R2, B3 }
-%else ; !cpuflag(ssse3)
-    movd           m0, [srcq+0]           ; (byte) { B0, G0, R0, B1 }
-    movd           m1, [srcq+2]           ; (byte) { R0, B1, G1, R1 }
-    movd           m4, [srcq+6]           ; (byte) { B2, G2, R2, B3 }
-    movd           m5, [srcq+8]           ; (byte) { R2, B3, G3, R3 }
-%if mmsize == 16
-    punpckldq      m0, m4                 ; (byte) { B0, G0, R0, B1, B2, G2, R2, B3 }
-    punpckldq      m1, m5                 ; (byte) { R0, B1, G1, R1, R2, B3, G3, R3 }
-    movd           m4, [srcq+12]          ; (byte) { B4, G4, R4, B5 }
-    movd           m5, [srcq+14]          ; (byte) { R4, B5, G5, R5 }
-%endif ; mmsize == 16
-    punpcklbw      m0, m7                 ; (word) { B0, G0, R0, B1, B2, G2, R2, B3 }
-    punpcklbw      m1, m7                 ; (word) { R0, B1, G1, R1, R2, B3, G3, R3 }
-%endif ; cpuflag(ssse3)
-    pmaddwd        m2, m0, coeffV1        ; (dword) { B0*BV + G0*GV, B1*BV, B2*BV + G2*GV, B3*BV }
-    pmaddwd        m3, m1, coeffV2        ; (dword) { R0*BV, G1*GV + R1*BV, R2*BV, G3*GV + R3*BV }
-    pmaddwd        m0, coeffU1            ; (dword) { B0*BU + G0*GU, B1*BU, B2*BU + G2*GU, B3*BU }
-    pmaddwd        m1, coeffU2            ; (dword) { R0*BU, G1*GU + R1*BU, R2*BU, G3*GU + R3*BU }
-    paddd          m0, m1                 ; (dword) { Bx*BU + Gx*GU + Rx*RU }[0-3]
-    paddd          m2, m3                 ; (dword) { Bx*BV + Gx*GV + Rx*RV }[0-3]
-%if cpuflag(ssse3)
-    pshufb         m5, m4, shuf_rgb2      ; (word) { R4, B5, G5, R5, R6, B7, G7, R7 }
-    pshufb         m4, shuf_rgb1          ; (word) { B4, G4, R4, B5, B6, G6, R6, B7 }
-%else ; !cpuflag(ssse3)
-%if mmsize == 16
-    movd           m1, [srcq+18]          ; (byte) { B6, G6, R6, B7 }
-    movd           m3, [srcq+20]          ; (byte) { R6, B7, G7, R7 }
-    punpckldq      m4, m1                 ; (byte) { B4, G4, R4, B5, B6, G6, R6, B7 }
-    punpckldq      m5, m3                 ; (byte) { R4, B5, G5, R5, R6, B7, G7, R7 }
-%endif ; mmsize == 16 && !cpuflag(ssse3)
-    punpcklbw      m4, m7                 ; (word) { B4, G4, R4, B5, B6, G6, R6, B7 }
-    punpcklbw      m5, m7                 ; (word) { R4, B5, G5, R5, R6, B7, G7, R7 }
-%endif ; cpuflag(ssse3)
-    add          srcq, 3 * mmsize / 2
-    pmaddwd        m1, m4, coeffU1        ; (dword) { B4*BU + G4*GU, B5*BU, B6*BU + G6*GU, B7*BU }
-    pmaddwd        m3, m5, coeffU2        ; (dword) { R4*BU, G5*GU + R5*BU, R6*BU, G7*GU + R7*BU }
-    pmaddwd        m4, coeffV1            ; (dword) { B4*BV + G4*GV, B5*BV, B6*BV + G6*GV, B7*BV }
-    pmaddwd        m5, coeffV2            ; (dword) { R4*BV, G5*GV + R5*BV, R6*BV, G7*GV + R7*BV }
-    paddd          m1, m3                 ; (dword) { Bx*BU + Gx*GU + Rx*RU }[4-7]
-    paddd          m4, m5                 ; (dword) { Bx*BV + Gx*GV + Rx*RV }[4-7]
-    paddd          m0, m6                 ; += rgb_UVrnd, i.e. (dword) { U[0-3] }
-    paddd          m2, m6                 ; += rgb_UVrnd, i.e. (dword) { V[0-3] }
-    paddd          m1, m6                 ; += rgb_UVrnd, i.e. (dword) { U[4-7] }
-    paddd          m4, m6                 ; += rgb_UVrnd, i.e. (dword) { V[4-7] }
-    psrad          m0, 9
-    psrad          m2, 9
-    psrad          m1, 9
-    psrad          m4, 9
-    packssdw       m0, m1                 ; (word) { U[0-7] }
-    packssdw       m2, m4                 ; (word) { V[0-7] }
-%if mmsize == 8
-    mova   [dstUq+wq], m0
-    mova   [dstVq+wq], m2
-%else ; mmsize == 16
-    mova   [dstUq+wq], m0
-    mova   [dstVq+wq], m2
-%endif ; mmsize == 8/16
-    add            wq, mmsize
-    jl .loop
-    REP_RET
-%endif ; ARCH_X86_64 && %0 == 3
-%endmacro
-
-; %1 = nr. of XMM registers for rgb-to-Y func
-; %2 = nr. of XMM registers for rgb-to-UV func
-%macro RGB24_FUNCS 2
-RGB24_TO_Y_FN %1, rgb
-RGB24_TO_Y_FN %1, bgr, rgb
-RGB24_TO_UV_FN %2, rgb
-RGB24_TO_UV_FN %2, bgr, rgb
-%endmacro
-
-%if ARCH_X86_32
-INIT_MMX mmx
-RGB24_FUNCS 0, 0
-%endif
-
-INIT_XMM sse2
-RGB24_FUNCS 10, 12
-
-INIT_XMM ssse3
-RGB24_FUNCS 11, 13
-
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-RGB24_FUNCS 11, 13
-%endif
-
-; %1 = nr. of XMM registers
-; %2-5 = rgba, bgra, argb or abgr (in individual characters)
-%macro RGB32_TO_Y_FN 5-6
-cglobal %2%3%4%5 %+ ToY, 6, 6, %1, dst, src, u1, u2, w, table
-    mova           m5, [rgba_Ycoeff_%2%4]
-    mova           m6, [rgba_Ycoeff_%3%5]
-%if %0 == 6
-    jmp mangle(private_prefix %+ _ %+ %6 %+ ToY %+ SUFFIX).body
-%else ; %0 == 6
-.body:
-%if ARCH_X86_64
-    movsxd         wq, wd
-%endif
-    add            wq, wq
-    sub            wq, mmsize - 1
-    lea          srcq, [srcq+wq*2]
-    add          dstq, wq
-    neg            wq
-    mova           m4, [rgb_Yrnd]
-    pcmpeqb        m7, m7
-    psrlw          m7, 8                  ; (word) { 0x00ff } x4
-.loop:
-    ; FIXME check alignment and use mova
-    movu           m0, [srcq+wq*2+0]      ; (byte) { Bx, Gx, Rx, xx }[0-3]
-    movu           m2, [srcq+wq*2+mmsize] ; (byte) { Bx, Gx, Rx, xx }[4-7]
-    DEINTB          1,  0,  3,  2,  7     ; (word) { Gx, xx (m0/m2) or Bx, Rx (m1/m3) }[0-3]/[4-7]
-    pmaddwd        m1, m5                 ; (dword) { Bx*BY + Rx*RY }[0-3]
-    pmaddwd        m0, m6                 ; (dword) { Gx*GY }[0-3]
-    pmaddwd        m3, m5                 ; (dword) { Bx*BY + Rx*RY }[4-7]
-    pmaddwd        m2, m6                 ; (dword) { Gx*GY }[4-7]
-    paddd          m0, m4                 ; += rgb_Yrnd
-    paddd          m2, m4                 ; += rgb_Yrnd
-    paddd          m0, m1                 ; (dword) { Y[0-3] }
-    paddd          m2, m3                 ; (dword) { Y[4-7] }
-    psrad          m0, 9
-    psrad          m2, 9
-    packssdw       m0, m2                 ; (word) { Y[0-7] }
-    mova    [dstq+wq], m0
-    add            wq, mmsize
-    jl .loop
-    sub            wq, mmsize - 1
-    jz .end
-    add            srcq, 2*mmsize - 2
-    add            dstq, mmsize - 1
-.loop2:
-    movd           m0, [srcq+wq*2+0]      ; (byte) { Bx, Gx, Rx, xx }[0-3]
-    DEINTB          1,  0,  3,  2,  7     ; (word) { Gx, xx (m0/m2) or Bx, Rx (m1/m3) }[0-3]/[4-7]
-    pmaddwd        m1, m5                 ; (dword) { Bx*BY + Rx*RY }[0-3]
-    pmaddwd        m0, m6                 ; (dword) { Gx*GY }[0-3]
-    paddd          m0, m4                 ; += rgb_Yrnd
-    paddd          m0, m1                 ; (dword) { Y[0-3] }
-    psrad          m0, 9
-    packssdw       m0, m0                 ; (word) { Y[0-7] }
-    movd    [dstq+wq], m0
-    add            wq, 2
-    jl .loop2
-.end:
-    REP_RET
-%endif ; %0 == 3
-%endmacro
-
-; %1 = nr. of XMM registers
-; %2-5 = rgba, bgra, argb or abgr (in individual characters)
-%macro RGB32_TO_UV_FN 5-6
-cglobal %2%3%4%5 %+ ToUV, 7, 7, %1, dstU, dstV, u1, src, u2, w, table
-%if ARCH_X86_64
-    mova           m8, [rgba_Ucoeff_%2%4]
-    mova           m9, [rgba_Ucoeff_%3%5]
-    mova          m10, [rgba_Vcoeff_%2%4]
-    mova          m11, [rgba_Vcoeff_%3%5]
-%define coeffU1 m8
-%define coeffU2 m9
-%define coeffV1 m10
-%define coeffV2 m11
-%else ; x86-32
-%define coeffU1 [rgba_Ucoeff_%2%4]
-%define coeffU2 [rgba_Ucoeff_%3%5]
-%define coeffV1 [rgba_Vcoeff_%2%4]
-%define coeffV2 [rgba_Vcoeff_%3%5]
-%endif ; x86-64/32
-%if ARCH_X86_64 && %0 == 6
-    jmp mangle(private_prefix %+ _ %+ %6 %+ ToUV %+ SUFFIX).body
-%else ; ARCH_X86_64 && %0 == 6
-.body:
-%if ARCH_X86_64
-    movsxd         wq, dword r5m
-%else ; x86-32
-    mov            wq, r5m
-%endif
-    add            wq, wq
-    sub            wq, mmsize - 1
-    add         dstUq, wq
-    add         dstVq, wq
-    lea          srcq, [srcq+wq*2]
-    neg            wq
-    pcmpeqb        m7, m7
-    psrlw          m7, 8                  ; (word) { 0x00ff } x4
-    mova           m6, [rgb_UVrnd]
-.loop:
-    ; FIXME check alignment and use mova
-    movu           m0, [srcq+wq*2+0]      ; (byte) { Bx, Gx, Rx, xx }[0-3]
-    movu           m4, [srcq+wq*2+mmsize] ; (byte) { Bx, Gx, Rx, xx }[4-7]
-    DEINTB          1,  0,  5,  4,  7     ; (word) { Gx, xx (m0/m4) or Bx, Rx (m1/m5) }[0-3]/[4-7]
-    pmaddwd        m3, m1, coeffV1        ; (dword) { Bx*BV + Rx*RV }[0-3]
-    pmaddwd        m2, m0, coeffV2        ; (dword) { Gx*GV }[0-3]
-    pmaddwd        m1, coeffU1            ; (dword) { Bx*BU + Rx*RU }[0-3]
-    pmaddwd        m0, coeffU2            ; (dword) { Gx*GU }[0-3]
-    paddd          m3, m6                 ; += rgb_UVrnd
-    paddd          m1, m6                 ; += rgb_UVrnd
-    paddd          m2, m3                 ; (dword) { V[0-3] }
-    paddd          m0, m1                 ; (dword) { U[0-3] }
-    pmaddwd        m3, m5, coeffV1        ; (dword) { Bx*BV + Rx*RV }[4-7]
-    pmaddwd        m1, m4, coeffV2        ; (dword) { Gx*GV }[4-7]
-    pmaddwd        m5, coeffU1            ; (dword) { Bx*BU + Rx*RU }[4-7]
-    pmaddwd        m4, coeffU2            ; (dword) { Gx*GU }[4-7]
-    paddd          m3, m6                 ; += rgb_UVrnd
-    paddd          m5, m6                 ; += rgb_UVrnd
-    psrad          m0, 9
-    paddd          m1, m3                 ; (dword) { V[4-7] }
-    paddd          m4, m5                 ; (dword) { U[4-7] }
-    psrad          m2, 9
-    psrad          m4, 9
-    psrad          m1, 9
-    packssdw       m0, m4                 ; (word) { U[0-7] }
-    packssdw       m2, m1                 ; (word) { V[0-7] }
-%if mmsize == 8
-    mova   [dstUq+wq], m0
-    mova   [dstVq+wq], m2
-%else ; mmsize == 16
-    mova   [dstUq+wq], m0
-    mova   [dstVq+wq], m2
-%endif ; mmsize == 8/16
-    add            wq, mmsize
-    jl .loop
-    sub            wq, mmsize - 1
-    jz .end
-    add            srcq , 2*mmsize - 2
-    add            dstUq, mmsize - 1
-    add            dstVq, mmsize - 1
-.loop2:
-    movd           m0, [srcq+wq*2]        ; (byte) { Bx, Gx, Rx, xx }[0-3]
-    DEINTB          1,  0,  5,  4,  7     ; (word) { Gx, xx (m0/m4) or Bx, Rx (m1/m5) }[0-3]/[4-7]
-    pmaddwd        m3, m1, coeffV1        ; (dword) { Bx*BV + Rx*RV }[0-3]
-    pmaddwd        m2, m0, coeffV2        ; (dword) { Gx*GV }[0-3]
-    pmaddwd        m1, coeffU1            ; (dword) { Bx*BU + Rx*RU }[0-3]
-    pmaddwd        m0, coeffU2            ; (dword) { Gx*GU }[0-3]
-    paddd          m3, m6                 ; += rgb_UVrnd
-    paddd          m1, m6                 ; += rgb_UVrnd
-    paddd          m2, m3                 ; (dword) { V[0-3] }
-    paddd          m0, m1                 ; (dword) { U[0-3] }
-    psrad          m0, 9
-    psrad          m2, 9
-    packssdw       m0, m0                 ; (word) { U[0-7] }
-    packssdw       m2, m2                 ; (word) { V[0-7] }
-    movd   [dstUq+wq], m0
-    movd   [dstVq+wq], m2
-    add            wq, 2
-    jl .loop2
-.end:
-    REP_RET
-%endif ; ARCH_X86_64 && %0 == 3
-%endmacro
-
-; %1 = nr. of XMM registers for rgb-to-Y func
-; %2 = nr. of XMM registers for rgb-to-UV func
-%macro RGB32_FUNCS 2
-RGB32_TO_Y_FN %1, r, g, b, a
-RGB32_TO_Y_FN %1, b, g, r, a, rgba
-RGB32_TO_Y_FN %1, a, r, g, b, rgba
-RGB32_TO_Y_FN %1, a, b, g, r, rgba
-
-RGB32_TO_UV_FN %2, r, g, b, a
-RGB32_TO_UV_FN %2, b, g, r, a, rgba
-RGB32_TO_UV_FN %2, a, r, g, b, rgba
-RGB32_TO_UV_FN %2, a, b, g, r, rgba
-%endmacro
-
-%if ARCH_X86_32
-INIT_MMX mmx
-RGB32_FUNCS 0, 0
-%endif
-
-INIT_XMM sse2
-RGB32_FUNCS 8, 12
-
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-RGB32_FUNCS 8, 12
-%endif
-
-;-----------------------------------------------------------------------------
-; YUYV/UYVY/NV12/NV21 packed pixel shuffling.
-;
-; void <fmt>ToY_<opt>(uint8_t *dst, const uint8_t *src, int w);
-; and
-; void <fmt>toUV_<opt>(uint8_t *dstU, uint8_t *dstV, const uint8_t *src,
-;                      const uint8_t *unused, int w);
-;-----------------------------------------------------------------------------
-
-; %1 = a (aligned) or u (unaligned)
-; %2 = yuyv or uyvy
-%macro LOOP_YUYV_TO_Y 2
-.loop_%1:
-    mov%1          m0, [srcq+wq*2]        ; (byte) { Y0, U0, Y1, V0, ... }
-    mov%1          m1, [srcq+wq*2+mmsize] ; (byte) { Y8, U4, Y9, V4, ... }
-%ifidn %2, yuyv
-    pand           m0, m2                 ; (word) { Y0, Y1, ..., Y7 }
-    pand           m1, m2                 ; (word) { Y8, Y9, ..., Y15 }
-%else ; uyvy
-    psrlw          m0, 8                  ; (word) { Y0, Y1, ..., Y7 }
-    psrlw          m1, 8                  ; (word) { Y8, Y9, ..., Y15 }
-%endif ; yuyv/uyvy
-    packuswb       m0, m1                 ; (byte) { Y0, ..., Y15 }
-    mova    [dstq+wq], m0
-    add            wq, mmsize
-    jl .loop_%1
-    REP_RET
-%endmacro
-
-; %1 = nr. of XMM registers
-; %2 = yuyv or uyvy
-; %3 = if specified, it means that unaligned and aligned code in loop
-;      will be the same (i.e. YUYV+AVX), and thus we don't need to
-;      split the loop in an aligned and unaligned case
-%macro YUYV_TO_Y_FN 2-3
-cglobal %2ToY, 5, 5, %1, dst, unused0, unused1, src, w
-%if ARCH_X86_64
-    movsxd         wq, wd
-%endif
-    add          dstq, wq
-%if mmsize == 16
-    test         srcq, 15
-%endif
-    lea          srcq, [srcq+wq*2]
-%ifidn %2, yuyv
-    pcmpeqb        m2, m2                 ; (byte) { 0xff } x 16
-    psrlw          m2, 8                  ; (word) { 0x00ff } x 8
-%endif ; yuyv
-%if mmsize == 16
-    jnz .loop_u_start
-    neg            wq
-    LOOP_YUYV_TO_Y  a, %2
-.loop_u_start:
-    neg            wq
-    LOOP_YUYV_TO_Y  u, %2
-%else ; mmsize == 8
-    neg            wq
-    LOOP_YUYV_TO_Y  a, %2
-%endif ; mmsize == 8/16
-%endmacro
-
-; %1 = a (aligned) or u (unaligned)
-; %2 = yuyv or uyvy
-%macro LOOP_YUYV_TO_UV 2
-.loop_%1:
-%ifidn %2, yuyv
-    mov%1          m0, [srcq+wq*4]        ; (byte) { Y0, U0, Y1, V0, ... }
-    mov%1          m1, [srcq+wq*4+mmsize] ; (byte) { Y8, U4, Y9, V4, ... }
-    psrlw          m0, 8                  ; (word) { U0, V0, ..., U3, V3 }
-    psrlw          m1, 8                  ; (word) { U4, V4, ..., U7, V7 }
-%else ; uyvy
-%if cpuflag(avx)
-    vpand          m0, m2, [srcq+wq*4]        ; (word) { U0, V0, ..., U3, V3 }
-    vpand          m1, m2, [srcq+wq*4+mmsize] ; (word) { U4, V4, ..., U7, V7 }
-%else
-    mov%1          m0, [srcq+wq*4]        ; (byte) { Y0, U0, Y1, V0, ... }
-    mov%1          m1, [srcq+wq*4+mmsize] ; (byte) { Y8, U4, Y9, V4, ... }
-    pand           m0, m2                 ; (word) { U0, V0, ..., U3, V3 }
-    pand           m1, m2                 ; (word) { U4, V4, ..., U7, V7 }
-%endif
-%endif ; yuyv/uyvy
-    packuswb       m0, m1                 ; (byte) { U0, V0, ..., U7, V7 }
-    pand           m1, m0, m2             ; (word) { U0, U1, ..., U7 }
-    psrlw          m0, 8                  ; (word) { V0, V1, ..., V7 }
-%if mmsize == 16
-    packuswb       m1, m0                 ; (byte) { U0, ... U7, V1, ... V7 }
-    movh   [dstUq+wq], m1
-    movhps [dstVq+wq], m1
-%else ; mmsize == 8
-    packuswb       m1, m1                 ; (byte) { U0, ... U3 }
-    packuswb       m0, m0                 ; (byte) { V0, ... V3 }
-    movh   [dstUq+wq], m1
-    movh   [dstVq+wq], m0
-%endif ; mmsize == 8/16
-    add            wq, mmsize / 2
-    jl .loop_%1
-    REP_RET
-%endmacro
-
-; %1 = nr. of XMM registers
-; %2 = yuyv or uyvy
-; %3 = if specified, it means that unaligned and aligned code in loop
-;      will be the same (i.e. UYVY+AVX), and thus we don't need to
-;      split the loop in an aligned and unaligned case
-%macro YUYV_TO_UV_FN 2-3
-cglobal %2ToUV, 4, 5, %1, dstU, dstV, unused, src, w
-%if ARCH_X86_64
-    movsxd         wq, dword r5m
-%else ; x86-32
-    mov            wq, r5m
-%endif
-    add         dstUq, wq
-    add         dstVq, wq
-%if mmsize == 16 && %0 == 2
-    test         srcq, 15
-%endif
-    lea          srcq, [srcq+wq*4]
-    pcmpeqb        m2, m2                 ; (byte) { 0xff } x 16
-    psrlw          m2, 8                  ; (word) { 0x00ff } x 8
-    ; NOTE: if uyvy+avx, u/a are identical
-%if mmsize == 16 && %0 == 2
-    jnz .loop_u_start
-    neg            wq
-    LOOP_YUYV_TO_UV a, %2
-.loop_u_start:
-    neg            wq
-    LOOP_YUYV_TO_UV u, %2
-%else ; mmsize == 8
-    neg            wq
-    LOOP_YUYV_TO_UV a, %2
-%endif ; mmsize == 8/16
-%endmacro
-
-; %1 = a (aligned) or u (unaligned)
-; %2 = nv12 or nv21
-%macro LOOP_NVXX_TO_UV 2
-.loop_%1:
-    mov%1          m0, [srcq+wq*2]        ; (byte) { U0, V0, U1, V1, ... }
-    mov%1          m1, [srcq+wq*2+mmsize] ; (byte) { U8, V8, U9, V9, ... }
-    pand           m2, m0, m5             ; (word) { U0, U1, ..., U7 }
-    pand           m3, m1, m5             ; (word) { U8, U9, ..., U15 }
-    psrlw          m0, 8                  ; (word) { V0, V1, ..., V7 }
-    psrlw          m1, 8                  ; (word) { V8, V9, ..., V15 }
-    packuswb       m2, m3                 ; (byte) { U0, ..., U15 }
-    packuswb       m0, m1                 ; (byte) { V0, ..., V15 }
-%ifidn %2, nv12
-    mova   [dstUq+wq], m2
-    mova   [dstVq+wq], m0
-%else ; nv21
-    mova   [dstVq+wq], m2
-    mova   [dstUq+wq], m0
-%endif ; nv12/21
-    add            wq, mmsize
-    jl .loop_%1
-    REP_RET
-%endmacro
-
-; %1 = nr. of XMM registers
-; %2 = nv12 or nv21
-%macro NVXX_TO_UV_FN 2
-cglobal %2ToUV, 4, 5, %1, dstU, dstV, unused, src, w
-%if ARCH_X86_64
-    movsxd         wq, dword r5m
-%else ; x86-32
-    mov            wq, r5m
-%endif
-    add         dstUq, wq
-    add         dstVq, wq
-%if mmsize == 16
-    test         srcq, 15
-%endif
-    lea          srcq, [srcq+wq*2]
-    pcmpeqb        m5, m5                 ; (byte) { 0xff } x 16
-    psrlw          m5, 8                  ; (word) { 0x00ff } x 8
-%if mmsize == 16
-    jnz .loop_u_start
-    neg            wq
-    LOOP_NVXX_TO_UV a, %2
-.loop_u_start:
-    neg            wq
-    LOOP_NVXX_TO_UV u, %2
-%else ; mmsize == 8
-    neg            wq
-    LOOP_NVXX_TO_UV a, %2
-%endif ; mmsize == 8/16
-%endmacro
-
-%if ARCH_X86_32
-INIT_MMX mmx
-YUYV_TO_Y_FN  0, yuyv
-YUYV_TO_Y_FN  0, uyvy
-YUYV_TO_UV_FN 0, yuyv
-YUYV_TO_UV_FN 0, uyvy
-NVXX_TO_UV_FN 0, nv12
-NVXX_TO_UV_FN 0, nv21
-%endif
-
-INIT_XMM sse2
-YUYV_TO_Y_FN  3, yuyv
-YUYV_TO_Y_FN  2, uyvy
-YUYV_TO_UV_FN 3, yuyv
-YUYV_TO_UV_FN 3, uyvy
-NVXX_TO_UV_FN 5, nv12
-NVXX_TO_UV_FN 5, nv21
-
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-; in theory, we could write a yuy2-to-y using vpand (i.e. AVX), but
-; that's not faster in practice
-YUYV_TO_UV_FN 3, yuyv
-YUYV_TO_UV_FN 3, uyvy, 1
-NVXX_TO_UV_FN 5, nv12
-NVXX_TO_UV_FN 5, nv21
-%endif
diff -uparN ffmpeg-4.1/libswscale/x86/output.asm ffmpeg-y/libswscale/x86/output.asm
--- ffmpeg-4.1/libswscale/x86/output.asm	2018-11-02 02:34:28.000000000 +0800
+++ ffmpeg-y/libswscale/x86/output.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,425 +0,0 @@
-;******************************************************************************
-;* x86-optimized vertical line scaling functions
-;* Copyright (c) 2011 Ronald S. Bultje <rsbultje@gmail.com>
-;*                    Kieran Kunhya <kieran@kunhya.com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-
-minshort:      times 8 dw 0x8000
-yuv2yuvX_16_start:  times 4 dd 0x4000 - 0x40000000
-yuv2yuvX_10_start:  times 4 dd 0x10000
-yuv2yuvX_9_start:   times 4 dd 0x20000
-yuv2yuvX_10_upper:  times 8 dw 0x3ff
-yuv2yuvX_9_upper:   times 8 dw 0x1ff
-pd_4:          times 4 dd 4
-pd_4min0x40000:times 4 dd 4 - (0x40000)
-pw_16:         times 8 dw 16
-pw_32:         times 8 dw 32
-pw_512:        times 8 dw 512
-pw_1024:       times 8 dw 1024
-
-SECTION .text
-
-;-----------------------------------------------------------------------------
-; vertical line scaling
-;
-; void yuv2plane1_<output_size>_<opt>(const int16_t *src, uint8_t *dst, int dstW,
-;                                     const uint8_t *dither, int offset)
-; and
-; void yuv2planeX_<output_size>_<opt>(const int16_t *filter, int filterSize,
-;                                     const int16_t **src, uint8_t *dst, int dstW,
-;                                     const uint8_t *dither, int offset)
-;
-; Scale one or $filterSize lines of source data to generate one line of output
-; data. The input is 15 bits in int16_t if $output_size is [8,10] and 19 bits in
-; int32_t if $output_size is 16. $filter is 12 bits. $filterSize is a multiple
-; of 2. $offset is either 0 or 3. $dither holds 8 values.
-;-----------------------------------------------------------------------------
-%macro yuv2planeX_mainloop 2
-.pixelloop_%2:
-%assign %%i 0
-    ; the rep here is for the 8-bit output MMX case, where dither covers
-    ; 8 pixels but we can only handle 2 pixels per register, and thus 4
-    ; pixels per iteration. In order to not have to keep track of where
-    ; we are w.r.t. dithering, we unroll the MMX/8-bit loop x2.
-%if %1 == 8
-%assign %%repcnt 16/mmsize
-%else
-%assign %%repcnt 1
-%endif
-
-%rep %%repcnt
-
-%if %1 == 8
-%if ARCH_X86_32
-    mova            m2, [rsp+mmsize*(0+%%i)]
-    mova            m1, [rsp+mmsize*(1+%%i)]
-%else ; x86-64
-    mova            m2,  m8
-    mova            m1,  m_dith
-%endif ; x86-32/64
-%else ; %1 == 9/10/16
-    mova            m1, [yuv2yuvX_%1_start]
-    mova            m2,  m1
-%endif ; %1 == 8/9/10/16
-    movsx     cntr_reg,  fltsizem
-.filterloop_%2_ %+ %%i:
-    ; input pixels
-    mov             r6, [srcq+gprsize*cntr_reg-2*gprsize]
-%if %1 == 16
-    mova            m3, [r6+r5*4]
-    mova            m5, [r6+r5*4+mmsize]
-%else ; %1 == 8/9/10
-    mova            m3, [r6+r5*2]
-%endif ; %1 == 8/9/10/16
-    mov             r6, [srcq+gprsize*cntr_reg-gprsize]
-%if %1 == 16
-    mova            m4, [r6+r5*4]
-    mova            m6, [r6+r5*4+mmsize]
-%else ; %1 == 8/9/10
-    mova            m4, [r6+r5*2]
-%endif ; %1 == 8/9/10/16
-
-    ; coefficients
-    movd            m0, [filterq+2*cntr_reg-4] ; coeff[0], coeff[1]
-%if %1 == 16
-    pshuflw         m7,  m0,  0          ; coeff[0]
-    pshuflw         m0,  m0,  0x55       ; coeff[1]
-    pmovsxwd        m7,  m7              ; word -> dword
-    pmovsxwd        m0,  m0              ; word -> dword
-
-    pmulld          m3,  m7
-    pmulld          m5,  m7
-    pmulld          m4,  m0
-    pmulld          m6,  m0
-
-    paddd           m2,  m3
-    paddd           m1,  m5
-    paddd           m2,  m4
-    paddd           m1,  m6
-%else ; %1 == 10/9/8
-    punpcklwd       m5,  m3,  m4
-    punpckhwd       m3,  m4
-    SPLATD          m0
-
-    pmaddwd         m5,  m0
-    pmaddwd         m3,  m0
-
-    paddd           m2,  m5
-    paddd           m1,  m3
-%endif ; %1 == 8/9/10/16
-
-    sub       cntr_reg,  2
-    jg .filterloop_%2_ %+ %%i
-
-%if %1 == 16
-    psrad           m2,  31 - %1
-    psrad           m1,  31 - %1
-%else ; %1 == 10/9/8
-    psrad           m2,  27 - %1
-    psrad           m1,  27 - %1
-%endif ; %1 == 8/9/10/16
-
-%if %1 == 8
-    packssdw        m2,  m1
-    packuswb        m2,  m2
-    movh   [dstq+r5*1],  m2
-%else ; %1 == 9/10/16
-%if %1 == 16
-    packssdw        m2,  m1
-    paddw           m2, [minshort]
-%else ; %1 == 9/10
-%if cpuflag(sse4)
-    packusdw        m2,  m1
-%else ; mmxext/sse2
-    packssdw        m2,  m1
-    pmaxsw          m2,  m6
-%endif ; mmxext/sse2/sse4/avx
-    pminsw          m2, [yuv2yuvX_%1_upper]
-%endif ; %1 == 9/10/16
-    mov%2   [dstq+r5*2],  m2
-%endif ; %1 == 8/9/10/16
-
-    add             r5,  mmsize/2
-    sub             wd,  mmsize/2
-
-%assign %%i %%i+2
-%endrep
-    jg .pixelloop_%2
-%endmacro
-
-%macro yuv2planeX_fn 3
-
-%if ARCH_X86_32
-%define cntr_reg fltsizeq
-%define movsx mov
-%else
-%define cntr_reg r7
-%define movsx movsxd
-%endif
-
-cglobal yuv2planeX_%1, %3, 8, %2, filter, fltsize, src, dst, w, dither, offset
-%if %1 == 8 || %1 == 9 || %1 == 10
-    pxor            m6,  m6
-%endif ; %1 == 8/9/10
-
-%if %1 == 8
-%if ARCH_X86_32
-%assign pad 0x2c - (stack_offset & 15)
-    SUB             rsp, pad
-%define m_dith m7
-%else ; x86-64
-%define m_dith m9
-%endif ; x86-32
-
-    ; create registers holding dither
-    movq        m_dith, [ditherq]        ; dither
-    test        offsetd, offsetd
-    jz              .no_rot
-%if mmsize == 16
-    punpcklqdq  m_dith,  m_dith
-%endif ; mmsize == 16
-    PALIGNR     m_dith,  m_dith,  3,  m0
-.no_rot:
-%if mmsize == 16
-    punpcklbw   m_dith,  m6
-%if ARCH_X86_64
-    punpcklwd       m8,  m_dith,  m6
-    pslld           m8,  12
-%else ; x86-32
-    punpcklwd       m5,  m_dith,  m6
-    pslld           m5,  12
-%endif ; x86-32/64
-    punpckhwd   m_dith,  m6
-    pslld       m_dith,  12
-%if ARCH_X86_32
-    mova      [rsp+ 0],  m5
-    mova      [rsp+16],  m_dith
-%endif
-%else ; mmsize == 8
-    punpcklbw       m5,  m_dith,  m6
-    punpckhbw   m_dith,  m6
-    punpcklwd       m4,  m5,  m6
-    punpckhwd       m5,  m6
-    punpcklwd       m3,  m_dith,  m6
-    punpckhwd   m_dith,  m6
-    pslld           m4,  12
-    pslld           m5,  12
-    pslld           m3,  12
-    pslld       m_dith,  12
-    mova      [rsp+ 0],  m4
-    mova      [rsp+ 8],  m5
-    mova      [rsp+16],  m3
-    mova      [rsp+24],  m_dith
-%endif ; mmsize == 8/16
-%endif ; %1 == 8
-
-    xor             r5,  r5
-
-%if mmsize == 8 || %1 == 8
-    yuv2planeX_mainloop %1, a
-%else ; mmsize == 16
-    test          dstq, 15
-    jnz .unaligned
-    yuv2planeX_mainloop %1, a
-    REP_RET
-.unaligned:
-    yuv2planeX_mainloop %1, u
-%endif ; mmsize == 8/16
-
-%if %1 == 8
-%if ARCH_X86_32
-    ADD             rsp, pad
-    RET
-%else ; x86-64
-    REP_RET
-%endif ; x86-32/64
-%else ; %1 == 9/10/16
-    REP_RET
-%endif ; %1 == 8/9/10/16
-%endmacro
-
-%if ARCH_X86_32
-INIT_MMX mmxext
-yuv2planeX_fn  8,  0, 7
-yuv2planeX_fn  9,  0, 5
-yuv2planeX_fn 10,  0, 5
-%endif
-
-INIT_XMM sse2
-yuv2planeX_fn  8, 10, 7
-yuv2planeX_fn  9,  7, 5
-yuv2planeX_fn 10,  7, 5
-
-INIT_XMM sse4
-yuv2planeX_fn  8, 10, 7
-yuv2planeX_fn  9,  7, 5
-yuv2planeX_fn 10,  7, 5
-yuv2planeX_fn 16,  8, 5
-
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-yuv2planeX_fn  8, 10, 7
-yuv2planeX_fn  9,  7, 5
-yuv2planeX_fn 10,  7, 5
-%endif
-
-; %1=outout-bpc, %2=alignment (u/a)
-%macro yuv2plane1_mainloop 2
-.loop_%2:
-%if %1 == 8
-    paddsw          m0, m2, [srcq+wq*2+mmsize*0]
-    paddsw          m1, m3, [srcq+wq*2+mmsize*1]
-    psraw           m0, 7
-    psraw           m1, 7
-    packuswb        m0, m1
-    mov%2    [dstq+wq], m0
-%elif %1 == 16
-    paddd           m0, m4, [srcq+wq*4+mmsize*0]
-    paddd           m1, m4, [srcq+wq*4+mmsize*1]
-    paddd           m2, m4, [srcq+wq*4+mmsize*2]
-    paddd           m3, m4, [srcq+wq*4+mmsize*3]
-    psrad           m0, 3
-    psrad           m1, 3
-    psrad           m2, 3
-    psrad           m3, 3
-%if cpuflag(sse4) ; avx/sse4
-    packusdw        m0, m1
-    packusdw        m2, m3
-%else ; mmx/sse2
-    packssdw        m0, m1
-    packssdw        m2, m3
-    paddw           m0, m5
-    paddw           m2, m5
-%endif ; mmx/sse2/sse4/avx
-    mov%2    [dstq+wq*2+mmsize*0], m0
-    mov%2    [dstq+wq*2+mmsize*1], m2
-%else ; %1 == 9/10
-    paddsw          m0, m2, [srcq+wq*2+mmsize*0]
-    paddsw          m1, m2, [srcq+wq*2+mmsize*1]
-    psraw           m0, 15 - %1
-    psraw           m1, 15 - %1
-    pmaxsw          m0, m4
-    pmaxsw          m1, m4
-    pminsw          m0, m3
-    pminsw          m1, m3
-    mov%2    [dstq+wq*2+mmsize*0], m0
-    mov%2    [dstq+wq*2+mmsize*1], m1
-%endif
-    add             wq, mmsize
-    jl .loop_%2
-%endmacro
-
-%macro yuv2plane1_fn 3
-cglobal yuv2plane1_%1, %3, %3, %2, src, dst, w, dither, offset
-    movsxdifnidn    wq, wd
-    add             wq, mmsize - 1
-    and             wq, ~(mmsize - 1)
-%if %1 == 8
-    add           dstq, wq
-%else ; %1 != 8
-    lea           dstq, [dstq+wq*2]
-%endif ; %1 == 8
-%if %1 == 16
-    lea           srcq, [srcq+wq*4]
-%else ; %1 != 16
-    lea           srcq, [srcq+wq*2]
-%endif ; %1 == 16
-    neg             wq
-
-%if %1 == 8
-    pxor            m4, m4               ; zero
-
-    ; create registers holding dither
-    movq            m3, [ditherq]        ; dither
-    test       offsetd, offsetd
-    jz              .no_rot
-%if mmsize == 16
-    punpcklqdq      m3, m3
-%endif ; mmsize == 16
-    PALIGNR         m3, m3, 3, m2
-.no_rot:
-%if mmsize == 8
-    mova            m2, m3
-    punpckhbw       m3, m4               ; byte->word
-    punpcklbw       m2, m4               ; byte->word
-%else
-    punpcklbw       m3, m4
-    mova            m2, m3
-%endif
-%elif %1 == 9
-    pxor            m4, m4
-    mova            m3, [pw_512]
-    mova            m2, [pw_32]
-%elif %1 == 10
-    pxor            m4, m4
-    mova            m3, [pw_1024]
-    mova            m2, [pw_16]
-%else ; %1 == 16
-%if cpuflag(sse4) ; sse4/avx
-    mova            m4, [pd_4]
-%else ; mmx/sse2
-    mova            m4, [pd_4min0x40000]
-    mova            m5, [minshort]
-%endif ; mmx/sse2/sse4/avx
-%endif ; %1 == ..
-
-    ; actual pixel scaling
-%if mmsize == 8
-    yuv2plane1_mainloop %1, a
-%else ; mmsize == 16
-    test          dstq, 15
-    jnz .unaligned
-    yuv2plane1_mainloop %1, a
-    REP_RET
-.unaligned:
-    yuv2plane1_mainloop %1, u
-%endif ; mmsize == 8/16
-    REP_RET
-%endmacro
-
-%if ARCH_X86_32
-INIT_MMX mmx
-yuv2plane1_fn  8, 0, 5
-yuv2plane1_fn 16, 0, 3
-
-INIT_MMX mmxext
-yuv2plane1_fn  9, 0, 3
-yuv2plane1_fn 10, 0, 3
-%endif
-
-INIT_XMM sse2
-yuv2plane1_fn  8, 5, 5
-yuv2plane1_fn  9, 5, 3
-yuv2plane1_fn 10, 5, 3
-yuv2plane1_fn 16, 6, 3
-
-INIT_XMM sse4
-yuv2plane1_fn 16, 5, 3
-
-%if HAVE_AVX_EXTERNAL
-INIT_XMM avx
-yuv2plane1_fn  8, 5, 5
-yuv2plane1_fn  9, 5, 3
-yuv2plane1_fn 10, 5, 3
-yuv2plane1_fn 16, 5, 3
-%endif
diff -uparN ffmpeg-4.1/libswscale/x86/rgb_2_rgb.asm ffmpeg-y/libswscale/x86/rgb_2_rgb.asm
--- ffmpeg-4.1/libswscale/x86/rgb_2_rgb.asm	2018-11-06 07:22:26.000000000 +0800
+++ ffmpeg-y/libswscale/x86/rgb_2_rgb.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,300 +0,0 @@
-;******************************************************************************
-;* Copyright Nick Kurshev
-;* Copyright Michael (michaelni@gmx.at)
-;* Copyright 2018 Jokyo Images
-;* Copyright Ivo van Poorten
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-
-pb_mask_shuffle2103_mmx times 8 dw 255
-pb_shuffle2103: db 2, 1, 0, 3, 6, 5, 4, 7, 10, 9, 8, 11, 14, 13, 12, 15
-pb_shuffle0321: db 0, 3, 2, 1, 4, 7, 6, 5, 8, 11, 10, 9, 12, 15, 14, 13
-pb_shuffle1230: db 1, 2, 3, 0, 5, 6, 7, 4, 9, 10, 11, 8, 13, 14, 15, 12
-pb_shuffle3012: db 3, 0, 1, 2, 7, 4, 5, 6, 11, 8, 9, 10, 15, 12, 13, 14
-pb_shuffle3210: db 3, 2, 1, 0, 7, 6, 5, 4, 11, 10, 9, 8, 15, 14, 13, 12
-
-SECTION .text
-
-%macro RSHIFT_COPY 3
-; %1 dst ; %2 src ; %3 shift
-%if cpuflag(avx)
-    psrldq  %1, %2, %3
-%else
-    mova           %1, %2
-    RSHIFT         %1, %3
-%endif
-%endmacro
-
-;------------------------------------------------------------------------------
-; shuffle_bytes_2103_mmext (const uint8_t *src, uint8_t *dst, int src_size)
-;------------------------------------------------------------------------------
-INIT_MMX mmxext
-cglobal shuffle_bytes_2103, 3, 5, 8, src, dst, w, tmp, x
-    mova   m6, [pb_mask_shuffle2103_mmx]
-    mova   m7, m6
-    psllq  m7, 8
-
-    movsxdifnidn wq, wd
-    mov xq, wq
-
-    add        srcq, wq
-    add        dstq, wq
-    neg          wq
-
-;calc scalar loop
-    and xq, mmsize*2 -4
-    je .loop_simd
-
-.loop_scalar:
-   mov          tmpb, [srcq + wq + 2]
-   mov [dstq+wq + 0], tmpb
-   mov          tmpb, [srcq + wq + 1]
-   mov [dstq+wq + 1], tmpb
-   mov          tmpb, [srcq + wq + 0]
-   mov [dstq+wq + 2], tmpb
-   mov          tmpb, [srcq + wq + 3]
-   mov [dstq+wq + 3], tmpb
-   add            wq, 4
-   sub            xq, 4
-   jg .loop_scalar
-
-;check if src_size < mmsize * 2
-cmp wq, 0
-jge .end
-
-.loop_simd:
-    movu     m0, [srcq+wq]
-    movu     m1, [srcq+wq+8]
-
-    pshufw   m3, m0, 177
-    pshufw   m5, m1, 177
-
-    pand     m0, m7
-    pand     m3, m6
-
-    pand     m1, m7
-    pand     m5, m6
-
-    por      m0, m3
-    por      m1, m5
-
-    movu      [dstq+wq], m0
-    movu  [dstq+wq + 8], m1
-
-    add              wq, mmsize*2
-    jl .loop_simd
-
-.end:
-    RET
-
-;------------------------------------------------------------------------------
-; shuffle_bytes_## (const uint8_t *src, uint8_t *dst, int src_size)
-;------------------------------------------------------------------------------
-; %1-4 index shuffle
-%macro SHUFFLE_BYTES 4
-cglobal shuffle_bytes_%1%2%3%4, 3, 5, 2, src, dst, w, tmp, x
-    VBROADCASTI128    m0, [pb_shuffle%1%2%3%4]
-    movsxdifnidn wq, wd
-    mov xq, wq
-
-    add        srcq, wq
-    add        dstq, wq
-    neg          wq
-
-;calc scalar loop
-    and xq, mmsize-4
-    je .loop_simd
-
-.loop_scalar:
-   mov          tmpb, [srcq + wq + %1]
-   mov [dstq+wq + 0], tmpb
-   mov          tmpb, [srcq + wq + %2]
-   mov [dstq+wq + 1], tmpb
-   mov          tmpb, [srcq + wq + %3]
-   mov [dstq+wq + 2], tmpb
-   mov          tmpb, [srcq + wq + %4]
-   mov [dstq+wq + 3], tmpb
-   add            wq, 4
-   sub            xq, 4
-   jg .loop_scalar
-
-;check if src_size < mmsize
-cmp wq, 0
-jge .end
-
-.loop_simd:
-    movu           m1, [srcq+wq]
-    pshufb         m1, m0
-    movu    [dstq+wq], m1
-    add            wq, mmsize
-    jl .loop_simd
-
-.end:
-    RET
-%endmacro
-
-INIT_XMM ssse3
-SHUFFLE_BYTES 2, 1, 0, 3
-SHUFFLE_BYTES 0, 3, 2, 1
-SHUFFLE_BYTES 1, 2, 3, 0
-SHUFFLE_BYTES 3, 0, 1, 2
-SHUFFLE_BYTES 3, 2, 1, 0
-
-;-----------------------------------------------------------------------------------------------
-; uyvytoyuv422(uint8_t *ydst, uint8_t *udst, uint8_t *vdst,
-;              const uint8_t *src, int width, int height,
-;              int lumStride, int chromStride, int srcStride)
-;-----------------------------------------------------------------------------------------------
-%macro UYVY_TO_YUV422 0
-cglobal uyvytoyuv422, 9, 14, 8, ydst, udst, vdst, src, w, h, lum_stride, chrom_stride, src_stride, wtwo, whalf, tmp, x, back_w
-    pxor         m0, m0
-    pcmpeqw      m1, m1
-    psrlw        m1, 8
-
-    movsxdifnidn            wq, wd
-    movsxdifnidn   lum_strideq, lum_strided
-    movsxdifnidn chrom_strideq, chrom_strided
-    movsxdifnidn   src_strideq, src_strided
-
-    mov     back_wq, wq
-    mov      whalfq, wq
-    shr      whalfq, 1     ; whalf = width / 2
-
-    lea srcq, [srcq + wq * 2]
-    add    ydstq, wq
-    add    udstq, whalfq
-    add    vdstq, whalfq
-
-.loop_line:
-    mov          xq, wq
-    mov       wtwoq, wq
-    add       wtwoq, wtwoq ; wtwo = width * 2
-
-    neg       wq
-    neg    wtwoq
-    neg   whalfq
-
-    ;calc scalar loop count
-    and       xq, mmsize * 2 - 1
-    je .loop_simd
-
-    .loop_scalar:
-        mov             tmpb, [srcq + wtwoq + 0]
-        mov [udstq + whalfq], tmpb
-
-        mov             tmpb, [srcq + wtwoq + 1]
-        mov     [ydstq + wq], tmpb
-
-        mov             tmpb, [srcq + wtwoq + 2]
-        mov [vdstq + whalfq], tmpb
-
-        mov             tmpb, [srcq + wtwoq + 3]
-        mov [ydstq + wq + 1], tmpb
-
-        add      wq, 2
-        add   wtwoq, 4
-        add  whalfq, 1
-        sub      xq, 2
-        jg .loop_scalar
-
-    ; check if simd loop is need
-    cmp      wq, 0
-    jge .end_line
-
-    .loop_simd:
-        movu    m2, [srcq + wtwoq             ]
-        movu    m3, [srcq + wtwoq + mmsize    ]
-        movu    m4, [srcq + wtwoq + mmsize * 2]
-        movu    m5, [srcq + wtwoq + mmsize * 3]
-
-        ; extract y part 1
-        RSHIFT_COPY    m6, m2, 1 ; UYVY UYVY -> YVYU YVY...
-        pand           m6, m1; YxYx YxYx...
-
-        RSHIFT_COPY    m7, m3, 1 ; UYVY UYVY -> YVYU YVY...
-        pand           m7, m1 ; YxYx YxYx...
-
-        packuswb       m6, m7 ; YYYY YYYY...
-        movu [ydstq + wq], m6
-
-        ; extract y part 2
-        RSHIFT_COPY    m6, m4, 1 ; UYVY UYVY -> YVYU YVY...
-        pand           m6, m1; YxYx YxYx...
-
-        RSHIFT_COPY    m7, m5, 1 ; UYVY UYVY -> YVYU YVY...
-        pand           m7, m1 ; YxYx YxYx...
-
-        packuswb                m6, m7 ; YYYY YYYY...
-        movu [ydstq + wq + mmsize], m6
-
-        ; extract uv
-        pand       m2, m1   ; UxVx...
-        pand       m3, m1   ; UxVx...
-        pand       m4, m1   ; UxVx...
-        pand       m5, m1   ; UxVx...
-
-        packuswb   m2, m3   ; UVUV...
-        packuswb   m4, m5   ; UVUV...
-
-        ; U
-        pand       m6, m2, m1 ; UxUx...
-        pand       m7, m4, m1 ; UxUx...
-
-        packuswb m6, m7 ; UUUU
-        movu   [udstq + whalfq], m6
-
-
-        ; V
-        psrlw      m2, 8  ; VxVx...
-        psrlw      m4, 8  ; VxVx...
-        packuswb   m2, m4 ; VVVV
-        movu   [vdstq + whalfq], m2
-
-        add   whalfq, mmsize
-        add    wtwoq, mmsize * 4
-        add       wq, mmsize * 2
-        jl .loop_simd
-
-    .end_line:
-        add        srcq, src_strideq
-        add        ydstq, lum_strideq
-        add        udstq, chrom_strideq
-        add        vdstq, chrom_strideq
-
-        ;restore initial state of line variable
-        mov           wq, back_wq
-        mov          xq, wq
-        mov      whalfq, wq
-        shr      whalfq, 1     ; whalf = width / 2
-        sub          hd, 1
-        jg .loop_line
-
-    RET
-%endmacro
-
-%if ARCH_X86_64
-INIT_XMM sse2
-UYVY_TO_YUV422
-
-INIT_XMM avx
-UYVY_TO_YUV422
-%endif
diff -uparN ffmpeg-4.1/libswscale/x86/scale.asm ffmpeg-y/libswscale/x86/scale.asm
--- ffmpeg-4.1/libswscale/x86/scale.asm	2018-11-03 08:17:30.000000000 +0800
+++ ffmpeg-y/libswscale/x86/scale.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,423 +0,0 @@
-;******************************************************************************
-;* x86-optimized horizontal line scaling functions
-;* Copyright (c) 2011 Ronald S. Bultje <rsbultje@gmail.com>
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or
-;* modify it under the terms of the GNU Lesser General Public
-;* License as published by the Free Software Foundation; either
-;* version 2.1 of the License, or (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;* Lesser General Public License for more details.
-;*
-;* You should have received a copy of the GNU Lesser General Public
-;* License along with FFmpeg; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-;******************************************************************************
-
-%include "libavutil/x86/x86util.asm"
-
-SECTION_RODATA
-
-max_19bit_int: times 4 dd 0x7ffff
-max_19bit_flt: times 4 dd 524287.0
-minshort:      times 8 dw 0x8000
-unicoeff:      times 4 dd 0x20000000
-
-SECTION .text
-
-;-----------------------------------------------------------------------------
-; horizontal line scaling
-;
-; void hscale<source_width>to<intermediate_nbits>_<filterSize>_<opt>
-;                               (SwsContext *c, int{16,32}_t *dst,
-;                                int dstW, const uint{8,16}_t *src,
-;                                const int16_t *filter,
-;                                const int32_t *filterPos, int filterSize);
-;
-; Scale one horizontal line. Input is either 8-bit width or 16-bit width
-; ($source_width can be either 8, 9, 10 or 16, difference is whether we have to
-; downscale before multiplying). Filter is 14 bits. Output is either 15 bits
-; (in int16_t) or 19 bits (in int32_t), as given in $intermediate_nbits. Each
-; output pixel is generated from $filterSize input pixels, the position of
-; the first pixel is given in filterPos[nOutputPixel].
-;-----------------------------------------------------------------------------
-
-; SCALE_FUNC source_width, intermediate_nbits, filtersize, filtersuffix, n_args, n_xmm
-%macro SCALE_FUNC 6
-%ifnidn %3, X
-cglobal hscale%1to%2_%4, %5, 7, %6, pos0, dst, w, src, filter, fltpos, pos1
-%else
-cglobal hscale%1to%2_%4, %5, 10, %6, pos0, dst, w, srcmem, filter, fltpos, fltsize
-%endif
-%if ARCH_X86_64
-    movsxd        wq, wd
-%define mov32 movsxd
-%else ; x86-32
-%define mov32 mov
-%endif ; x86-64
-%if %2 == 19
-%if mmsize == 8 ; mmx
-    mova          m2, [max_19bit_int]
-%elif cpuflag(sse4)
-    mova          m2, [max_19bit_int]
-%else ; ssse3/sse2
-    mova          m2, [max_19bit_flt]
-%endif ; mmx/sse2/ssse3/sse4
-%endif ; %2 == 19
-%if %1 == 16
-    mova          m6, [minshort]
-    mova          m7, [unicoeff]
-%elif %1 == 8
-    pxor          m3, m3
-%endif ; %1 == 8/16
-
-%if %1 == 8
-%define movlh movd
-%define movbh movh
-%define srcmul 1
-%else ; %1 == 9-16
-%define movlh movq
-%define movbh movu
-%define srcmul 2
-%endif ; %1 == 8/9-16
-
-%ifnidn %3, X
-
-    ; setup loop
-%if %3 == 8
-    shl           wq, 1                         ; this allows *16 (i.e. now *8) in lea instructions for the 8-tap filter
-%define wshr 1
-%else ; %3 == 4
-%define wshr 0
-%endif ; %3 == 8
-    lea      filterq, [filterq+wq*8]
-%if %2 == 15
-    lea         dstq, [dstq+wq*(2>>wshr)]
-%else ; %2 == 19
-    lea         dstq, [dstq+wq*(4>>wshr)]
-%endif ; %2 == 15/19
-    lea      fltposq, [fltposq+wq*(4>>wshr)]
-    neg           wq
-
-.loop:
-%if %3 == 4 ; filterSize == 4 scaling
-    ; load 2x4 or 4x4 source pixels into m0/m1
-    mov32      pos0q, dword [fltposq+wq*4+ 0]   ; filterPos[0]
-    mov32      pos1q, dword [fltposq+wq*4+ 4]   ; filterPos[1]
-    movlh         m0, [srcq+pos0q*srcmul]       ; src[filterPos[0] + {0,1,2,3}]
-%if mmsize == 8
-    movlh         m1, [srcq+pos1q*srcmul]       ; src[filterPos[1] + {0,1,2,3}]
-%else ; mmsize == 16
-%if %1 > 8
-    movhps        m0, [srcq+pos1q*srcmul]       ; src[filterPos[1] + {0,1,2,3}]
-%else ; %1 == 8
-    movd          m4, [srcq+pos1q*srcmul]       ; src[filterPos[1] + {0,1,2,3}]
-%endif
-    mov32      pos0q, dword [fltposq+wq*4+ 8]   ; filterPos[2]
-    mov32      pos1q, dword [fltposq+wq*4+12]   ; filterPos[3]
-    movlh         m1, [srcq+pos0q*srcmul]       ; src[filterPos[2] + {0,1,2,3}]
-%if %1 > 8
-    movhps        m1, [srcq+pos1q*srcmul]       ; src[filterPos[3] + {0,1,2,3}]
-%else ; %1 == 8
-    movd          m5, [srcq+pos1q*srcmul]       ; src[filterPos[3] + {0,1,2,3}]
-    punpckldq     m0, m4
-    punpckldq     m1, m5
-%endif ; %1 == 8
-%endif ; mmsize == 8/16
-%if %1 == 8
-    punpcklbw     m0, m3                        ; byte -> word
-    punpcklbw     m1, m3                        ; byte -> word
-%endif ; %1 == 8
-
-    ; multiply with filter coefficients
-%if %1 == 16 ; pmaddwd needs signed adds, so this moves unsigned -> signed, we'll
-             ; add back 0x8000 * sum(coeffs) after the horizontal add
-    psubw         m0, m6
-    psubw         m1, m6
-%endif ; %1 == 16
-    pmaddwd       m0, [filterq+wq*8+mmsize*0]   ; *= filter[{0,1,..,6,7}]
-    pmaddwd       m1, [filterq+wq*8+mmsize*1]   ; *= filter[{8,9,..,14,15}]
-
-    ; add up horizontally (4 srcpix * 4 coefficients -> 1 dstpix)
-%if mmsize == 8 ; mmx
-    movq          m4, m0
-    punpckldq     m0, m1
-    punpckhdq     m4, m1
-    paddd         m0, m4
-%elif notcpuflag(ssse3) ; sse2
-    mova          m4, m0
-    shufps        m0, m1, 10001000b
-    shufps        m4, m1, 11011101b
-    paddd         m0, m4
-%else ; ssse3/sse4
-    phaddd        m0, m1                        ; filter[{ 0, 1, 2, 3}]*src[filterPos[0]+{0,1,2,3}],
-                                                ; filter[{ 4, 5, 6, 7}]*src[filterPos[1]+{0,1,2,3}],
-                                                ; filter[{ 8, 9,10,11}]*src[filterPos[2]+{0,1,2,3}],
-                                                ; filter[{12,13,14,15}]*src[filterPos[3]+{0,1,2,3}]
-%endif ; mmx/sse2/ssse3/sse4
-%else ; %3 == 8, i.e. filterSize == 8 scaling
-    ; load 2x8 or 4x8 source pixels into m0, m1, m4 and m5
-    mov32      pos0q, dword [fltposq+wq*2+0]    ; filterPos[0]
-    mov32      pos1q, dword [fltposq+wq*2+4]    ; filterPos[1]
-    movbh         m0, [srcq+ pos0q   *srcmul]   ; src[filterPos[0] + {0,1,2,3,4,5,6,7}]
-%if mmsize == 8
-    movbh         m1, [srcq+(pos0q+4)*srcmul]   ; src[filterPos[0] + {4,5,6,7}]
-    movbh         m4, [srcq+ pos1q   *srcmul]   ; src[filterPos[1] + {0,1,2,3}]
-    movbh         m5, [srcq+(pos1q+4)*srcmul]   ; src[filterPos[1] + {4,5,6,7}]
-%else ; mmsize == 16
-    movbh         m1, [srcq+ pos1q   *srcmul]   ; src[filterPos[1] + {0,1,2,3,4,5,6,7}]
-    mov32      pos0q, dword [fltposq+wq*2+8]    ; filterPos[2]
-    mov32      pos1q, dword [fltposq+wq*2+12]   ; filterPos[3]
-    movbh         m4, [srcq+ pos0q   *srcmul]   ; src[filterPos[2] + {0,1,2,3,4,5,6,7}]
-    movbh         m5, [srcq+ pos1q   *srcmul]   ; src[filterPos[3] + {0,1,2,3,4,5,6,7}]
-%endif ; mmsize == 8/16
-%if %1 == 8
-    punpcklbw     m0, m3                        ; byte -> word
-    punpcklbw     m1, m3                        ; byte -> word
-    punpcklbw     m4, m3                        ; byte -> word
-    punpcklbw     m5, m3                        ; byte -> word
-%endif ; %1 == 8
-
-    ; multiply
-%if %1 == 16 ; pmaddwd needs signed adds, so this moves unsigned -> signed, we'll
-             ; add back 0x8000 * sum(coeffs) after the horizontal add
-    psubw         m0, m6
-    psubw         m1, m6
-    psubw         m4, m6
-    psubw         m5, m6
-%endif ; %1 == 16
-    pmaddwd       m0, [filterq+wq*8+mmsize*0]   ; *= filter[{0,1,..,6,7}]
-    pmaddwd       m1, [filterq+wq*8+mmsize*1]   ; *= filter[{8,9,..,14,15}]
-    pmaddwd       m4, [filterq+wq*8+mmsize*2]   ; *= filter[{16,17,..,22,23}]
-    pmaddwd       m5, [filterq+wq*8+mmsize*3]   ; *= filter[{24,25,..,30,31}]
-
-    ; add up horizontally (8 srcpix * 8 coefficients -> 1 dstpix)
-%if mmsize == 8
-    paddd         m0, m1
-    paddd         m4, m5
-    movq          m1, m0
-    punpckldq     m0, m4
-    punpckhdq     m1, m4
-    paddd         m0, m1
-%elif notcpuflag(ssse3) ; sse2
-%if %1 == 8
-%define mex m6
-%else
-%define mex m3
-%endif
-    ; emulate horizontal add as transpose + vertical add
-    mova         mex, m0
-    punpckldq     m0, m1
-    punpckhdq    mex, m1
-    paddd         m0, mex
-    mova          m1, m4
-    punpckldq     m4, m5
-    punpckhdq     m1, m5
-    paddd         m4, m1
-    mova          m1, m0
-    punpcklqdq    m0, m4
-    punpckhqdq    m1, m4
-    paddd         m0, m1
-%else ; ssse3/sse4
-    ; FIXME if we rearrange the filter in pairs of 4, we can
-    ; load pixels likewise and use 2 x paddd + phaddd instead
-    ; of 3 x phaddd here, faster on older cpus
-    phaddd        m0, m1
-    phaddd        m4, m5
-    phaddd        m0, m4                        ; filter[{ 0, 1,..., 6, 7}]*src[filterPos[0]+{0,1,...,6,7}],
-                                                ; filter[{ 8, 9,...,14,15}]*src[filterPos[1]+{0,1,...,6,7}],
-                                                ; filter[{16,17,...,22,23}]*src[filterPos[2]+{0,1,...,6,7}],
-                                                ; filter[{24,25,...,30,31}]*src[filterPos[3]+{0,1,...,6,7}]
-%endif ; mmx/sse2/ssse3/sse4
-%endif ; %3 == 4/8
-
-%else ; %3 == X, i.e. any filterSize scaling
-
-%ifidn %4, X4
-%define dlt 4
-%else ; %4 == X || %4 == X8
-%define dlt 0
-%endif ; %4 ==/!= X4
-%if ARCH_X86_64
-%define srcq    r8
-%define pos1q   r7
-%define srcendq r9
-    movsxd  fltsizeq, fltsized                  ; filterSize
-    lea      srcendq, [srcmemq+(fltsizeq-dlt)*srcmul] ; &src[filterSize&~4]
-%else ; x86-32
-%define srcq    srcmemq
-%define pos1q   dstq
-%define srcendq r6m
-    lea        pos0q, [srcmemq+(fltsizeq-dlt)*srcmul] ; &src[filterSize&~4]
-    mov      srcendq, pos0q
-%endif ; x86-32/64
-    lea      fltposq, [fltposq+wq*4]
-%if %2 == 15
-    lea         dstq, [dstq+wq*2]
-%else ; %2 == 19
-    lea         dstq, [dstq+wq*4]
-%endif ; %2 == 15/19
-    movifnidn  dstmp, dstq
-    neg           wq
-
-.loop:
-    mov32      pos0q, dword [fltposq+wq*4+0]    ; filterPos[0]
-    mov32      pos1q, dword [fltposq+wq*4+4]    ; filterPos[1]
-    ; FIXME maybe do 4px/iteration on x86-64 (x86-32 wouldn't have enough regs)?
-    pxor          m4, m4
-    pxor          m5, m5
-    mov         srcq, srcmemmp
-
-.innerloop:
-    ; load 2x4 (mmx) or 2x8 (sse) source pixels into m0/m1 -> m4/m5
-    movbh         m0, [srcq+ pos0q     *srcmul] ; src[filterPos[0] + {0,1,2,3(,4,5,6,7)}]
-    movbh         m1, [srcq+(pos1q+dlt)*srcmul] ; src[filterPos[1] + {0,1,2,3(,4,5,6,7)}]
-%if %1 == 8
-    punpcklbw     m0, m3
-    punpcklbw     m1, m3
-%endif ; %1 == 8
-
-    ; multiply
-%if %1 == 16 ; pmaddwd needs signed adds, so this moves unsigned -> signed, we'll
-             ; add back 0x8000 * sum(coeffs) after the horizontal add
-    psubw         m0, m6
-    psubw         m1, m6
-%endif ; %1 == 16
-    pmaddwd       m0, [filterq]                 ; filter[{0,1,2,3(,4,5,6,7)}]
-    pmaddwd       m1, [filterq+(fltsizeq+dlt)*2]; filter[filtersize+{0,1,2,3(,4,5,6,7)}]
-    paddd         m4, m0
-    paddd         m5, m1
-    add      filterq, mmsize
-    add         srcq, srcmul*mmsize/2
-    cmp         srcq, srcendq                   ; while (src += 4) < &src[filterSize]
-    jl .innerloop
-
-%ifidn %4, X4
-    mov32      pos1q, dword [fltposq+wq*4+4]    ; filterPos[1]
-    movlh         m0, [srcq+ pos0q     *srcmul] ; split last 4 srcpx of dstpx[0]
-    sub        pos1q, fltsizeq                  ; and first 4 srcpx of dstpx[1]
-%if %1 > 8
-    movhps        m0, [srcq+(pos1q+dlt)*srcmul]
-%else ; %1 == 8
-    movd          m1, [srcq+(pos1q+dlt)*srcmul]
-    punpckldq     m0, m1
-%endif ; %1 == 8
-%if %1 == 8
-    punpcklbw     m0, m3
-%endif ; %1 == 8
-%if %1 == 16 ; pmaddwd needs signed adds, so this moves unsigned -> signed, we'll
-             ; add back 0x8000 * sum(coeffs) after the horizontal add
-    psubw         m0, m6
-%endif ; %1 == 16
-    pmaddwd       m0, [filterq]
-%endif ; %4 == X4
-
-    lea      filterq, [filterq+(fltsizeq+dlt)*2]
-
-%if mmsize == 8 ; mmx
-    movq          m0, m4
-    punpckldq     m4, m5
-    punpckhdq     m0, m5
-    paddd         m0, m4
-%else ; mmsize == 16
-%if notcpuflag(ssse3) ; sse2
-    mova          m1, m4
-    punpcklqdq    m4, m5
-    punpckhqdq    m1, m5
-    paddd         m4, m1
-%else ; ssse3/sse4
-    phaddd        m4, m5
-%endif ; sse2/ssse3/sse4
-%ifidn %4, X4
-    paddd         m4, m0
-%endif ; %3 == X4
-%if notcpuflag(ssse3) ; sse2
-    pshufd        m4, m4, 11011000b
-    movhlps       m0, m4
-    paddd         m0, m4
-%else ; ssse3/sse4
-    phaddd        m4, m4
-    SWAP           0, 4
-%endif ; sse2/ssse3/sse4
-%endif ; mmsize == 8/16
-%endif ; %3 ==/!= X
-
-%if %1 == 16 ; add 0x8000 * sum(coeffs), i.e. back from signed -> unsigned
-    paddd         m0, m7
-%endif ; %1 == 16
-
-    ; clip, store
-    psrad         m0, 14 + %1 - %2
-%ifidn %3, X
-    movifnidn   dstq, dstmp
-%endif ; %3 == X
-%if %2 == 15
-    packssdw      m0, m0
-%ifnidn %3, X
-    movh [dstq+wq*(2>>wshr)], m0
-%else ; %3 == X
-    movd [dstq+wq*2], m0
-%endif ; %3 ==/!= X
-%else ; %2 == 19
-    PMINSD        m0, m2, m4
-%ifnidn %3, X
-    mova [dstq+wq*(4>>wshr)], m0
-%else ; %3 == X
-    movq [dstq+wq*4], m0
-%endif ; %3 ==/!= X
-%endif ; %2 == 15/19
-%ifnidn %3, X
-    add           wq, (mmsize<<wshr)/4          ; both 8tap and 4tap really only do 4 pixels (or for mmx: 2 pixels)
-                                                ; per iteration. see "shl wq,1" above as for why we do this
-%else ; %3 == X
-    add           wq, 2
-%endif ; %3 ==/!= X
-    jl .loop
-    REP_RET
-%endmacro
-
-; SCALE_FUNCS source_width, intermediate_nbits, n_xmm
-%macro SCALE_FUNCS 3
-SCALE_FUNC %1, %2, 4, 4,  6, %3
-SCALE_FUNC %1, %2, 8, 8,  6, %3
-%if mmsize == 8
-SCALE_FUNC %1, %2, X, X,  7, %3
-%else
-SCALE_FUNC %1, %2, X, X4, 7, %3
-SCALE_FUNC %1, %2, X, X8, 7, %3
-%endif
-%endmacro
-
-; SCALE_FUNCS2 8_xmm_args, 9to10_xmm_args, 16_xmm_args
-%macro SCALE_FUNCS2 3
-%if notcpuflag(sse4)
-SCALE_FUNCS  8, 15, %1
-SCALE_FUNCS  9, 15, %2
-SCALE_FUNCS 10, 15, %2
-SCALE_FUNCS 12, 15, %2
-SCALE_FUNCS 14, 15, %2
-SCALE_FUNCS 16, 15, %3
-%endif ; !sse4
-SCALE_FUNCS  8, 19, %1
-SCALE_FUNCS  9, 19, %2
-SCALE_FUNCS 10, 19, %2
-SCALE_FUNCS 12, 19, %2
-SCALE_FUNCS 14, 19, %2
-SCALE_FUNCS 16, 19, %3
-%endmacro
-
-%if ARCH_X86_32
-INIT_MMX mmx
-SCALE_FUNCS2 0, 0, 0
-%endif
-INIT_XMM sse2
-SCALE_FUNCS2 7, 6, 8
-INIT_XMM ssse3
-SCALE_FUNCS2 6, 6, 8
-INIT_XMM sse4
-SCALE_FUNCS2 6, 6, 8
diff -uparN ffmpeg-4.1/tests/checkasm/x86/checkasm.asm ffmpeg-y/tests/checkasm/x86/checkasm.asm
--- ffmpeg-4.1/tests/checkasm/x86/checkasm.asm	2018-11-02 02:34:28.000000000 +0800
+++ ffmpeg-y/tests/checkasm/x86/checkasm.asm	1970-01-01 08:00:00.000000000 +0800
@@ -1,244 +0,0 @@
-;*****************************************************************************
-;* Assembly testing and benchmarking tool
-;* Copyright (c) 2008 Loren Merritt
-;* Copyright (c) 2012 Henrik Gramner
-;*
-;* This file is part of FFmpeg.
-;*
-;* FFmpeg is free software; you can redistribute it and/or modify
-;* it under the terms of the GNU General Public License as published by
-;* the Free Software Foundation; either version 2 of the License, or
-;* (at your option) any later version.
-;*
-;* FFmpeg is distributed in the hope that it will be useful,
-;* but WITHOUT ANY WARRANTY; without even the implied warranty of
-;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
-;* GNU General Public License for more details.
-;*
-;* You should have received a copy of the GNU General Public License
-;* along with this program; if not, write to the Free Software
-;* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02111, USA.
-;*****************************************************************************
-
-%define private_prefix checkasm
-%include "libavutil/x86/x86inc.asm"
-
-SECTION_RODATA
-
-error_message: db "failed to preserve register", 0
-error_message_emms: db "failed to issue emms", 0
-
-%if ARCH_X86_64
-; just random numbers to reduce the chance of incidental match
-ALIGN 16
-x6:  dq 0x1a1b2550a612b48c,0x79445c159ce79064
-x7:  dq 0x2eed899d5a28ddcd,0x86b2536fcd8cf636
-x8:  dq 0xb0856806085e7943,0x3f2bf84fc0fcca4e
-x9:  dq 0xacbd382dcf5b8de2,0xd229e1f5b281303f
-x10: dq 0x71aeaff20b095fd9,0xab63e2e11fa38ed9
-x11: dq 0x89b0c0765892729a,0x77d410d5c42c882d
-x12: dq 0xc45ea11a955d8dd5,0x24b3c1d2a024048b
-x13: dq 0x2e8ec680de14b47c,0xdd7b8919edd42786
-x14: dq 0x135ce6888fa02cbf,0x11e53e2b2ac655ef
-x15: dq 0x011ff554472a7a10,0x6de8f4c914c334d5
-n7:  dq 0x21f86d66c8ca00ce
-n8:  dq 0x75b6ba21077c48ad
-n9:  dq 0xed56bb2dcb3c7736
-n10: dq 0x8bda43d3fd1a7e06
-n11: dq 0xb64a9c9e5d318408
-n12: dq 0xdf9a54b303f1d3a3
-n13: dq 0x4a75479abd64e097
-n14: dq 0x249214109d5d1c88
-%endif
-
-SECTION .text
-
-cextern fail_func
-
-; max number of args used by any asm function.
-; (max_args % 4) must equal 3 for stack alignment
-%define max_args 15
-
-%if ARCH_X86_64
-
-;-----------------------------------------------------------------------------
-; int checkasm_stack_clobber(uint64_t clobber, ...)
-;-----------------------------------------------------------------------------
-cglobal stack_clobber, 1,2
-    ; Clobber the stack with junk below the stack pointer
-    %define argsize (max_args+6)*8
-    SUB  rsp, argsize
-    mov   r1, argsize-8
-.loop:
-    mov [rsp+r1], r0
-    sub   r1, 8
-    jge .loop
-    ADD  rsp, argsize
-    RET
-
-%if WIN64
-    %assign free_regs 7
-    DECLARE_REG_TMP 4
-%else
-    %assign free_regs 9
-    DECLARE_REG_TMP 7
-%endif
-
-%macro report_fail 1
-    mov  r9, rax
-    mov r10, rdx
-    lea  r0, [%1]
-    xor eax, eax
-    call fail_func
-    mov rdx, r10
-    mov rax, r9
-%endmacro
-
-;-----------------------------------------------------------------------------
-; void checkasm_checked_call(void *func, ...)
-;-----------------------------------------------------------------------------
-INIT_XMM
-%macro CHECKED_CALL 0-1
-cglobal checked_call%1, 2,15,16,max_args*8+8
-    mov  t0, r0
-
-    ; All arguments have been pushed on the stack instead of registers in order to
-    ; test for incorrect assumptions that 32-bit ints are zero-extended to 64-bit.
-    mov  r0, r6mp
-    mov  r1, r7mp
-    mov  r2, r8mp
-    mov  r3, r9mp
-%if UNIX64
-    mov  r4, r10mp
-    mov  r5, r11mp
-    %assign i 6
-    %rep max_args-6
-        mov  r9, [rsp+stack_offset+(i+1)*8]
-        mov  [rsp+(i-6)*8], r9
-        %assign i i+1
-    %endrep
-%else ; WIN64
-    %assign i 4
-    %rep max_args-4
-        mov  r9, [rsp+stack_offset+(i+7)*8]
-        mov  [rsp+i*8], r9
-        %assign i i+1
-    %endrep
-
-    ; Move possible floating-point arguments to the correct registers
-    movq m0, r0
-    movq m1, r1
-    movq m2, r2
-    movq m3, r3
-
-    %assign i 6
-    %rep 16-6
-        mova m %+ i, [x %+ i]
-        %assign i i+1
-    %endrep
-%endif
-
-%assign i 14
-%rep 15-free_regs
-    mov r %+ i, [n %+ i]
-    %assign i i-1
-%endrep
-    call t0
-%assign i 14
-%rep 15-free_regs
-    xor r %+ i, [n %+ i]
-    or  r14, r %+ i
-    %assign i i-1
-%endrep
-
-%if WIN64
-    %assign i 6
-    %rep 16-6
-        pxor m %+ i, [x %+ i]
-        por  m6, m %+ i
-        %assign i i+1
-    %endrep
-    packsswb m6, m6
-    movq r5, m6
-    or  r14, r5
-%endif
-
-    ; Call fail_func() with a descriptive message to mark it as a failure
-    ; if the called function didn't preserve all callee-saved registers.
-    ; Save the return value located in rdx:rax first to prevent clobbering.
-    jz .clobber_ok
-    report_fail error_message
-.clobber_ok:
-%ifidn %1, _emms
-    emms
-%elifnidn %1, _float
-    fstenv [rsp]
-    cmp  word [rsp + 8], 0xffff
-    je   .emms_ok
-    report_fail error_message_emms
-    emms
-.emms_ok:
-%endif
-    RET
-%endmacro
-
-%else
-
-; just random numbers to reduce the chance of incidental match
-%define n3 dword 0x6549315c
-%define n4 dword 0xe02f3e23
-%define n5 dword 0xb78d0d1d
-%define n6 dword 0x33627ba7
-
-%macro report_fail 1
-    mov  r3, eax
-    mov  r4, edx
-    lea  r0, [%1]
-    mov [esp], r0
-    call fail_func
-    mov  edx, r4
-    mov  eax, r3
-%endmacro
-
-%macro CHECKED_CALL 0-1
-;-----------------------------------------------------------------------------
-; void checkasm_checked_call(void *func, ...)
-;-----------------------------------------------------------------------------
-cglobal checked_call%1, 1,7
-    mov  r3, n3
-    mov  r4, n4
-    mov  r5, n5
-    mov  r6, n6
-%rep max_args
-    PUSH dword [esp+20+max_args*4]
-%endrep
-    call r0
-    xor  r3, n3
-    xor  r4, n4
-    xor  r5, n5
-    xor  r6, n6
-    or   r3, r4
-    or   r5, r6
-    or   r3, r5
-    jz .clobber_ok
-    report_fail error_message
-.clobber_ok:
-%ifidn %1, _emms
-    emms
-%elifnidn %1, _float
-    fstenv [esp]
-    cmp  word [esp + 8], 0xffff
-    je   .emms_ok
-    report_fail error_message_emms
-    emms
-.emms_ok:
-%endif
-    add  esp, max_args*4
-    REP_RET
-%endmacro
-
-%endif ; ARCH_X86_64
-
-CHECKED_CALL
-CHECKED_CALL _emms
-CHECKED_CALL _float
